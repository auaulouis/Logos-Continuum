**AI Ethics Case Neg**

**[Inherency Resps]{.underline}**

**1. No Inherency -- the Status Quo system solves best -- DOD policy
ensures ethical standards and responsible use, but retains flexibility
to address future contingencies**

**Allen, 2022 - Director, AI Governance Project, Strategic Technologies
Program at CSIS** \[Gregory C. June 6 "DOD Is Updating Its Decade-Old
Autonomous Weapons Policy, but Confusion Remains Widespread"
[https://www.csis.org/analysis/dod-updating-its-decade-old-autonomous-weapons-policy-confusion-remains-widespread
Acc
6/6/22](https://www.csis.org/analysis/dod-updating-its-decade-old-autonomous-weapons-policy-confusion-remains-widespread%20Acc%206/6/22)
TA\]

The DOD recently announced that it is planning to update DODD 3000.09
this year. [Michael Horowitz, director of the DOD's Emerging
Capabilities Policy Office]{.underline} and the Pentagon official with
responsibility for shepherding the policy, [praised DODD
3000.09]{.underline} in a recent interview, stating that "the
fundamental approach in the directive remains sound, that [the directive
laid out a very responsible approach to the incorporation of autonomy
and weapons systems]{.underline}." While not making any firm
predictions, Horowitz suggested that [major revisions]{.underline} to
DODD 3000.09 [were unlikely. In general, this is good news.
The]{.underline} [DOD's existing policy recognizes that some categories
of autonomous weapons,]{.underline} such as cyber weapons and missile
defense systems, [are already in]{.underline} widespread and [broadly
accepted use]{.underline} by dozens of militaries worldwide. [It also
allows for the possibility that future technological progress and
changes in the global security landscape, such as Russia's potential
deployment of artificial intelligence (AI)-enabled lethal autonomous
weapons in Ukraine, might make new types of autonomous weapons
desirable.]{.underline} [This requires proposals for such weapons to
clear a high procedural and technical bar. In addition to demonstrating
compliance with U.S. obligations under domestic and international law,
DOD system safety standards, and DOD AI-ethics principles, proposed
autonomous weapons systems must clear an additional senior review
process]{.underline} where the chairman of the Joint Chiefs of Staff,
under secretary of defense for policy; and the under secretary of
defense for acquisition, technology, and logistics certify that the
proposed system meets 11 additional requirements, each of which require
presenting considerable evidence.

**2. The JAIC solves now -- it promotes cooperation on interoperability
and ethical norms for responsible AI use through the Partnership for
Defense.**

**Stanley-Lockman, 2021 - Center for Security and Emerging Technology**
\[Zoe CSET Issue Brief August "Responsible and Ethical Military AI
Allies and Allied Perspectives"
https://cset.georgetown.edu/publication/responsible-and-ethical-military-ai/
Acc. 4/21/22 TA\]

Since DOD adopted these five principles, the JAIC has led their
implementation both in staffing and in processes. Further,
implementation has also included efforts related to "procurement
guidance, technological safeguards, organizational controls, risk
mitigation strategies and training measures."24 On training measures in
particular, the JAIC organized a RAI Champions Pilot to educate
multidisciplinary military AI stakeholders on AI ethics and
implementation.[25 The eventual development and implementation of
"governance standards]{.underline}" that encompass these measures, as
[included in the responsibilities of the JAIC Head of AI Ethics
Policy]{.underline}, are namely geared toward internal use.26 Further,
[such governance standards can also guide alignment efforts with allies
and partners]{.underline}---as then-[JAIC Director Lieutenant General
Jack Shanahan mentioned with regards to using ethical principles to
"\[forge\] a path to increase dialogue and cooperation abroad to include
the goal of advancing interoperability]{.underline}."27 [These
priorities are also seen in the JAIC's international engagement. The
JAIC is focused on "shaping norms around democratic values]{.underline}"
as one if its three pillars of international engagement.28 [The other
pillars of international military AI policy--- "ensuring data
interoperability and working to create pipelines to enable the secure
transfer of technology"---also partially depend on ethics,]{.underline}
safety, principles, and possibly even regulations.29 Importantly, some
technical aspects of this engagement concerns adoption issues that are
not discussed at length here.

**3. No inherency -- NATO is solving now -- they are setting ethical
guidelines now.**

**Heikkilä, 2021 - Politico's AI Correspondent in London** \[Melissa,
Politico March 31 "AI Decoded: NATO on AI warfare --- AI treaty
consultation --- Unions call for more AI protections"
https://www.politico.eu/newsletter/ai-decoded/politico-ai-decoded-nato-on-ai-warfare-ai-treaty-consultation-unions-call-for-more-ai-protections/
Acc 4/9/22 TA\]

THE AI WARS: [NATO is working on an AI strategy]{.underline} it hopes to
unveil before the summer as part of its bid [to maintain an edge over
increasingly assertive rivals.]{.underline} "We need each other more
than ever. No country alone or no continent alone can compete in this
era of great power competition," NATO Deputy Secretary-General Mircea
Geoană, the alliance's second in command, told me in an interview. What
to expect: [The strategy will identify ways to operate AI systems
responsibly,]{.underline} name different uses for the technology in the
military sector and provide a "platform for allies to test their AI to
see whether it's up to NATO standards," [David van Weel, NATO's
assistant secretary-general for emerging security challenges, told
me.]{.underline} [The strategy will also set ethical guidelines to
govern AI systems, for example by ensuring systems can be shut down by a
human at all times]{.underline}, [and make them accountable by ensuring
a human is responsible for the actions of AI systems]{.underline}.

**4. No inherency -- The EU and the US established the Global
Partnership for AI to increase collaboration and create norms for use.**

**Lawrence and Cordey, 2020 -- researchers for The Cyber Project at the
Belfer Center for Science and International Affairs** \[Christie and
Sean, August, The Cyber Project Paper "The Case for Increased
Transatlantic Cooperation on Artificial Intelligence Edited by Lauren
Zabierek and Julia Voo
https://www.belfercenter.org/sites/default/files/2020-08/TransatlanticAI.pdf
Acc. 4/21/22 TA\]

[Another topic of de facto transatlantic collaboration and alignment is
international principles for AI (i.e., norms for AI's development, use,
and governanc]{.underline}e). In that regard and as confirmed by
Assistant Research Director at the Berkman Klein Center Ryan Budish, a
significant conduit of collaboration has been the OECD' Values-based
Principles for the Responsible Stewardship of Trustworthy AI272 which
were adopted in May 2019 by 42 countries.273 [These
principles---supported by the US, the EU, and most European Member
States---were developed by a group of international experts from member
countries]{.underline}, think tanks, industry, and civil society.
Amongst these were staff from the NSF, Departments of Commerce and
State, as well as from the European Commission and various European
Member States, such as the French, Dutch and German ministries of the
economy.274 Indeed, during a speech at the 2019 White House Summit on
AI, Deputy Chief Technology Officer Michael Kratsios stated that the US
was working with "democracies of the world that share our common
values," as illustrated by the adoption of the OECD's AI principles.275
A more recent and promising example of and conduit for cooperation on
responsible behavior is the newly founded [Global Partnership for
AI]{.underline} (GPAI). This initiative, which is grounded in the OECD
AI principles [was]{.underline} [co-founded in]{.underline} June [2020
by the US and the EU.]{.underline}276 Its aims [to develop AI "grounded
in human rights, inclusion, diversity, innovation, and economic
growth."]{.underline}277 [It is one of the most extensive collaborations
on AI policy that exists, notably in terms of scale, diversity of
experts and geographical span.]{.underline} As such, it is the first
major coalition of likeminded states and is seen by many as an attempt
to form a bulwark against China and its AI leadership ambitions. A last
conduit for international norms for AI has been the G20. In June 2019,
the G20 drew upon the OECD's principles to publish its Human-Centered AI
Principles.278

**5. No inherency -- the military won't build autonomous weapons because
they don't want them. We have a training advantage now.**

**Freedberg, 2019 -- deputy editor for Breaking Defense** \[Sydney J
"The frontline of a new age in defense Artificial Intelligence"
https://cdn2.hubspot.net/hubfs/2097098/MCM120_BreakingDefense_AI_ebookR1%20(1).pdf
Acc 5/25/22 TA\]

The Real Barrier So [what is stopping the Defense Department from
developing AI weapons that can kill humans? The real barrier, it turns
out, is not legal or technological]{.underline}: It's cultural. [The US
military isn't developing killer robots because it doesn't want
them]{.underline}. [Every officer and official I've ever talked to on
the subject]{.underline}, for at least eight years, [has said they want
AI and robotics to help the human, not replace them --- and even then,
they want AI primarily in non-combat functions like logistics and
maintenance.]{.underline} In fact, [Pentagon leaders seem to think
taking the human out of the loop would be giving up one of American
military's most crucial advantages: the training, creativity, and, yes,
ethics of its people]{.underline}. "The last thing I want is you to go
away from this thinking this is all about technology," then-Deputy
Secretary Robert Work told us in 2015. Work, whose Third Offset Strategy
first made AI a top priority for the Pentagon, has remained deeply
engaged in the debate. "The number one advantage we have is the people
in uniform, in our civilian work force, in our defense industrial base,
and the contractors who support us."

**6. No inherency -- the DOD AI Strategy creates policies to adopt
ethical AI standards and set international norms.**

**Stanley-Lockman, 2021 - Center for Security and Emerging Technology**
\[Zoe CSET Issue Brief August "Responsible and Ethical Military AI
Allies and Allied Perspectives"
https://cset.georgetown.edu/publication/responsible-and-ethical-military-ai/
Acc. 4/21/22 TA\]

[Following the adoption of the DOD AI Strategy in 2018]{.underline},
[the U.S. approach to AI ethics in the defense realm]{.underline} can be
generally broken down into three phases: [(1) the DIB leading the
process to define AI ethics principles, (2) DOD adopting these
principles]{.underline} for safe and ethical AI, and most recently, (3)
the beginning of more visible efforts to implement RAI across the
Department and armed forces. Starting in July 2018, the DIB began its
15-month process on safe and ethical AI for defense, with the mandate of
recommending principles to DOD in its capacity as an independent federal
advisory committee.17 This process took the form of public
consultations, listening sessions, the formation of an informal DOD
Principles and Ethics Working Group, expert roundtables, a classified
"red team" session, and a tabletop exercise.18 As part of these
consultations, government officials from "close partner nations" were
also involved---including as part of the monthly meetings of the
informal DOD Principles and Ethics Working Group.19 [The role of allies
in the resulting DIB recommendations largely focuses on]{.underline}
[the intersection between AI ethics and international norm
development.]{.underline} More specifically, [the DIB conceived of the
role of allies mainly through the lens of DOD leadership, focusing on
"how AI will be developed]{.underline} and used, and whether there ought
to be any regulation on particular applications" to mitigate potential
harms.20 This is seen hand-in-hand with DOD's "duty [to the American
people and its allies to preserve its strategic and technological
advantage]{.underline} over competitors and adversaries who would use AI
for purposes inconsistent with the Department's values."21 In other
words, [the DIB sees aligning technological development]{.underline},
deployment, and intended outcomes [with democratically informed values
as a strategic obligation]{.underline} just as much as a departure point
[for the U.S. to lead norm development in the international
community.]{.underline}

**[\--Extend -- JAIC]{.underline}**

**The JAIC hosted the PfD Symposium -- it is a starting point for a
global approach to ethical AI development.**

**Trabucco, 2020 - Research Assistant at the Centre for Military Studies
at the University of Copenhagen** \[Lena, May 10 "AI Partnership for
Defense is a Step in the Right Direction -- But Will Face Challenges"
http://opiniojuris.org/2020/10/05/ai-partnership-for-defense-is-a-step-in-the-right-direction-but-will-face-challenges/
Acc 4/17/22 TA\]

On September 15 and 16, 2020, [the Defense Department's Joint Artificial
Intelligence Center, or JAIC, held an symposium which hosted delegations
from 12 nations]{.underline} including Australia, Canada, Denmark,
Estonia, Finland, France, Israel, Japan, Norway, the Republic of Korea,
Sweden, and the United Kingdom. According to the JAIC, the symposium
gathered nations [furthest along in their AI development to discuss
"shared lessons learned and best practices in harnessing AI for their
respective and shared defense missions]{.underline}." The goal of the
partnership, according to DoD officials, is to promote standards for
responsible AI development and establish avenues and tools for data
sharing, cooperative development, and enhanced interoperability. The DoD
offered few details about the framework and functioning of the
partnership beyond a "forum \[that\] seeks to provide values-based
global leadership in defense for policies and approaches in adopting
AI," according to a DoD statement. Nevertheless, some benefits and
challenges are clear from the outset. [The AI partnership is a step in
the right direction. It offers an avenue of AI innovation that will
signal consensus and cooperation between the US and crucial partners. At
first glance, this cooperation may seem little more than a symbolic
gesture, but this partnership was a necessary step in fostering a more
reasoned and global approach to military AI development]{.underline}.
Additionally, [the partnership grants accessibility to a wider network
of military and defense organizations to engage in research and
development and draw on global AI talent]{.underline}. [Adding diverse
voices to the complex discussion]{.underline} of AI as a defense
technology [will yield more]{.underline} insightful dialogue and
[solutions.]{.underline}

**The PfD shares best practices, reaches out to new allies, and creates
an ongoing forum to establish norms.**

**Stanley-Lockman, 2021 - Center for Security and Emerging Technology**
\[Zoe CSET Issue Brief August "Responsible and Ethical Military AI
Allies and Allied Perspectives"
https://cset.georgetown.edu/publication/responsible-and-ethical-military-ai/
Acc. 4/21/22 TA\]

AI Partnership for Defense (PfD) [In September 2020, the JAIC convened
the inaugural PfD meeting]{.underline}, featuring virtual delegations
from Australia, Canada, Denmark, Estonia, Finland, France, Israel,
Japan, Norway, the Republic of Korea, Sweden, the U.K., and the United
States [to "shape what responsible AI looks like."]{.underline}162 As of
May 2021, three additional countries joined for the third PfD meeting:
Germany, the Netherlands, and Singapore. [As the grouping of countries
makes clear, the ability to include non-treaty allies in the PfD makes
it a useful format to borrow from each other's approaches to RAI, be it
to establish, refine, or implement nation-level views.]{.underline} Just
two months before joining the PfD, for instance, Singapore prepared
"preliminary guiding principles to be applied to the defence
establishment in Singapore, and Singapore's contributions to the global
discussion on international norms for defence AI applications" in March
2021.163 [Further, there may also be the possibility of taking aspects
of responsible military AI from other countries that focus more on norms
of responsible state behavior on board in the PfD format.]{.underline}
Some allies explicitly mention a focus on norms, including the U.K. in
its new national security and international policy, and Germany via its
focus on arms control and emerging technologies. This normative emphasis
harkens back to the U.S. approach to responsible and ethical AI in
defense---which saw norms as one of the primary areas of engagement with
like-minded countries. This normative focus could also benefit
engagement with allies that have not yet begun any public iteration of
views on responsible military AI, including Japan and South Korea. As
such, [the PfD's focus on responsible AI makes it an important venue to
encompass technology norms that are based on democratic values and that
focus on minimizing risks in the international security
environment.]{.underline} As a final note, it is not a coincidence that
all allies surveyed here participate in the PfD. It is an important
forum for them to exchange views---not only on aspects covered in this
report, but potentially also the impact of civilian AI ethics frameworks
and developments, as well as questions about autonomy-related aspects of
human-machine teaming.

**The JAIC establishes US leadership on responsible AI use.**

**Trabucco, 2020 - Research Assistant at the Centre for Military Studies
at the University of Copenhagen** \[Lena, May 10 "AI Partnership for
Defense is a Step in the Right Direction -- But Will Face Challenges"
http://opiniojuris.org/2020/10/05/ai-partnership-for-defense-is-a-step-in-the-right-direction-but-will-face-challenges/
Acc 4/17/22 TA\]

On September 16, 2020, [the US Defense Department (DoD) announced the
launch of the AI Partnership for Defense -- a multi-national partnership
which will "engage military and defense organizations from more than 10
nations with a focus on incorporating ethical principles into the AI
delivery pipeline," according to Secretary Esper]{.underline}. Secretary
Esper noted in his announcement: In February, [we became the first
military in the world to adopt ethical principles for the use of
AI,]{.underline} based on core values of transparency, reliability, and
governability. [These principles make clear to the]{.underline} American
people -- and the [world -- that the United States will once again lead
the way in the responsible development and application of emerging
technologies, reinforcing our role as the global security partner of
choice.]{.underline}

**[AT Cohesion]{.underline}**

**Ethics principles cannot create cohesion -- there are many other
issues that must be resolved for the US to coordinate NATO**

**Trabucco, 2020 - Research Assistant at the Centre for Military Studies
at the University of Copenhagen** \[Lena, May 10 "AI Partnership for
Defense is a Step in the Right Direction -- But Will Face Challenges"
http://opiniojuris.org/2020/10/05/ai-partnership-for-defense-is-a-step-in-the-right-direction-but-will-face-challenges/
Acc 4/17/22 TA\]

Which brings me to the third challenge. The AI partnership symposium did
not offer a coherent strategy for the partnership beyond advancing core
values the participating nations find important to the AI pipeline.
[Peter Singer, New America Foundation fellow]{.underline} and
strategist, noted that the US has not yet offered a coherent strategy to
contrast its "near peers." In one article, Singer [said, "China has a
fairly clear and robust vision of this \[AI and its applications\] and
it is actively exporting that vision. There is absolutely no way the US
can compete without offering a different and compelling vision and one
that involves our friends and allies."]{.underline} On the one hand, the
absence of an overarching strategy gives the DoD and the AI partnership
latitude to address inevitable issues that will arise. Secretary Esper
noted in his address that this kind of partnership is the first of its
kind and needs time to operate in the face of unforeseen challenges.
[But to accomplish the goal as the preferred security partner in AI, the
partnership will need to substantiate its agreement with a vision more
concrete than just ethical values.]{.underline} The AI partnership
accomplishes getting some allies on board, but does not clarify what
vision is driving the newfound partnership. At some point, this will
need to change. In essence, the AI partnership is a necessary and
welcome development in the US AI strategy, but significant legal and
policy challenges are on the horizon. The three outlined here -- [legal
interoperability, trans-Atlantic cooperation, and an uncertain coherent
strategy]{.underline} -- are certainly not exhaustive. But they
[represent a span of legal and policy issues the partnership are sure to
encounter as it moves forward.]{.underline}

**No solvency --alternate causalities - adversarial disinformation will
undermine cohesion by sowing discord.**

**Lin-Greenberg, 2020 - member of the MIT Security Studies Program**
\[Erik Vol 3, Iss 2 Spring, Texas National Security Review "Allies and
Artificial Intelligence: Obstacles to Operations and Decision-Making"
http://dx.doi.org/10.26153/tsw/8866 Acc 4/22/22 TA\]

Adversary Manipulation and Interference Even if states were to trust
their own AI technologies, rivals and malicious actors can use AI to sow
discord that can hamper decision-making. [Trust and close relationships
are crucial when multiple states coordinate security-related decisions
since policymakers must be confident that allies will not renege on
commitments. Leaders have long held fears of being abandoned by allies
or of being drawn into unwanted conflicts.]{.underline}105 These fears
are magnified when leaders suggest they might not follow through with
their alliance commitments or engage in provocative actions.106 Trump,
for instance, raised questions about Washington's commitment to its
allies when he publicly questioned the value of defending certain NATO
member states.107 [An adversary could use AI to drive misinformation
campaigns that latch onto these concerns in an effort to strain ties or
deepen cleavages between allies.]{.underline} Just as
[adversaries]{.underline} can use deepfakes to interfere with
operational-level coordination, they [can]{.underline} also [use AI
technologies to breed confusion and mistrust that hamper strategic
decision-making. Actors seeking to disrupt alliance cohesion might
create deepfakes depicting leaders of alliance member states questioning
the value of an alliance, criticizing other leaders, or threatening to
take actions that could draw an alliance into an unwanted
conflict]{.underline}. [These falsified videos]{.underline} or
recordings [could boost uncertainty of an ally's commitments or induce
panic over fears of abandonment during a crisis]{.underline}. The
decision-making process may be slowed as policymakers try to understand
their allies' true intentions and preferences, or convince domestic
publics that an ally's "statements" are in fact AI-produced
misinformation.

**France blocks cohesion -- French commitment to digital sovereignty
prevents collaboration.**

**Stanley-Lockman, 2021 - Center for Security and Emerging Technology**
\[Zoe CSET Issue Brief August "Responsible and Ethical Military AI
Allies and Allied Perspectives"
https://cset.georgetown.edu/publication/responsible-and-ethical-military-ai/
Acc. 4/21/22 TA\]

On this note, [the French conception of controlled AI goes a step
further and ties auditability to the core value of sovereignty in French
strategic culture.54 This is because the relationship between
auditability and control stems from geopolitical concerns. The strategy
states, "France cannot resign itself to being dependent on technologies
over which it has no control. \[...\] Preserving digital sovereignty
therefore also involves controlling the algorithms and their
configuration, and the governance of data."55 This need for control
comes from a desire to exert independence from the "stranglehold on AI
exerted by China and the United States," including by strengthening
European cooperation.56]{.underline} While [the]{.underline}
geopolitical aspects and [prospects of France to assert this
independence]{.underline} are beyond the scope of this study, it is
notable that they [trickle into the French approach to trace the
provenance of models and data.]{.underline} [In particular, weapons are
"critical applications" that will need to be auditable]{.underline}.57
If enforced, this means that questions about data rights and legal
authorities to transfer data (including from foreign suppliers) could
render AI "uncontrolled" per the French definition. [Here, protectionism
straddles the line of ethics and adoption, with digital sovereignty as a
potential factor that determines acceptability of both]{.underline}.
[This can also be seen in the imperative to maintain "freedom of
action.]{.underline}"58

**Domestic politics prevent alliance cohesion and interoperability --
many nations cannot commit to AI**

**Lin-Greenberg, 2020 - member of the MIT Security Studies Program**
\[Erik Vol 3, Iss 2 Spring, Texas National Security Review "Allies and
Artificial Intelligence: Obstacles to Operations and Decision-Making"
http://dx.doi.org/10.26153/tsw/8866 Acc 4/22/22 TA\]

Although [alliances and multilateral coalitions]{.underline} can bolster
the security of member states and the efficiency of their military
operations, membership [can create complications for decision-making and
the coordination of military operations.]{.underline} First, alliances
and coalitions must overcome operational challenges surrounding the
integration and coordination of military forces. Modern military
operations require the close coordination of participating forces,
shared intelligence to guide planning and mission execution, and weapon
systems capable of communicating with and operating alongside each
other. [The military of each alliance or coalition member state brings
with it different equipment, policies, and tactics, meaning that a
state's forces may not fully integrate with the forces of its
allies]{.underline}.42 [Moreover, partners are often reluctant to share
sensitive operational and intelligence information]{.underline}.43
Beyond these institutional issues, more commonplace matters --- such as
the [different languages and military cultures]{.underline} of each
member state --- [can hinder interoperability during contingency
operations]{.underline}.44 Second, [alliance and coalition leaders may
have trouble deciding what policies their coalition should
pursue]{.underline}. Although allies typically face a common threat and
share many policy objectives, [each state still maintains its own
priorities and goals. State leaders therefore respond to domestic
constituencies]{.underline} and pursue their own national interests,
[which,]{.underline} at times, [may be at odds with alliance
goals]{.underline}.45 At best, these divergent interests result in
coordination problems that draw out decision-making timelines.46 At
worst, they generate mistrust between partners and raise concerns of
being abandoned during a crisis or "chain-ganged" into unwanted wars.47

**France is committed to digital sovereignty**

**Stanley-Lockman, 2021 - Center for Security and Emerging Technology**
\[Zoe CSET Issue Brief August "Responsible and Ethical Military AI
Allies and Allied Perspectives"
https://cset.georgetown.edu/publication/responsible-and-ethical-military-ai/
Acc. 4/21/22 TA\]

54 [The French imperative to maintain technological independence is
stronger than any other European ally, and largely motivates French
defence industrial policy as well as the political agenda of "strategic
autonomy" and "digital sovereignty" at the national and European
levels]{.underline}. [Other documents]{.underline} that [reinforce
this]{.underline} include those referenced in footnotes 47 and 49, as
well as the 2019 Defence Innovation Orientation Document (2019) and the
Ministry of Armed Forces Digital Transformation: Key Concepts (2020). 55
AI Task Force, Artificial Intelligence in Support of Defence, 10. 56
[This strong language intends to set the political tone for
adoption]{.underline}, [and is not purely about ethics.]{.underline}
Further, [while the "stranglehold" motivates sovereignty, there are few
specifics in the strategy]{.underline} about hardware components or
cloud capabilities, beyond the recognition that these are not French or
European strengths. Auditability is only tied, here, to models and data.
AI Task Force, Artificial Intelligence in Support of Defence, 10, 24.

**[AT Crisis Instability]{.underline}**

**AI doesn't lower the threshold for war -- it is a Political, not
Technological issue -- banning weapons won't solve it.**

**Del Re, 2017 -- US Army Major** \[Amanda "Lethal Autonomous Weapons:
Take the Human Out of the Loop A paper submitted to the Faculty of the
US Naval War College in partial satisfaction of the requirements for the
Ethics of Emerging Military Technology Graduate Certificate. 16 June
2017 https://apps.dtic.mil/sti/citations/AD1041804 Acc 12/27/20 TA\]

The above arguments against employing LAW are valid reasons to be
hesitant. However, there are a few points to consider in juxtaposition
with them. [The "tacticization of strategy" is a problem at the higher
levels of government. However, this issue has been a problem long before
the idea of LAWs has been considered. The "tacticization of strategy" or
the idea that technology could lower the bar to entry into war is not
the problem, it is only the symptom of the problem. The problem is the
misconception that technology wins wars]{.underline}. [LAWs may
contribute to that way of thinking but banning them will not make it go
away.]{.underline}

**Turn -- Banning LAWs increases the risk of hacking because it
undermines research that helps cyber Defense against hacking.**

**Messinger 2015 - Research Associate at the Center on Law and
Security** \[Eric January 15, Just Security "Is it Possible to Ban
Autonomous Weapons in Cyberwar?"
https://www.justsecurity.org/19119/ban-autonomous-weapons-cyberwar/ Acc
12/27/20 TA\]

Of course, those familiar with the debate over AWS in kinetic warfare
have already heard arguments about potential upsides for efficacy. Yet
the nature of the cyber battleground, and especially cyber defense, will
provide strong incentives to employ autonomous offensive cyber systems.
The cyber theater consists in whole or in part of computerized systems,
where the speed of movement is not constrained by the physical
limitations of feet and engines and rockets, and where the scope and
scale of combat may proceed beyond the ability of human observers to
comprehend in real-time. [As Dorothy Denning argues, "\[a\]t the speed
of cyber, placing humans in the loop at every step is neither practical
nor desirable."]{.underline} As a result, in direct analogy to defense
systems such as anti-missile systems, [\[m\]ost anti-malware and
intrusion prevention systems have]{.underline} both manual and
[automated components]{.underline}. Humans determine what goes into the
signature database, and they install and configure the security
software. The processes of signature distribution, malicious code and
packet detection, and initial response are automated, but humans may be
involved in determining the final response. [Effective cyber defenses,
in short, will have to rely upon automatic routines]{.underline}.
Further, to date, [the technological development and practical adoption
of autonomous weapons systems for defense has progressed further than
that of autonomous offensive systems]{.underline}.

**Speed is non-unique -- adversaries with autonomous weapons will still
compress time for decision making.**

**Lin-Greenberg, 2020 - member of the MIT Security Studies Program**
\[Erik Vol 3, Iss 2 Spring, Texas National Security Review "Allies and
Artificial Intelligence: Obstacles to Operations and Decision-Making"
http://dx.doi.org/10.26153/tsw/8866 Acc 4/22/22 TA\]

[An adversary's use of AI-enabled systems can also compress timelines
and complicate alliance decision-making. Just as AI can boost the tempo
of allied operations, it can increase the frequency and speed of a
rival's military actions]{.underline}. AI-enabled autonomous weapon
systems that allow states to launch military operations without putting
personnel in harm's way may lead rival leaders to launch operations that
they might not otherwise carry out.93 [China, for instance, has
developed and exported autonomous drones capable of identifying targets
and carrying out lethal strikes with little or no human
oversight]{.underline}.94 Further, a rival's integration of AI into its
command-and-control networks may speed its decision-making process.
Indeed, China's military has expressed an interest in leveraging AI for
military decision-making.95 A publication from the Central Military
Commission Joint Operations Command Center, for example, described how
the use of AI to play the complex board game Go "demonstrated the
enormous potential of artificial intelligence in combat command, program
deduction, and decisionmaking."96 [These systems could be employed
against the United States and its allies in the Indo-Pacific region,
forcing allied commanders to respond more quickly to these
threats]{.underline}.

**No solvency -- even if the plan eliminates the risk of crisis
instability from NATO weapons, Russia and China will still use Their
AI.**

**Allen, 2022 - director of the AI Governance Project at the CSIS**
\[Gregory, May 20, "One Key Challenge for Diplomacy on AI: China's
Military Does Not Want to Talk"
[https://www.csis.org/analysis/one-key-challenge-diplomacy-ai-chinas-military-does-not-want-talk
Acc
6/6/22](https://www.csis.org/analysis/one-key-challenge-diplomacy-ai-chinas-military-does-not-want-talk%20Acc%206/6/22)
TA\]

Over the past 10 years, artificial intelligence (AI) technology has
become increasingly critical to scientific breakthroughs and technology
innovation across an ever-widening set of fields, and warfare is no
exception. In pursuit of new sources of competitive advantage,
militaries around the world are working to accelerate the integration of
AI technology into their capabilities and operations. However, [the rise
of military AI has brought with it fears of a new AI arms race and a
potential new source of unintended conflict escalation]{.underline}. In
the May/June 2022 issue of Foreign Affairs, Michael C.
[Horowitz,]{.underline} Lauren [Kahn, and]{.underline} Laura Resnick
[Samotin write]{.underline}: ["The United States, then, faces dueling
risks from AI.]{.underline} If it moves too slowly, Washington could be
overtaken by its competitors, jeopardizing national security. But [if it
moves too fast, it may compromise on safety and build AI systems that
breed deadly accidents.]{.underline} Although the former is a larger
risk than the latter, it is critical that the United States take safety
concerns seriously." Such fears are not entirely unfounded. Machine
learning, the technology paradigm at the heart of the modern AI
revolution, brings with it not only opportunities for radically improved
performance, but also new failure modes. When it comes to traditional
software, the U.S. military has decades of institutional muscle memory
related to preventing technical accidents, but building machine learning
systems that are reliable enough to be trusted in safety-critical or
use-of-force applications is a newer challenge. To its credit, the
Department of Defense (DOD) has devoted significant resources and
attention to the problem: partnering with industry to make commercial AI
test and evaluation capabilities more widely available, announcing AI
ethics principles and releasing new guidelines and governance processes
to ensure their robust implementation, updating longstanding DOD system
safety standards to pay extra attention to machine learning failure
modes, and funding a host of AI reliability and trustworthiness research
efforts through organizations like the Defense Advanced Research
Projects Agency (DARPA). [However, even if the United States were
somehow to successfully eliminate the risk of AI accidents in its own
military systems---]{.underline}a bold and incredibly challenging goal,
to be sure---[it still would not have solved risks to the United States
from technical failures in Russian and Chinese military AI systems. What
if a Chinese AI-enabled early warning system erroneously announces that
U.S. forces are launching a surprise attack? The resulting Chinese
strike---wrongly believed to be a counterattack---could be the opening
salvo of a new war]{.underline}.

**Defense planners face uncertainty even without AI weapons.**

**Lin-Greenberg, 2020 - member of the MIT Security Studies Program**
\[Erik Vol 3, Iss 2 Spring, Texas National Security Review "Allies and
Artificial Intelligence: Obstacles to Operations and Decision-Making"
http://dx.doi.org/10.26153/tsw/8866 Acc 4/22/22 TA\]

[Allied decision-makers will also face uncertainty when confronting a
rival's use of AI-enabled technologies. Leaders will be forced to
wrestle with whether to respond to actions carried out by AI-enabled
systems --- like autonomous aircraft or ships --- in the same way as
actions carried out by traditionally manned assets]{.underline}.
[Existing doctrine and law are generally silent on these issues,
providing no guidance on the appropriate response.]{.underline} States
have drafted domestic policies to govern their own use of autonomous
weapon systems, but these regulations and international law make no
distinction between how states should react to a rival's AI-enabled
military actions versus "traditional" military actions.103 Yet,
decision-makers may believe that a rival's use of AI technologies
demands different responses than those involving manned platforms.104
What happens if a rival claims that an attack carried out by an
AI-enabled system was the result of a flawed algorithm? Should air
defense forces respond differently to an adversary's autonomous drones
that penetrate friendly airspace than to a manned aircraft that does the
same? Decision-makers may find themselves with little time to consider
these complicated issues, particularly as AI technology accelerates the
speed of a rival's military operations.

**[AT Nuclear Instability]{.underline}**

**While countries will adopt autonomous systems, most countries will
avoid using Destabilizing AI. Nations are Cautious about Nuclear
Systems, and will only utilize autonomous AI if they enhance
stability.**

**Horowitz, 2019 -- prof of Political Science at Univ of Pennsylvania**
\[Michael with Paul Scharre, and Alexander Velez-Green "A Stable Nuclear
Future? The Impact of Autonomous Systems and Artificial Intelligence"
December arXiv:1912.05291 https://arxiv.org/abs/1912.05291 Acc 12/27/20
TA\]

[What might lead to variation in how countries make choices about the
relative utility of autonomous systems]{.underline}? The answer could
depend on how secure they feel about their non-autonomous nuclear
systems. [States that feel extremely secure in their second strike
capabilities at present may see fewer advantages to
automation.]{.underline} In that case, the advantages greater speed and
precision but might not appear worth the potential risk of accidents.
[Instead, states like the United States would likely prefer to use
existing non-autonomous systems for nuclear command and control and
delivery]{.underline}. [In contrast, countries whose nuclear arsenals
are more insecure]{.underline} may be more accepting of risk and [may
find the perceived advantages of automation more valuable]{.underline}.
If a country thinks that its nuclear command and control might be at
risk of severe degradation or destruction, [it might be more likely to
automate early warning to increase its response speed, deploy autonomous
nuclear delivery platforms with higher endurance]{.underline}, automate
new aspects of target selection for nuclear delivery vehicles, or shift
towards more automated nuclear launch postures. All may not happen in
unison, of course, but as a general relationship, countries whose
arsenals are more insecure may be more willing to take risks to better
enhance their arsenal's survivability.

**AI enhances nuclear deterrence because they provide the Speed
necessary to handle the information overload in a crisis, and the
Resilience to enhance deterrence.**

**Horowitz, 2019 -- prof of Political Science at Univ of Pennsylvania**
\[Michael with Paul Scharre, and Alexander Velez-Green "A Stable Nuclear
Future? The Impact of Autonomous Systems and Artificial Intelligence"
December arXiv:1912.05291 https://arxiv.org/abs/1912.05291 Acc 12/27/20
TA\]

A critical question is thus how militaries will employ advances in AI to
influence their early warning and NC2 systems. There may be many places
where militaries could employ new forms of autonomous systems to bolster
the reliability and effectiveness of early warning and NC2.
Human-machine teaming could help offset automation bias and thus enable
the use of more autonomous systems. [More advanced automation in nuclear
early warning systems could allow greater situational awareness,
reducing the risk of false alarms. It could also play a valuable role in
helping human decision-makers process large amounts of information
quickly]{.underline}. In this regard, automated data processing may play
a critical role in helping human nuclear early warning operators to
identify threats -- and false cues -- in an increasingly data-saturated
and complex strategic environment. [Increased automation in NC2 could
also help to reduce the risk of accidents or unauthorized use. And an
expanded role for automation in communications could help ensure that
command-and-control signals reach their targets quickly and uncorrupted
in highly contested electromagnetic environments]{.underline}.
Automation could also be used to enhance defenses -- physical or cyber
-- against attacks on nuclear early warning, command-and-control,
delivery, and support systems, [thereby enhancing deterrence and
fortifying stability. It could also be used to bolster the resilience of
vulnerable NC2 networks.]{.underline} For instance, long-endurance
uninhabited aircraft that act as pseudo-satellites ("pseudo-lites") to
create an airborne communications network could increase NC2 resilience
by providing additional redundant communications pathways in the event
of satellite disruption. [Automation could even enable autonomously
self-healing networks]{.underline} -- in physical or cyberspace -- [in
response to jamming]{.underline} or kinetic attacks against
command-and-control nodes, [thereby sustaining situational awareness and
command and control and enhancing deterrence.]{.underline}

**Autonomous AI systems are essential to Counter cyberwarfare -- only
Defensive LAWs can react to hacking and accidents.**

**Messinger 2015 - Research Associate at the Center on Law and
Security** \[Eric January 15, Just Security "Is it Possible to Ban
Autonomous Weapons in Cyberwar?"
https://www.justsecurity.org/19119/ban-autonomous-weapons-cyberwar/ Acc
12/27/20 TA\]

Of course, those familiar with the debate over AWS in kinetic warfare
have already heard arguments about potential upsides for efficacy. Yet
the nature of the cyber battleground, and especially cyber defense, will
provide strong incentives to employ autonomous offensive cyber systems.
The cyber theater consists in whole or in part of computerized systems,
where the speed of movement is not constrained by the physical
limitations of feet and engines and rockets, and where the scope and
scale of combat may proceed beyond the ability of human observers to
comprehend in real-time. [As Dorothy Denning argues, "\[a\]t the speed
of cyber, placing humans in the loop at every step is neither practical
nor desirable."]{.underline} As a result, in direct analogy to defense
systems such as anti-missile systems, [\[m\]ost anti-malware and
intrusion prevention systems have]{.underline} both manual and
[automated components]{.underline}. Humans determine what goes into the
signature database, and they install and configure the security
software. The processes of signature distribution, malicious code and
packet detection, and initial response are automated, but humans may be
involved in determining the final response. [Effective cyber defenses,
in short, will have to rely upon automatic routines]{.underline}.
Further, to date, [the technological development and practical adoption
of autonomous weapons systems for defense has progressed further than
that of autonomous offensive systems]{.underline}.

**AI enhances the security of a stable second strike, which is key to
deterrence.**

**Horowitz, 2019 -- prof of Political Science at Univ of Pennsylvania**
\[Michael with Paul Scharre, and Alexander Velez-Green "A Stable Nuclear
Future? The Impact of Autonomous Systems and Artificial Intelligence"
December arXiv:1912.05291 https://arxiv.org/abs/1912.05291 Acc 12/27/20
TA\]

[Uninhabited nuclear launch platforms may be seen to offer some
strategic benefits to nuclear-armed states. Nuclear-armed UAVs, for
instance, could be kept aloft for far longer than is possible with human
pilots, decreasing fear of a disarming first strike.]{.underline} B-2
bomber pilots, for instance, have flown a maximum of 40-hour missions.88
By contrast, refuelable UAVs could stay aloft for several days, limited
only by engine lubricants and other reliability factors. Uninhabited
aircraft have already conducted 80-hour flights.89 The maximum endurance
record for a refuelable aircraft is 64 days.90 [The ability to keep
nuclear bombers in the air for longer periods of time might offer
policymakers new tools for managing escalation.]{.underline}
Long-endurance nuclear-armed UAVs could provide policymakers with
additional options for nuclear signaling, since they could be kept
on-station longer than would otherwise be possible. Likewise, if they
are sufficiently survivable against adversary countermeasures,
nuclear-armed UAVs might improve a state's ability to deliver nuclear
weapons in a timely manner since they could be kept aloft closer to
potential targets longer than their manned counterparts. For some less
powerful nuclear-armed states, UAVs may also be seen as a lower-cost,
longer-range alternative to human-inhabited nuclear bombers. Lower-cost
systems are unlikely to be as survivable as their more expensive
counterparts, however, thus limiting their utility. [Nuclear delivery
vehicles that leverage AI for certain functions may]{.underline} also be
seen to provide strategic benefits. For instance, the Status-6's
notional AI-enabled counter-ASW capabilities may help to [improve
Russian leaders' confidence in their secure second strike]{.underline}
regardless of advances in U.S. missile defenses by convincing them that
their nuclear-armed torpedoes will always be able to reach their
targets. [This might constitute an improvement to U.S.-Russian nuclear
stability]{.underline}.91 But any such reassurance will be limited by
the fact that, while torpedoes may pose a threat to coastal targets,
they cannot strike inland strategic targets, such as enemy leadership
redoubts, command centers, strategic forces, critical infrastructure, or
population centers.92 As a result, even if automation does improve
Status-6 survivability, it would constitute only a marginal improvement
to the overall viability of Russia's nuclear deterrent, since from
Moscow's perspective, U.S. missile defenses and strike capabilities
could still prevent it from using missiles to hold the full range of
necessary targets at risk.93

**Specifically, a fear of accidents will limit nuclear LAWs to only
positive ones.**

**Horowitz, 2019 -- prof of Political Science at Univ of Pennsylvania**
\[Michael with Paul Scharre, and Alexander Velez-Green "A Stable Nuclear
Future? The Impact of Autonomous Systems and Artificial Intelligence"
December arXiv:1912.05291 https://arxiv.org/abs/1912.05291 Acc 12/27/20
TA\]

Many of these ways that autonomous systems could increase the resiliency
and accuracy of NC2 are speculative, however. [Existing automation, as
the Petrov incident shows, already creates the risk of automation bias.
Knowledge of this will probably make most nuclear-armed states unlikely
to further automate the early warning or command-and-control processes,
with two exceptions: first, in situations where human-machine teaming
might be further integrated to mitigate potential false alarms; second,
in situations where a state fears for its secure second strike, and
believes that further automation would reinforce deterrence of a
potential aggressor]{.underline}. It is also possible, though less
likely, that more automation could occur via a highly risk-tolerant
nuclear-armed state that believes automated NC2 protocols would improve
its ability to manage escalation.

**[AT Human Dignity]{.underline}**

**Autonomous Weapons will be more ethical than humans because human
errors cause poor decisions**

**Arkin, 2008 -- the Mobile Robot Laboratory at Georgia Institute of
Technology** \[Ronald "Technical Report GIT-GVU-07-11 Governing Lethal
Behavior: Embedding Ethics in a Hybrid Deliberative/Reactive Robot
Architecture" <https://dl.acm.org/doi/abs/10.1145/1349822.1349839> Acc
12/27/20 TA\]

[It is not my belief that an unmanned system will be able to be
perfectly ethical in the battlefield, but I am convinced that they can
perform more ethically than human soldiers]{.underline} are capable of.
Unfortunately [the trends in human behavior in the battlefield regarding
adhering to legal and ethical requirements are questionable at
best]{.underline}. A recent report from the Surgeon General's Office
\[Surgeon General 06\] assessing the battlefield ethics of soldiers and
marines deployed in Operation Iraqi Freedom is disconcerting. The
following findings are taken directly from that report: 1.
[Approximately 10% of Soldiers]{.underline} and Marines [report
mistreating noncombatants]{.underline} (damaged/destroyed Iraqi property
when not necessary or hit/kicked a noncombatant when not necessary).
Soldiers that have high levels of anger, experience high levels of
combat or those who screened positive for a mental health problem were
nearly twice as likely to mistreat non-combatants as those who had low
levels of anger or combat or screened negative for a mental health
problem. 2. [Only 47% of Soldiers]{.underline} and 38% of Marines
[agreed that noncombatants should be treated with dignity]{.underline}
and respect. 3. [Well over a third of Soldiers]{.underline} and Marines
[reported torture should be]{.underline} allowed, whether to save the
life of a fellow Soldier or Marine or to obtain important information
about insurgents. 4. [17% of Soldiers]{.underline} and Marines agreed or
strongly [agreed that all noncombatants should be treated as
insurgents]{.underline}. 5. Just under 10% of soldiers and marines
reported that their unit modifies the ROE to accomplish the mission. 6.
45% of Soldiers and 60% of Marines did not agree that they would report
a fellow soldier/marine if he had injured or killed an innocent
noncombatant. 7. Only 43% of Soldiers and 30% of Marines agreed they
would report a unit member for unnecessarily damaging or destroying
private property. 8. [Less than half of Soldiers and Marines would
report a team member for an unethical behavior.]{.underline} 9. A third
of Marines and over a quarter of Soldiers did not agree that their NCOs
and Officers made it clear not to mistreat noncombatants. 10. Although
they reported receiving ethical training, 28% of Soldiers and 31% of
Marines reported facing ethical situations in which they did not know
how to respond. 11. Soldiers and Marines are more likely to report
engaging in the mistreatment of Iraqi noncombatants when they are angry,
and are twice as likely to engage in unethical behavior in the
battlefield than when they have low levels of anger. 12. [Combat
experience, particularly losing a team member, was related to an
increase in ethical violations]{.underline}.

**Even if AI is not Perfect, it is Better than human soldiers**

**Arkin, 2008 -- the Mobile Robot Laboratory at Georgia Institute of
Technology** \[Ronald "Technical Report GIT-GVU-07-11 Governing Lethal
Behavior: Embedding Ethics in a Hybrid Deliberative/Reactive Robot
Architecture" <https://dl.acm.org/doi/abs/10.1145/1349822.1349839> Acc
12/27/20 TA\]

Along other lines \[Sparrow 07\], points out several clear challenges to
the roboticist attempting to create a moral sense for a battlefield
robot: • "Controversy about right and wrong is endemic to ethics". o
Response: While that is true, we have reasonable guidance by the agreed
upon and negotiated Laws of War as well as the Rules of Engagement as a
means to constrain behavior when compared to ungoverned solutions for
autonomous robots. • "I suspect that [any decision structure that a
robot is capable of instantiating is still likely to leave open the
possibility that robots will act unethically]{.underline}." o [Response:
Agreed -- It is the goal of this work to create systems that can perform
better ethically than human soldiers do in the battlefield, albeit they
will still be imperfect. This challenge seems achievable]{.underline}.
Reaching perfection in almost anything in the real world, including
human behavior, seems beyond our grasp.

**No Solvency -- ALL weapons violate human dignity -- not just AI**

**Pop, 2018 -- diplomat for the Swiss Federal Department of Foreign
Affairs** \[Ariadna April 10 "Autonomous weapon systems: A threat to
human dignity?"
<https://blogs.icrc.org/law-and-policy/2018/04/10/autonomous-weapon-systems-a-threat-to-human-dignity/>
Acc 12/27/20 TA\]

Human dignity in anti-AWS arguments As we have seen above[, anti-AWS
arguments frequently employ Kantian terminology when invoking the
concept of human dignity. Unfortunately, however, in none of these
arguments do we get an account of how exactly human dignity is violated
by the employment of AWS. This is not satisfactory]{.underline}. Unless
it can be shown why the concept of human dignity mandates the
prohibition of AWS, such a prohibition is not justified. [Let us
therefore assume that human dignity stands for unconditional, intrinsic
value]{.underline}. Let us assume further that [this intrinsic value has
its source in our autonomy, understood as our ability to make
self-determined choices]{.underline}. What can plausibly follow from
such an understanding regarding the use of AWS? [To be sure, if we are
hurt or killed against our will, it severely impacts our ability to make
our own choices. Hence, to allow AWS to deliver force is certainly
incompatible with the moral value of autonomy. But how are AWS different
in this regard from regular weapons? After all, any type of weapon or
method of warfare, be it a remote-controlled long-distance missile, a
drone, an air strike or conventional ballistic weapon, is designed to
seriously harm human beings.]{.underline} What is it about AWS that
renders them particularly reprehensible from the point of view of human
dignity? I fail to see what the relevant argument could look like and
have also not found any satisfactory explanation in the literature. Note
that I am not saying that there are no morally questionable aspects in
the employment AWS, there certainly are. My point is simply that [if the
morally significant dimension is taken to be human dignity, and if human
dignity is understood as a value that has its source in our capacity for
self-determination, then the relevant incompatibility is not restricted
to a specific weapon system,]{.underline} but is shared by any type of
force that harms our agency.

**Human dignity arguments enforce anthropocentrism -- they assign humans
a higher position of hierarchy than weapons based on speciesism.**

**Pop, 2018 -- diplomat for the Swiss Federal Department of Foreign
Affairs** \[Ariadna April 10 "Autonomous weapon systems: A threat to
human dignity?"
<https://blogs.icrc.org/law-and-policy/2018/04/10/autonomous-weapon-systems-a-threat-to-human-dignity/>
Acc 12/27/20 TA\]

[Anti-AWS arguments are]{.underline} therefore [either obscure by
drawing unjustified inferences or, if the inferences can be justified,
they must be based on a different conception of human dignity than the
one that seems to be endorsed]{.underline}. Finally, note that endorsing
[a Waldron-style account of human dignity]{.underline} to make sense of
anti-AWS arguments is also not without its difficulties. To begin with,
it [would imply that it is also a violation of human dignity to be
killed by other entities that occupy a lower status in the hierarch of
beings, such as animals, bacteria, or even viruses. This strikes me as
counter-intuitive. Moreover, it would have to be clarified why AWS that
are endowed with artificial intelligence would necessarily occupy a
lower status than human beings]{.underline}. It is problematic to simply
stipulate that it is a matter of respect for the high-ranking status of
human beings that they do not get hurt or killed by non-humans. This
gives rise to the suspicion that [ultimately, it all boils down to a
form of speciesism: that in the hierarchy of being we simply consider
ourselves to be the most valuable form of existence]{.underline} and
demand to be treated accordingly, without bothering to explain why this
is supposed to be the case.

**"Human Dignity" is not theoretically coherent -- the aff manipulates
it for political reasons**

**Pop, 2018 -- diplomat for the Swiss Federal Department of Foreign
Affairs** \[Ariadna April 10 "Autonomous weapon systems: A threat to
human dignity?"
<https://blogs.icrc.org/law-and-policy/2018/04/10/autonomous-weapon-systems-a-threat-to-human-dignity/>
Acc 12/27/20 TA\]

It might well be that proponents of a pre-emptive ban of AWS presuppose
a very different understanding of human dignity which would make it much
more evident why such a ban would follow. But if that is the case, they
must make the relevant argument and spell out the specific understanding
of human dignity they presuppose. Otherwise, [the use of human dignity
is reduced to a rhetoric maneuver, an empty shell, which, depending on
the context and purpose of its use, has a very different theoretical
underpinning.]{.underline} As I see it, [this renders the employment of
human dignity not only extremely unattractive in trying to make
a]{.underline} specific legal or [political argument. It also renders
the concept vulnerable to manipulation and thereby undermines the
credibility of the debate in question]{.underline}. [Given the
importance of a serious legal and political engagement with the
possibilities and limitations of AWS, such an outcome should be avoided
at all cost. In order to advance the AWS debate it is therefore
preferable to refrain from using the concept of human dignity
altogether]{.underline}, at least as long as no consensus regarding its
proper meaning has emerged and the relevant assumptions remain
unarticulated.

**[Solvency Resps]{.underline}**

**1. A focus on Ethics distracts attention from other aspects of
Responsible Use, and leaves us vulnerable to Unethical states.**

**Stanley-Lockman, 2021 - Center for Security and Emerging Technology**
\[Zoe CSET Issue Brief August "Responsible and Ethical Military AI
Allies and Allied Perspectives"
https://cset.georgetown.edu/publication/responsible-and-ethical-military-ai/
Acc. 4/21/22 TA\]

[Allies with articulated views also translate their obligation to
protect into]{.underline} language on [military AI. This can be seen
in]{.underline} arguments for [the moral imperative to pursue AI-enabled
capability development to maintain freedom of action and protect from
adversaries whose uses of AI do not respect legal and ethical
obligations.]{.underline}190 In these views, [maintaining freedom of
action can also mean maintaining interoperability, or even developing AI
systems that help protect friendly forces, as most allies depend on
cooperation to fill capability gaps]{.underline}.191 Beyond operational
risks, [responsibility also means incorporating ways to minimize risks
in the international security environment.]{.underline} DOD has a role
to play off the battlefield in this regard as well, including [by
developing norms around arms control]{.underline}. Allied concerns about
diffusion and access, as embedded in the German international security
and AI governance agenda, as well as risks that the Dutch identify, make
this a compelling area for responsible AI cooperation between defense
ministries. In doing so, [the United States could find complementary
areas of interest with allies that see responsible military AI as
encompassing norms in the international environment]{.underline}. In
fact, this may be a palatable way to move debates beyond questions
exclusive to autonomous weapon systems. While autonomy in weapon systems
undoubtedly introduces important [questions for ethics]{.underline},
legality, and responsibility, the dominant attention it receives [tends
to overshadow other aspects of military AI. This not only includes
responsible AI implementation, but potentially even the responsibility
states have to defend against AI-enabled threats from less ethical
adversaries]{.underline}. The fact that most allies are still
transitioning from ethical questions wrapped up in autonomy in weapons
means that DOD can facilitate and complement their views on ethical and
technical dimensions of AI in non-lethal or non-autonomous systems. In
doing so, it could help steward the conversation toward other,
underrated aspects of ethical design, development, and deployment of
military technology.

**2. NATO cooperation on ethical principles ignore incoherence in
national viewpoints on AI -- principles will fail in implementation**

**Trabucco and Stanley-Lockman, 2022 -- prof of Political Science,
University of Copenhagen and prof of Defense and Strategic Studies,
Nanyang Technological University** \[Lena and Zoe, The Oxford Handbook
of AI Governance, March, "NATO's Role in Responsible AI Governance in
Military Affairs"
https://www.oxfordhandbooks.com/view/10.1093/oxfordhb/9780197579329.001.0001/oxfordhb-9780197579329-e-69
Acc 4/16/22 TA\]

Externally, [as AI-enabled autonomous systems enter the arsenals of more
technologically advanced countries, uncoordinated ethical frameworks
between Allies could pose operational risks.]{.underline} Without wider
alignment, AI systems will have "varying technical specifications based
on the legal and policy decisions made by individual governments when
answering the key questions."64 Further, although one motivation of
autonomous systems is the increased safety of military personnel by
removing them from dangerous situations, [the lack of alignment could
lead some Allies to perceive other capitals' deployments of unmanned
forces as a lack of commitment to put lives on the line, therein posing
credibility risks for Allies to assure one another]{.underline}.65 These
credibility risks can be mitigated by accountability and verification
standards and procedures that NATO can implement for multinational
operations, and efforts to institutionalize these procedures for AI are
underway.66 [While the NATO AI Strategy is expected to create a common
foundation for the Alliance's pursuit of AI, it is the implementation of
principles for safe, ethical, legal, and interoperable AI that will
reveal how coherent different national perspectives are]{.underline}.
[As of August 2021, only the United States and France have]{.underline}
[publicly issued their military AI strategies]{.underline}.67 [Other
allies]{.underline}, including Canada and the United Kingdom, [have
emerging views on responsible military AI, but little official
information about how they implement their ethical risk assessments is
publicly available]{.underline}.68 NATO's influence in the functioning
of joint operations and multinational military operations situates the
Alliance to coordinate between how Allies implement ethical principles
in their own national AI development. Specifically, NATO is
well-situated to advocate for transparency, accountability, and data
governance, which are also adoption factors that can translate into
operational benefits, among other values.69 For example, these factors
can promote coordination among Allies on ethical guidelines on the
development and use of AI, as this will be a necessary foundation in any
future joint operation that uses this technology. "The transatlantic
partnership must focus on coordinating these core principles and
systematic governance to ensure AI systems development aligns with the
rule of law and democracy. In particular, this must ensure answering
questions about human dignity, human control, and accountability ...
NATO remains the organization that can bring these two (U.S. and EU)
together and establishes the ethical bottom line."70 The issues of
transparency and accountability will define the scope of future
implementation. Many remaining questions and uncertainty will be
addressed in [NATO's forthcoming AI ethical principles
guidelines]{.underline}. But the guidelines adopted in 2021 [do not
address every ethical dilemma]{.underline}. Regarding accountability,
especially, likely [major questions will continue to affect the
Allianc]{.underline}e. As Assistant Secretary-General for Emerging
Security Challenges David van Weel recently clarified, [NATO will offer
a framework of responsible use for the Allies---but the question of
accountability for member states]{.underline}, as opposed to civilian
technology manufacturers for example, is one principle that [will not
have an easy solution]{.underline}.71

**3. No solvency - Human control is vague which prevents it from
establishing a norm.**

**Bode, 2021 - Professor of International Relations at the University of
Southern Denmark** \[Ingvild June 25, AutoNorms "Reflecting on the
Future Norms of Warfare"
https://www.autonorms.eu/reflecting-on-the-future-norms-of-warfare-2/
Acc. 5/27/22 TA\]

We may therefore continue to have two parallel processes shaping the
norms of war and conflict that do not necessarily overlap. Further[,
even if states agree on setting a legal norm defining a necessary
quality of human control, that legal norm is likely going to be
ambiguous in character, providing states with a significant amount of
leeway]{.underline}. States may therefore continue to engage in use of
force practices with AWS in much the same way as before the legal norm
was in place[. Norms that have emerged as part of practices
of]{.underline} designing and [using AWS therefore run the risk of
undercutting deliberative legal efforts]{.underline}. To counter these
dynamics, it is vital that such silent norm-making processes are closely
examined closely and publicly expressed. Autonomous weapon systems and
an emerging norm of human control The debate about autonomous weapons
systems poses fundamental questions to the extent to which the use of
force in conflict and warfare, as well as the very application of
international law, remains in human hands. At first glance, practically
all states parties addressing the Group of Governmental Experts (GGE) on
emerging technologies in the area of LAWS highlight the importance of
maintaining human control over the use of (lethal) force. The most
substantial outcome of the GGE yet, the Guiding Principles on LAWS,
includes a principle on human-machine interaction. We can therefore
observe the potential making of a new legal norm, if states proceed
towards a negotiation stage. This option has arguably gathered steam
after the ICRC's clear positioning in favour of new international law
around LAWS in May 2021. But [any consensus on what quality of human
control is appropriate is yet to emerge. Many states favour a long-term
view on human control as something that should be present throughout the
entire life cycle of a weapon system from design to operation. Further,
the US and Australia have argued that autonomous features can enhance
human control in specific use of force situations by "effectuating the
intent of commanders".]{.underline} This perspective treats AI as a
straightforward extension of human agency. [Such thinking downplays the
complexity of human-machine interaction and how this challenges the
decision-making capacity of humans operatin]{.underline}g (or working in
teams) with AI-driven weapon systems. Indeed, we must consider the
extent to which the technology itself, having been designed and
conceptualised in a certain way, can itself become a change agent for
shaping (new) legal norms.

**4. Russia and China take out solvency -- they will not model plan.**

**Thornton, 2019 - Senior Lecturer in the Centre for Defence Education
Research and Analysis, King's College** \[Rod, "One to ponder: the UK's
ethical stance on the use of Artificial Intelligence in weapons systems
https://defenceindepth.co/2019/06/17/one-to-ponder-the-uks-ethical-stance-on-the-use-of-artificial-intelligence-in-weapons-systems/
Acc 4/16/22 TA\]

With such sentiments abroad, it is no surprise then that the official UK
line is that none of its offensive weapons systems will be capable of
attacking targets without some degree of human control. As one Ministry
of Defence spokesperson put it: 'The United Kingdom does not possess
fully autonomous weapon systems and has no intention of developing
them'. This is a laudable but debateable statement. Surely, the UK will
be developing purely defensive AI system that are fully autonomous --
anti-missile missile systems, for instance -- where speed of reaction
cannot be left to dithering humans. In a broader sense, though, this is
a declared restriction that could put UK forces at a serious
disadvantage on future battlefields when it comes to the employment of
LAWS. [It cannot really be imagined that the likes of China and Russia,
as they develop their AI systems, will feel limited by ethical
sentiment. Their view will be that they cannot afford to be. They both
see themselves as weaker militarily than the combined forces of
NATO]{.underline} and its partner countries [and, as such, have
doctrinally declared that they will be seeking out any asymmetric
advantage they can. If these Western powers]{.underline} -- including
the UK -- [want to self-restrict their use of LAWS]{.underline}, for
instance, then [this will be seen by Beijing and Moscow as a weakness to
be exploited in an asymmetric sense. There may then come a future
scenario where UK force elements, facing adversaries with different
ethical standards and free to deploy their 'killer robots', would be
unable to reciprocate with their own. They could be left exposed;
fighting with one arm behind their back.]{.underline}

**5. No solvency - The US cannot lead AI collaboration -- we are not the
AI leader, we lack the AI workforce, and funding is insufficient**

**Lawrence and Cordey, 2020 -- researchers for The Cyber Project at the
Belfer Center for Science and International Affairs** \[Christie and
Sean, August, The Cyber Project Paper "The Case for Increased
Transatlantic Cooperation on Artificial Intelligence Edited by Lauren
Zabierek and Julia Voo
https://www.belfercenter.org/sites/default/files/2020-08/TransatlanticAI.pdf
Acc. 4/21/22 TA\]

[Despite the momentum within the US federal government to prioritize AI
and align efforts across the interagency to maintain America's AI
leadership, there are]{.underline} three [key challenges that imperil
the ability of the US to achieve its strategic goals.]{.underline}
Private Sector Landscape: The US leads in many metrics of AI innovation.
Notably, the US has the greatest number of AI-related startups and the
largest amount of venture capital and private equity funding for AI.151
However, as already described in The Case for Transatlantic Cooperation,
[China's AI-related private industry and private funding, combined with
government funding, a lack of regulation, and widespread economic
espionage constitute threats to America's edge.152 The decentralized US
approach, uncertainty across the US private sector on how to balance
sometimes competing economic and ethical considerations,]{.underline}
and the weak funding and information sharing links between government,
academia, and industry could also hinder US efforts at AI leadership.153
[Workforce & AI Talent]{.underline}: As already mentioned, since 2016
[the US government has recognized that it needs to build up its domestic
workforce of AI talent as the demand exceeds the supply]{.underline}.
According to Acting Director of the JAIC Nand Mulchandani, a significant
amount of AI talent chooses to work in the private sector. As a result,
the US Government's focus is on leveraging commercial AI offerings
instead of only focusing on building internal AI talent.154 Recognizing
the growing lucrative nature of the private tech industry, students at
American universities are increasingly demanding classwork focused on
AI-related fields like computer science. However, universities are
unable to match this demand with an appropriate expansion of
tenure-track faculty in the same areas.155 US immigration policy also
threatens America's AI edge. Countries like Canada, the UK, and China
have reformed their visa processes to attract foreign talent focused on
AI research.156 The US may lose its attractiveness to foreign
researchers and AI experts if it does not similarly ease immigration
procedures including vis-à-vis China, whose nationals have historically
been part of America's science and technology innovation workforce.157
[AI Funding: Although the Administration has pledged to
increase]{.underline} (non-defense and defense) [AI-related
spending]{.underline} and absolute AI R&D budget numbers have increased,
[there are concerns that these numbers may not accurately reflect
development]{.underline}. First, as AI-related expenditures have
increased, the budget for all government R&D has decreased.158 For
example, the President's budget request for cuts in R&D at NSF, NIH,
DOE, and other agencies, would force these government entities to
prioritize AI R&D to the detriment of other, potentially equally useful
R&D.159 Second, without full transparency about the procedures
undertaken to re-classify projects as AI-related, it is not possible to
fully credit the supposed increase in AI-related R&D to new AI projects.
One analysis by Bloomberg Government of the Pentagon's FY2020 budget
found that approximately 27% of the legacy AI-related activities had not
included any AI components or descriptors in the previous year
budget.160,161 Observers have suggested that the DoD was partaking in
"AI-washing," or exaggerating the increase in its AI-related R&D to meet
government imposed objectives. 162 The US government will need to be
careful that initiatives to enhance AI innovation do not foster a
zero-sum competition between AI and other S&T research but instead
foster genuine innovation.

**6. No solvency -- most AI technology is available from private
companies -- this guarantees circumvention.**

**Horowitz, 2018 -- prof of political science at the Univ of
Pennsylvania** \[Michael C., May, Texas National Security Review, The
Scholar "Artificial Intelligence, International Competition, and the
Balance of Power"
<https://tnsr.org/2018/05/artificial-intelligence-international-competition-and-the-balance-of-power/>
Acc 5/14/22 TA\]

What countries benefit from AI will depend in part on where
militarily-relevant innovations come from. [Non-military institutions,
such as private companies]{.underline} and academic departments, are
pushing the boundaries of what is possible in the [realm of artificial
intelligence.]{.underline} While some AI and robotics companies, such as
Boston Dynamics, receive military research and development funding,
others, such as DeepMind, do not, and actively reject engaging with
military organizations.12 Unlike stealth technology, which has a
fundamentally military purpose, artificial intelligence has uses as
varied as shopping, agriculture, and stock trading. [If
commercially-driven AI continues to fuel innovation, and the types of
algorithms militaries might one day use are closely related to civilian
applications, advances in AI are likely to diffuse more rapidly to
militaries around the world.]{.underline} AI competition could feature
actors across the globe developing AI capabilities, much like late-
19th-century competition in steel and chemicals. [The potential for
diffusion would make it more difficult to maintain "first-mover
advantages" in applications of narrow AI]{.underline}. This could change
the balance of power, narrowing the gap in military capabilities not
only between the United States and China but between others as well.

**7. It is impossible to ban autonomous AI systems -- there is no agreed
upon definition of lethal autonomy.**

**Horowitz, 2019 - Professor of Political Science, University of
Pennsylvania** \[Michael C. May 2"When Speed Kills: Autonomous Weapon
Systems, Deterrence, and Stability" https://ssrn.com/abstract=3348356
Acc 12/27/20 TA\]

The arms control dilemma, in this case, is that the more the possession
of LAWS improves the ability of a military to fight and win the nation's
wars, the harder it will become for the international community to
effectively regulate them. That does not make regulation impossible, to
be clear. But the dynamic described above whereby the [dual-use
character of AI means some types of LAWS could be accessible to many
militaries, not just major powers]{.underline}, [means a broader set of
states would have something to lose]{.underline}, from a capabilities
perspective, through regulation. It also gives a broader number of
states potential interest in regulation. [Uncertainty surrounding the
definition of a lethal autonomous weapon system,]{.underline} though not
a central focus of this paper, [could also make traditional arms control
more difficult.]{.underline} From nuclear treaties like the Limited Test
Ban Treaty to the Ottawa Convention, successful arms control agreements
have generally tackled discrete technologies. [The]{.underline} breadth
of the category of AI and [difficulties in defining what constitutes a
lethal autonomous weapon system at the margins are making reaching
agreement on a definition of lethal autonomous weapon system challenging
at the international level.]{.underline}

**8. Technological Determinism takes out solvency -- NATO is too
committed to competition for innovation.**

**Trabucco and Stanley-Lockman, 2022 -- prof of Political Science,
University of Copenhagen and prof of Defense and Strategic Studies,
Nanyang Technological University** \[Lena and Zoe, The Oxford Handbook
of AI Governance, March, "NATO's Role in Responsible AI Governance in
Military Affairs"
https://www.oxfordhandbooks.com/view/10.1093/oxfordhb/9780197579329.001.0001/oxfordhb-9780197579329-e-69
Acc 4/16/22 TA\]

A number of NATO entities carry out strategic and policy planning,
recognizing the importance of policy alignment to sustain political
strength and military effectiveness. As relates to S&T, allies'
representations to NATO, defense ministries, and policy entrepreneurs
from the relevant entities summarized in Table 69.1 support and
negotiate how the Alliance approaches EDTs[. NATO's strategic
documentation and forward-looking policy analysis incorporates hints of
technological determinism]{.underline}, including noting how
technological change inevitably shapes the future strategic and
operating environment[. Further, the connections between technology and
competitive advantage over adversaries and competitors are embodied in
the Alliance's desire to maintain its "technological edge" as the
"foundation upon which NATO's ability to deter and defend against
potential threats ultimately rests."]{.underline}50 This places
technology squarely within NATO's core purpose of deterrence and
defense---and while this signals NATO's express commitment to technology
through these channels, [this reliance on technology also obscures
whether NATO's governance capacity will be adaptive, anticipatory, or
participatory. This position of technological determinism may result in
more limitations for AI governance.]{.underline}

**9. No solvency -- it is easy to circumvent Human Control Norm --
weapons can shift easily from autonomous to non-autonomous.**

**Kania 2018 - fellow with the Technology and National Security Program
at the Center for a New American Security** \[Elsa B. "China's Strategic
Ambiguity and Shifting Approach to Lethal Autonomous Weapons Systems"
April 17, 2018
<https://www.lawfareblog.com/chinas-strategic-ambiguity-and-shifting-approach-lethal-autonomous-weapons-systems>
Acc 12/27/20 TA\]

China argues that [lethal autonomous weapons are characterized
by]{.underline}: [lethality; autonomy, "which means absence of human
intervention]{.underline} and control during the entire process of
executing a task" "impossibility for termination" such that "once
started there is no way to terminate the device"; ["indiscriminate
effect,"]{.underline} in that it will "execute the task of killing and
maiming regardless of conditions, scenarios and targets"; [and
"evolution,"]{.underline} "through interaction with the environment the
device can learn autonomously, expand its functions and capabilities in
a way exceeding human expectations" (emphasis added throughout).
[Banning weapons systems with those characteristics could]{.underline}
be a symbol, while [implicitly legitimizing the development of
semi-autonomous or even fully autonomous systems that do not possess
such qualities. By such a standard, a weapons system that operates with
a high degree of autonomy but involves even limited human involvement,
with the capability for distinction between legitimate and illegitimate
targets, would not technically be a LAWS]{.underline}, nor would a
system with a failsafe to allow for shutdown in case of malfunction.
Interestingly, this particular definition is much more stringent than
the Chinese military's own definition of the concept of "artificial
intelligence weapon." According to the dictionary of People's Liberation
Army Military Terminology, an artificially intelligent weapon is "a
weapon that utilizes AI to automatically \[\] pursue, distinguish, and
destroy enemy targets; often composed of information collection and
management systems, knowledge base systems, assistance to decision
systems, mission implementation systems, etc.," such as military
robotics. Because this definition dates back to 2011, the Chinese
military's thinking has evolved as technology has advanced. It is
important, therefore, to consider that there may be daylight between
China's diplomatic efforts on autonomous weapons and the military's
approach.

**10. No solvency -- it is impossible to Verify human control**

**Horowitz, 2019 - Professor of Political Science, University of
Pennsylvania** \[Michael C. May 2"When Speed Kills: Autonomous Weapon
Systems, Deterrence, and Stability" <https://ssrn.com/abstract=3348356>
Acc 12/27/20 TA\]

Jervis argues that arms races occur due to a security dilemma when
states have the ability to measure each other's capabilities, but not
their intentions.57 [LAWS could be especially risky in this regard
because of potential opacity about capabilities]{.underline}, in
addition to the "normal" opacity that exists about intentions. First,
[it will be extremely difficult for states to credibly demonstrate
autonomous weapon capabilities. The difference between a
remotely-piloted system and an autonomous system is software, not
hardware, meaning verification that a given country is operating an
autonomous system at all would be difficult]{.underline}. Second,
uncertainty about the technological trajectory of machine learning and
specific military applications means that countries might have
significant uncertainty about other countries' capabilities. Thus,
countries might invest a lot in artificial intelligence applications to
military systems due to fear of what others are developing.

**[\--Extend - Trade Off]{.underline}**

**Focusing on autonomy in AI weapons trades off with other AI Issues
like cyber security and reliability**

**Stanley-Lockman, 2021 - Center for Security and Emerging Technology**
\[Zoe CSET Issue Brief August "Responsible and Ethical Military AI
Allies and Allied Perspectives"
https://cset.georgetown.edu/publication/responsible-and-ethical-military-ai/
Acc. 4/21/22 TA\]

Overall, the absence of concrete German, European, or transatlantic
military AI frameworks means that industry has a different starting
point when determining the most appropriate framework for responsible
military AI. There is no immediately available information on
government-guided implementation and requirements validation. [With the
government more focused on arms control, the German interest in a
whole-of-lifecycle approach to AI governance may turn into a delegation
of labor]{.underline}---with government looking at responsible use and
diffusion and industry focusing on development. [The mix here of both
self-regulation and waiting for multilateral guidance indicates a clear
German interest in military AI governance and ethics---even if more
narrow questions]{.underline} around autonomy in weapon systems
[continues to monopolize public debate. This focus on autonomy in
weapons is an important factor in assessing the degree of coherence
between U.S. and German approaches to military AI, as it risks
overwhelming less controversial issues.]{.underline} Beyond FCAS, [the
limited bandwidth for AI ethics beyond the tip of the spear could also
mean the countries have different bases for how they develop and procure
defensive systems and countermeasures. Further, it is not yet clear how
the German military government is looking at AI ethics that are separate
from autonomy in weapons. Without this separation, it could be more
difficult to coalesce on views like the importance of cybersecurity and
operator training to building trustworthy, reliable AI for
defense.]{.underline}

**[\--Extend - Political Obstacles]{.underline}**

**Political obstacles block NATO cooperation and leadership on
Responsible AI use.**

**Trabucco and Stanley-Lockman, 2022 -- prof of Political Science,
University of Copenhagen and prof of Defense and Strategic Studies,
Nanyang Technological University** \[Lena and Zoe, The Oxford Handbook
of AI Governance, March, "NATO's Role in Responsible AI Governance in
Military Affairs"
https://www.oxfordhandbooks.com/view/10.1093/oxfordhb/9780197579329.001.0001/oxfordhb-9780197579329-e-69
Acc 4/16/22 TA\]

On that note [NATO]{.underline}, or any other international
organization, [is not exempt from]{.underline} these [political hurdles.
As EDTs increasingly become a focal point in the geopolitical space, any
approach of AI governance in the international security environment will
have global political undertones. This will undoubtedly be a significant
hurdle for NATO]{.underline} as it balances responsible AI development
and Allied coordination and cooperation in a changing geopolitical
landscape. And certainly, [the political realities may well represent
the greatest challenge and disincentivize NATO to emerge as a leader in
responsible military AI.]{.underline} Nevertheless, the three pillars
indicate that NATO is an institution with considerable opportunity to
shape responsible AI governance. More specifically, this entails urging
and facilitating Allied standards and policies to establish foundations
for emerging military technology built on informed and ethical
principles and enhance the international security environment.

**[\--Extend - Russia and China]{.underline}**

**The plan will not solve Chinese AI -- they are not concerned about
ethical issues.**

**Heikkilä, 2021 - Politico's AI Correspondent in London** \[Melissa,
Politico March 31 "AI Decoded: NATO on AI warfare --- AI treaty
consultation --- Unions call for more AI protections"
https://www.politico.eu/newsletter/ai-decoded/politico-ai-decoded-nato-on-ai-warfare-ai-treaty-consultation-unions-call-for-more-ai-protections/
Acc 4/9/22 TA\]

The China question: [Beijing supports banning the use of these weapons,
but not their development or production. The U.S. and NATO have argued
that they need to keep autonomous lethal weapons in their arsenals as
deterrence and defense against "malign" actors]{.underline} --- all very
reminiscent of the Cold War and nuclear politics. [Van Weel, the
assistant secretary-general, said: "China has a seamless flow of these
new technologies from the private sector into the defense realm. I'm ...
not sure that they're having the same debates on principles of
responsible use or they're definitely not applying our democratic values
to these technologies]{.underline}." [Coyle doesn't trust Beijing
either, calling it an "insincere partner in many of their multilateral
agreements]{.underline}."

**Chinese motives on ethics in AI are unknown due to secrecy.**

**Allen, 2022 - director of the AI Governance Project at the CSIS**
\[Gregory, May 20, "One Key Challenge for Diplomacy on AI: China's
Military Does Not Want to Talk"
[https://www.csis.org/analysis/one-key-challenge-diplomacy-ai-chinas-military-does-not-want-talk
Acc
6/6/22](https://www.csis.org/analysis/one-key-challenge-diplomacy-ai-chinas-military-does-not-want-talk%20Acc%206/6/22)
TA\]

[The truth]{.underline}, unfortunately, [is that]{.underline}---despite
the United States' efforts at transparency and requests for
dialogue---[the United States knows very little about how seriously the
Chinese military considers ethics in its use of AI, how robust Chinese
test and evaluation processes are, and what governance structures and
procedures exist to reduce the risk of military AI accidents. That
secrecy in and of itself is a source of risk to international peace and
security.]{.underline} But, then again, [what incentive does China have
to substantively engage? The United States is already providing a great
deal of transparency around its own risk reduction effor]{.underline}ts,
and China is already garnering many reputational benefits from calling
for dialogue without any of the costs of substantively participating.
[Perhaps]{.underline} [neither]{.underline} [the U.S.
government]{.underline} [nor the Chinese scholarly community can succeed
in persuading the PLA that it is in everyone's best interest for this
dialogue to occur]{.underline}. At the very least, however, it should be
clear to the international community that China is the one refusing to
talk.

**[\--Extend - No Definition]{.underline}**

**States the utilize Autonomous Weapons have an incentive to disagree
over definitions**

**Thornton, 2019 - Senior Lecturer in the Centre for Defence Education
Research and Analysis, King's College** \[Rod, "One to ponder: the UK's
ethical stance on the use of Artificial Intelligence in weapons systems
https://defenceindepth.co/2019/06/17/one-to-ponder-the-uks-ethical-stance-on-the-use-of-artificial-intelligence-in-weapons-systems/
Acc 4/16/22 TA\]

Given its declared position, it might seem logical for the UK to push
for an international ban on the use of LAWS. Trying to level the playing
field so that no other state possessed them would seemingly work to the
UK's advantage. A ban is also the favoured UN option. UN Secretary
General António Guterres has, for instance, described LAWS as 'morally
repugnant'. Within the UN, however, the UK is part of a group of states
(alongside Australia, Israel, Russia and US) that has collectively
stated that currently they do not want to see any regulation that
forbids the use of LAWS. To explain the UK's position, an MOD
spokesperson said that, 'We believe a pre-emptive ban is premature as
[there is still no international agreement on the characteristics of
lethal autonomous weapons systems'. We are thus back to the thorny
problem of definitions. If we do not know what something is then how can
it be banned?]{.underline} The question here, though, is why is the UK
trying to prevent a ban on a weapon it has 'no intention' of developing
itself? This does not look very ethical or, indeed, sensible. It seems
to be giving licence to potential adversaries to continue with their own
development of LAWS while the UK sits on its AI hands. Whatever the UK's
position, it seems that LAWS will prove impossible to ban anyway.
Firstly, [because the world's major states will be seeing the benefits
of LAWS there will probably (and maybe conveniently?) never be an
internationally agreed definition on them]{.underline}, which would then
allow any ban to accrue. Secondly, the technology that underpins any
'killer robot' will come to be developed anyway in the civilian sector
-- with systems designed, for instance, to deliver parcels or to tackle
forest fires. Any military organisation could simply buy such systems
off the shelf and convert them readily into LAWS. The genie will thus be
out of the bottle on LAWS fairly soon anyway and can never be put back
in. It will therefore, and unfortunately, be very hard for the UK to
maintain a credible stance as a 'pioneer in ethical AI'.

**No solvency -- it is impossible to ban LAWs because there is no solid
definition of LAWs.**

**Lewis, 2020 - Research Director of the Harvard Law School Program on
International Law and Armed Conflict** \[Dustin September 28, "An
Enduring Impasse on Autonomous Weapons"
<https://www.justsecurity.org/72610/an-enduring-impasse-on-autonomous-weapons/>
Acc 12/27/20 TA\]

The Current Impasse [The current impasse on autonomous weapons might be
traced to at least two factors. The first concerns definitions. There
are widely differing conceptions of autonomous weapons and their
technical characteristics]{.underline} (at least for purposes of the
GGE). [And there is also a divergence on sequencing, especially whether
States must first agree on minimal definitional elements before taking
more concrete steps]{.underline} or, alternatively, whether countries
can develop a political declaration (or even a legal instrument) without
first establishing agreement on what specific technologies are of
concern. The second factor is a significant difference of views on what
international law already permits, mandates, and prohibits in practice
and corresponding positions on whether or not the law is satisfactory.

**[1NC - Military Technology DA]{.underline}**

**NATO's current AI policy focuses on competition, innovation and speed,
not ethics regulations**

**Lanata, 2020 - General, French Air Force Supreme Allied Commander
Transformation** \[Andre, NDC Research Paper No.15 -- December forward
to ""NATO-Mation": Strategies for Leading in the Age of Artificial
Intelligence" by Andrea Gilli
https://www.ndc.nato.int/news/news.php?icode=1514 Acc 4/21/22 TA\]

Many of its proposals closely match [Allied Command Transformation (ACT)
efforts to lead the transformation of]{.underline} NATO's military
instrument of power, and principally ACT's own contribution to the
establishment of a [NATO AI strategy]{.underline}. This strategy [should
aim at: leveraging AI to out-think, out-perform, and out-pace our
potential adversaries]{.underline}; improving decision making at every
NATO echelon; optimizing performance of priority NATO capabilities; and
driving agility and continuous improvement. Such a strategy requires
efforts in various directions. We should start by sourcing commercially
mature AI applications to demonstrate value and create early momentum
for the adoption of AI. We must also improve our ability to pro-actively
shape the AI technology investment landscape and development of the
defence specificities. Moreover[, NATO has to rethink its operating
model and organization with a prime focus on speed.]{.underline}
Finally, NATO will need to scale AI with a technology and talent
foundation, meaning we need to establish a NATO Infostructure. A key
condition to implement successfully these lines of effort is truly
embracing innovation and agility. [ACT, as one of the leading agents for
innovation at NATO, has taken bold steps in this direction in the last
two years. We established an Innovation Branch, a place where our
innovators are "protected and nurtured]{.underline}", to use the words
of the "NATO-mation" study. As part of it, our Innovation Hub has been
equipped with an "Open Innovation Lab" capability, implementing
state-of-the-art agile methodologies ("DevSecOps"4). Our Innovation
motto, "Start Small, Think Big, and Scale Up towards Full Scale
Continuous Innovation", is a concrete way to phrase the "3S" strategy
approach proposed by the "NATO-mation" study. The Lab is notably
exploring currently the potential of AI to mine and analyze open source
datasets to provide Intelligence communities easily-accessible,
supplementary data facilitating target area risk assessment during
mission planning and real time operations. We need to build upon these
successful experimentations and adopt the agile approach at the NATO
Enterprise level. This is the intent of the proposals put forward with
our sister command, Allied Command Operations, and of the partnership we
established this summer with the NATO Communications and Information
Agency (NCIA) to work towards the adoption of DevSecOps in NATO at
scale. Scaling up is also about appropriately funding innovation, an
imperative that we have raised on numerous occasions and that I will
continue to advocate with passion. On another note, ACT strives to
cultivate an innovative workforce through our regular "i3" (initiate,
innovate, imagine) events, and bring fresh perspective to NATO through
our young disruptor forum, Hackatons and Innovation Challenges. We also
explore the potential of emerging technologies through Disruptive
Technology Assessment Games in order to help build concretely common
understanding among the Allies, and address the interoperability issues
upstream via our TIDE Sprint events. We have established a pool of
expertise on data within ACT, notably missioned to advocate for NATO to
become a Data Driven Organisation. Last but not least, we support NATO
HQ to progress on the key ethical questions raised by the adoption of AI
for military purposes. On this topic, I am convinced that we will
identify these issues as we progress -- we are just at the beginning of
our journey. [My message to our political leaders is that the ethical
reflections are essential but must not slow us down in the exploration
of the potential of this technology and in our R&D efforts]{.underline}.
This study comes in handy to make our case on the necessity to continue
these efforts!

**Ethical limits undermine military readiness and deterrence -- they
will give a strategic edge to other nations.**

**Wilner, 2018 - Professor of International Affairs, Carleton
University** \[Alex "Artificial Intelligence and Deterrence: Science,
Theory and Practice"
<https://www.sto.nato.int/publications/STO%20Meeting%20Proceedings/STO-MP-SAS-141/MP-SAS-141-14.pdf>
Acc 6/4/22 TA\]

And finally, [ethical]{.underline} and legal l[imitations on how AI is
used in battle may dictate how some countries behave and others respond.
While some states, notably the United States and several European
allies, are openly against providing AI with the right or the means to
kill individuals without human intervention]{.underline} -- French
President Emanuel Macron explained, for instance, while promoting his
country's new AI innovation strategy in 2018, that he was "dead against"
the idea48 -- [other countries appear far less concerned.]{.underline}
[China, Russia,]{.underline} Israel, and others, for example, [may be
more willing to delegate decisions]{.underline} -- including those that
result in human death -- [to Artificial Intelligence. Under certain
conditions, doing so may provide these countries with a tactical,
strategic, or coercive advantage over those inclined to keep humans in
or on the loop. It may likewise provide these countries with a means to
counter-coerce, influence, or otherwise manipulate countries that are
more constrained and refrained in the way they use their AI in
battle.]{.underline}

**Human control undermines military AI development -- autonomy is the
key to improving speed and communications.**

**Sauer, 2021 - Senior Research Fellow at Bundeswehr University**
\[Frank serves on the International Panel on the Regulation of
Autonomous Weapons IRRC No. 913 March "Stepping back from the brink: Why
multilateral regulation of autonomy in weapons systems is difficult, yet
imperative and feasible"
https://international-review.icrc.org/articles/stepping-back-from-brink-regulation-of-autonomous-weapons-systems-913
Acc 4/5/22 TA\]

The second reason why regulating autonomy in weapons systems is
difficult is the enormous military significance ascribed to it. This
pertains to the five permanent members of the UN Security Council, but
also to other countries with technologically advanced militaries such
as, to give but two examples, Israel and Australia. The hurdle itself is
not new, of course. It is observable in other regulatory processes of
the recent past, such as the ones on landmines, cluster munitions and
blinding laser weapons, with the latter being achieved within the CCW
framework.30 However, blinding lasers always represented an exotic niche
capability that States could forego without great perceived military
costs. Landmines and cluster munitions, too, had specific fields of use
and were at least partly substitutable. This is not the case with
[weapon autonomy]{.underline}. Its impact [is perceived to be
game-changing for militaries in at least two domains of major
significance. First, weapon autonomy promises a whole range of
operational and strategic advantages by rendering constant control and
communication links obsolete. The militarily beneficial effects of this
innovation, proponents argue, are manifold. It allows for a new level of
force multiplication]{.underline} (with a single human operating
several, dozens or hundreds of systems at once), [creates the
possibility of "swarming" (opening up new possibilities for overwhelming
the enemy and evading counter-fire]{.underline}),31 reduces personnel
costs [and increases a system\'s stealth in the electromagnetic
spectrum]{.underline} (offering insurance against communications
disruption or hijacking). Most importantly, however[, it removes the
inevitable delay between a remote human operator\'s command and the
system\'s response. Swifter reaction times generate a key tactical
advantage over a]{.underline} remotely controlled and thus
[slower-reacting adversarial system]{.underline}. In fact, the promise
of gaining the upper hand by allowing for the completion of the
targeting cycle at machine speed is arguably the main motivation behind
increasing weapon autonomy.32 [Second, weapon autonomy promises to help
prevent some of the atrocities of war and render warfare more humane.
Since machines know no fear, stress or fatigue and are devoid of
negative human emotions, they never panic, overreact or seek
revenge]{.underline}, it is argued. [Since they lack a self-preservation
instinct, they can always delay returning fire. They supposedly allow
not only for greater restraint but also]{.underline} -- eventually, when
technology permits -- [for better discrimination between civilians and
combatants]{.underline}, thus resulting in the potential to apply
military force in stricter accordance with the rules of international
humanitarian law (IHL). [This would add up to an overall ethical benefit
-- in a utilitarian sense]{.underline}.33 In sum, the perceived
transformative potential of weapon autonomy and the quantity and quality
of military benefits ascribed to it render it more significant when
compared to specific weapon categories, such as landmines or cluster
munitions, that have been subject to humanitarian disarmament in the
recent past.

**Technological superiority is the key internal link to military success
-- AI enables every level of the battlefield. Without AI superiority, we
cannot deter or defeat adversaries.**

**Schmidt and Work, 2021 -- Chairs of The National Security Commission
on Artificial Intelligence** \[Eric and Robert, "National Security
Commission on Artificial Intelligence Final Report Executive Summary"
<https://www.nscai.gov/2021-final-report/> Acc 6/7/22 TA\]

Technology so ubiquitous in other facets of society will have an
equivalent impact on international competition and conflict.4 [We must
adopt AI to change the way we defend America, deter
adversaries]{.underline}, use intelligence to make sense of the world,
[and fight and win wars]{.underline}. The men and women who protect the
United States must be able to leverage the AI and associated
technologies that can help them accomplish their missions as quickly and
safely as possible. AI is the quintessential "dual use" technology---it
can be used for civilian and military purposes. The AI promise---that a
machine can perceive, decide, and act more quickly, in a more complex
environment, with more accuracy than a human---represents a competitive
advantage in any field. It will be employed for military ends, by
governments and non-state groups. [We can expect the large-scale
proliferation of AI-enabled capabilities.]{.underline} Many national
security applications of AI will require only modest resources and good,
but not great, expertise to use. AI algorithms are often accessible. The
hardware is "off-the-shelf" and in most cases generally available to
consumers (as with graphics processing units, for example). "Deepfake"
capabilities can be easily downloaded and used by anyone.5 AI enabled
tools and mutating malware are in the hands of hackers.6 Cheap, lethal
drones will be common. Azerbaijan's use of Turkish drones and Israeli
loitering munitions in combat against Armenia in October 2020 confirmed
that autonomous military capabilities are spreading.7 Many states are
watching and learning from these experiences. [The likelihood of
reckless or unethical uses of AI-enabled technologies by rogue
states]{.underline}, criminals, or terrorists [is
increasing.]{.underline} [AI-enabled capabilities will be tools of first
resort]{.underline} [in a new era of conflict. State and non-state
actors determined to challenge the United States,]{.underline} but avoid
direct military confrontation, [will use AI to amplify existing
tools]{.underline} and develop new ones. Adversaries are exploiting our
digital openness through AI-accelerated information operations and cyber
attacks. Adtech will become natsec-tech as adversaries recognize what
advertising and technology firms have recognized for years: that machine
learning is a powerful tool for harvesting and analyzing data and
targeting activities. Using espionage and publicly available data,
adversaries will gather information and use AI to identify
vulnerabilities in individuals, society, and critical infrastructure.
They will model how best to manipulate behavior, and then act. AI will
transform all aspects of military affairs. [AI applications will help
militaries prepare, sense and understand, decide, and execute faster and
more efficiently.]{.underline} Numerous weapon systems will leverage one
or more AI technologies[. AI systems will generate options for
commanders and create battle networks connecting systems across all
domains. It will transform logistics, procurement, training, and the
design and development of new hardware]{.underline}. Adopting AI will
demand the development of new tactics and operational concepts. In the
future, warfare will pit algorithm against algorithm. [The sources of
battlefield advantage will shift from traditional factors like force
size and levels of armaments to factors like superior data collection
and assimilation, connectivity, computing power, algorithms, and system
security.]{.underline} Competitors are actively developing AI concepts
and technologies for military use. [Russia has plans to automate a
substantial portion of its military systems.]{.underline}8 It has
irresponsibly deployed autonomous systems in Syria for testing on the
battlefield.9 [China sees AI as the path to offset U.S. conventional
military superiority]{.underline} by "leapfrogging" to a new generation
of technology. Its military has embraced "intelligentized
war"----investing, for example, in swarming drones to contest U.S. naval
supremacy.10 China's military leaders talk openly about using AI systems
for "reconnaissance, electromagnetic countermeasures and coordinated
firepower strikes."11 China is testing and training AI algorithms in
military games designed around real-world scenarios. As these
authoritarian states field new AI enabled military systems, we are
concerned that [they will not be constrained by the same]{.underline}
rigorous testing and [ethical code that guide the U.S.
military]{.underline}. AI will revolutionize the practice of
intelligence. There may be no national security function better suited
for AI adoption than intelligence tradecraft and analysis. Machines will
sift troves of data amassed from all sources, locate critical
information, translate languages, fuse data sets from different domains,
identify correlations and connections, redirect assets, and inform
analysts and decision-makers. To protect the American people, perhaps
the most urgent and compelling reason to accelerate the use of AI for
national security is the possibility that more advanced machine analysis
could find and connect the dots before the next attack, when human
analysis alone may not see the full picture as clearly. [Defending
against AI-capable adversaries without employing AI is an invitation to
disaster. AI will compress decision time frames]{.underline} from
minutes to seconds, [expand the scale of attacks, and demand responses
that will tax the limits of human cognition. Human operators will not be
able to defend against AI-enabled cyber]{.underline} or disinformation
[attacks, drone swarms, or missile attacks without the assistance of
AI-enabled machines]{.underline}. The best human operator cannot defend
against multiple machines making thousands of maneuvers per second
potentially moving at hypersonic speeds and orchestrated by AI across
domains. Humans cannot be everywhere at once, but software can.

**Declining military readiness invites aggression and causes war.
Revisionist powers will exploit rapid technological change.**

**O'Hanlon and Miller 2019 - director of research in Foreign Policy at
the Brookings** **and former Under Secretary of Defense for Policy**
\[Michael and James, Dec 2 \"Why we need a more modern and ready
military, not a larger one,\"
[https://www.brookings.edu/blog/order-from-chaos/2019/10/04/why-we-need-a-more-modern-and-ready-military-not-a-larger-one/](https://www.brookings.edu/blog/order-from-chaos/2019/10/04/why-we-need-a-more-modern-and-ready-military-not-a-larger-one/%5d//GJ)
Acc. 2/2/21 TA\]

Better management of existing forces by the military services would help
a great deal, too. The Army is overworked partly because it maintains
deployments of several thousand soldiers in South Korea and Poland
through frequent rotations of multiple units, rather than the more
efficient approach of permanently stationing individual brigades in
these locations. The Air Force could consider similar changes in how it
maintains key units in parts of the Middle East. Several fighter
squadrons could, for example, be based in Gulf states rather than
rotated in and out. The Navy still focuses too rigidly on maintaining
permanent presence in the broader Persian Gulf and Western Pacific
regions. More flexible and unpredictable deployments can ease strain on
the force without giving adversaries any solace. The Navy can also
consider crew swaps while ships remain at sea, rather than bringing
crews and ships home from deployment together every six to eight months
as is now the norm. With these kinds of adaptations, and improved
readiness resulting from more consistent budgets, the size of today's
force can prove adequate to the tasks at hand. By contrast, [quality
must improve, and modernization must intensify.]{.underline} That is not
because the U.S. military is obsolescent. Rather, [the pace of
innovation in key areas of military technology, and the way in which
vulnerabilities in our existing military could be exploited by Russia or
China, require it. If we fail to make the U.S. military more modern,
resilient, lethal, and survivable, the perception could grow that
relative American combat power was fading --- or that the American
military had developed systemic vulnerabilities that an enemy could
exploit to produce catastrophic failure. Deterrence could weaken. War
could result]{.underline}. [And we could quite possibly even lose such a
war]{.underline}. The years 2020-40 seem likely to see even more change
in the technologies, and the character, of warfare than have recent
decades. For the years 2000-20, [revolutionary technological change
occurred]{.underline} mainly in various aspects of computers and
robotics. For the next two decades, those areas will remain fast-moving,
and they will be joined [by]{.underline} various [breakthroughs in
artificial intelligence (AI) including the use of big data. The
battlefield implications in domains such as swarms of robotic systems
usable as both sensors and weapons may truly come of age.]{.underline}
In addition, progress in laser weapons, reusable rockets, hypersonic
missiles, unmanned submarines, biological pathogens, and nanomaterials
may occur rapidly. The sum total may or may not add up to a revolution.
But the potential cannot be dismissed. [The rise of China and the return
of Russia supercharge the competition and raise the strategic
stakes]{.underline}. [The marriage of rapid technological progress with
hegemonic change could prove especially potent. The return of
great-power competition during an era of rapid progress in science and
technology could]{.underline} reward innovators and [expose
vulnerabilities]{.underline}, [much more than has been the case in the
21st century to date.]{.underline} Not every existing Department of
Defense weapons program is equally defensible, of course. Some programs
should be reassessed, or delayed, in order to make room for more
survivable and effective systems --- for example, reducing procurement
of surface ships in favor of attack submarines and unmanned undersea
vehicles for the Navy, and emphasizing longer-range aircraft more than
fighters for the Air Force as well as the Navy. On balance, however, in
broad strokes and in overall resource requirements, the Pentagon agenda
for modernization makes sense. It is important to prioritize, and
preserve, it. Today's already-excellent American military is big enough
to meet the reasonable requirements of ongoing commitments and great
power competition --- provided, that is, that it improves further. It
needs to repair readiness. Most of all, it must be modernized for
greater lethality, and made more resilient and survivable against the
kinds of precision-strike, cyber, anti-satellite, and other asymmetric
attacks future adversaries would be sure to employ. We need to keep our
eye focused clearly on the ball, and our resource allocations focused
clearly on the strategy. We need a more modern and ready force, not a
larger one.

**Maintaining technological superiority deters Russia and China
aggression -- rapid great power shifts and revisionist powers eliminate
bargaining, escalating conflicts to nuclear war.**

**Kroenig and Gopalaswamy, 2018 - Prof of Government and Foreign Service
at Georgetown and director of the South Asia Center at the Atlantic
Council** \[Matthew and Bharath "Will disruptive technology cause
nuclear war?" November 12.
<https://thebulletin.org/2018/11/will-disruptive-technology-cause-nuclear-war>
//pipk\]

Recently, [analysts have argued that emerging technologies with military
applications may undermine nuclear stability]{.underline} (see here,
here, and here), [but the logic of these arguments is debatable and
overlooks a more straightforward reason why]{.underline} [new technology
might cause nuclear conflict: by upending the existing balance of power
among nuclear-armed states.]{.underline} [This latter concern is more
probable and dangerous]{.underline} and demands an immediate policy
response. For more than 70 years, the world has avoided major power
conflict, and many attribute this era of peace to nuclear weapons. In
situations of mutually assured destruction (MAD), neither side has an
incentive to start a conflict because doing so will only result in its
own annihilation. The key to this model of deterrence is the maintenance
of secure second-strike capabilities---the ability to absorb an enemy
nuclear attack and respond with a devastating counterattack. Recently
analysts have begun to worry, however, that new strategic military
technologies may make it possible for a state to conduct a successful
first strike on an enemy. For example, Chinese colleagues have
complained to me in Track II dialogues that the United States may decide
to launch a sophisticated cyberattack against Chinese nuclear command
and control, essentially turning off China's nuclear forces. Then,
Washington will follow up with a massive strike with conventional cruise
and hypersonic missiles to destroy China's nuclear weapons. Finally, if
any Chinese forces happen to survive, the United States can simply mop
up China's ragged retaliatory strike with advanced missile defenses.
China will be disarmed and US nuclear weapons will still be sitting on
the shelf, untouched. If the United States, or any other state acquires
such a first-strike capability, then the logic of MAD would be
undermined. Washington may be tempted to launch a nuclear first strike.
Or China may choose instead to use its nuclear weapons early in a
conflict before they can be wiped out---the so-called "use 'em or lose
'em" problem. According to this logic, therefore, the appropriate policy
response would be to ban outright or control any new weapon systems that
might threaten second-strike capabilities. This way of thinking about
new technology and stability, however, is open to question. Would any US
president truly decide to launch a massive, bolt-out-of-the-blue nuclear
attack because he or she thought s/he could get away with it? And why
does it make sense for the country in the inferior position, in this
case China, to intentionally start a nuclear war that it will almost
certainly lose? More important, this conceptualization of how new
technology affects stability is too narrow, focused exclusively on how
new military technologies might be used against nuclear forces directly.
Rather, we should think more broadly about how new technology might
affect global politics, and, for this, it is helpful to turn to
scholarly international relations theory. The dominant theory of the
causes of war in the academy is [the "bargaining model of
war."]{.underline} This theory [identifies rapid shifts in the balance
of power as a primary cause of conflict. International politics often
presents states with conflicts that they can settle through peaceful
bargaining, but when bargaining breaks down, war results. Shifts in the
balance of power are problematic because they undermine effective
bargaining]{.underline}. After all, why agree to a deal today if your
bargaining position will be stronger tomorrow? And, a clear
understanding of the military balance of power can contribute to peace.
(Why start a war you are likely to lose?) But [shifts in the balance of
power muddy understandings of which states have the
advantage]{.underline}. You may see where this is going. [New
technologies threaten to create potentially destabilizing shifts in the
balance of power. For decades, stability in Europe]{.underline} [and
Asia]{.underline} [has been supported by US military power]{.underline}.
In recent years, however, [the balance of power in Asia has begun to
shift, as China has increased its military capabilities]{.underline}.
Already, Beijing has become more assertive in the region, claiming
contested territory in the South China Sea. And the results of [Russia's
military modernization have been on full display in its ongoing
intervention in Ukraine]{.underline}. Moreover, [China may have the lead
over the United States in emerging technologies that could be decisive
for the future of military]{.underline} acquisitions and
[warfare]{.underline}, [including]{.underline} 3D printing, hypersonic
missiles, quantum computing, 5G wireless connectivity, and [artificial
intelligence]{.underline} (AI). And Russian President Vladimir Putin is
building new unmanned vehicles while ominously declaring, "Whoever leads
in AI will rule the world." [If China or Russia are able to incorporate
new technologies into their militaries before the United States, then
this could lead to the kind of rapid shift in the balance of power that
often causes war. If Beijing believes emerging technologies provide it
with a newfound, local military advantage over the United States, for
example, it may be more willing than previously to initiate conflict
over Taiwan]{.underline}. And [if Putin thinks new tech has strengthened
his hand, he may be more tempted to launch a Ukraine-style invasion of a
NATO member]{.underline}. [Either scenario could bring these nuclear
powers into direct conflict with the United States, and once nuclear
armed states are at war, there is an inherent risk of nuclear
conflict]{.underline} through limited nuclear war strategies, nuclear
brinkmanship, or simple accident or inadvertent escalation. This framing
of the problem leads to a different set of policy implications. The
concern is not simply technologies that threaten to undermine nuclear
second-strike capabilities directly, but, rather, any technologies that
can result in a meaningful shift in the broader balance of power. And
[the solution is]{.underline} not to preserve second-strike
capabilities, but [to preserve prevailing power balances]{.underline}
more broadly. When it comes to new technology, this means that [the
United States should seek to maintain an innovation edge]{.underline}.
Washington should also work with other states, including its
nuclear-armed rivals, to develop a new set of arms control and
nonproliferation agreements and export controls to deny these newer and
potentially destabilizing technologies to potentially hostile states.
These are no easy tasks, but [the consequences of Washington losing the
race for technological superiority]{.underline} to its autocratic
challengers [just might mean nuclear Armageddon.]{.underline}

**[\--Extend Uniqueness]{.underline}**

**The US has increased funding for AI development and minimized
regulations to promote innovation**

**Lawrence and Cordey, 2020 -- researchers for The Cyber Project at the
Belfer Center for Science and International Affairs** \[Christie and
Sean, August, The Cyber Project Paper "The Case for Increased
Transatlantic Cooperation on Artificial Intelligence Edited by Lauren
Zabierek and Julia Voo
https://www.belfercenter.org/sites/default/files/2020-08/TransatlanticAI.pdf
Acc. 4/21/22 TA\]

United States: [The United States views American leadership in AI as
necessary]{.underline} [to]{.underline} safeguard American values and
[maintain defense and economic superiority.]{.underline} Recognizing the
need to develop a national AI approach and reclaim the AI R&D global
leadership position from China, which had already surpassed the US in
several research output metrics by 2016,10 [the Obama Administration
developed an AI R&D prioritization]{.underline} in October 2016.11
Building on this urgency, [the Trump Administration has prioritized AI
and established the American AI Initiative]{.underline} in February
2019.12 This Initiative identified the need for a whole-of-government
approach to prioritize AI R&D and deployment throughout the entire
federal government. [The Initiative also identifies the need to grow the
US AI workforce, set national and global norms and standards, and work
with industry and allies to promote an AI environment favorable to the
United States]{.underline}.13 The United States' federal government has
made key strategic and tactical changes to achieve these goals. Federal
AI R&D and the American AI Initiative are coordinated by several
committees and subcommittees within the Executive Office. President
Trump pledged to more than double non-defense AI R&D to \$2 billion by
2022.14 Federal AI R&D, guided by the National AI R&D Strategic Plan,
must now be reported annually for each federal entity.15 [The United
States has taken a "light-touch" approach to regulation, fearing overly
burdensome laws will stifle innovation.]{.underline} However, guidance
is not completely absent. The Office of Management and Budget released a
memo to guide Federal agencies as they develop regulatory and
non-regulatory approaches to non-government applications of AI and the
Department of Defense published five AI principles to guide AI design,
deployment, and adoptions in defense.16

**The DOD is accelerating programs to develop AI and autonomy**

**Nurkin and Konaev, 2022 - senior fellows at the Center for Strategy
and Security at the Atlantic Council** \[Tate and Margarita, May 25,
"Eye to eye in AI: Developing artificial intelligence for national
security and defense"
<https://www.atlanticcouncil.org/in-depth-research-reports/report/eye-to-eye-in-ai/>
Acc 6/12/22 TA\]

[The Pentagon's interest and urgency related to AI is due]{.underline}
both [to the accelerating pace of development of technology]{.underline}
and, increasingly, the transformative capabilities it can enable.
Indeed, [AI is poised to fundamentally alter how militaries think about,
prepare for, carry out, and sustain operations]{.underline}. Drawing on
a previous Atlantic Council report outline, the "Five Revolutions"
framework for classifying the potential impact of AI across five broad
capability areas, Figure 3 below illustrates the different ways in which
AI could augment human cognitive and physical capabilities, fuse
networks and systems for optimal efficiency and performance, and usher
in a new era of cyber conflict and chaos in the information space, among
other effects.38 [The DoD currently has more than six hundred AI-related
efforts in progress, with a vision to integrate AI into every element of
the DoD's mission]{.underline}---from warfighting operations to support
and sustainment functions to the business operations and processes that
undergird the vast DoD enterprise.39 [A February 2022 report by the US
Government Accountability Office (GAO) has found that the DoD is
pursuing AI capabilities for warfighting that predominantly focus
on]{.underline} "(1) [recognizing targets through intelligence and
surveillance analysis,]{.underline} (2) providing recommendations to
operators on the battlefield (such as where to move troops or which
weapon is best positioned to respond to a threat), and (3) [increasing
the autonomy of uncrewed systems]{.underline}."40 Most of the DoD's AI
capabilities, especially the efforts related to warfighting, are still
in development, and not yet aligned with or integrated into specific
systems. And, despite notable progress in experimentation and some
experience with deploying AI-enabled capabilities in combat operations,
there are still significant challenges ahead for wide-scale adoption. In
September 2021, the Air Force's first chief software officer, Nicolas
Chaillan, resigned in protest of the bureaucratic and cultural
challenges that have slowed technology adoption and hindered the DoD
from moving fast enough to effectively compete with China. In Chaillan's
view, in twenty years, the United States and its allies "will have no
chance competing in a world where China has the drastic advantage in
population."41 Later, he added that China has essentially already won,
saying, "Right now, it's already a done deal."42 Chaillan's assessment
of the United States engaged in a futile competition with China is
certainly not shared across the DoD, but it reflects what many see as a
lack of urgency within the risk-averse and ponderous culture of the
department. [Lt. General Michael Groen, the head of the
JAIC,]{.underline} agreed that "inside the department, there is a
cultural change that has to occur."43 However, he also [touted the
innovative capacity of the United States and highlighted the
establishment of an AI accelerator and the finalization of a Joint
Common Foundation (JCF) for AI development]{.underline}, testing, and
sharing of AI tools across DoD entities.44 [The]{.underline}
cloud-enabled [JCF is an important step forward that will]{.underline}
allow for AI development based on common standards and architectures.
This should help [encourage sharing between the military services and
DoD components]{.underline} and, according to the JAIC, ensure that
"progress by one DoD AI initiative will build momentum across the entire
DoD enterprise."45

**[\--Extend Links]{.underline}**

**Ethical limits that prevent the US from Using autonomous AI weapons
gives China a technological edge.**

**Horowitz, 2018 -- prof of political science at the Univ of
Pennsylvania** \[Michael C., May, Texas National Security Review, The
Scholar "Artificial Intelligence, International Competition, and the
Balance of Power"
<https://tnsr.org/2018/05/artificial-intelligence-international-competition-and-the-balance-of-power/>
Acc 5/14/22 TA\]

From a balance-of-power perspective, this scenario would be more likely
to feature disruption among emerging and great powers but not a broader
leveling of the military playing field. [The ability to exclude many
countries from advances in AI would concentrate military competition
among current leading militaries, such as the United States, China, and
Russia.]{.underline} There could be significant disruption within those
categories, though. [A Chinese military that more rapidly developed
critical algorithms for broader battle management, or that was more
willing to use them than the United States, might gain advantages that
shifted power in the Asia- Pacific.]{.underline} This assumes that these
algorithms operate as they are designed to operate. All
militarily-useful AI will have to be hardened against hacking and
spoofing. Operators will use narrow AI applications only if they are as
or more effective or reliable as existing inhabited or remotely-piloted
options.111

**Autonomy is key to develop AI systems that enable all levels of
military innovation**

**Vestner, 2021 - Head of Security and Law Programme at Geneva Centre
for Security Policy** \[Tobias, July 8 "Warfare and Artificial
Intelligence" in Robin Geiß and Henning Lahmann (eds), Research Handbook
on Warfare and Artificial Intelligence -forthcoming
https://www.gcsp.ch/publications/military-operations-and-artificial-intelligenceGCSP
Acc 5/27/22 TA\]

Military operations are the essence of warfare. The introduction of
[military applications of artificial intelligence (AI) will most likely
transform the preparation and conduct of military operations. AI can
increasingly support and replace humans for military tasks as they are
becoming faster and more accurate as well as able to consider more
information and higher levels of complexity]{.underline}. [This may lead
to an increased speed of military operations and better military
decision-making, ultimately offering armed forces with performant AI
significant advantage]{.underline}s. The military use of AI may indeed
lead to another revolution in military affairs.1 AI can be used for
various military purposes. In multi-dimensional battlefields, [AI
technologies can be utilized as sensors, planners, and
fighters]{.underline}, or a combination thereof.2 [More concretely,
military applications of AI can range from systems supporting
intelligence, surveillance, and reconnaissance (ISR) to autonomous
navigation and target recognition systems]{.underline}.3 This can lead
to diverse forms of interaction between military staff and AI systems as
well as various levels of delegation of military tasks to AI systems.
[AI systems may assist commanders and soldiers in decision-making
processes,]{.underline} unmanned AI systems may operate together with
manned systems, and AI systems may operate autonomously under minimal
human supervision, for instance.4 While currently only narrow and
task-specific AI exist,5 significant efforts for the development of
artificial general intelligence (AGI) -- systems with an ability to
reason across a wide range of domains akin to that of the human mind -
are underway.6 This is in line with the continuous trend towards
increased autonomy of AI systems.

**Regulations stifle AI innovation that is necessary to win the military
competition**

**Schmidt and Work, 2021 -- Chairs of The National Security Commission
on Artificial Intelligence** \[Eric and Robert, "National Security
Commission on Artificial Intelligence Final Report Executive Summary"
<https://www.nscai.gov/2021-final-report/> Acc 6/7/22 TA\]

AI systems will also be used in the pursuit of power. We fear [AI tools
will be weapons of first resort in future conflicts]{.underline}. AI
will not stay in the domain of superpowers or the realm of science
fiction. AI is dual-use, often open-source, and diffusing rapidly[.
State adversaries are already using AI-enabled disinformation
attacks]{.underline} to sow division in democracies and jar our sense of
reality. [States]{.underline}, criminals, and terrorists [will conduct
AI-powered cyber attacks and pair AI software with commercially
available drones to create "smart weapons]{.underline}." [It is no
secret that America's military rivals are integrating AI concepts and
platforms to challenge the United States' decades-long technology
advantage. We will not be able to defend against AI-enabled threats
without ubiquitous AI capabilities]{.underline} and new warfighting
paradigms. [We want]{.underline} the men and women in national security
departments and [agencies to have access to the best technology in the
world to defend]{.underline} themselves and us, and to protect [our
interests and those of our allies]{.underline} and partners. Despite
exciting experimentation and a few small AI programs, the U.S.
government is a long way from being "AI-ready." The Commission's
business leaders are most frustrated by slow government progress because
they know it's possible for large institutions to adopt AI. AI
integration is hard in any sector---and the national security arena
poses some unique challenges. Nevertheless, committed leaders can drive
change. We need those leaders in the Pentagon and across the Federal
Government to build the technical infrastructure and connect ideas and
experimentation to new concepts and operations. By 2025, the Department
of Defense and the Intelligence Community must be AI-ready. [We should
embrace the AI competition. Competition already infuses the quests for
data, computing power, and the holy grail: the rare talent to make AI
breakthroughs]{.underline}. The fact that AI courses through so many
adjacent technologies and is leveraged across so many fields explains
its power and leads inexorably to another critical point: AI is part of
a broader global technology competition. [Competition will speed up
innovation]{.underline}. We should race together with partners when AI
competition is directed at the moonshots that benefit humanity like
discovering vaccines. But [we must win the AI competition that is
intensifying strategic competition with China.]{.underline} China's
plans, resources, and progress should concern all Americans. It is an AI
peer in many areas and an AI leader in some applications. We take
seriously China's ambition to surpass the United States as the world's
AI leader within a decade. The AI competition is also a values
competition. China's domestic use of AI is a chilling precedent for
anyone around the world who cherishes individual liberty. Its employment
of AI as a tool of repression and surveillance---at home and,
increasingly, abroad---is a powerful counterpoint to how we believe AI
should be used. The AI future can be democratic, but we have learned
enough about the power of technology to strengthen authoritarianism
abroad and fuel extremism at home to know that we must not take for
granted that future technology trends will reinforce rather than erode
democracy. We must work with fellow democracies and the private sector
to build privacy-protecting standards into AI technologies and advance
democratic norms to guide AI uses so that democracies can responsibly
use AI tools for national security purposes. Given these convictions,
the Commission concludes that the United States must act now to field AI
systems and invest substantially more resources in AI innovation to
protect its security, promote its prosperity, and safeguard the future
of democracy. Today, the government is not organizing or investing to
win the technology competition against a committed competitor, nor is it
prepared to defend against AI-enabled threats and rapidly adopt AI
applications for national security purposes. This is not a time for
incremental toggles to federal research budgets or adding a few new
positions in the Pentagon for Silicon Valley technologists. This will be
expensive and require a significant change in mindset. America needs
White House leadership, Cabinet-member action, and bipartisan
Congressional support to win the AI era.

**Ethical limits put our allies at a technological and military
disadvantage.**

**Thornton, 2019 - Senior Lecturer in the Centre for Defence Education
Research and Analysis, King's College** \[Rod, "One to ponder: the UK's
ethical stance on the use of Artificial Intelligence in weapons systems
https://defenceindepth.co/2019/06/17/one-to-ponder-the-uks-ethical-stance-on-the-use-of-artificial-intelligence-in-weapons-systems/
Acc 4/16/22 TA\]

The potential use, thus, of AI in weapons systems has raised
controversy, not least in the UK. A recent report by the House of Lords
Select Committee on AI recognised that, [while UK spending on AI could
not match that of the US or China, the UK could still be a world leader
in terms of the ethics involved.]{.underline} [The report
warned]{.underline}: 'The [autonomous power to]{.underline} hurt,
[destroy]{.underline} or deceive [human beings should never be vested in
artificial intelligence'. It advised that the UK should be acting to
'lead the international community in AI's ethical development, rather
than passively accept its consequences'. It went on to urge the UK to
'forge a distinctive role for itself as a pioneer in ethical
AI'.]{.underline} The UK is, it seems, to be a global leader in the
governance of the ethical use of AI. In light of this report, one
journal article rather pointedly carried the headline: 'The UK says it
can't lead on AI spending, so will have to lead on AI ethics instead'.
With such sentiments abroad, it is no surprise then that [the official
UK line is that none of its offensive weapons systems will be capable of
attacking targets without some degree of human control]{.underline}. As
one Ministry of Defence spokesperson put it: '[The United Kingdom does
not possess fully autonomous weapon systems and has no intention of
developing them'.]{.underline} This is a laudable but debateable
statement. Surely, the UK will be developing purely defensive AI system
that are fully autonomous -- anti-missile missile systems, for instance
-- where speed of reaction cannot be left to dithering humans. In a
broader sense, though, [this is a declared restriction that could put UK
forces at a serious disadvantage on future battlefields when it comes to
the employment of LAWS.]{.underline} It cannot really be imagined that
the likes of China and Russia, as they develop their AI systems, will
feel limited by ethical sentiment. Their view will be that they cannot
afford to be. They both see themselves as weaker militarily than the
combined forces of NATO and its partner countries and, as such, have
doctrinally declared that they will be seeking out any asymmetric
advantage they can. If these Western powers -- including the UK -- want
to self-restrict their use of LAWS, for instance, then this will be seen
by Beijing and Moscow as a weakness to be exploited in an asymmetric
sense.

**[\--Extend Internal Link]{.underline}**

**Technology is the key to military superiority -- improving speed and
control are necessary to deal with future military threats.**

**Schmidt and Work, 2021 -- Chairs of The National Security Commission
on Artificial Intelligence** \[Eric and Robert, "National Security
Commission on Artificial Intelligence Final Report Executive Summary"
<https://www.nscai.gov/2021-final-report/> Acc 6/7/22 TA\]

[AI-enhanced capabilities will be the tools of first resort in a new era
of conflict as strategic competitors develop AI]{.underline} concepts
and [technologies for military]{.underline} and other malign [uses and
cheap and commercially available AI applications]{.underline} ranging
from "deepfakes" to lethal drones [become available to rogue
states]{.underline}, terrorists, and criminals. [The United States must
prepare to defend against these threats by quickly]{.underline} and
responsibly [adopting AI for national security and defense purposes.
Defending against AI-capable adversaries operating at machine speeds
without employing AI is an invitation to disaster]{.underline}. [Human
operators will not be able to keep up with or defend against AI-enabled
cyber]{.underline} or disinformation [attacks, drone swarms, or missile
attacks without the assistance of AI-enabled machines]{.underline}.
[National security professionals must have access to the world's best
technology to]{.underline} protect themselves, [perform their missions,
and defend us]{.underline}. The Commission recommends that the
government take the following actions: Defend against emerging
AI-enabled threats to America's free and open society. Digital
dependence in all walks of life is transforming personal and commercial
vulnerabilities into potential national security weaknesses. Adversaries
are using AI systems to enhance disinformation campaigns and cyber
attacks. They are harvesting data on Americans to build profiles of
their beliefs, behavior, and biological makeup for tailored attempts to
manipulate or coerce individuals. This gathering storm of foreign
influence and interference requires organizational and policy reforms to
bolster our resilience. The government needs to stand up a task force
and 24/7 operations center to confront digital disinformation. It needs
to better secure its own databases and prioritize data security in
foreign investment screening, supply chain risk management, and national
data protection legislation. [The government should leverage AI-enabled
cyber defenses to protect against AI-enabled cyber attacks.]{.underline}
And biosecurity must become a top-tier priority in national security
policy. Prepare for future warfare. [Our armed forces' competitive
military-technical advantage could be lost within the next decade if
they do not accelerate the adoption of AI across their
missions]{.underline}. This will require marrying top-down leadership
with bottom-up innovation to put operationally relevant AI applications
into place. The Department of Defense (DoD) should: First, establish the
foundations for widespread integration of AI by 2025. This includes
building a common digital infrastructure, developing a
digitally-literate workforce, and instituting more agile acquisition,
budget, and oversight processes. It also requires strategically
divesting from military systems that are ill-equipped for AI-enabled
warfare and instead investing in next-generation capabilities.

**Technological superiority is the key internal link to America's
military advantage. China can legitimately challenge US dominance.**

**Schmidt and Work, 2021 -- Chairs of The National Security Commission
on Artificial Intelligence** \[Eric and Robert, "National Security
Commission on Artificial Intelligence Final Report Executive Summary"
<https://www.nscai.gov/2021-final-report/> Acc 6/7/22 TA\]

The National Security Commission on Artificial Intelligence (NSCAI)
humbly acknowledges how much remains to be discovered about AI and its
future applications. Nevertheless, we know enough about AI today to
begin with two convictions. First, the rapidly improving ability of
computer systems to solve problems and to perform tasks that would
otherwise require human intelligence---and in some instances exceed
human performance---is world altering. AI technologies are the most
powerful tools in generations for expanding knowledge, increasing
prosperity, and enriching the human experience. [AI is also the
quintessential "dual-use" technology. The ability of a machine to
perceive, evaluate, and act more quickly and accurately than a human
represents a competitive advantage in any field]{.underline}---civilian
or [military. AI technologies will be a source of enormous power for the
companies and countries that harness them]{.underline}. Second, [AI is
expanding the window of vulnerability the United States has already
entered. For the first time since World War II, America's technological
predominance---the backbone of its economic and military power---is
under threat. China possesses the might, talent, and ambition to surpass
the United States as the world's leader in AI in the next
decade]{.underline} if current trends do not change. Simultaneously[, AI
is deepening the threat posed by cyber attacks]{.underline} and
disinformation campaigns [that Russia, China, and others are using
to]{.underline} infiltrate our society, [steal our data]{.underline},
and interfere in our democracy. [The limited uses of AI-enabled attacks
to date represent the tip of the iceberg]{.underline}. Meanwhile, global
crises exemplified by the COVID-19 pandemic and climate change highlight
the need to expand our conception of national security and find
innovative AI-enabled solutions.

**Losing the AI arms race encourages China to invade Taiwan.**

**Horowitz and Kahn, 2022 - Senior and Research Fellows for Defense
Technology and Innovation at the Council on Foreign Relations**
\[Michael and Lauren, with Laura Samotin, May/June Foreign Affairs "A
Force for the Future A High-Reward, Low-Risk Approach to AI Military
Innovation"
[https://www.foreignaffairs.com/articles/united-states/2022-04-19/force-future
Acc
5/28/22](https://www.foreignaffairs.com/articles/united-states/2022-04-19/force-future%20Acc%205/28/22)
TA\]

Leading military powers often forgo innovation and resist change. But
today, the United States risks being more like the United Kingdom---or
even France. The Defense Department appears to be biased in favor of
tried-and-true capabilities over new tools, and its pace of innovation
has slowed: the time it takes to move new technology from the lab and to
the battlefield went from roughly five years, on average, in the early
1960s to a decade or more today. Sometimes, the Pentagon has seemingly
dragged its feet on AI and autonomous systems because it fears that
adopting those technologies could require disruptive changes that would
threaten existing, successful parts of the armed forces, as the story of
the X-45, the X-47A, and the X-47B clearly illustrates. Some projects
have struggled to even make it off the drawing board. Multiple
experiments have shown that Loyal Wingman, an uncrewed aircraft that
employs AI, can help aircraft groups better coordinate their attacks.
But the U.S. military has yet to seriously implement this technology,
even though it has existed for years. It's no wonder that [the National
Security Commission on Artificial Intelligence concluded in
2021,]{.underline} in its final report, that the United States "is not
prepared to defend or compete in the AI era." [If the United States
fails to develop effective AI, it could find itself at the mercy of
increasingly sophisticated adversaries. China, for example, is already
employing AI to war-game a future conflict over Taiwan. Beijing plans to
use AI in combination with cyberweapons, electronic warfare, and
robotics]{.underline} to make an amphibious assault on Taiwan more
likely to succeed. It is investing in AI-enabled systems to track
undersea vehicles and U.S. Navy ships and to develop the ability to
launch swarm attacks with low-cost, high-volume aircraft[. If the United
States lacks advanced AI capabilities, it will find itself inevitably
moving at a slower pace---and would therefore be less able to help
Taiwan fend off an invasion.]{.underline}

**[\--Extend Internal Links - Specific AI Missions]{.underline}**

**[Planning]{.underline}**

**Autonomous AI is key to military planning -- speed allows multiple
iterations of battle plans.**

**Vestner, 2021 - Head of Security and Law Programme at Geneva Centre
for Security Policy** \[Tobias, July 8 "Warfare and Artificial
Intelligence" in Robin Geiß and Henning Lahmann (eds), Research Handbook
on Warfare and Artificial Intelligence -forthcoming
https://www.gcsp.ch/publications/military-operations-and-artificial-intelligenceGCSP
Acc 5/27/22 TA\]

Such [AI applications will probably have strong ramifications on
planning. Planning military operations is a slow and burdensome process,
which relies on estimations of 'outcomes, attrition, consumption of
supplies, and enemy reaction]{.underline}'.75 It involves understanding
a given situation, time-space analysis, and logistics concerns. [Time
and labour limitations restrict how many options of plans can be
explored]{.underline}.76 Moreover, prediction is arguably 'one of the
most vexing tasks of the operational commander'.77 [Provided that
sufficient quantity and quality of data can be made available, AI may
excel in prediction making both in quality and speed. Data analytics
further enables the processing of much more information than human
computing, eventually reducing the 'fog of war']{.underline}.78 As AI
programmes can deconstruct operations into specific tasks to then
allocate resources accordingly, predict enemy actions, and estimate
risks, this would improve the general speed and accuracy of
decision-making.79 [An increase]{.underline} in the number of COA that
can be considered [would]{.underline} further [allow a qualitative
improvement of planning processes.80]{.underline}

**Battle planning is crucial for NATO's advantage -- it improves threat
assessment and analysis and improves interoperability through
coordination of resources.**

**de Maizière and Mitchell, 2020 - Former German Defense Minister and
former U.S. Assistant Secretary of State** \[Thomas and Wess, November
25, "NATO 2030: United for a New Era Analysis and Recommendations of the
Reflection Group Appointed by the NATO Secretary General"
https://www.nato.int/nato_static_fl2014/assets/pdf/2020/12/pdf/201201-Reflection-Group-Final-Report-Uni.pdf
Acc. 4/12/22 TA\]

Emerging and Disruptive Technologies [Maintaining a technological edge
is the foundation upon which NATO's ability to deter and defend against
potential threats ultimately rests. EDTs pose]{.underline} a fundamental
challenge but also---if harnessed correctly---[an opportunity for the
Alliance]{.underline}. Without a strategic surge in this area[, allowing
adversaries to gain competitive advantage would impede NATO's ability to
win on the battlefield, challenge strategic stability and change the
fundamentals of deterrence, but also offer]{.underline} state and even
non-state actors, including eventually [terrorists, the potential to
threaten our societies from within. They also could undermine NATO's
political cohesion, by raising questions about technology sharing within
the Alliance, impairing interoperability]{.underline}, and potentially
fuelling dependencies on rival states. At the same time, new
technologies offer historic opportunities for strategic advantage, from
dealing with new types of conflicts to sharing and analysing data at an
unprecedented level and, more broadly, for the enrichment and betterment
of society. Against that backdrop, the acquisition of, and [access to,
EDTs in the arenas of]{.underline} e.g., big data, [Artificial
Intelligence, autonomous capabilities]{.underline}, space, cloud
technologies, hypersonic and new missile technologies, quantum
technologies and biotechnologies, and human augmentation/enhancement,
[is fundamental to the future security of NATO and its Allies -- and
should be reflected in the capabilities NATO asks its Allies to deliver.
This must begin with a common understanding and approach of the major
challenges the Alliance is facing in this domain]{.underline}. NATO and
its Allies have acknowledged the profound impact of new technologies by
launching the Emerging and Disruptive Technologies Roadmap during the
London Leaders Meeting in December 2019. However, NATO has to increase
the pace and scale of its political focus on this area if it really
wants to counter the threats and to reap the fruits resulting from new
technologies. Recommendations: 1. NATO Allies should agree to, and begin
to enact, NATO's EDT Implementation Strategy as soon as possible. [The
development and introduction of cutting-edge capabilities is the primary
responsibility of national governments. However, NATO has an important
role to play in prompting the development of a common strategy, grounded
in an Alliance-wide EDT threat assessment and an analysis of
opportunities,]{.underline} whereby members can conceptualise how their
national efforts fit together for purposes of common security and where
the Alliance can benefit from new technologies. 2. [Competing with the
efforts underway by large authoritarian states to achieve dominance in
key EDTs must be a strategic priority for the Alliance]{.underline} and
its members. [It should enhance its role as the key coordinating
institution on security-related EDTs for its members. While key aspects
of technological innovation lie at the national or EU levels, NATO has
an appropriate]{.underline} and as-yet underdeveloped [role to play in
providing a forum for discussion on all aspects of EDTs that have a
direct bearing on the security of the Euro-Atlantic area.]{.underline}
3. [NATO should serve as a crucial coordinating institution for
information sharing and collaboration between Allies on the security
dimensions of EDTs. At present, no transatlantic coordination tool
exists for this purpose. These consultations could, when NATO security
requires, be extended to included non-allies that are cleared for
intelligence-sharing.]{.underline} 4[. NATO should hold a digital summit
of governments and private sector with the aim of identifying gaps in
collective defence cooperation in security-related AI strategies, norms
and R&D, and safeguarding against the malign and aggressive use of AI,
including militarily, and via the spread of digital
authoritarianism]{.underline}. 5. [NATO should anchor EDTs in the
defence planning process]{.underline} (NDPP) [to ensure that all Allies
modernise their forces appropriately, and that technological adaptation
is included in evaluating fair burden-sharing]{.underline}. Against that
backdrop, the NDPP should be analysed and potentially adapted to reflect
NATO's capabilities to respond to threats from EDTs. NATO should review
whether, in light of the fast-moving nature of technological change, the
four-year time span allotted for incorporation of EDTs should be
shortened.

**[Cyberdefense]{.underline}**

**AI at Machine speed is essential for cyber defense, because
Adversaries will be using AI for cyber attacks.**

**Flournoy and Haines, 2020 - former Under Secretary of Defense for
Policy and Director of National Intelligence** \[Michèle and Avril
October, "Building Trust through Testing Adapting DOD's Test &
Evaluation, Validation & Verification (TEVV) Enterprise for Machine
Learning Systems, including Deep Learning Systems" https://cset.
georgetown.edu/wp-content/uploads/Building-Trust-Through-Testing.pdf Acc
6/7/22 TA\]

The United States is at an inflection point in an age of mounting
transnational threats, unprecedented global interdependence, and
resurgent great power competition. This moment is taking place in the
context of a technological revolution that exacerbates the challenges we
face while simultaneously offering potential solutions, providing
breakthroughs in climate, medicine, communications, transportation,
intelligence, and many other fields. [Many of these breakthroughs will
come through the exploitation of artificial intelligence (AI) and its
related technologies---chief among them machine learning
(ML).]{.underline} These advances will likely shape the economic and
military balance of power among nations and the future of work, wealth,
and inequality within them. [Innovations in ML have the potential to
transform fundamentally how the U.S. military fights, and how
the]{.underline} Department of Defense [(DOD) operates. Machine learning
applications can increase the speed and quality of human decision-making
on the battlefield, enable human-machine teaming to maximize performance
and minimize the risk to soldiers, and greatly improve the accuracy and
speed of analysis that relies on very large data sets. ML can also
strengthen the United States' ability to defend its networks against
cyberattacks at machine speeds]{.underline} and has the power to
automate critical components of labor-intensive enterprise functions,
such as predictive maintenance and personnel management.

**Cyber Defense is crucial to our military dominance -- it is the
vulnerability that enemies will exploit.**

**Schmidt and Work, 2021 -- Chairs of The National Security Commission
on Artificial Intelligence** \[Eric and Robert, "National Security
Commission on Artificial Intelligence Final Report Executive Summary"
<https://www.nscai.gov/2021-final-report/> Acc 6/7/22 TA\]

Second, [AI is expanding the window of vulnerability the United States
has already entered. For the first time since World War II, America's
technological predominance---the backbone of its economic and military
power---is under threat.]{.underline} China possesses the might, talent,
and ambition to surpass the United States as the world's leader in AI in
the next decade if current trends do not change. Simultaneously, [AI is
deepening the threat posed by cyber attacks]{.underline} and
disinformation campaigns [that Russia, China, and others are using
to]{.underline} infiltrate our society, [steal our data]{.underline},
and interfere in our democracy. [The limited uses of AI-enabled attacks
to date represent the tip of the iceberg]{.underline}. Meanwhile, global
crises exemplified by the COVID-19 pandemic and climate change highlight
the need to expand our conception of national security and find
innovative AI-enabled solutions.

**[Swarming]{.underline}**

**Autonomous AI is necessary to defeat autonomous drone swarms.**

**Miller, 2021 - Flight Chief, US Air Force** \[Amanda, Dec 14 Air Force
Magazine "UN Addresses Lethal Autonomous Weapons---aka 'Killer
Robots'---Amid Calls for a Treaty"
https://www.airforcemag.com/un-addresses-lethal-autonomous-weapons-aka-killer-robots-amid-calls-for-a-treaty/
Acc. 4/5/22 TA\]

Speaking during an American Enterprise Institute webinar about AI on
Dec. 7, [NATO's David van Weel articulated why countries such as the
U.S. might oppose a treaty on autonomous weapons. Van Weel put the issue
in terms of a hypothetical attack by a swarm of drones. "How do we
defend against them? Well, we can't, frankly, because you need AI in
that case in order to be able to counter AI," he said.]{.underline}

**Drone swarms threaten European security.**

**van Weel, 2021 -- Assistant Secretary General for Emerging Security
Challenges, NATO** \[David, Dec 7, "Artificial intelligence: Can we go
from chaos to cooperation?" AEI Panel Discussion - Moderator: Elisabeth
Braw
https://www.aei.org/events/artificial-intelligence-can-we-go-fromchaos-to-cooperation/
Acc 5/11/22 TA\]

And there I would agree on the [EU]{.underline}, that they are far ahead
when it comes down to thinking about regulating AI. But what they [don't
take into account is the security and defense aspects and uses of
AI]{.underline}, for example. And that is a miss from two angles.
[One]{.underline}, [you don't want the security and defense realms not
to be regulated,]{.underline} which is what the current EU legislation
proposes: is to leave an exempt category for all national security and
defense cases. [Second, you don't want to overregulate]{.underline} [if
you don't know that you can defend yourself within the
regulations]{.underline} that you're proposing. So [you need to have
this angle to those new technologies]{.underline} being fed into the
general regulation debate. And [let me give you an example on
autonomy]{.underline}. So, at the moment, if we want to catch drones
with explosives --- and you know drones are very cheap and easy to buy
--- then the ways to catch them are quite primitive. So you either jam
the signal and run the risk that your jam an airfield, which might be
the objective of the attacker in any case. Or the drone might return
home. Or you use a net to catch the drone. Now, that's all fine. Now [we
get drone swarms]{.underline}. So [we get hundreds of drones that
collectively, powered by AI, are able to follow an intrinsic pattern of
attack]{.underline} on, for example, our water supply or one of our
cities. So [how do we defend against them]{.underline}? Well, we can't,
frankly, because [you need AI in that case in order to be able to
counter AI.]{.underline} So it's all these kinds of questions. That
means that we need to be in that discussion and shaping it.

**[1NC - Civilian Deaths DA]{.underline}**

**Banning autonomous AI weapons prevents the technological research that
will make them more ethical and efficient.**

**Arkin, 2013 -- prof at Georgia Tech** \[Ronald "Lethal Autonomous
Systems and the Plight of the Non-combatant" AISB Quarterly, No. 137,
July https://smartech.gatech.edu/handle/1853/50079?show=full Acc
12/27/20 TA\]

Limited autonomy is also present or under development in many systems as
well, ranging from the Phalanx system "capable of autonomously
performing its own search, detect, evaluation, track, engage and kill
assessment functions"2, fire-and-forget munitions, loitering torpedoes,
and intelligent antisubmarine or anti-tank mines among numerous other
examples. Continued advances in autonomy will result in changes
involving tactics, precision, and just perhaps, if done correctly, a
reduction in atrocities as outlined in research conducted at the Georgia
Tech Mobile Robot Laboratory (GT-MRL)3. This paper asserts that [it may
be possible to ultimately create intelligent autonomous robotic military
systems that are capable of reducing civilian casualties and property
damage when compared to the performance of human
warfighters.]{.underline} Thus, it is a contention that [calling for an
outright ban on this technology is premature,]{.underline} as some
groups already are doing4. Nonetheless, if this technology is to be
deployed, then restricted, careful and graded introduction into the
battlefield of lethal autonomous systems must be standard policy as
opposed to haphazard deployments, which I believe is consistent with
existing International Humanitarian Law (IHL). [Multiple potential
benefits of intelligent war machines have already been declared by the
military, including: a reduction in friendly casualties; force
multiplication; expanding the battlespace; extending the warfighter's
reach; the ability to respond faster given the pressure of an ever
increasing battlefield tempo; and greater precision due to persistent
stare]{.underline} \[constant video surveillance that enables more time
for decision making and more eyes on target\]. This argues for the
inevitability of development and deployment of lethal autonomous systems
from a military efficiency and economic standpoint, unless limited by
IHL.

**Researching AI is essential to both improve the military And find ways
to program ethical autonomous weapons. We need to develop Artificial
Intelligence into Artificial Conscience.**

**Park, 2013 - graduate of New York University School of Law** \[Jiou
August 22, 2013 Just Security "Book Synopsis: Governing Lethal Behavior
in Autonomous Robots"
https://www.justsecurity.org/books-read/book-synopsis-governing-lethal-behavior-autonomous-robots/
Acc 12/27/20 TA\]

Arkin first describes [two reasons why it is necessary to develop
ethical military robots. First, there is an unmistakable and
irreversible trend toward greater autonomy in weapon
systems.]{.underline} Arkin points to a number of existing unmanned
weapons systems, ranging from ground robots such as Packbots to air
units such as the Reaper, commonly known as "drones," and also cites
military and technology experts asserting that the trend toward
autonomous military robots is accelerating. According to Arkin, there is
a significant possibility that robots with the capacity to identify and
engage targets without human supervision will be operating side-by-side
with human soldiers within the next twenty to thirty years.
[Second]{.underline}, Arkin argues that [in addition to having the
potential to solve many of the problems related to human soldiers, robot
soldiers may also be able to perform better than human
soldiers]{.underline}. For example, Arkin argues that [soldiers are
prone to behavior that results in atrocities due to emotional and
psychological factors and are vulnerable to psychological
injuries]{.underline}. Moreover, Arkin refers to [studies]{.underline}
that [have found the general reluctance of human soldiers to "shoot to
kill" problematic for effective battlefield performance]{.underline}.
Thus, according to Arkin, [military robots have the potential to behave
not only more ethically but also more effectively on the battlefield
compared to human soldiers.]{.underline} However, whether lethal robots
with an "artificial conscience" capable of behaving "more humanely than
humans" could ever actually come into existence is a separate question.
Arkin devotes the second half of Governing Lethal Behavior in Autonomous
Robots to proving that [it would be possible to develop such "ethical
robots."]{.underline} Arkin focuses on how a [military robot's
programming would work to ensure ethical behavior]{.underline}, starting
from a hypothetical situation where all other necessary technologies are
present. An autonomous robot decides how to act through a "behavioral
mapping" which translates specific sensory inputs, such as what the
robot sees or hears, into specific actions, like shooting or moving away
from an object. According to Arkin, the very basic way to embed ethical
behavior into robots is to impose a set of constraints on the behavioral
mappings. The set of constraints, Arkin says, would be derived from laws
of war (including the principles of necessity, humanity,
proportionality, and discrimination), the rules of engagement, and any
other applicable rules for peace enforcement missions, depending on the
context. As a result, upon encountering a certain sensory input, the
robot will only be able to take an action that does not violate the
constraints programmed into its behavioral mappings. [The ultimate goal
is to ensure, through these constraints, that only actions complying
with laws of war and rules of engagement will occur.]{.underline} Arkin
presents four different architectural choices to achieve that goal. The
first is the "ethical governor," which reviews the robot's action prior
to its enactment. The second is an "ethical behavioral control," which
ensures that any action the robot can select is ethical in the first
place. In other words, the "ethical governor" will act as a reviewer
once the behavior is selected, while the "ethical behavioral control"
will act as a constraining principle prior to the selection of behavior.
The third is "the ethical adaptor," which reviews the robot's action
after the fact and updates the robot's ethical constraints accordingly.
The fourth and final component is the "responsibility advisor," which
makes it possible to assign responsibility to a human agent when the
robot acts in an unethical way. Arkin believes that [by using a
combination of the above architectural designs, it is possible to strike
a balance between the robot's ability to execute missions effectively
and absolute compliance with laws of war.]{.underline} To facilitate
this result, Arkin presents a basic protocol that a robot will have to
follow: (i) prior to engagement, confirm that specific people have
accepted responsibility for the robot's actions; (ii) ensure that the
mission at hand complies with the principle of necessity; (iii) maximize
discrimination between enemy combatants and non-combatants; and (iv) use
the minimum force required. According to Arkin, [by following this
protocol and ensuring]{.underline} that all other constraints derived
from rules of engagement and [laws of war are programmed into the robot,
"ethical" military robots will be able to avoid atrocities and sustain
fewer non-combatant casualties than human soldiers.]{.underline}

**Autonomous AI systems reduce civilian casualties -- they are better
able to distinguish innocent civilians, and they are less likely to
commit civilian atrocities.**

**Arkin, 2008 -- the Mobile Robot Laboratory at Georgia Institute of
Technology** \[Ronald "Technical Report GIT-GVU-07-11 Governing Lethal
Behavior: Embedding Ethics in a Hybrid Deliberative/Reactive Robot
Architecture" <https://dl.acm.org/doi/abs/10.1145/1349822.1349839> Acc
12/27/20 TA\]

This is no simple task however. [In the fog of war it is hard enough for
a human to be able to effectively discriminate whether or not a target
is legitimate. Fortunately]{.underline} for a variety of reasons, it may
be anticipated, despite the current state of the art, that i[n the
future autonomous robots may be able to perform better than humans under
these conditions, for the following reasons:]{.underline} [1. The
ability to act conservatively]{.underline}: i.e., [they do not need to
protect themselves in cases of low certainty of target
identification.]{.underline} UxVs do not need to have self-preservation
as a foremost drive, if at all. They can be used in a self-sacrificing
manner if needed and appropriate without reservation by a commanding
officer, [2. The eventual development and use of a broad range of
robotic sensors better equipped for battlefield observations than
humans' currently possess]{.underline}. [3. They can be designed without
emotions that cloud their judgment or result in anger and
frustration]{.underline} with ongoing battlefield events. In addition,
"Fear and hysteria are always latent in combat, often real, and they
press us toward fearful measures and criminal behavior" \[Walzer 77, p.
251\]. Autonomous agents need not suffer similarly. [4. Avoidance of the
human psychological problem of "scenario fulfillment" is
possible,]{.underline} a factor believed partly contributing to the
downing of an Iranian Airliner by the USS Vincennes in 1988 \[Sagan
91[\]. This phenomena leads to distortion or neglect of contradictory
information in stressful situations,]{.underline} where humans use new
incoming information in ways that only fit their pre-existing belief
patterns, a form of premature cognitive closure. Robots need not be
vulnerable to such patterns of behavior. [5. They can integrate more
information from more sources far faster before responding with lethal
force than a human possibly could in real-time.]{.underline} This can
arise from multiple remote sensors and intelligence (including human)
sources, as part of the Army's network-centric warfare concept and the
concurrent development of the Global Information Grid. [6.]{.underline}
When working in a team of combined human soldiers and [autonomous
systems]{.underline}, they [have the potential capability of
independently and objectively monitoring ethical behavior in the
battlefield by all parties and reporting infractions that might be
observed. This presence alone might possibly lead to a reduction in
human ethical infractions]{.underline}.

**When ethical robots go to war, they place themselves in harms way,
rather than human soldiers. The proliferation of autonomous AI weapons
will dramatically decrease casualties of war.**

**Del Re, 2017 -- US Army Major** \[Amanda "Lethal Autonomous Weapons:
Take the Human Out of the Loop A paper submitted to the Faculty of the
US Naval War College in partial satisfaction of the requirements for the
Ethics of Emerging Military Technology Graduate Certificate. 16 June
2017 https://apps.dtic.mil/sti/citations/AD1041804 Acc 12/27/20 TA\]

[Lethal Autonomous Weapons]{.underline} (LAWS) should be employed by the
United States on the field of battle. LAWS [will save lives because they
are potentially more proficient on the battlefield than humans. There
will not be as many combat-related deaths or injuries which will result
in a healthier, more resilient military.]{.underline} It behooves the
United States to employ this emerging technology because other nations
already are. [As a superpower, the United States bears the burden of
setting the example in warfare]{.underline} and foreign policy. A Lethal
Autonomous Weapon is a robot that is designed to select and attack
military targets without direct intervention by a human operator. The
idea of not having a human operator is called "human-out-of-the-loop."
Autonomous weapons also have the capability of operating with a
"human-in-the-loop" (like a drone and drone operator) or a
"human-on-the-loop" in which a human operator supervises the targeting
process and can intervene at any time during the cycle. Currently, the
US employs both human-in-the-loop and human-on-the-loop weapons in
combat. Lethal autonomous Weapons are also called LAWS, LARS (lethal
autonomous robots), robotic weapons, or killer robots. For these
purposes, LAWS, robots, or lethal autonomous weapon will be used. This
study will present a somewhat unrepresented argument, that LAWS should
be developed and employed by the United States on the battlefield. There
are several premises to support this conclusion. First, humans are
overall inferior on the battlefield as compared to robots. Historically
humans deal poorly with the traumatic effects of combat resulting in war
atrocities, posttraumatic stress disorder, increased veteran suicide and
homelessness; robots would not be negatively affected by combat like
humans are. Additionally, human soldiers are more expensive in the long
run than robots. 7 Second, the United States needs to stay on the
cutting edge of technology especially in warfare. History provides
examples of the United States using ethically questionable strategies in
war such as unrestricted submarine warfare and strategic bombing, both
in WWII, without having had the chance to fully examine the potential
ramifications of those strategies prior to the heat of conflict.
Moreover, other nations are already employing LAWS. The United States
needs to lead the development of these weapons in terms of technological
capabilities and ethical standards so that an international agreement
can be achieved before they are misused by another nation.

**Advanced autonomous weapons would be More [Ethical]{.underline} than
human soldiers -- human flaws make war atrocities more likely.**

**Arkin, 2008 -- the Mobile Robot Laboratory at Georgia Institute of
Technology** \[Ronald "Technical Report GIT-GVU-07-11 Governing Lethal
Behavior: Embedding Ethics in a Hybrid Deliberative/Reactive Robot
Architecture" <https://dl.acm.org/doi/abs/10.1145/1349822.1349839> Acc
12/27/20 TA\]

[It is not my belief that an unmanned system will be able to be
perfectly ethical in the battlefield, but I am convinced that they can
perform more ethically than human soldiers]{.underline} are capable of.
Unfortunately [the trends in human behavior in the battlefield regarding
adhering to legal and ethical requirements are questionable at
best]{.underline}. A recent report from the Surgeon General's Office
\[Surgeon General 06\] assessing the battlefield ethics of soldiers and
marines deployed in Operation Iraqi Freedom is disconcerting. The
following findings are taken directly from that report: 1.
[Approximately 10% of Soldiers]{.underline} and Marines [report
mistreating noncombatants]{.underline} (damaged/destroyed Iraqi property
when not necessary or hit/kicked a noncombatant when not necessary).
Soldiers that have high levels of anger, experience high levels of
combat or those who screened positive for a mental health problem were
nearly twice as likely to mistreat non-combatants as those who had low
levels of anger or combat or screened negative for a mental health
problem. 2. [Only 47% of Soldiers]{.underline} and 38% of Marines
[agreed that noncombatants should be treated with dignity]{.underline}
and respect. 3. [Well over a third of Soldiers]{.underline} and Marines
[reported torture should be]{.underline} allowed, whether to save the
life of a fellow Soldier or Marine or to obtain important information
about insurgents. 4. [17% of Soldiers]{.underline} and Marines agreed or
strongly [agreed that all noncombatants should be treated as
insurgents]{.underline}. 5. Just under 10% of soldiers and marines
reported that their unit modifies the ROE to accomplish the mission. 6.
45% of Soldiers and 60% of Marines did not agree that they would report
a fellow soldier/marine if he had injured or killed an innocent
noncombatant. 7. Only 43% of Soldiers and 30% of Marines agreed they
would report a unit member for unnecessarily damaging or destroying
private property. 8. [Less than half of Soldiers and Marines would
report a team member for an unethical behavior.]{.underline} 9. A third
of Marines and over a quarter of Soldiers did not agree that their NCOs
and Officers made it clear not to mistreat noncombatants. 10. Although
they reported receiving ethical training, 28% of Soldiers and 31% of
Marines reported facing ethical situations in which they did not know
how to respond. 11. Soldiers and Marines are more likely to report
engaging in the mistreatment of Iraqi noncombatants when they are angry,
and are twice as likely to engage in unethical behavior in the
battlefield than when they have low levels of anger. 12. [Combat
experience, particularly losing a team member, was related to an
increase in ethical violations]{.underline}.

**[\--Extend - Civilian Links]{.underline}**

**The Affirmative focuses on distinguishing Autonomous from
Non-autonomous weapons is counterproductive - it prevents us from
focusing on how to use them effectively and responsibly.**

**Noyes, 2019 - Major, US Army Reserve** \[Matthew MPP, Harvard Kennedy
School 14-06-2019 "Autonomous Weapons: The Future Behind Us"
https://apps.dtic.mil/sti/pdfs/AD1085435.pdf Acc 12/27/20 TA\]

[Militaries have long used weapons with autonomous capabilities and are
likely to increasingly use autonomous weapons as their effectiveness is
demonstrated.]{.underline} Autonomous weapons predate remotely operated
weapons in most domains. The air domain is instructive in this regard,
as the invention of television in the 1930s disrupted prior work on more
autonomous aircraft by allowing for the remote operation of aircraft out
of sight from the pilot. Remotely operated systems are likely to
continue to be generally preferred by militaries, but those systems will
have increasingly autonomous capabilities, particularly in targeting
enemy systems that disrupt their command and control links. This
suggests [rather than viewing "autonomous weapons" as a distinct class
of systems, instead we should consider the autonomous capabilities of a
weapon and how it is controlled by its user. Correctly considering
autonomy in weapons begins with recognizing weapons are
tools,]{.underline} and like all tools have a user. "Full autonomy," in
the sense of absolute independence, is not a desirable property of a
tool. Rather, users will maintain some control over their tools to
ensure they are accomplishing their desired purpose. [Instead of trying
to distinguish between what is autonomous and what is not, we should
focus on the relationship between the user and the tool.]{.underline} To
paraphrase David Mindell: "Where are the \[users\]? Who are they? What
are they doing? When? Why does it matter?"152 [Unencumbered with trying
to distinguish between what is autonomous and non-autonomous allows
analysis to instead focus on autonomy in executing particular
functions.]{.underline} Recognizing militaries have long used autonomous
weapons makes predicting their future use a matter of identifying the
capabilities militaries have sought to achieve through autonomous
weapons in the past, and how technology is changing what is possible.
Considering this history, autonomous weapons have predominately been
used to address at least one of four challenges: the mundane, the fast,
the denied, and mass. [Autonomous weapons aid militaries in
addressing]{.underline} the mundane, or [long duration, by providing
greater persistence in observing and reacting to events over an extended
period]{.underline} than is efficiently achieved with a manned system.
In terms of both reaction speed and at high accelerations, [autonomous
weapons can operate at speeds exceeding what people are capable of. A
particularly attractive feature of autonomous weapons is the ability to
operate in denied environment,]{.underline} either due to environmental
conditions or enemy action, that present unacceptable risk to manned
systems or to the control channels for remotely operated systems.
Finally, [autonomous weapons provide increased mass, like most any
military technology, by increasing the military power of a
state]{.underline} relative to its available population for military
service.

**Banning autonomous weapons prevents us from focusing on ways to use
them to Counter uncontrolled human deaths.**

**Noyes, 2019 - Major, US Army Reserve** \[Matthew MPP, Harvard Kennedy
School 14-06-2019 "Autonomous Weapons: The Future Behind Us"
https://apps.dtic.mil/sti/pdfs/AD1085435.pdf Acc 12/27/20 TA\]

The normative and ethical considerations related to autonomous weapons
are an area that needs further research. [Existing literature on the
ethics of autonomous weapons tends to describe them as a potential
future capability not an existing capability, which may create a status
quo bias in arguing against autonomous weapons]{.underline}. It also
appears a substantial element in some moral arguments against autonomous
weapons is a reaction to a challenge in what it means to be human.
[There seems to be a deeply held moral intuition that humans ought to
have control over killing other humans. Autonomous weapons may be a key
means for providing for that control, for example by targeting systems
to disrupt command and control channels. If autonomous weapons are seen
not as a means to kill humans, but as a means to target adversarial
military systems that undermine control over who is killed, I suspect it
fundamentally alters normative perspectives.]{.underline} [It also
suggests rather than seeking to ban autonomous weapons, to seek
banning]{.underline} electronic warfare and other [systems that erode
control over the use of force.]{.underline}

**[\--Extend -- Soldier Links]{.underline}**

**Even when human soldiers are not killed in combat, they live with the
trauma of war for the rest of their lives, fueling an epidemic of
suicide.**

**Del Re, 2017 -- US Army Major** \[Amanda "Lethal Autonomous Weapons:
Take the Human Out of the Loop A paper submitted to the Faculty of the
US Naval War College in partial satisfaction of the requirements for the
Ethics of Emerging Military Technology Graduate Certificate. 16 June
2017 https://apps.dtic.mil/sti/citations/AD1041804 Acc 12/27/20 TA\]

The economic costs of losing soldiers and wounded soldiers pales in
comparison to the societal costs. [The psychological consequences of war
are often costlier than the physical costs. A 2016 Veteran's Affairs
study found that 20 veterans commit suicide a day. The risk of suicide
for veterans is 21 percent higher when compared to civilian
adults]{.underline}. The civilian suicide rate rose 23.3% between 2001
and 2014 while the veteran rate rose 32%. More disturbing is the female
veteran suicide rate which rose 85% during that time compared to their
civilian counterparts which rose 40%.42 [Suicide rates are at all an
all-time high. It is an epidemic. This human cost could be alleviated by
substituting robots for many of the combat roles]{.underline} now filled
by their inherently fragile human counterparts. Posttraumatic Stress
Disorder (PTSD) is also more common in the military as compared to the
civilian population. [According to a recent Veteran Affairs study,
approximately 7-8% of the population will have PTSD]{.underline} at some
point in their lives. The veteran rate is higher and varies by war.
Approximately 30% of Vietnam veterans were diagnosed with PTSD according
to the National Vietnam Veterans Readjustment Study (NVVRS).43
Approximately 20% of Operation Iraqi Freedom and Operation Enduring
Freedom veterans have been diagnosed per year.44 The latter number will
most likely rise as more cases are diagnosed. PTSD can and does affect a
service member's ability to do their job and can lead to a service
member to be medically retired. [The societal burden of warfare is
costly and will continue to rise if the nation continues to rely heavily
on large numbers of troops]{.underline}. A recent study claimed that
veterans are 50% more likely to become homeless as compared to
non-veterans. The study stated that the causes were due to: "poverty,
lack of support networks, and dismal living conditions in overcrowded or
substandard housing."45 While those reasons are the immediate causes for
homelessness they are indicative of addressing a symptom and not the
actual problem. Veterans also struggle with the fact that after their
uniformed service many do not have translatable skills that can be used
in a non-wartime environment. Using robots would require soldiers to be
able to troubleshoot and maintain the robots resulting in skills that
are desired by many high-tech corporations. Additionally, it would
decrease the number of soldiers needed for jobs that are not as
applicable in a non-wartime environment such as route clearance,
explosive ordnance disposal, and base security. In conclusion, humans
are vulnerable to the detrimental effects of warfare while robots are
not. The physiological effects of combat trauma may render a human
physically and psychologically unable to make sound decisions. If
employed in battle, robots will not be as unethical as humans have the
potential to be. Humans are emotional, desire revenge, and are prone to
commit atrocities as a result. Human service members are also expensive,
especially when they are killed or wounded. The psychological impacts of
war are unavoidable and destructive. Robots have no need for revenge,
are unemotional, and are less expensive than human soldiers. [Using
robots on the battlefield will reduce economic and societal costs and
save lives.]{.underline}

**[\--Extend - Impacts]{.underline}**

**Human soldiers make terrible battlefield decisions -- they are
overloaded with stress, trauma, and revenge.**

**Arkin, 2013 -- prof at Georgia Tech** \[Ronald "Lethal Autonomous
Systems and the Plight of the Non-combatant" AISB Quarterly, No. 137,
July https://smartech.gatech.edu/handle/1853/50079?show=full Acc
12/27/20 TA\]

It must be noted that past and present trends in [human behavior in the
battlefield regarding adhering to legal and ethical requirements are
questionable at best. Unfortunately, humanity has a rather dismal record
in ethical behavior in the battlefield]{.underline}. Potential
[explanations for the persistence of war crimes include: high friendly
losses leading to a tendency to seek revenge;]{.underline} high turnover
in the chain of command leading to weakened leadership; [dehumanisation
of the enemy through the use of derogatory names and
epithets;]{.underline} poorly trained or inexperienced troops; no
clearly defined enemy; unclear orders where intent of the order may be
interpreted incorrectly as unlawful; youth and immaturity of troops;
[external pressure,]{.underline} e.g., [for a need to produce a high
body count of the enemy; and pleasure from power of killing]{.underline}
or an overwhelming sense of frustration. There is clear room for
improvement and [autonomous systems may help address some of these
problems]{.underline}. [Robotics technology, suitably deployed may
assist with the plight of the innocent noncombatant caught in the
battlefield]{.underline}. If used without suitable precautions, however,
it could potentially exacerbate the already existing violations by human
soldiers. While I have the utmost respect for our young men and women
warfighters, they are placed into conditions in modern warfare under
which no human being was ever designed to function. In such a context,
expecting a strict adherence to the Laws of War (LOW) seems unreasonable
and unattainable by a significant number of soldiers6. Battlefield
atrocities have been present since the beginnings of warfare, and
despite the introduction of International Humanitarian Law (IHL) over
the last 150 years or so, these tendencies persist and are well
documented,7 even more so in the days of CNN and the Internet. 'Armies,
armed groups, political and religious movements have been killing
civilians since time immemorial.'8 'Atrocity. . . is the most repulsive
aspect of war, and that which resides within man and permits him to
perform these acts is the most repulsive aspect of mankind'.9 The
dangers of abuse of unmanned robotic systems in war, such as the
Predator and Reaper drones, are well documented; they occur even when a
human operator is directly in charge.10 Given this, questions then arise
regarding if and how these [new robotic systems c]{.underline}an conform
as well as, or better than, our soldiers with respect to adherence to
the existing IHL. If achievable, this [would result in a reduction in
collateral damage]{.underline}, i.e., [noncombatant casualties
and]{.underline} damage to civilian property, which translates into
[saving innocent lives. If achievable this could result in a moral
requirement necessitating the use of these systems]{.underline}.

**Autonomous AI will reduce civilian deaths by reducing the impact of
human error.**

**Arkin, 2013 -- prof at Georgia Tech** \[Ronald "Lethal Autonomous
Systems and the Plight of the Non-combatant" AISB Quarterly, No. 137,
July https://smartech.gatech.edu/handle/1853/50079?show=full Acc
12/27/20 TA\]

Is there any cause for optimism that [this form of technology can lead
to a reduction in non-combatant deaths]{.underline} and casualties? I
believe so, [for the following reasons.]{.underline} -- [The ability to
act conservatively: i.e., they do not need to protect themselves in
cases of low certainty of target identification. Autonomous armed
robotic vehicles do not need to have self-preservation as a foremost
drive]{.underline}, if at all. They can be used in a selfsacrificing
manner if needed and appropriate without reservation by a commanding
officer. [There is no need for a 'shoot first, ask-questions later'
approach, but rather a 'first-do-no-harm' strategy can be utilized
instead]{.underline}. They can truly assume risk on behalf of the
noncombatant, something that soldiers are schooled in, but which some
have difficulty achieving in practice. [-- The eventual development and
use of a broad range of robotic sensors better equipped for battlefield
observations than humans currently possess.]{.underline} This includes
ongoing technological advances in electro-optics, synthetic aperture or
wall penetrating radars, acoustics, and seismic sensing, to name but a
few. There is reason to believe in the future that robotic systems will
be able to pierce the fog of war more effectively than humans ever
could. [-- Unmanned robotic systems can be designed without emotions
that cloud their judgment or result in anger and frustration with
ongoing battlefield events.]{.underline} In addition, 'Fear and hysteria
are always latent in combat, often real, and they press us toward
fearful measures and criminal behavior'13. Autonomous agents need not
suffer similarly. [-- Avoidance of the human psychological problem of
'scenario fulfilment' is possible.]{.underline} This phenomenon leads to
distortion or neglect of contradictory information in stressful
situations, where humans use new incoming information in ways that only
fit their pre-existing belief patterns. Robots need not be vulnerable to
such patterns of premature cognitive closure. Such failings are believed
to have led to the downing of an Iranian airliner by the USS Vincennes
in 1988.14 [-- Intelligent electronic systems can integrate more
information from more sources far faster before responding with lethal
force than a human possibly could in real-time.]{.underline} These data
can arise from multiple remote sensors and intelligence (including
human) sources, as part of the Army's network-centric warfare concept
and the concurrent development of the Global Information Grid. 'Military
systems (including weapons) now on the horizon will be too fast, too
small, too numerous and will create an environment too complex for
humans to direct'15. -- [When working in a team of combined human
soldiers and autonomous systems]{.underline} as an organic asset, they
have the potential capability of independently and objectively
monitoring ethical behavior in the battlefield by all parties, providing
evidence and reporting infractions that might be observed. [This
presence alone might possibly lead to a reduction in human ethical
infractions.]{.underline}

**Even if LAWs don't make Perfect choices, they are Much better than
humans, who face Six obstacles to decision making in combat.**

**Arkin, 2008 -- the Mobile Robot Laboratory at Georgia Institute of
Technology** \[Ronald "Technical Report GIT-GVU-07-11 Governing Lethal
Behavior: Embedding Ethics in a Hybrid Deliberative/Reactive Robot
Architecture" <https://dl.acm.org/doi/abs/10.1145/1349822.1349839> Acc
12/27/20 TA\]

[It is not my belief that an unmanned system will be able to be
perfectly ethical in the battlefield, but I am convinced that they can
perform more ethically than human soldiers]{.underline} are capable of.
Unfortunately [the trends in human behavior in the battlefield regarding
adhering to legal and ethical requirements are questionable at
best]{.underline}. A recent report from the Surgeon General's Office
\[Surgeon General 06\] assessing the battlefield ethics of soldiers and
marines deployed in Operation Iraqi Freedom is disconcerting. The
following findings are taken directly from that report: 1.
[Approximately 10% of Soldiers]{.underline} and Marines [report
mistreating noncombatants]{.underline} (damaged/destroyed Iraqi property
when not necessary or hit/kicked a noncombatant when not necessary).
Soldiers that have high levels of anger, experience high levels of
combat or those who screened positive for a mental health problem were
nearly twice as likely to mistreat non-combatants as those who had low
levels of anger or combat or screened negative for a mental health
problem. 2. [Only 47% of Soldiers]{.underline} and 38% of Marines
[agreed that noncombatants should be treated with dignity]{.underline}
and respect. 3. [Well over a third of Soldiers]{.underline} and Marines
[reported torture should be]{.underline} allowed, whether to save the
life of a fellow Soldier or Marine or to obtain important information
about insurgents. 4. [17% of Soldiers]{.underline} and Marines agreed or
strongly [agreed that all noncombatants should be treated as
insurgents]{.underline}. 5. Just under 10% of soldiers and marines
reported that their unit modifies the ROE to accomplish the mission. 6.
45% of Soldiers and 60% of Marines did not agree that they would report
a fellow soldier/marine if he had injured or killed an innocent
noncombatant. 7. Only 43% of Soldiers and 30% of Marines agreed they
would report a unit member for unnecessarily damaging or destroying
private property. 8. [Less than half of Soldiers and Marines would
report a team member for an unethical behavior.]{.underline} 9. A third
of Marines and over a quarter of Soldiers did not agree that their NCOs
and Officers made it clear not to mistreat noncombatants. 10. Although
they reported receiving ethical training, 28% of Soldiers and 31% of
Marines reported facing ethical situations in which they did not know
how to respond. 11. Soldiers and Marines are more likely to report
engaging in the mistreatment of Iraqi noncombatants when they are angry,
and are twice as likely to engage in unethical behavior in the
battlefield than when they have low levels of anger. 12. [Combat
experience, particularly losing a team member, was related to an
increase in ethical violations]{.underline}.

**[\--Extend - Turns Ethics]{.underline}**

**AI's ability to avoid human emotions is what makes it more ethical --
autonomous weapons are able to avoid the atrocities that come from human
psychological reactions**

**Del Re, 2017 -- US Army Major** \[Amanda "Lethal Autonomous Weapons:
Take the Human Out of the Loop A paper submitted to the Faculty of the
US Naval War College in partial satisfaction of the requirements for the
Ethics of Emerging Military Technology Graduate Certificate. 16 June
2017 https://apps.dtic.mil/sti/citations/AD1041804 Acc 12/27/20 TA\]

[Humans are high-maintenance, flawed, and deal poorly with the horrific
effects of war. Robots can be far more proficient at warfare.
Humans]{.underline} after all, are human. They [make mistakes often and
emotion clouds their judgment. Humans require a great deal of
maintenance]{.underline} to be at peak performance. [Humans fall prey to
the horrors of war, often resulting in atrocities and long-term physical
and psychological damage.]{.underline} Human troops are expensive and,
in the long run, more expensive than robots. [Robots are unemotional,
cheaper, and not susceptible to committing an atrocity for
self-preservation or revenge.]{.underline} This chapter examines why
humans are inferior warriors and why [robots may be]{.underline}
superior and [more ethical combatants]{.underline}. The stresses of
combat affect judgment. Consider a few personal accounts of dealing with
combat stress. James R. McDonough, a Platoon Leader in Vietnam,
describes his uncontrollable emotions after surviving his first
firefight: "My emotions were breaking through the fatigue that had
numbed them, and my mood shifted dramatically from one extreme to
another."2 McDonough is describing the effects of shock after trauma. He
initially felt detached or "numbed," and in the aftermath of battle his
erratic emotions are irrepressible. Napoleon had described that the most
dangerous point in battle is immediately after victory because that is
when the soldier is most vulnerable to counterattack.3 Although Napoleon
did not realize it at the time, he was describing "parasympathetic
backlash" which occurs immediately after the attack "has halted and the
soldier briefly believes himself to be safe."4 During this
parasympathetic backlash, a soldier becomes "physiologically and
psychologically incapacitated" or, in McDonough's case, "numbed."5 Karl
Marlantes, also a Platoon Leader in Vietnam, echoes this theory when he
describes his experience in war as "the predominant feeling when you win
in battle, is numbed exhaustion."6 [Critics of autonomous weapons state
that humans possess judgment, something that a robot never could.
However, the concept of the parasympathetic backlash undermines reliance
on human judgment since humans may be unable to control their basic
functions and emotions during and even some duration after the trauma of
battle]{.underline}.

**If AI can reduce civilian deaths, then it is the most ethical option**

**Del Re, 2017 -- US Army Major** \[Amanda "Lethal Autonomous Weapons:
Take the Human Out of the Loop A paper submitted to the Faculty of the
US Naval War College in partial satisfaction of the requirements for the
Ethics of Emerging Military Technology Graduate Certificate. 16 June
2017 https://apps.dtic.mil/sti/citations/AD1041804 Acc 12/27/20 TA\]

The human body, like a robot, is a complex system of systems. Yet
[unlike their robot counterparts, human shortcomings are magnified in
combat when physiological and cognitive systems are stressed. Robots'
have no sympathetic nervous system susceptible to rapid changes in heart
rate, blood pressure, and blood glucose levels during
combat]{.underline}. Unencumbered by a sympathetic nervous system,
[robots are not susceptible to the "parasympathetic backlash;" they will
not be "numbed" or "stunned" after battle. Rather, robots will remain
consistent and calm during and after all combat
engagements]{.underline}. Moreover, [the heat of battle will not disrupt
a robot's decision-making abilities, but it will disrupt a human's which
could lead to errors in judgment and ethical mistakes.]{.underline}
Humans have been struggling with ethical conduct throughout the long
history of warfare. [Robots may help solve some of the ethical problems
that emerge in warfare and may be the answer to some of the ethical
questions that warfare asks]{.underline}. Consider several examples from
the history of warfare in which human emotion drove an unethical
decision and had dire consequences.

**[\]{.underline}**

**[AT NATO Key]{.underline}**

**NATO is not key -- its diversity and complexity have are not well
analyzed**

**Trabucco and Stanley-Lockman, 2022 -- prof of Political Science,
University of Copenhagen and prof of Defense and Strategic Studies,
Nanyang Technological University** \[Lena and Zoe, The Oxford Handbook
of AI Governance, March, "NATO's Role in Responsible AI Governance in
Military Affairs"
https://www.oxfordhandbooks.com/view/10.1093/oxfordhb/9780197579329.001.0001/oxfordhb-9780197579329-e-69
Acc 4/16/22 TA\]

To be sure, [NATO is far from the only institution that impacts military
AI governance and its security implications. Indeed, international
technology governance is inherently complex because it includes diverse
stakeholders in a system of "organizations, regimes, and other forms of
principles, norms, regulations, and decision-making procedures" with a
shared interest and responsibility in a given issue-area of world
politics]{.underline}.9 Existing discussions of the impact of AI on
international security have looked to nation-states, regional
institutions like the European Union (EU), or international bodies like
the United Nations Convention on Certain Conventional Weapons (UN CCW)
for discussions on the military governance of AI.10 Without expanding on
the role of these other stakeholders, this chapter begins to explore
pressing questions for NATO and international relations scholars that
illustrate [NATO's role in AI governance]{.underline}, which [has not
had a comprehensive analysis]{.underline}.11

**NATO is not a leader in AI technology -- it has to coordinate as a
group, and not every nation is committed to AI**

**Tucker, 2018 - Technology Editor at Defense One** \[Patrick, May 18,
"How NATO's Transformation Chief Is Pushing the Alliance to Keep Up in
AI"
https://www.defenseone.com/technology/2018/05/how-natos-transformation-chief-pushing-alliance-keep-ai/148301/
Acc 4/22/22 TA\]

[The United States, Russia, and China]{.underline} --- in varying ways
--- [have described artificial intelligence as central to the way they
intend to develop weapons and fight]{.underline}. In September, Vladimir
Putin famously said that the leading player in AI will rule the world.
[China is sprinting forward with a massive plan to become the world's
center for AI innovation by 2030.]{.underline} NATO is rushing to keep
up, said Mercier. But [as an organization of nations --- as various in
wealth and technological capability as they are in language and culture
--- the alliance can't move as quickly as individual great powers to
adopt emerging technologies and integrate them into operations. Some
NATO members,]{.underline} such as Estonia, [punch well above their
weight in AI]{.underline} and robotics. [Other NATO member states...
less so.]{.underline}

**[1NC - OSCE CP]{.underline}**

**CP Text - The Organization for Security and Cooperation in Europe
should adopt ethical principles to ensure human control in military
artificial intelligence systems.**

**Solvency -- The OSCE can establish norms on AI ethics. A declaration
would promote international norms due to its history of working across
the East West gap and its reputation for building trust.**

**Nadibaidze, 2021 - Ph.D. Student at the University of Southern
Denmark** \[Anna Entry submitted for the Second OSCE-IFSH Essay
Competition "Commitment to Control over Weaponised Artificial
Intelligence: A Step Forward for the OSCE and European Security"
https://www.osce.org/files/f/documents/8/3/507341.pdf Acc. 4/21/22 TA\]

In sum, [a political declaration should contain a commitment to keeping
human control over existant AI-driven weapons systems. Agreeing to
enshrine such a commitment]{.underline} in a document with a soft law
nature [would be a step forward not only towards addressing regional
security threats, but also towards an international framework on
weaponised AI.]{.underline} As highlighted in this section, [the history
and membership of the OSCE make it the organisation to built trust and
take a step forward on weaponised AI, at a moment when global discussion
at the UN is stalling and when operational trends continue to increase
autonomy.]{.underline} Conclusion This essay has argued that current
practices related to the use of weaponised AI already impact European
stability and security. Operational trends are diminishing the level of
human control over weapons with increasingly autonomous features in
their critical functions, which poses significant legal, ethical and
security risks. Moreover, the lack of definition of LAWS and a common
conception of an appropriate level of human control among participating
States creates uncertainty and potential for misinterpretation. At the
same time, the trajectory of AI is not set to be an 'arms race', as
"arms races are not inevitable, but can be managed, channeled or even
stopped" (Maas 2019, 303). States can intervene in a variety of
political ways to address the impacts of existing weaponised AI. Finding
a common agreement is a challenging, but not impossible task. In this
essay, I have argued that [the OSCE is a promising platform to build
upon the stalled discussions at the CCW. This institution has a history
of acting as a bridge between Eastern and Western perspectives of
European security. It is an inclusive organisation which brings together
not only the Euro-Atlantic Community, but also the Russian Federation,
one of the key developers of weaponised AI and players in European
security. By debating the issue of weaponised AI at the OSCE and
agreeing on a political declaration with commitment to human control,
participating States will address the risks of autonomy in weapons
systems]{.underline}, as well demonstrate as the relevance of the OSCE
in tackling the impact of new technologies and their use in conventional
weapons.

**The OSCE solves crisis miscalculation by sharing information and
increasing confidence**

**Egel, 2021 - Nuclear Security Visiting Fellow at the Truman Center for
National Policy** \[Naomi, PhD candidate at Cornell, January, Geneva
Centre for Security Policy Strategic Security Analysis" Issue 16,
"Reducing Military Risks through OSCE Instruments: The Untapped
Potential in the European Arms Control Framework"
https://www.gcsp.ch/publications/reducing-military-risks-through-osce-instruments-the-untapped-potential-in-the-European-arms-control-framework
Acc 2/27/22 TA\]

[Information about the types of AWS that states employ, the aspects of
autonomy and human control in AWS, the intended uses of AWS, and
demonstrations of AWS use would together provide greater information
regarding the scope of AWS deployment and use in military
operations]{.underline}. [They would also strengthen confidence that AWS
are being designed and deployed in ways that aim to avoid inadvertent
conflict escalation]{.underline}. Although these CBMs would not fully
eliminate the military risks posed by AWS (including the risks posed by
AWS malfunctions), [they could help to reduce misunderstandings and
thereby moderate the responses by other states]{.underline}.
Additionally, [such CBMs could be applied to future AWS]{.underline}.
Much of the concern over the risks posed by AWS is centred on
cutting-edge and future systems of this kind. Although the exact nature
of future AWS cannot be predicted, CBMs that are designed to reduce the
risks posed by various elements of autonomy would be useful for
[reducing the risks posed by both existing and future AWS.]{.underline}
Given the ever-increasing integration of autonomy into weapons systems,
these AWS CBMs could take the form of an addendum to the Vienna
Document, which covers all conventional weapons systems. [The CBMs
proposed here build on precedents in the Vienna Document, and thus
amending the Vienna Document to address risks arising from AWS would be
a logical progression that would be acceptable to states.]{.underline}
Alternatively, AWS CBMs could be agreed as a stand-alone set of CBMs, in
the style of the OSCE's CBMs for Information and Communication
Technologies. Regarding the institutional format used to discuss,
negotiate and develop AWS CBMs, the OSCE Structured Dialogue
(established in 2016) could provide a dedicated venue for negotiating
such CBMs. An alternate approach would be for the OSCE to set up a new
venue for such negotiations.

**The OSCE solves better than NATO because of its inclusive membership.
Russia will only participate if NATO is not involved, due to their
adversarial relationship.**

**Nadibaidze, 2021 - Ph.D. Student at the University of Southern
Denmark** \[Anna Entry submitted for the Second OSCE-IFSH Essay
Competition "Commitment to Control over Weaponised Artificial
Intelligence: A Step Forward for the OSCE and European Security"
https://www.osce.org/files/f/documents/8/3/507341.pdf Acc. 4/21/22 TA\]

[Other international institutions have shown their ambitions in AI
regulation.]{.underline} In April 2021, the European Commission
presented its legal framework proposal, which, while not touching upon
security and defence, could set a path forward towards a regional
approach to governing weaponised AI (European Commission 2021).1 [NATO
is also due to present an AI strategy and set forward its "principles of
responsible use of AI in defence"]{.underline} (NATO Newsroom 2021).
[The key difference is that these institutions have favored exclusive
membership,]{.underline} where prospective countries need to fulfill
specific conditions to join. Meanwhile, the OSCE has relatively easy
accession rules, as it was initially based on the concept of
geopolitical diversity (Pourchot 2011, 183). [Crucially, the OSCE
includes not only the Euro-Atlantic community, but also other major
security actors, notably the Russian Federation. Settling the
differences and misunderstandings between Russia and the US is a key
step in achieving a security agreement such as a commitment to human
control over weaponised AI.]{.underline} In recent years, [the Russian
discourse has shown a disappointment towards Western countries and their
making NATO as the main European security organisation]{.underline}
(Kropatcheva 2012, 386). [By engaging with Russia upon the issue of
weaponised AI within the OSCE framework, the US and the EU would
contribute to easing the tensions, while also diminishing the chances of
misunderstanding and misinterpretation which could lead to severe
security risks]{.underline}, as outlined in the first section. [The
OSCE's inclusive membership is thus a valuable advantage when it comes
to building trust and mitigating the security implications of emerging
technologies]{.underline} (Dunay 2006, 25).

**[\--Extend - OSCE Solves Best]{.underline}**

**The OSCE solves best due to its inclusive membership -- that is best
for building trust and cooperation with Russia.**

**Nadibaidze, 2021 - Ph.D. Student at the University of Southern
Denmark** \[Anna Entry submitted for the Second OSCE-IFSH Essay
Competition "Commitment to Control over Weaponised Artificial
Intelligence: A Step Forward for the OSCE and European Security"
https://www.osce.org/files/f/documents/8/3/507341.pdf Acc. 4/21/22 TA\]

Strengthening Human Control: The Role of the OSCE [Arriving to a common
position on weaponised AI regulation will be challenging]{.underline}.
The current global and European political atmosphere is one of distrust.
There is distrust between two major European security players: Russia
and the US. In June 2021, both President Joe Biden and President
Vladimir Putin said that the bilateral relationship "has deteriorated to
its lowest point in recent years" (Walters 2021). There is also distrust
among state leaders towards technologies such as the Internet, AI, 5G
and robotics, not least because they can be weaponised and used for
threatening activities such as cyberattacks. This environment makes it
difficult to find a common understanding and commit to principles on the
use of weaponised AI. [Nevertheless,]{.underline} in this section I
argue that [the OSCE possesses some key advantages to become the
platform for making a step forward in the global debate.]{.underline} In
2019, the GGE on LAWS adopted a set of principles to guide the work of
the group in the next years, which were endorsed by the CCW High
Contracting Parties. These Guiding Principles are broad and have no
legally binding force. As pointed out by the Campaign to Stop Killer
Robots, the "CCW principles were simply intended to guide the
deliberations. They were never supposed to be an end in themselves or
intended to provide the structure for or outcome to CCW work on lethal
autonomous weapons systems" (2020, 1). Moreover, they do not provide
further clarifications on the concept of human control, only stating
that "human responsibility for decisions on the use of weapons systems
must be retained since accountability cannot be transferred to machines"
(Principle b). Thus, while member states accept in principle the
importance of human control, they have until now not been able to agree
on a common definition of this concept, which is stalling the progress
of the CCW debate (Bolton et al. 2021). Just like the CCW, [the
OSCE]{.underline} operates via consensus, which requires seeking a
compromise between participating States and can often hamper the
decision-making process. However, this institution [has been
historically known for its ambition to form an inclusive security
community and to build practices that "suggest a new model of
international security", described as "comprehensive", "indivisible",
and "cooperative"]{.underline} (Adler 1998, 119). The former Conference
on Security and Cooperation in Europe (CSCE) was a symbol of détente
between the US and the Soviet Union, and a place for two rivals to find
compromise on security issues, demonstrating the possibility of
coexistence on the European continent (Rittberger et al. 2012, 42). The
Helsinki Final Act negotiations were a cooperative process, with the
goal of promoting communication, as well as increasing confidence
between States (Sandole 2007, 65). The negotiations resulted in a
declaration of common norms and values of the participating States.
Following the Cold War, the OSCE was not only able to survive, but also
to change its goals and adapt to the rising security challenges of the
new world order. Its [broad membership and comprehensive approach
towards security make it a key, if not the most, legitimate institution
for European security]{.underline} (Mosser 2015, 584). [At a time
when]{.underline} some experts debate whether [Russia-US relations have
entered a 'new Cold War']{.underline} (Polyakova 2019), [the OSCE's
inclusive approach towards security is the one that is needed to show
that tensions can be dealt with in a forum, rather than in the
battlefield]{.underline}.

**The OSCE solves best because it bridges East West tensions and builds
consensus historically.**

**Nadibaidze, 2021 - Ph.D. Student at the University of Southern
Denmark** \[Anna Entry submitted for the Second OSCE-IFSH Essay
Competition "Commitment to Control over Weaponised Artificial
Intelligence: A Step Forward for the OSCE and European Security"
https://www.osce.org/files/f/documents/8/3/507341.pdf Acc. 4/21/22 TA\]

This essay examines the impacts of weaponised Artificial Intelligence
(AI) for European security and proposes solutions to mitigate the risks
caused by the lack of regulation of autonomy in weapons systems.
Studying how OSCE participating States use and talk about weaponised AI,
it demonstrates that [the diminishing human control over the use of
force and the differences in States' discourses pose a considerable risk
for regional stability.]{.underline} [At the same time]{.underline}, it
suggests that [the trajectory of the impact of AI is not inevitable,
and]{.underline} that [States should address this issue via political
means, specifically a political declaration with a commitment to human
control over the use of force.]{.underline} Finally, it argues that [due
to its inclusive membership, the OSCE can become the platform which can
build trust and consensus, two necessary elements to make a step forward
in the global debate on weaponised AI.]{.underline} Introduction Recent
technological and political developments in participating States of the
Organization for Security and Co-operation in Europe (OSCE) suggest a
strong interest to pursue, test and use weaponised Artificial
Intelligence (AI), specifically weapons systems with increasingly
autonomous features operating on the base of algorithms. In May 2021,
Defence Minister Sergei Shoigu announced that the Russian Federation has
begun producing "combat robots capable of fighting on their own" (TASS
2021). According to media reports, the French Land Army is planning to
introduce robots by 2040 (Barotte 2021). The UK Government stated its
objective of achieving "a leading role in critical and emerging
technologies" (HM Government 2021, 38) and has established a Defence
Artificial Intelligence and Autonomy Unit to better understand them
(Ministry of Defence 2020, 15). In the United States, the National
Security Commission on Artificial Intelligence (NSCAI) has urged the
government to "not be a witness to the AI revolution in military
affairs" and "deliver it with leadership from the top, new operating
concepts, relentless experimentation, and a system that rewards agility
and risk" (2021, 77). Nadibaidze 2 While the global discussion about
autonomy in weapons systems is often framed in a futuristic way and
focuses on fully lethal autonomous weapons systems (LAWS) --
colloquially called 'killer robots' -- or the 'AI arms race',
[weaponised AI is already a reality of European security]{.underline}.
[Due to its current wide-ranging impacts, this issue deserves the
attention of the OSCE and especially the military-political component of
its multidimensional approach to regional security]{.underline}.
Nevertheless, [so far participating States have been reluctant to
benefit from the OSCE platform to address the risks]{.underline} [caused
by increasing autonomy in weapons systems]{.underline}. Building upon
this puzzle, this essay intends to address the following questions: How
does the lack of regulation of weaponised AI affect security and
stability in Europe? What role can the OSCE play in mitigating the risks
related to weaponised AI? In the first section, I argue that the
international debate on weaponised AI should take existing weapons
systems as a starting point and highlight the impacts of practices
related to weaponised AI for European security and stability. By
analysing how weaponised AI is used and is talked about, I point to the
issues of diminishing human control over the use of force as well as the
uncertainty caused by the lack of a common definition of LAWS among OSCE
participating States, while examining the case studies of France, Russia
and the UK. Further, I argue that the trajectory of the impact of
weaponised AI for European security and stability is not set in stone
and that the current framing of the debate overestimates the agency of
AI and its military uses. Therefore, there is a possibility of changing
the trajectory of this impact by, as a first step, agreeing on basic
principles about responsible use of weaponised AI. In the second
section, I argue that, [given its large membership which includes the
Russian perspective, as well as its historical role as a
consensus-builder and a forum for bridging East-West tensions, the OSCE
has a key role to play in terms of re-structuring the global debate on
weaponised AI.]{.underline} The global political tensions between Russia
and the United States make an agreement on emerging technologies such as
AI more challenging, but not impossible. At a moment when the debates on
LAWS at the United Nations are stalling, [the OSCE can and should take a
step forward by building consensus on guiding principles, issuing a
political declaration with a commitment to human control over weapons
systems, and demonstrating that the impact of AI depends on how states
decide to use it]{.underline}.

**[\--Extend -- OSCE CBM Solvency]{.underline}**

**OSCE declarations are a confidence building measure -- OSCE CBMs are
more legitimate due to their broad membership. Empirically proven.**

**Egel, 2021 - Nuclear Security Visiting Fellow at the Truman Center for
National Policy** \[Naomi, PhD candidate at Cornell, January, Geneva
Centre for Security Policy Strategic Security Analysis" Issue 16,
"Reducing Military Risks through OSCE Instruments: The Untapped
Potential in the European Arms Control Framework"
https://www.gcsp.ch/publications/reducing-military-risks-through-osce-instruments-the-untapped-potential-in-the-European-arms-control-framework
Acc 2/27/22 TA\]

[CBMs are voluntary measures designed to communicate 'credible evidence
of the absence of feared threats by reducing uncertainties]{.underline}
and by constraining opportunities for exerting pressure through military
activity'. [Given the OSCE's membership and history, OSCE CBMs would
make a particularly significant contribution to reducing the risks posed
by AWS. The OSCE's membership is both broad]{.underline} (the
organisation has 57 participating States, encompassing not only all
European states, but Central Asian and North American states as well)
[and includes seven of the top ten arms producers]{.underline} in the
period 2015-2019. [OSCE CBMs, thus, have a legitimacy based on both the
breadth and number of states that subscribe to them and the involvement
of key arms producers. Among international organisations]{.underline}
(both regional and global), [the OSCE is distinctive for its history of
establishing strong norms for using CBMs as tools for risk
reduction]{.underline}. [AWS CBMs would build on the OSCE's robust
record of reducing military risk through CBMs]{.underline}. The Vienna
Document - the cornerstone of OSCE CBMs - has provided a strong
framework for building confidence among OSCE participating states,
reducing military risks and increasing security in the OSCE region.
Other OSCE instruments like the Conventional Forces in Europe Treaty and
the Open Skies Treaty provide further information about states'
activities and are thus able to build trust among OSCE members (even
though both treaties are currently under strain). However, these
agreements are designed to address the risks posed by conventional
weapons rather than risks from AWS. Still, [the OSCE's recent adoption
of CBMs for Information and Communication Technologies]{.underline}
(adopted in 2013 and updated in 2016) [demonstrates that the OSCE and
its approach to CBMs are well suited to developing risk-reduction CBMs
applicable to advanced technologies]{.underline}. The OSCE's cyber CBMs
have also provided a template for other states or regional organisations
to develop their own cyber CBMs, building confidence and reducing risk
both within and beyond the OSCE's membership; [CBMs for AWS could play a
similar role]{.underline}. Although the original Vienna Document was
negotiated at the end of the Cold War, its updates and revisions,
together with the development of other OSCE CBMs, reflect how CBMs are
not merely an outdated Cold War legacy, but continue to play an
important role in building confidence and reducing risks.

**OSCE CBMs build trust and transparency -- this helps establish global
norms for AI autonomy.**

**Egel, 2021 - Nuclear Security Visiting Fellow at the Truman Center for
National Policy** \[Naomi, PhD candidate at Cornell, January, Geneva
Centre for Security Policy Strategic Security Analysis" Issue 16,
"Reducing Military Risks through OSCE Instruments: The Untapped
Potential in the European Arms Control Framework"
https://www.gcsp.ch/publications/reducing-military-risks-through-osce-instruments-the-untapped-potential-in-the-European-arms-control-framework
Acc 2/27/22 TA\]

• [As autonomous weapons systems (AWS) increase in military importance,
they pose significant risks of miscommunication, miscalculation and
inadvertent conflict escalation]{.underline}. • The Organization for
Security and Co-operation in Europe ([OSCE) has a strong history of
developing confidence-building measures]{.underline} (CBMs) [to reduce
military risks]{.underline} stemming from other types of weapons. • [The
OSCE should develop CBMs for partially autonomous weapons
systems]{.underline}. [Such CBMs]{.underline} [should]{.underline}
provide information about AWS features and doctrine for their use, to
[increase transparency and build trust between states.]{.underline} •
[OSCE CBMs could provide a foundation for the global governance of
autonomous weapons in other multilateral venues]{.underline}. Autonomous
weapons systems (AWS) are widely regarded as a game changer in the field
of international security and an increasingly important element of
military operations. However, they pose heightened risks of
miscommunication, miscalculation, and the inadvertent escalation of a
conflict that could increase tensions and conflict between states.
[Although the development of AWS and their use in military operations
vary widely among]{.underline} Organization for Security and
Co-operation in Europe [(OSCE) states, many states]{.underline} both
within and outside the OSCE [are incorporating increasing numbers of AWS
into their armed forces, raising concerns over the unintended risks
associated with these weapons systems.]{.underline} Even states that do
not currently employ AWS have concerns over other states' use of this
technology and the implications of AWS for international security and
strategic stability. [In the absence of formal treaties]{.underline}
governing the use of AWS, [confidence-building measures (CBMs) would
provide a valuable tool for reducing military risks associated with
AWS.]{.underline} This essay proposes that [the OSCE develop CBMs for
partially autonomous weapons systems, building on its successful history
of developing CBMs to reduce other military risks]{.underline}.

**[\--Net Benefit - UN Modelling]{.underline}**

**OSCE CBMs are the first step toward building consensus in the UN -- it
demonstrates that the political will exists.**

**Nadibaidze, 2021 - Ph.D. Student at the University of Southern
Denmark** \[Anna Entry submitted for the Second OSCE-IFSH Essay
Competition "Commitment to Control over Weaponised Artificial
Intelligence: A Step Forward for the OSCE and European Security"
https://www.osce.org/files/f/documents/8/3/507341.pdf Acc. 4/21/22 TA\]

[As a realistic starting point, the OSCE's Confidence and Security
Building Measures (CBMs) could provide a framework to exchange
information and observations on the use of weaponised AI, in order to
facilitate communication and dialogue]{.underline} (Egel 2021). [The
OSCE already has CBMs for Information Communication Technologies (ICTs),
which, like weaponised AI, create "an area with much room for
speculation, doubt, and ambiguity",]{.underline} and "increase the
potential for tensions between States" (OSCE website). There is also
potential to go further than exchanging information informally within
the CBMs framework. [The next step should be,]{.underline} based on the
recommendations of the International Panel on the Regulation of
Autonomous Weapons (iPRAW), [to "focus on the obligation to maintain
human control over the use of force" which would "apply to all
conventional weapons]{.underline}" (2021, 6). Taking this path will
avoid the debate on defining LAWS, which has been hindering the progress
of the CCW discussions. Based on the GGE Guiding Principles, [the
commitment to human control should be enshrined into a normative
framework, for example a political declaration or a manual of best
practices]{.underline}. Any document with a soft law standing, or a
politically binding status, would already be a step forward. [Such a
political declaration could be]{.underline} part of the Vienna Document,
or [the result of a new Working Group established at the OSCE.
Importantly, an OSCE political declaration]{.underline} or guiding code
[on weaponised AI would not go against the efforts at the CCW but would
complement and build upon them. Shifting the discussion towards the
current impacts of weaponised AI]{.underline}, rather than the potential
future impact of 'killer robots' [would be a progressive step to
mitigate the risks coming from this emerging technology. An agreement at
the OSCE would also be a preliminary step towards building consensus at
the UN]{.underline}. [It would demonstrate that finding consensus,
especially in an atmosphere of political distrust, is
possible]{.underline}. [This is a]{.underline} relevant and [crucial
step for European and international security.]{.underline} While debates
at the CCW continue, the operational trend towards further autonomy in
the armed forces of OSCE participating States continues. As noted in the
previous section, practices related to the use of weaponised AI have the
potential to shape warfare norms. Yet, this trajectory is not inevitable
and with the right approach, [a political declaration committing to a
common definition of human control is a realistic
achievement.]{.underline}

**UN AI governance is failing now -- OSCE declarations would
reinvigorate it -- empirically proven by the Small Arms Treaty.**

**Egel, 2021 - Nuclear Security Visiting Fellow at the Truman Center for
National Policy** \[Naomi, PhD candidate at Cornell, January, Geneva
Centre for Security Policy Strategic Security Analysis" Issue 16,
"Reducing Military Risks through OSCE Instruments: The Untapped
Potential in the European Arms Control Framework"
https://www.gcsp.ch/publications/reducing-military-risks-through-osce-instruments-the-untapped-potential-in-the-European-arms-control-framework
Acc 2/27/22 TA\]

The OSCE's contribution to risk reduction [Currently, global governance
of AWS is under discussion within the]{.underline} framework of the
Convention on Certain Conventional Weapons ([CCW). This process,
however, has been slow moving and fraught with disagreement]{.underline}
over what AWS are, particularly regarding the distinction between fully
and partially autonomous weapons systems. Rather than relying solely on
the CCW to address the risks posed by AWS, [regional organisations could
develop mechanisms to reduce risks and build confidence that states'
increasing use of partially autonomous weapons would not be
destabilising and escalatory. Such efforts could also help to energise
the process under way in the CCW and facilitate a global agreement
there. The OSCE has made such contributions before: OSCE information
exchanges on small arms and light weapons]{.underline}, beginning with
the 1993 OSCE Principles Governing Conventional Arms Transfers, [helped
pave the way for the 2013 Arms Trade Treaty.]{.underline}

**[\--AT Russia Blocks OSCE]{.underline}**

**The OSCE has been successful in incorporating Russia -- cyber CBMs
prove.**

**Egel, 2021 - Nuclear Security Visiting Fellow at the Truman Center for
National Policy** \[Naomi, PhD candidate at Cornell, January, Geneva
Centre for Security Policy Strategic Security Analysis" Issue 16,
"Reducing Military Risks through OSCE Instruments: The Untapped
Potential in the European Arms Control Framework"
https://www.gcsp.ch/publications/reducing-military-risks-through-osce-instruments-the-untapped-potential-in-the-European-arms-control-framework
Acc 2/27/22 TA\]

Moreover, [the OSCE's success in negotiating cyber CBMs in the context
of tensions between the United States and the Russian Federation
indicates that such tensions are not an insurmountable barrier to
reaching agreement on CBMs of various kinds within the
OSCE]{.underline}. Although negotiating new agreements in international
organisations is rarely easy, [the OSCE's track record shows that it is
capable of doing so. This stands in contrast to many other international
organisations in which geopolitical tensions have prevented progress on
agreements to govern new risks. The OSCE's resilience in the face of
such geopolitical challenges demonstrates its importance and efficacy as
a vehicle for addressing security threats and military
risks.]{.underline}

**[\--AT OSCE is Voluntary]{.underline}**

**Voluntary CBMs reduce military risks by avoiding miscommunication and
reducing tensions.**

**Egel, 2021 - Nuclear Security Visiting Fellow at the Truman Center for
National Policy** \[Naomi, PhD candidate at Cornell, January, Geneva
Centre for Security Policy Strategic Security Analysis" Issue 16,
"Reducing Military Risks through OSCE Instruments: The Untapped
Potential in the European Arms Control Framework"
https://www.gcsp.ch/publications/reducing-military-risks-through-osce-instruments-the-untapped-potential-in-the-European-arms-control-framework
Acc 2/27/22 TA\]

Given the difficulty of negotiating legally binding treaties governing
weapons in the current geopolitical environment, [informal and voluntary
CBMs provide a valuable and practical way to reduce military risks
arising from AWS. CBMs provide information about other states'
capabilities and intentions in order to reduce risks arising from
miscalculation and miscommunication and to build trust between parties.
Although CBMs are, by design, voluntary measures, OSCE CBMs have
provided a valuable framework for reducing tensions and lowering risks
associated with other weapons and military activities]{.underline}.
Additionally, in 2019 the OSCE Parliamentary Assembly called on OSCE
members to support international negotiations to ban lethal autonomous
weapons (also referred to as fully autonomous weapons). CBMs for
partially autonomous weapons would support efforts in this area while
also reducing the risks posed by these kinds of weapons systems.

**Voluntary CBMs build support for Codes of Conduct -- empirically
proven.**

**Egel, 2021 - Nuclear Security Visiting Fellow at the Truman Center for
National Policy** \[Naomi, PhD candidate at Cornell, January, Geneva
Centre for Security Policy Strategic Security Analysis" Issue 16,
"Reducing Military Risks through OSCE Instruments: The Untapped
Potential in the European Arms Control Framework"
https://www.gcsp.ch/publications/reducing-military-risks-through-osce-instruments-the-untapped-potential-in-the-European-arms-control-framework
Acc 2/27/22 TA\]

Even if the CCW eventually bans fully autonomous weapons, OSCE CBMs
would still be very useful for reducing the risks posed by partially
autonomous weapons systems. Autonomy is and will continue to be
important in military operations, regardless of whether fully autonomous
systems are banned. CBMs for partially autonomous weapons could also
strengthen confidence in a future agreement banning fully autonomous
weapons. By providing information about the extent of human control over
AWS and how AWS would be used, such CBMs could be used in assessments of
states' compliance with such an agreement. [OSCE AWS CBMs could also
provide the basis for a future OSCE code of conduct for AWS involving
doctrinal restrictions on use. The OSCE has precedents here, too, with
its Principles Governing Conventional Arms Transfers]{.underline}, [its
Code of Conduct on Politico-Military Aspects of Security, and its
Principles Governing Non-Proliferation]{.underline}. Like CBMs, codes of
conduct (including principles governing behaviour) are voluntary rather
than legally binding agreements. However, [whereas CBMs reduce risk and
build trust by providing information about other states' capabilities
and intentions, codes of conduct go further by endorsing certain
behaviours and proscribing others. An OSCE AWS code of conduct could
further reduce military risks by proscribing certain uses of AWS or
forms of autonomy]{.underline} (e.g. fully autonomous weapons that lack
human supervision) and/or endorsing certain requirements for human
control over AWS. [By building confidence and trust among states, CBMs
could facilitate further cooperation and agreements.]{.underline}

**[\--AT OSCE Fragmented]{.underline}**

**Empirically, the OSCE CBMs have reduced the risk of war and
miscalculation, even during times of political tensions**

**Egel, 2021 - Nuclear Security Visiting Fellow at the Truman Center for
National Policy** \[Naomi, PhD candidate at Cornell, January, Geneva
Centre for Security Policy Strategic Security Analysis" Issue 16,
"Reducing Military Risks through OSCE Instruments: The Untapped
Potential in the European Arms Control Framework"
https://www.gcsp.ch/publications/reducing-military-risks-through-osce-instruments-the-untapped-potential-in-the-European-arms-control-framework
Acc 2/27/22 TA\]

Conclusion [CBMs for AWS would make a significant contribution to
reducing the military risks posed by AWS and]{.underline} - equally
important - [are a realistic goal. Although agreement within the OSCE
cannot be assured, its strong history of developing CBMs and its ability
to make progress towards reducing military risk even in a climate of
geopolitical tensions makes the OSCE uniquely well positioned to develop
AWS CBMs.]{.underline} The growing proliferation of AWS and their
increasing importance in military operations means that risk reduction
is an urgent priority. Given the numerous challenges to arms control and
cooperative security measures in other international organisations,
[OSCE CBMs offer one of the best opportunities for making progress in
reducing the military risks of AWS.]{.underline} Moreover, they would
provide a foundation for further governance of AWS by other
international organisations[. The CBMs proposed here would provide
greater transparency regarding states' capabilities and intentions
regarding AWS, and in doing so, would reduce the risks of
miscommunication, miscalculation and inadvertent conflict escalation
that these systems pose.]{.underline}

**[\--AT China not in OSCE]{.underline}**

**OSCE CBMs can promote global agreements that include China**

**Egel, 2021 - Nuclear Security Visiting Fellow at the Truman Center for
National Policy** \[Naomi, PhD candidate at Cornell, January, Geneva
Centre for Security Policy Strategic Security Analysis" Issue 16,
"Reducing Military Risks through OSCE Instruments: The Untapped
Potential in the European Arms Control Framework"
https://www.gcsp.ch/publications/reducing-military-risks-through-osce-instruments-the-untapped-potential-in-the-European-arms-control-framework
Acc 2/27/22 TA\]

Far-reaching benefits of AWS CBMs In addition to reducing the military
risks posed by AWS, [the process of developing AWS CBMs in the OSCE
would also help to strengthen policymakers' understanding of the role of
autonomy in military operations. This would facilitate further
international cooperation on AWS governance.]{.underline} [Within
the]{.underline} context of the [CCW]{.underline} - at present the
primary global venue considering AWS governance - [debates over AWS are
often stymied by diplomats' unfamiliarity with these
systems.]{.underline} Greater transparency and information regarding
states' AWS capabilities (including the varying elements of autonomy in
these systems) and use would ameliorate this impediment to reaching an
agreement[. Developing AWS CBMs in the OSCE would thus help to provide a
foundation for the global governance of AWS]{.underline}. Moreover,
while OSCE CBMs would not include China (a leader in the development and
use of AWS), [OSCE CBMs could lay the groundwork for a global agreement
that would include China]{.underline}. An OSCE-developed template of
CBMs could also be applied outside the OSCE region, either in a global
agreement or through agreements in other regional organisations.

**[1NC - UN CP]{.underline}**

**CP Text - The United Nations should adopt ethical principles to ensure
human control in military artificial intelligence systems.**

**The UN is best to create a global norm for human control because it
builds on its principles from the CCW. UN Leadership would restore the
credibility of global governance**

**Bolton, 2021 - professor of political science at Pace University**
\[Matthew with Matilda Byrne, Ryan Gariepy, Emilia Javorsky, Volker
Lehmann, and Laura Nolan, January "Addressing The Threat Of Autonomous
Weapons Maintaining Meaningful Human Control"
http://library.fes.de/pdf-files/iez/17215.pdf Acc 5/27/22 TA\]

[The 2019 Meeting of High Contracting Parties to the Convention on
Certain Conventional Weapons (CCW) adopted 11 »Guiding
Principles]{.underline} affirmed by the Group of Governmental Experts
(GGE ) on Emerging Technologies in the Area of Lethal Autonomous Weapons
Systems.« (see Box 1). [Some 40 UN Member States belonging to the
»Alliance for Multilateralism« are now championing these 11 Guiding
Principles]{.underline} and calling on States to contribute actively to
the clarification and development of a normative and operational
framework.2 The civil society-led Campaign to Stop Killer Robots is
working for a preemptive ban on the development, production, and use of
fully autonomous weapons. Similarly, UN Secretary- General António
Guterres as well as his High Representative for Disarmament, Izumi
Nakamitsu, have called for LAWS to be banned by international law.3 [A
growing number of Member States have also called for a prohibition of
LAWS. At a minimum, there seems to be broad agreement that it is
necessary that States have an obligation to maintain meaningful human
control over the lethal use of force.]{.underline} Member States, with
the support and active participation of the United Nations and other
international organizations, civil society and the private sector,
quickly need to reach common understandings on how to ensure human
beings retain control over the use of force. Improving communication
between policymakers and scientific and technical experts is crucial
given the dual-use nature of artificial intelligence. Indeed, a
prohibition on weapons outside of human control would not be
counterproductive to technological development. Rather, there is a need
to fully harness technological progress while maintaining and advancing
international law that safeguards humanitarian protections, human rights
and international peace and security. Therefore, in September and
October 2020, the Friedrich- Ebert-Stiftung New York Office (FESN Y)
launched the »New Alliances for Meaningful Human Control« project aimed
at building on the 11 Guiding Principles on LAWS and moving them to the
next level, towards international legal instruments that regulate
high-tech weapons and prohibit LAWS. Linked to the UN 75 Global
Governance Forum,4 FESN Y convened three discussions with
representatives of governments, the United Nations, academia, civil
society and the private sector to discuss the moral, ethical, legal and
humanitarian challenges posed by LAWS (see Annex I for list of
participants). Participants, speaking under Chatham House Rule, offered
many diverse viewpoints, representing different perspectives on LAWS and
pathways to addressing the risk they pose. 5 This paper is rooted in the
discussions facilitated by FESN Y. While it summarizes the agreements
between the co-authors, who participated, it does not purport to
represent the views of all members of the discussions. For instance,
participants disagreed on the precise meaning, scope and value of the 11
Guiding Principles. However, a significant majority agreed that good
faith interpretation of [the 11 Guiding Principles]{.underline} --
particularly paragraphs b), c) and d) (see Box 1) -- [required progress
toward negotiation of a legally-binding multilateral instrument
mandating a positive obligation to maintain meaningful human
control]{.underline} over the use of force, at the level of individual
attacks. [Given the contested international political environment,
progress toward negotiating a legally-binding instrument would also
demonstrate the potential for effective global governance]{.underline},
as currently spearheaded by the Alliance for Multilateralism. [Towards
this end]{.underline}, the subsequent paper argues that [a potential
venue for negotiating such a mandate could be within the CC W, in the
form of a new protocol on meaningful human control.]{.underline} While
positive obligations would be the most suitable starting point, [a new
protocol should also entail principles about technologies not to be
developed and deployed.]{.underline} In what follows, two main issues
will be addressed to help States as they move toward negotiations on a
treaty to ensure meaningful human control: First, meaningful positive
obligations, and second, legally-binding control.

**UN action solves for the case -- it establishes global norms, provides
flexibility, and serves as a clearinghouse for information even without
a formal treaty.**

**Sauer, 2021 - Senior Research Fellow at Bundeswehr University**
\[Frank serves on the International Panel on the Regulation of
Autonomous Weapons IRRC No. 913 March "Stepping back from the brink: Why
multilateral regulation of autonomy in weapons systems is difficult, yet
imperative and feasible"
https://international-review.icrc.org/articles/stepping-back-from-brink-regulation-of-autonomous-weapons-systems-913
Acc 4/5/22 TA\]

Fostering norm development in the CCW LAWS keep steadily gathering media
attention around the globe.104 [With mounting public
pressure]{.underline} and increased scrutiny, [there will be a strong
incentive for CCW States Parties to produce tangible
results]{.underline} for the 2021 Review Conference. The "aspects of the
normative and operational framework" that are to be further developed
over the course of 2021 could take a more concrete shape in three steps.
[First, consensus seems achievable on shared language that adopts the by
now widely accepted functionalist view of weapon autonomy as well as a
common understanding that some form of positive obligation and
affirmation of the principle of human control over weapons systems is
required]{.underline}.105 The CCW\'s guiding principle (b) already
points this way in stating that "\[h\]uman responsibility for decisions
on the use of weapons systems must be retained since accountability
cannot be transferred to machines".106 The Forum for Supporting the 2020
GGE on LAWS conducted in April 2020 as a webcast by the German Federal
Foreign Office, with 320 registered participants representing
sixty-three CCW States Parties, underlined the importance of further
conceptualizing the human element. Controllability of weapons is
arguably a proto-norm already,107 and a shared terminology -- be it
"meaningful human control" or some other formulation -- could be found
to stipulate in a general sense when humans and when machines are to be
performing which function in the targeting cycle. The ICRC and the
Stockholm International Peace Research Institute (SIPRI) recently
presented a conceptual framework that can support this effort of
operationalizing human control -- that is, of clarifying the "who, what,
when and how" of controlling weapons and limiting their autonomy.108
[Second, since there is no one-size-fits-all standard of meaningful
human control, the sharing of best practices and, more importantly, of
case studies of specific weapons systems and operational scenarios could
allow CCW States Parties to develop a deeper, shared conceptual grasp
o]{.underline}f the intricacies involved with implementing human control
in design and use. [The GGE is uniquely suited to facilitate these sorts
of deep dives with analyses from multiple stakeholders and a sharing of
legal, ethical and operational views]{.underline}. Smaller expert groups
such as the International Panel on the Regulation of Autonomous Weapons
(iPRAW) and the commission on the responsible use of technologies in the
Franco-German Future Combat Air System are already beginning to organize
their research toward that end. [Third, a differentiated implementation
scheme could be developed that conceives of human control as being
exercised in a context-dependent way]{.underline} -- that is, contingent
on the weapons system, its mission environment, "target profiles"109 and
additional factors such as mission duration.110 This human control
scheme could prescribe minimum standards for controllability by design,
for example regarding the ergonomics of human--machine interfaces, and
determine "levels of human supervisory control"111 in use -- that is,
the tactics, techniques and procedures required to keep human control
and responsibility intact during the system\'s operation. It currently
seems unlikely that the CCW process, even if it were to complete these
three steps, will end up yielding more than "soft law", such as a
consensual political declaration or a catalogue of best practices. In
fact, a complete breakdown of the CCW process in Geneva is also within
the realm of possibility. But [even if the CCW turns out not to be the
venue from which a legally binding regulation for weapon autonomy
emerges, it has already served as an information hub and norm
incubator]{.underline} for the last six years -- [and will continue to
do]{.underline}. Especially considering the effect of the COVID-19
crisis on meeting schedules around the globe, it is currently too early
to tell if other fora -- and if so, which ones -- can and should pick up
the ball on regulation where the CCW leaves it in 2021, in order to
further develop and codify the human control norm as binding
international law.

**[Extend -- Human Control Solvency]{.underline}**

**UN action through the CCW would establish the human control norm and
reduce the instability and escalation from AI weapons**

**International Panel on the Regulation of Autonomous Weapons, 2021**
\[(iPRAW) coordinated by: German Institute for International and
Security Affairs, July "Building Blocks for a Regulation on LAWS and
Human Control Updated Recommendations to the GGE on LAWS"
https://www.readkong.com/page/building-blocks-for-a-regulation-on-laws-and-human-control-8617434
Acc 2/27/22 TA\]

iPRAW recommends that [the principle of human control should be
internationally recognized within the CCW and]{.underline} possibly
other documents of [international law and be the basis from which
requirements can be developed as part of a norm-shaping
process]{.underline}. The elements presented below could be helpful to
shape a regulation, be it legally binding or not. [What is important
though is to create a normative framework and operational guidance
around the]{.underline} (development and) [use of weapon systems with
autonomous functions.]{.underline} For example, [the ICRC]{.underline}
presented one approach to such a framework by [calling for a prohibition
of unpredictable and anti-personnel AWS and a regulation of other AWS
that considers the specific operational context.]{.underline} As iPRAW's
scenario-based discussions about the adequate type and level of human
control illustrated, [a definition of human control that adequately
considers the operational context requires many details about technical
capabilities and indicators for the targets. Hence, a
'one-size-of-control-fits-all' solution does not exist. Rather, a
combination of minimum requirements and individual solutions is
necessary]{.underline}. Individual solutions based on a case-by-case
assessment will ultimately lead to different levels of granularity when
it comes to formulating human control in a regulatory framework, e.g. a
rather abstract declaration or treaty and more granular best practices
and manuals. [These could inform a regulation of LAWS and human control
by creating the normative baseline and align with recommendations from
other actors.]{.underline} As for example the ICRC discussed, it is
crucial to link specific regulatory elements to the challenges raised by
LAWS. Within the CCW such a regulation could cover:  military
considerations: fulfilling the operational objective and translating the
commanders intent to the battlefield,  legal concerns: abide by IHL
principles, especially the principle of precaution by avoiding
unpredictable effects, ensure human judgment to take legal decisions, 
ethical concerns: retain moral agency. [A CCW regulation entailing an
obligation to maintain human control may also mitigate security
challenges, such as conflict escalation]{.underline}, even though the
CCW does not address them explicitly. Further important aspects, such as
technology diffusion, will most likely not be addressed in a future
regulation on LAWS but would have to be addressed in other fora. iPRAW's
model discussed below is not meant to read in opposition to the ICRC
propositions on a prohibition and regulation of AWS but rather as an
additional perspective with the same objective to keep human control in
the use of force.

**A UN mandate for Human Control would effectively ban LAWs -- it sets
the stage for future treaties.**

**International Panel on the Regulation of Autonomous Weapons, 2021**
\[(iPRAW) coordinated by: German Institute for International and
Security Affairs, July "Building Blocks for a Regulation on LAWS and
Human Control Updated Recommendations to the GGE on LAWS"
https://www.readkong.com/page/building-blocks-for-a-regulation-on-laws-and-human-control-8617434
Acc 2/27/22 TA\]

General Obligations[: A regulation of LAWS, e.g. a CCW Protocol, could
consist of a general obligation to maintain human control over the use
of force when deploying conventional weapons]{.underline}. The GGE
Guiding Principles adopted in 2019 could lay the groundwork to further
shape a future regulation. The Guiding Principles emphasize that IHL
continues to fully apply to LAWS. Furthermore, they stress that [human
responsibility for the decision to use LAWS must be retained since
accountability cannot be transferred to machines. This aspect should be
considered throughout the entire life-cycle of a LAWS]{.underline}. They
also stipulate that human-machine interaction must be in compliance with
international law and refer, among others, to questions relating to
accountability, weapon reviews, and risk assessments, including the
development stage of weapon systems. In addition, other principles that
were not explicitly mentioned in the Guiding Principles but found entry
into other documents adopted by the GGE could also supplement a future
regulation. Examples are the principle of predictability, reliability,
and transparency. Specific Obligations: [Specific obligations and more
nuanced rules on the concept of human control could play a pivotal role
in a future treaty focusing on human control. It could entail concrete
rules stipulating that human control encompasses]{.underline} both
[situational understanding]{.underline} and the option to intervene,
enabled by design and in use. The term situational understanding could
be elaborated in more detail by stipulating that it refers to the
ability to monitor information about the environment, the target and the
weapon system. The human operator shall monitor the system and the
operational environment to the extent necessary in a specific operation.
Furthermore, the different modes of operation should allow the human
operator to intervene if necessary. All people in the chain of command
are equally obliged to abide by the rules of international law and
should be held accountable for any violations of the law. [These and
other obligations could be an integral part of "specific obligations" in
a future treaty on LAWS. The ICRC mentions two specifics that would call
for tighter restrictions, namely AWS with unpredictable effects and the
anti-personnel use of AWS.]{.underline}

**[Extend - Modelling Solvency]{.underline}**

**The UN establishes a global model because it brings together the
experience and information from many states.**

**International Panel on the Regulation of Autonomous Weapons, 2021**
\[(iPRAW) coordinated by: German Institute for International and
Security Affairs, July "Building Blocks for a Regulation on LAWS and
Human Control Updated Recommendations to the GGE on LAWS"
https://www.readkong.com/page/building-blocks-for-a-regulation-on-laws-and-human-control-8617434
Acc 2/27/22 TA\]

SOFT LAW Best Practices: [States Parties to a CCW Protocol]{.underline}
(or any other regulatory instrument) [could meet regularly with the
specific purpose of sharing experiences]{.underline} that were already
made at the domestic level regarding the design, development,
acquisition, deployment, and use of weapon systems with autonomous
functions (under the assumption that the use of such weapons is lawful,
meaning that human control is maintained). [Especially States with
significant experience in this area could provide knowledge and
information about regulating LAWS and could show how human control is
maintained in practice. Such experiences could serve as exemplary model
for other States. Best practices are also a helpful instrument to
establish additional standards on the design, development, acquisition,
deployment, and use of LAWS based on cooperation, transparency, trust,
and confidentiality.]{.underline} The sharing of best practices could
promote the adoption of domestic laws on LAWS, ensuring that human
control is maintained as required by military, legal, and ethical
considerations.

**The UN can build consensus over time for a global norm.**

**Sauer, 2021 - Senior Research Fellow at Bundeswehr University**
\[Frank serves on the International Panel on the Regulation of
Autonomous Weapons IRRC No. 913 March "Stepping back from the brink: Why
multilateral regulation of autonomy in weapons systems is difficult, yet
imperative and feasible"
https://international-review.icrc.org/articles/stepping-back-from-brink-regulation-of-autonomous-weapons-systems-913
Acc 4/5/22 TA\]

Conclusion [A multilateral regulation of autonomy in weapons
systems]{.underline} -- that is, codifying a legally binding obligation
to retain meaningful human control over the use of force -- [is
difficult yet imperative to achieve]{.underline}. Severe strategic as
well as ethical mid- and long-term risks, such as unintended conflict
escalation at machine speed and the violation of human dignity, outweigh
any short-term military benefits. This analysis has illustrated how
[regulating weapon autonomy is feasible, presenting a three-step process
to facilitate stepping back from the brink: step one, foster the
emerging consensus on the notion that a positive obligation to retain
human control over weapons systems is]{.underline} prudent and [urgently
required; step two, further develop the insight that there is no
one-size-fits-all standard of meaningful human control; and step three,
devise differentiated, context-dependent human control schemes for
weapons systems]{.underline}. Given the current geopolitical landscape
and the lack of political will to engage in arms control efforts, [the
taking of these steps will resemble a marathon, not a
sprint.]{.underline} After all, the perceived military value of weapon
autonomy is exceptionally high, and [the issue itself is elusive,
requiring an innovative, qualitative approach to arms
control.]{.underline} But [history clearly suggests that great powers
are not devoid of sensitivity to the accumulation of collective risks --
otherwise arms control on nuclear, chemical and biological weapons would
never have seen the light of day]{.underline}. The emerging technologies
of the twenty-first century present humankind with the opportunity to
demonstrate that it has learned from history before the risks have
manifested themselves to their full extent. Humans do terrible things to
each other in war, and there is no technological fix for that. But [the
international community can at least set rules to curb against
uncontrolled escalation and the crossing of fundamental moral
lines]{.underline}. If we fail to do so, we will not only lose the
breathing room to ponder and deliberate responses,112 an essential
requirement of political conflict management, as the Cuban Missile
Crisis strongly suggests;113 we will also allow "the ultimate indignity"
of war turning into "death by algorithm".114

**[Extend - Global Governance Net Benefit]{.underline}**

**CCW action on AI weapons would demonstrate the potential for global
governance.**

**Bolton, 2021 - professor of political science at Pace University**
\[Matthew with Matilda Byrne, Ryan Gariepy, Emilia Javorsky, Volker
Lehmann, and Laura Nolan, January "Addressing The Threat Of Autonomous
Weapons Maintaining Meaningful Human Control"
http://library.fes.de/pdf-files/iez/17215.pdf Acc 5/27/22 TA\]

[The Guiding Principles]{.underline} therefore [are]{.underline} not a
ceiling, but a [steppingstone towards a more legally binding agreement
that ensures binding human control]{.underline}, has meaningful positive
obligations, and respects IH L and IHR L. [In this regard, it is good
news that the Principles' political visibility increased after the
endorsement of the Alliance for Multilateralism]{.underline}. Member
States of [the Alliance should therefore use this political momentum and
lead the effort to negotiate an additional protocol to the CC
W]{.underline}. After the November 2020 meetings of the GGE had to be
cancelled due to Covid-19, attention is now on the next meeting of the
High Contracting Parties to the CC W and the CC W Review Conference
scheduled for 2021. Tangible progress by then is sorely needed to
achieve some form of arms regulation in an area of rapid technological
development. [Moreover, progress toward new international law would
demonstrate the potential for effective global governance, called for by
the Alliance for Multilateralism]{.underline}. The Alliance itself is
still in a phase of self-definition and is a rather loose collection of
states. In fact, among the countries who supported the Alliance's
Guiding Principles, there are also 20 countries that have explicitly
endorsed the call for a ban on LAWS. P[rogress on an additional protocol
with meaningful obligations for human control of LAWS would demonstrate
the bridge-building capacity of the Alliance and would increase its
standing as an actor for effective global governance.]{.underline}

**[1NC - Ban LAWS CP]{.underline}**

**CP Text -- The United Nations should adopt a mandate prohibiting
Lethal Autonomous Weapons.**

**Banning LAWs is a de facto mandate for human control -- humans are the
check on autonomy**

**International Panel on the Regulation of Autonomous Weapons, 2021**
\[(iPRAW) coordinated by: German Institute for International and
Security Affairs, July "Building Blocks for a Regulation on LAWS and
Human Control Updated Recommendations to the GGE on LAWS"
https://www.readkong.com/page/building-blocks-for-a-regulation-on-laws-and-human-control-8617434
Acc 2/27/22 TA\]

General Obligations[: A regulation of LAWS]{.underline}, e.g. [a CCW
Protocol]{.underline}, [could consist of a general obligation to
maintain human control over the use of force when deploying conventional
weapons]{.underline}. The GGE Guiding Principles adopted in 2019 could
lay the groundwork to further shape a future regulation. The Guiding
Principles emphasize that IHL continues to fully apply to LAWS.
Furthermore, they stress that [human responsibility for the decision to
use LAWS must be retained since accountability cannot be transferred to
machines. This aspect should be considered throughout the entire
life-cycle of a LAWS]{.underline}. They also stipulate that
human-machine interaction must be in compliance with international law
and refer, among others, to questions relating to accountability, weapon
reviews, and risk assessments, including the development stage of weapon
systems. In addition, other principles that were not explicitly
mentioned in the Guiding Principles but found entry into other documents
adopted by the GGE could also supplement a future regulation. Examples
are the principle of predictability, reliability, and transparency.
Specific Obligations: [Specific obligations and more nuanced rules on
the concept of human control could play a pivotal role in a future
treaty focusing on human control.]{.underline} It could entail concrete
rules stipulating that human control encompasses both situational
understanding and the option to intervene, enabled by design and in use.
The term situational understanding could be elaborated in more detail by
stipulating that it refers to the ability to monitor information about
the environment, the target and the weapon system. The human operator
shall monitor the system and the operational environment to the extent
necessary in a specific operation. Furthermore, the different modes of
operation should allow the human operator to intervene if necessary. All
people in the chain of command are equally obliged to abide by the rules
of international law and should be held accountable for any violations
of the law. These and other obligations could be an integral part of
"specific obligations" in a future treaty on LAWS. The ICRC mentions two
specifics that would call for tighter restrictions, namely AWS with
unpredictable effects and the anti-personnel use of AWS.

**Only banning LAWs can avoid proliferation and an arms race**

**Docherty, 2014 - senior researcher in the Arms Division of Human
Rights Watch** \[Bonnie "Shaking the Foundations The Human Rights
Implications of Killer Robots" Human Rights Watch
http://www.hrw.org/sites/default/files/reports/ arms0514_ForUpload_0.pdf
Acc 12/27/20 TA\]

[Fully autonomous weapons threaten to contravene foundational elements
of human rights law.]{.underline} They could violate the right to life,
a prerequisite for all other rights. [Deficiencies in judgment,
compassion, and capacity to identify with human beings could lead to
arbitrary killing of civilian]{.underline}s during law enforcement or
armed conflict operations. Fully [autonomous weapons could also cause
harm for which individuals could not be held accountable,]{.underline}
thus undermining the right to a remedy. Robots could not be punished,
and superior officers, programmers, and manufacturers would all be
likely to escape liability. Finally, as machines, fully autonomous
weapons could not comprehend or respect the inherent dignity of human
beings. The inability to uphold this underlying principle of human
rights raises serious moral questions about the prospect of allowing a
robot to take a human life. Proponents of fully autonomous weapons might
argue that technology could eventually help address the problems
identified in this report, and it is impossible to know where science
will lead.85 In a 2013 public letter, however, more than 270
roboticists, artificial intelligence experts, and other scientists
expressed their skepticism that adequate developments would be
possible.86 Given this uncertainty, [the potential of fully autonomous
weapons to violate human rights law]{.underline}, combined with other
ethical, legal, policy, and scientific concerns, [demands a
precautionary approach]{.underline}. The precautionary principle of
international law states that "\[w\]here there are threats of serious or
irreversible damage, lack of full scientific certainty shall not be used
as a reason for postponing costeffective measures."87 [When applied to
fully autonomous weapons, this principle calls for preventive action to
be taken now]{.underline}. [Human Rights Watch]{.underline} and IHRC
[recommend a preemptive ban on fully autonomous weapons, which would
forestall the troubling consequences]{.underline} described in this
report and have great humanitarian benefits. [It would also help prevent
an arms race, block proliferation, and stop development before countries
invest so heavily in this technology that they do not want to give it
up.]{.underline}88 In determining the future of fully autonomous
weapons, the international community should seriously consider their
human rights implications and ensure the core components of this body of
law receive protection.

**Respect for human dignity requires a ban on LAWs.**

**Johnson and Axinn, 2013 - Prof of Philosophy at the Univ of South
Florida and PhD Candidate in Engineering at Penn** \[Aaron and Sidney,
Journal of Military Ethics, Volume 12, Issue 2, August "The Morality of
Autonomous Robots" www.tandfonline.com/10.1080/15027570.2013.818399 Acc
12/27/20 TA\]

Conclusion [For]{.underline} the reasons presented above, [reasons both
military and humanitarian, we propose that autonomous robots, carrying
lethal weapons and operated by computer programs alone, be treated on
the same basis as the United States now treats chemical weapons
(]{.underline}gas warfare among them). As noted earlier, [the U.S. and
all other nations should agree not use such weapons. A second
Ottawa-style conference would be an appropriate mechanism to accomplish
this moral goal internationally. Nuclear weapons are an example of
technology that was brought into use before civilization and the laws of
war could react to them. We need to act now to establish the moral and
legal standing of automatic robots before they enter into common
usage]{.underline}. The Ottawa conference prohibiting anti-personnel
mines (August 2007), as mentioned, would be an excellent model for what
should be done about automatic robots. A nation that relies on such
weapons ignores the humanitarian basis for the laws of war, and when
there is an international convention banning them, such a nation will be
acting dishonorably. [As technology continues to progress there will
certainly be borderline questions, but the central notion cannot be
abandoned, that a lethality decision is to be made only by a human and
not a machine. That should remain the key focus of debate and be the
guiding moral principle.]{.underline}

**[\--Extend -- Ban LAWs Solvency]{.underline}**

**The best solution is to ban lethal autonomous weapons. Empirically, it
has worked for other weapons and has broad international support.**

**Garcia, 2019 -- International Panel for the Regulation of Autonomous
Weapons at Northeastern Univ** \[Denise, with Justin Haner -- PhD
student Global Policy, Sept 26 "The Artificial Intelligence Arms Race:
Trends and World Leaders in Autonomous Weapons Development"
<https://onlinelibrary.wiley.com/doi/full/10.1111/1758-5899.12713> Acc
12/27/20 TA\]

[Ending the artificial intelligence arms race with a ban on killer
robots]{.underline} As the AI arms race rages on, the stakes remain high
yet public debate is lacking. Sixty‐one per cent of citizens polled
across more than twenty countries oppose the development of lethal AWS,
and yet billions of their tax dollars are being spent on their
development each year (CSKR, 2019). France, Germany, and others have
advocated use of the Convention on Certain Weapons (CCW) process to
develop 'Possible Guiding Principles' as a code of conduct to encourage
AWS development to stay in accordance with existing international law
(Convention on Certain Weapons, 2018). Beyond that, [28 states have
called for a ban on killer robots, and further the Non‐aligned
Movement]{.underline} and a group of African states [desire to negotiate
a new international treaty to set limits on robotic killing. Previous
weapons bans, from chemical and biological weapons to
landmines]{.underline} and cluster munitions, [have been effective
policy tools which significantly curtailed the use of these problematic
weapons]{.underline}. While the United States is not currently in a
position to lead with its ill‐fated 'America First' policy, the EU and
other [forward‐thinking countries should attempt to set solid global
norms and push for a ban on the use of AWS now]{.underline}. China
announced last year that it wishes to ban the battlefield use of AWS,
but not their development and production. This could serve as a basis
for coalition negotiations with the rest of the world and would
represent a key step forward in preventative security governance
(Garcia, 2018).

**A Treaty mandating Human control is necessary for safe AI because AI
lacks a conscience**

**Miller, 2021 - Flight Chief, US Air Force** \[Amanda, Dec 14 Air Force
Magazine "UN Addresses Lethal Autonomous Weapons---aka 'Killer
Robots'---Amid Calls for a Treaty"
https://www.airforcemag.com/un-addresses-lethal-autonomous-weapons-aka-killer-robots-amid-calls-for-a-treaty/
Acc. 4/5/22 TA\]

Van Weel represented a minority on the panel. His counterparts---[an
Oxford scholar and a tech attorney---supported a treaty to
"de-weaponize" AI.]{.underline} The University of Oxford's [Xiaolan Fu,
professor of technology and international development, thought that even
to start a dialog would amount to progress.]{.underline} Considering AI
to have "the risk to be as toxic as a nuclear weapon, if not more,"
[Tech Group co-head Jonathan Kewley of the firm Clifford Chance said
AI-enabled weapons need people in the loop the same way nuclear weapons
do. AI "doesn't have a conscience. It doesn't have a moral fiber unless
it's programmed in," Kewley said. "AI has the risk to be as toxic as a
nuclear weapon, if not more, and if we don't have the equivalent of that
moral compass]{.underline}, the finger on the button designed in through
a treaty---[because we're not going to design the technology to prevent
this unless there is a treaty involving China, the U.S., and
others---we're going to have a similar issue to nuclear
risk."]{.underline}

**Regulating LAWs does not protect dignity -- only a ban solves.
Regulation maintains robot decision making over human life -- it just
tries to make those decisions better. But Dignity requires that they not
make those decisions at all.**

**Docherty, 2018 - senior researcher in the Arms Division of Human
Rights Watch** \[Bonnie August 21, "Heed the Call A Moral and Legal
Imperative to Ban Killer Robots"
[https://www.hrw.org/report/2018/08/21/heed-call/moral-and-legal-imperative-ban-killer-robots#](https://www.hrw.org/report/2018/08/21/heed-call/moral-and-legal-imperative-ban-killer-robots)
Acc 12/27/20 TA\]

[There is no way to regulate fully autonomous weapons short of a ban
that would ensure compliance with the principles of
humanity.]{.underline} Fully autonomous weapons would lack the
compassion and legal and ethical judgment that facilitate humane
treatment of humans. They would face significant challenges in
respecting human life. [Even if they could comply with legal rules of
protection, they would not have the capacity to respect human
dignity.]{.underline} Limiting the use of fully autonomous weapons to
certain locations, such as those where civilians are rare, would not
sufficiently address these problems. "Harm to others," which the
principle of humane treatment seeks to avoid, encompasses harm to
civilian objects, which might be present where civilians themselves are
not. The requirement to respect human dignity applies to combatants as
well as civilians, so the weapons should not be permitted where enemy
troops are positioned. Furthermore, [allowing fully autonomous weapons
to be developed and to enter national arsenals would raise the
possibility of their misuse. They would likely proliferate to actors
with no regard for human suffering and no respect for human life or
dignity.]{.underline} The 2017 letter from [technology company CEOs
warned that the weapons could be "weapons of terror, weapons that
despots and terrorists use against innocent populations, and weapons
hacked to behave in undesirable ways]{.underline}."\[178\] [Regulation
that allowed for the existence of fully autonomous weapons would open
the door to violations of the principles of humanity.]{.underline}

**[\--Extend -- Ban Solves Dignity]{.underline}**

**Upholding the principle of dignity Demands a preemptive ban on LAWs**

**Docherty, 2014 - senior researcher in the Arms Division of Human
Rights Watch** \[Bonnie "Shaking the Foundations The Human Rights
Implications of Killer Robots" Human Rights Watch
http://www.hrw.org/sites/default/files/reports/ arms0514_ForUpload_0.pdf
Acc 12/27/20 TA\]

[Fully autonomous weapons threaten to contravene foundational elements
of human rights law. They could violate the right to life, a
prerequisite for all other rights. Deficiencies in judgment, compassion,
and capacity to identify with human beings could lead to arbitrary
killing of civilian]{.underline}s during law enforcement or armed
conflict operations. Fully [autonomous weapons could also cause harm for
which individuals could not be held accountable, thus undermining the
right to a remedy]{.underline}. Robots could not be punished, and
superior officers, programmers, and manufacturers would all be likely to
escape liability. Finally, [as machines, fully autonomous weapons could
not comprehend or respect the inherent dignity of human
beings]{.underline}. The inability to uphold this underlying principle
of human rights raises serious moral questions about the prospect of
allowing a robot to take a human life. Proponents of fully autonomous
weapons might argue that technology could eventually help address the
problems identified in this report, and it is impossible to know where
science will lead.85 In a 2013 public letter, however, more than 270
roboticists, artificial intelligence experts, and other scientists
expressed their skepticism that adequate developments would be
possible.86 Given this uncertainty, [the potential of fully autonomous
weapons to violate human rights law]{.underline}, combined with other
ethical, legal, policy, and scientific concerns, [demands a
precautionary approach]{.underline}. The precautionary principle of
international law states that "\[w\]here there are threats of serious or
irreversible damage, lack of full scientific certainty shall not be used
as a reason for postponing costeffective measures."87 [When applied to
fully autonomous weapons, this principle calls for preventive action to
be taken now]{.underline}. [Human Rights Watch]{.underline} and IHRC
[recommend a preemptive ban on fully autonomous weapons, which would
forestall the troubling consequences]{.underline} described in this
report and have great humanitarian benefits. [It would also help prevent
an arms race, block proliferation, and stop development before countries
invest so heavily in this technology that they do not want to give it
up.]{.underline}88 In determining the future of fully autonomous
weapons, the international community should seriously consider their
human rights implications and ensure the core components of this body of
law receive protection.

**Only a ban solves -- Regulating AI would fail because it cannot meet
the moral demand for dignity.**

**Docherty, 2018 - senior researcher in the Arms Division of Human
Rights Watch** \[Bonnie August 21, "Heed the Call A Moral and Legal
Imperative to Ban Killer Robots"
[https://www.hrw.org/report/2018/08/21/heed-call/moral-and-legal-imperative-ban-killer-robots#](https://www.hrw.org/report/2018/08/21/heed-call/moral-and-legal-imperative-ban-killer-robots)
Acc 12/27/20 TA\]

[A ban is also necessary to promote compliance with the dictates of
public conscience. An overview of public opinion shows that ordinary
people and experts alike have objected to the prospect of fully
autonomous weapons on moral grounds]{.underline}. Public opinion surveys
have illuminated significant opposition to these weapons based on the
problems of delegating life-and-death decisions to machines. Experts
have continually called for a preemptive ban on fully autonomous
weapons, citing moral along with legal and security concerns.
[Regulation that allows for the existence of fully autonomous weapons,
even if they could only be used in limited circumstances, would be
inconsistent with the widespread public belief that fully autonomous
weapons are morally wrong.]{.underline}

**[\--Extend - - Bans Solves Instability]{.underline}**

**Regulations fail to solve accountability or technology concerns.**

**Human Rights Watch, 2018** \[August 21, News Release "Killer Robots
Fail Key Moral, Legal Test Principles and Public Conscience Call for
Preemptive Ban"
https://www.hrw.org/news/2018/08/21/killer-robots-fail-key-moral-legal-test#
Acc 12/27/20 TA\]

The groups found that fully autonomous weapons would undermine the
principles of humanity, because they would be unable to apply either
compassion or nuanced legal and ethical judgment to decisions to use
lethal force. [Without]{.underline} these [human qualities, the weapons
would face significant obstacles in ensuring the humane treatment of
others]{.underline} and showing respect for human life and
dignity[.]{.underline} Fully autonomous weapons would also run contrary
to the dictates of public conscience. Governments, experts, and the
broader public have widely condemned the loss of human control over the
use of force. [Partial measures, such as regulations]{.underline} or
political declarations short of a legally binding prohibition, [would
fail to eliminate the many dangers posed by fully autonomous weapons. In
addition to violating the Martens Clause, the weapons raise other legal,
accountability, security, and technological concerns]{.underline}.

**[\--Extend -- Ban solves Prolif]{.underline}**

**Banning LAWs is essential to prevent proliferation of weapons.**

**Human Rights Watch, 2018** \[August 21, News Release "Killer Robots
Fail Key Moral, Legal Test Principles and Public Conscience Call for
Preemptive Ban"
https://www.hrw.org/news/2018/08/21/killer-robots-fail-key-moral-legal-test#
Acc 12/27/20 TA\]

[Basic humanity and the public conscience support a ban on fully
autonomous weapons,]{.underline} Human Rights Watch said in a report
released today. [Countries participating in an upcoming international
meeting on such "killer robots" should agree to negotiate a prohibition
on the weapons systems' development, production, and use.]{.underline}
The 46-page report, Heed the Call: A Moral and Legal Imperative to Ban
Killer Robots, finds that fully autonomous weapons would violate what is
known as the Martens Clause. This long-standing provision of
international humanitarian law requires emerging technologies to be
judged by the "principles of humanity" and the "dictates of public
conscience" when they are not already covered by other treaty
provisions. "[Permitting the development and use of killer robots would
undermine established moral and legal standards," said Bonnie Docherty,
senior arms researcher at Human Rights Watch, which coordinates the
Campaign to Stop Killer Robots. "Countries should work together to
preemptively ban these weapons systems before they proliferate around
the world."]{.underline}

**Only a global treaty can prevent mass production of genocide swarms**

**Freedberg, 2019 -- deputy editor for Breaking Defense** \[Sydney J.,
March 8 "Genocide Swarms & Assassin Drones: The Case For Banning Lethal
AI"
https://breakingdefense.com/2019/03/genocide-swarms-assassin-drones-the-case-for-banning-lethal-ai/
Acc 5/25/22 TA\]

Small Drones, Big Kills Now what happens when you scale this up? Russell
and fellow activists actually produced a video, Slaughterbots, in which
swarms of mini-drones attack, among other groups, every member of
Congress from a particular party. But that's still thinking small.
Remember, [once you've written the software, you can make infinite
copies; lone cranks can make explosives; and mini-drones are getting
cheaper by the day]{.underline}. Remember also that the Chinese
government has personal information on some 22.1 million federal
employees, contractors, and their family members from the Office of
Personnel Management breach two years ago. Now imagine one out of every
thousand shipping containers imported from China is actually full of
mini-drones programmed to go to those addresses and explode in the face
of the first person to leave the house. Imagine they do this the day
before China invades Taiwan. How effectively would the US government
react? [A rogue state or terrorist group could go further. How about
programming your mini-drones to kill everyone who looks white, or black
or Asian?]{.underline} (One Google facial recognition algorithm
classified African-Americans as "gorillas," not humans, so racist AI is
a mature technology). [It would be genocide by swarm]{.underline}. Such
a tactic might only work once, much like hijacking airliners with box
cutters on 9/11. "Small drones are vulnerable to jamming, to
high-powered microwaves, to other drones that might intercept them, to
nets," said Paul Scharre, an Army Ranger turned thinktank analyst.
"Bullets work pretty well... I have a buddy who shot a drone out of the
sky back in Iraq in 2005." (Unfortunately, the drone was American). At
least some object-recognition algorithms can be tricked by carefully
applied reflective tape. "People are working on countermeasures today,"
Scharre told me, "and the bigger the threat becomes, the more people
have an incentive to invest in countermeasures." [But how do you stop
tiny drones from becoming a big threat in the first place? While
technology to build a "working prototype" already exists, Russell told
me, the barrier is mass production.]{.underline} No national spy agency
or international monitoring regime can find and stop everyone trying to
make small numbers of drones. But, [Russell argues fervently, a treaty
banning "lethal autonomous weapons systems" would prevent countries and
companies from openly producing swarms of them, and a robust inspection
mechanism]{.underline} --- perhaps modeled on the Organisation for the
Prohibition of Chemical Weapons --- [could detect covert attempts at
mass production]{.underline}. [Without a ban, Russell said, legal mass
production could make lethal swarms as easy to obtain as, say, assault
rifles]{.underline} --- except, of course, one person can't aim and fire
thousands of rifles at once. Thousands of drones? Sure. So don't fear
robots who rebel against their human masters. Fear robots in the hands
of the wrong human.

**[\--AT UN Nations Oppose a Ban]{.underline}**

**A preemptive ban solves -- many countries support human control.**

**Docherty, 2018 - senior researcher in the Arms Division of Human
Rights Watch** \[Bonnie August 21, "Heed the Call A Moral and Legal
Imperative to Ban Killer Robots"
[https://www.hrw.org/report/2018/08/21/heed-call/moral-and-legal-imperative-ban-killer-robots#](https://www.hrw.org/report/2018/08/21/heed-call/moral-and-legal-imperative-ban-killer-robots)
Acc 12/27/20 TA\]

The statements of governments, another element of the public conscience,
illuminate that opposition to weapons that lack human control over the
selection and engagement of targets extends beyond individuals to
countries. [More than two dozen countries have explicitly called for a
preemptive ban on these weapons]{.underline},\[179\] [and consensus is
emerging regarding the need for human control over the use of
force.]{.underline} As noted above, [the requirement for human control
is effectively equivalent to a ban on weapons without it.]{.underline}
Therefore, a ban would best ensure that the dictates of public
conscience are met. The principles of humanity and dictates of public
conscience bolster the case against fully autonomous weapons although as
discussed above they are not the only matter of concern. [Fully
autonomous weapons]{.underline} are also problematic under other legal
provisions [and raise accountability, technological, and security risks.
Collectively, these dangers to humanity more than justify the creation
of new law that maintains human control over the use of force and
prevents fully autonomous weapons from coming into
existence.]{.underline}

**[\--AT No Enforcement]{.underline}**

**Enforcement problems should not prevent a Ban -- empirical evidence,
improved enforcement and moral stigmas will improve compliance**

**Freedberg, 2019 -- Breaking Defense writer** \[Sydney " Should We Ban
'Killer Robots'? Can We? The Pentagon insists it doesn\'t want them. But
could a global ban really rein in Russia or China? March 11,
<https://breakingdefense.com/2019/03/should-we-ban-killer-robots-can-we/>
Acc 12/27/20 TA\]

What about [an international treaty banning lethal]{.underline} AI,
which would have the force of law? "Such a treaty [would be very
difficult to monitor and enforce," the official said. "That doesn't mean
an international treaty should be ruled out."]{.underline} The
Enforcement Problem "Autonomous weapons have all of the features that
make arms control hard," said Paul Scharre, a former Army Ranger, now at
CNAS, who worked on the current Pentagon policy. International
inspectors can't tell by looking at an unmanned tank, plane, or warship
whether it's programmed to ask a human for permission before opening
fire. Even if they get to see the actual code --- a security breach few
countries would allow --- "there's nothing to stop you from upgrading
the software as soon as the inspectors leave," he told me. "Having said
that," [Scharre continued]{.underline}, "I think that [the kind of arms
control that]{.underline} Stuart [Russell is advocating for is actually
more feasible."]{.underline} If someone's building vast swarms of lethal
mini-drones, you don't have to see the code to know they have to be
fully autonomous: There's no practical way, Scharre told me, for humans
to review and approve "a million targets." Conversely, such mini-drones
are only truly threatening in vast numbers. "A country or an
individual... might be able to build a few hundred of these," Scharre
said, "but if you're going to build millions of them, there's no way to
hide that." So how would you find them? [The best model is probably the
Chemical Weapons Convention, which]{.underline}, unlike many other
treaties --- the Biological Weapons Convention, the landmine ban, and so
on --- [has a robust enforcement mechanism]{.underline}. The scope of
the problem is similar. Lethal chemicals like chlorine and phosgene are
widely used in legitimate industry, so you can't ban them outright any
more than mini-drones; they're relatively easy to turn into weapons,
again like drones; and yet only rogue states like Syria and Iraq have
used them since the end of World War I. Much of the reason militaries
abandoned poison gas is that a weapon that blows with the wind is hard
to control --- yet another similarity with AI, since even "narrow"
machine-learning algorithms modify themselves in ways beyond human
understanding. But there is also a robust monitoring regime, run by the
Organisation to Prevent Chemical Weapons, which has about 250 inspectors
who can rapidly respond to reported violations. Such "challenge
inspections" are a crucial tool, said Irakli Beridze, a Georgian-born
veteran of both OPCW and the UNICRI chem, bio, radiological, & nuclear
program --- with service in Afghanistan, Iraq, Libya, and Syria --- who
now runs the Centre for AI & Robotics at UNICRI, the United Nations
Interregional Crime and Justice Research Institute. (Beridze emphasized
he was only expressing his personal opinion as an expert, not as a UN
official). Mini-drone production would be easier to hide than chemical
plants --- for one thing, it doesn't stink like a lot of toxic chemicals
--- but [investigative techniques have advanced since the CWC entered
into force]{.underline} in 1997. It might even be possible, Beridze
said, to set an AI to catch an AI: use artificial intelligence to crunch
big data --- social media or parts orders, for example --- and correlate
subtle clues no human inspector could catch. Robust inspections,
however, are only one part of the solution, he told me. [Countries
need]{.underline} not only [to sign the treaty]{.underline} but use
their own intelligence agencies and domestic law enforcement to watch
for violations. And, after initial reluctance in the private sector,
"buy-in and participation of the chemical industry... was absolutely
essential," he said. "Otherwise this treaty would not work." Once
[compliance became a norm in the chemical industry, in large part
because of the moral stigma that attached to chemical
weapons]{.underline}, it became much harder to produce poison gas in
militarily significant amounts. [Given widespread anxiety in the tech
community about lethal AI, it should be possible to reach a similar
consensus among drone manufacturers]{.underline} --- eventually. Getting
private industry, law enforcement, and national governments on board,
even simply making them aware of the problem, would take years.

**[1NC - EU Counterplan]{.underline}**

**CP Text -- The United States should increase is security cooperation
with the European Union to adopt ethical principles ensuring human
control in military artificial intelligence systems.**

**US/EU collaboration is the best solution to ethical AI -- visits and
dialogues prove that information sharing and joint R&D are effective.**

**Lawrence and Cordey, 2020 -- researchers for The Cyber Project at the
Belfer Center for Science and International Affairs** \[Christie and
Sean, August, The Cyber Project Paper "The Case for Increased
Transatlantic Cooperation on Artificial Intelligence Edited by Lauren
Zabierek and Julia Voo
https://www.belfercenter.org/sites/default/files/2020-08/TransatlanticAI.pdf
Acc. 4/21/22 TA\]

National Interests & Common Priorities These [challenges are many but
are not insurmountable. We recommend the following]{.underline} 16
[actions to facilitate the full realization of US-EU AI
collaboration]{.underline}. The complete rationale, recommendation,
sub-recommendations, and additional considerations are found in the
Challenges to Collaboration & Recommendations section. Summary of
Recommendations A1 [Shift the Narrative from Adversarial to
Collaborative:]{.underline} [The US should recognize the EU has its own,
sometimes competing, interests that will not change through antagonistic
demands alone.]{.underline} The EU should soften its stance on certain
issues, recognizing both that adversarial rhetoric against the US may
threaten collaboration and total technological sovereignty is unlikely.
A2 [Increase High-level Engagements]{.underline}: [High-level visits
highlight the importance placed on US-EU collaboration]{.underline},
enhancing understanding and providing opportunities for greater
alignment. Engagements should restart across the full interagency at the
highest levels (e.g. Director, Secretary, and Commissioner level) once
travel reopens or virtual substitutes are established. A3 Foster a
Like-Minded Coalition: Work together to build a larger coalition of
nations that share their AI vision. Combined efforts will act as a force
multiplier in strengthening alliances that serve as a counterweight to
China and authoritarian regimes' efforts on the global stage. B1
[Establish US-EU Dialogues:]{.underline} Establish a Track 1 dialogue,
potentially modelled after the Canada-EU Digital Dialogues, [to
strengthen relations, communicate points of agreement and disagreement,
share best practices, and identify collaboration across the entire AI
ecosystem]{.underline}. Additional considerations: These dialogues
should be inclusive, with not only government officials but also
representatives from academia, business, and civil society present, and
could be incorporated into existing Track 1 dialogues or an upcoming
US-EU summit. Track 1.5 and 2 should supplement this formalized
engagement. Related recommendations: [Dialogue can enable and strengthen
the execution of all other recommendations in this paper. B2 Increase
and Formalize AI-Related Joint R&D:]{.underline} Increase joint R&D
through various avenues (joint ventures, greater US involvement in
Horizon 2020, formal R&D agreement, coordinating international private
partnerships). Pool resources for greater impact and larger scale
research on topics of importance for both the US and the EU.
Sub-recommendation: Research partnerships should span across the entire
AI ecosystem, but we believe the healthcare, defense, and environmental
sciences sectors should be prioritized, as well as joint efforts to
operationalize principles, verification, and standards. Related
recommendations: C1, D3 B3 [Share Best Practices: Facilitate
coordination on priorities and findings, increase capacity building
through information sharing and best practices. This can occur between
the US and EU's various networks of Centers of
Excellence]{.underline}46, establishing a shared platform (like BILAT
4.0), or dialogues and networking events. Sub-recommendation: To guide
decisions and ensure AI R&D and use respects shared values, a focus on
applied AI ethics and operationalizing principles should be at the
table.

**The counterplan captures all of their "NATO Key" warrants --
like-mindedness and shared values**

**Lawrence and Cordey, 2020 -- researchers for The Cyber Project at the
Belfer Center for Science and International Affairs** \[Christie and
Sean, August, The Cyber Project Paper "The Case for Increased
Transatlantic Cooperation on Artificial Intelligence Edited by Lauren
Zabierek and Julia Voo
https://www.belfercenter.org/sites/default/files/2020-08/TransatlanticAI.pdf
Acc. 4/21/22 TA\]

[There are three key reasons the United States and the European Union
must increase collaboration across]{.underline} the entire
[AI]{.underline} ecosystem. 4. Global Good: [Transatlantic AI
partnerships and cooperation encourages innovation and applications that
enhance human welfare, strengthen the economies of the US and the EU,
and advance global security.]{.underline} 5. [Great Power Competition:
US-EU leadership of like-minded nations is needed in this age of great
power competition to tip the scales against efforts by authoritarian
governments]{.underline}---particularly China and Russia---to undermine
democracies. 6. [Shared Values: The US and the EU share fundamental
values and would benefit from joint efforts to establish AI
norms]{.underline} [that would more effectively advance their common
vision of AI and ripple throughout the global AI ecosystem.]{.underline}

**[\--Extend -- EU Solvency]{.underline}**

**The US and EU can shift the debate from defining LAWs to addressing AI
more broadly.**

**Lawrence and Cordey, 2020 -- researchers for The Cyber Project at the
Belfer Center for Science and International Affairs** \[Christie and
Sean, August, The Cyber Project Paper "The Case for Increased
Transatlantic Cooperation on Artificial Intelligence Edited by Lauren
Zabierek and Julia Voo
https://www.belfercenter.org/sites/default/files/2020-08/TransatlanticAI.pdf
Acc. 4/21/22 TA\]

Defense H1 [Shift the Narrative Away from Lethal Autonomous
Weapons]{.underline} (LAWS): [The development of lethal autonomous
weapons is causing tension that may prohibit substantive discussion
around new areas for collaboration. The US and the EU should shift the
conversation away from potential disagreement around LAWS and towards
shared defense priorities such as ensuring military interoperability
separate from autonomous weapons development.]{.underline} Related
Recommendation: A2 H2 Relax Restrictions on Third-Country Funding, IP
Rights: The EU should consider reviewing and changing its EDF and PESCO
regulations, allowing non-EU companies to receive funds and maintain IP
rights in certain collaborative research projects. The EU and member
state governments should consider not replicating these restrictions in
other defense-related R&D mechanisms and collaborative efforts with the
US. H3 The US and the EU should Strengthen their Defense-Related AI
Talent: [The US and the EU should work together to pool their defense AI
talent to address workforce gaps. This could include defense-related
talent exchanges, talent exchanges/secondments into industry to
strengthen AI literacy and skills, coordination on AI training and
educational programs, and sharing of associated best
practices.]{.underline}

**[\--Extend -- Global Modelling]{.underline}**

**US / EU collaboration on AI can establish norms for autonomous
weapons.**

**Lawrence and Cordey, 2020 -- researchers for The Cyber Project at the
Belfer Center for Science and International Affairs** \[Christie and
Sean, August, The Cyber Project Paper "The Case for Increased
Transatlantic Cooperation on Artificial Intelligence Edited by Lauren
Zabierek and Julia Voo
https://www.belfercenter.org/sites/default/files/2020-08/TransatlanticAI.pdf
Acc. 4/21/22 TA\]

[Although the US consistently sounds the alarm bells around China's
AI]{.underline} aspirations and the EU urges international efforts
against AI that violates fundamental rights, increasingly noting China's
actions with concern,8 [little concrete international action has taken
place. The United States and the European Union's ongoing reassessment
of their respective AI strategies and legislation provides a window of
opportunity to align and collaborate. Transatlantic AI cooperation is at
a critical juncture and the United States and the European Union should
seize this opportunity to take concrete actions.]{.underline} The
Current State The United States and the European Union are separately
assessing and updating their AI strategies. However, it is a myth to
assume they are not collaborating at all to advance their AI-related
goals. [Transatlantic cooperation on AI norms, standards, research and
development, and data sharing should increase, but the United States and
the European Union can build upon an existing foundation for a stronger
alliance.]{.underline}

**[\--AT EU and US cannot Cooperate]{.underline}**

**The US and the EU can overcome differences -- we have more shared
commonalities than difference.**

**Lawrence and Cordey, 2020 -- researchers for The Cyber Project at the
Belfer Center for Science and International Affairs** \[Christie and
Sean, August, The Cyber Project Paper "The Case for Increased
Transatlantic Cooperation on Artificial Intelligence Edited by Lauren
Zabierek and Julia Voo
https://www.belfercenter.org/sites/default/files/2020-08/TransatlanticAI.pdf
Acc. 4/21/22 TA\]

Challenges to Collaboration & Recommendations This paper argues that
[the US and the EU should increase their AI collaboration and
partnerships to advance a world where AI is used to]{.underline} better
peoples' lives, grow national economies, safeguard liberal values and
civil liberties, [increase global security and safety, and promote
peace]{.underline}. However, [it would be naïve to assume such
collaboration would be without difficulties.]{.underline} This section
provides an overview of the obstacles---real and perceived---that
experts and government officials cite as potential impediments to
transatlantic cooperation. We broadly consider the challenges to full
collaboration as falling into five buckets that, although distinct,
interconnect with each other (see Figure 23 below). [Although the list
of challenges is extensive, we believe the US and the EU share more
commonalities than differences]{.underline}. In this section, we
therefore 1) provide an overview of the obstacles and explain why they
are not insurmountable and 2) recommend actions that both narrow
divergences and build on areas of agreement. [Ultimately, the US and the
EU can undertake deliberative, strategic steps towards the full
realization of AI collaboration that benefits their citizens, societies,
and economies.]{.underline}

**[Cybernetics Links]{.underline}**

**The Affirmative plan and discourse rely on humanist assumptions. The
focus on human control reinforces the human/machine binary, which
justifies human and team violence by condemning violence by AI. The
focus on human dignity centers Value on human life, which Devalues and
Marginalizes machine or cyborg life as "artificial."**

**Jones, 2018 - Lecturer in Law at the University of Essex** \[Emily "A
Posthuman-Xenofeminist Analysis of the Discourse on Autonomous Weapons
Systems and Other Killing Machines" Australian Feminist Law Journal
44(1) August 1 https://doi.org/10.1080/13200968.2018.1465333 Acc 2/2/21
TA\]

4.1 Challenging the Humanist Discourse around Autonomous Weapons: A
Posthuman Analysis Despite the challenges posed, [calls for a ban remain
strong. However, there are very large differences between the people who
call for a ban. NGOs]{.underline}, for example, [tend to take a
humanitarian stance, noting the need to promote a humanist
ethics]{.underline} and uphold the rules of IHL. Alternatively, however,
there are those in this group who do not fundamentally come from a
humanist background but come from the perspective of futurism.113 Whilst
[the position of NGOs]{.underline} comes from the humanist discourse of
IHL and the need to protect human life over all others, [aligning more
with the human dignity standpoint]{.underline} of religious leaders,
some tech experts like Elon Musk fully embrace the posthuman future
while working to ensure that this future remains ethical, albeit working
within the confines of capitalism.114 The will to construct the future
from the now can be seen through the project OpenAI, which Musk
co-founded.115 OpenAI is a project which seeks to disrupt the current
corporate trend in AI research, where most of the research into AI is
being done by large companies such as Google or in research centres such
as MIT, with many of the findings of this research kept private in the
hope of using them for profit. OpenAI is a non-profit organisation which
seeks to disrupt corporate monopolies on AI research by committing to
make all of its research and patents public, as well as through offering
to work freely with any group or organisation.116 Noting the threat AI
could pose to humanity, OpenAI aims to create a friendly General
Artificial Intelligence (GAI) - a system which can do more than just one
thing such as speak or play chess; one which has 'human'
characteristics.117 OpenAI thus represents a clear, strategic effort to
disrupt current trends in technology and AI development, including the
will to make profit, aiming to bring a different ethical standpoint to
the realm of AI research working, hopefully, to create a better
future.118 [All the groups who call for a preventative ban fundamentally
agree that machines should not make life/death decisions.]{.underline}
However, while NGOs seek to protect humans now within the confines of
the present, with the risk of becoming out of date, futurist tech
experts fundamentally believe that 'the best way to predict the future
is to invent it'.119 OpenAI represents a move towards shaping
technological advancement now to prevent the existence of killer
machines. Such aims are very similar to xenofeminist aims: the wish to
appropriate technology for feminist aims. Whilst xenofeminism does not
explicitly address the threat to life technology could pose, I argue
that the wish to define and use technology for feminist aims inherently
includes the wish to ensure that technology remains 'friendly'.120 While
OpenAI wishes to create a friendly AI, however, they do not explicitly
define what 'friendly' means. Xenofeminism, on the other hand, is
explicitly informed by a feminist ethos which seeks to ensure that
'friendly' means non-killer as a minimum. There is a need, as I will
suggest, to bring these two bodies of thought together. Gender theory is
required in this area due to its long history of theorising and applying
a set of complex and nuanced tools which seek to promote equality and
justice, these ethics forming what I argue should constitute 'friendly'
technology. [While there is a need to ensure]{.underline} that IHL is
upheld and that [robots do not kill, it is also clear that this ethical
dilemma does not just apply to autonomous weapons]{.underline}. While
the organisations mentioned above clearly have at least some form
anti-militarism as a core aim,121 the temporal horizon of such NGOs
remains limited. [NGOs largely maintain a strict humanist
stance,]{.underline} focusing on the realm of international law and the
need to promote and protect existing legal frameworks such as IHL [which
situate the human at the centre of the paradigm.]{.underline} Whilst
this has historically produced many great achievements, [such a humanist
stance cannot be applied given the rapid pace at which technology is now
developing. AI may not only pose a threat to the lives of humans where
designed, purposefully, to kill]{.underline}, but may also pose a threat
to life in and of itself, as it exists. There is a need for all groups
who are working to promote ethical technologies to consider not only
what may seem attainable now, but what is feasibly attainable in the
future, noting the ways in which the now can be used to construct the
future. [NGOs continue to situate the human as the centre of their
paradigm, seeing the machine as the 'other' to the human. This can be
seen in the way in which autonomy is discussed in relation to the human
who is imagined as either in/on/out of the loop, which does not account
for the ways in which humans and machines work in
connection.]{.underline}122 [Posthumanism,]{.underline} on the other
hand, [challenges the centrality of the human within Western thinking,
working to re- think the human/machine binary]{.underline}. Feminist
posthumanism notes that something else is needed; a new way of defining
subjectivity which sees the complexities and interconnections between
humans and others: nature, technology, animals, etc., rejecting the
human as the central paradigm and noting how the human is located
instead 'in the flow of relations with multiple others'.123 [The
humanist discourse around autonomous weapons ignores the posthuman
reality that humans and machines are already working in connection with
one another. Life/death decisions are already being made by
human-machine combinations]{.underline}, as the next section
illustrates. [Banning autonomous weapons is not enough: there is a need
to]{.underline} consider the ways in which machines are already making
these decisions and to [create ethical frameworks for these and future
technologies, rebutting the false exceptionalisation which surrounds the
current discourse on autonomous weapons]{.underline}. 4.2 Machine-Human
Life/Death Decision Making As Jasanoff notes, technology is based around
a set of decisions.124 Jasanoff observes that we often do not question
these decisions until there is an accident -- at which point we find who
made the mistake.125 However, she argues that people 'have spent a great
deal more energy thinking about how to make good laws than about how to
design good technological objects'.126 Yet, she continues, 'in
democratic societies, uncontrolled delegation of power is seen as a
basic threat to freedom'.127 We must 'understand how power is delegated
to technological systems'.128 The need to understand how power and
decision-making are delegated to technology is ever more urgent in the
realm of life/death decision-making. However, as noted, the humanist
discourse on autonomous weapons fails to account for the ways in which
humans and machines are already working together to make life/death
decisions. The most obvious example is the use of programming and
algorithms in drone warfare. While many drone strikes are conducted as
'personality strikes' -- i.e. strikes on a particular, key, well-known
person -- these occur only a few times a year, with 'signature strikes'
happening a few times a week.129 These attacks are conducted on the
basis of a 'pattern of life' analysis. 'Pattern of life' analysis
develops a profile of an individual or a network of individuals by
drawing on all the intelligence available, which includes things like
drone and other aerial surveillance intelligence, communications
interceptions, as well as phone tapping information and GPS tracking
information.130 What becomes clear in 'drone warfare', therefore, is
that the drone itself is only one part of a broader system which
includes big data, algorithms, intelligence collection, chains of
command, and bureaucratic formations, among other technologies and
practices.131 This data is then often combined with individual tracking
through the use of mobile phone and GPS tracking systems in order to
both watch movements as well as to target individuals.132 The gathering
of this information builds up to create a file of information collected
by machines which, as Chamayou has noted 'once it becomes thick enough,
will constitute a death warrant'.133 This is an example of part-machine
life/death decision-making. Part of the decision-making process here is
already done by machines which gather this data and predict the
likelihood of an individual's involvement with terrorist organisations.
While the human is clearly involved, in that they then must note the
results of the data collected, deem it enough to act upon and then
operate the drone to kill the subject in question, the machine and the
human are making life/death decisions together. It is also worth
nothing, as Wilcox has shown, the ways in which this data is often
interpreted in racialised and gendered ways.134 Such processes of
human-machine life/death decision-making would not be covered under a
ban of autonomous weapons. It thus seems that part of [the problem with
the debate around autonomous weapons is the debate around autonomy
itself. By trying to define autonomy instead of working to understand
automation and autonomy as in continuum, international debates on
autonomous weapons other the machine from the human, creating a false
paradigm.]{.underline} Such a limited account of autonomy works to set
the standard so high for machine decision-making that, in the end,
almost nothing may be covered under a ban. In the meantime, machines are
already making of life/death decisions alongside humans. Machine
involvement in such decision-making processes is only set to increase,
as the next section will illustrate.

**Military Doctrine determines the Status and Value of artificial
intelligence -- it can define AI as a tool, or as an autonomous agent.**

**Vestner, 2021 - Head of Security and Law Programme at Geneva Centre
for Security Policy** \[Tobias, July 8 "Warfare and Artificial
Intelligence" in Robin Geiß and Henning Lahmann (eds), Research Handbook
on Warfare and Artificial Intelligence -forthcoming
https://www.gcsp.ch/publications/military-operations-and-artificial-intelligenceGCSP
Acc 5/27/22 TA\]

In response to AI's particular characteristics, [military doctrine is
the appropriate means to define how armed forces perceive, understand,
and value AI. Due to AI's high levels of autonomy, armed forces may need
to specify whether AI is considered as a technical tool or rather as an
agent]{.underline}. In this sense, [doctrine can define if the armed
forces perceive AI as simply a mathematical, technical system, or rather
a tool with cognitive abilities which can act as an autonomous
influencer]{.underline}.51 As a corollary and based on doctrine's
function to shape armed forces' culture, principles, and identity,
[doctrine can define the value, place, and role of humans in the
organization and its processes]{.underline}. [Since military operations
and warfare remain endeavours for human purposes in a human world,
doctrine can specify what this means]{.underline}. In this context,
[doctrines can also define values and principles on human interaction
with AI systems, including that AI needs to serve humans and not the
opposite.]{.underline}

**[Security K Links]{.underline}**

**State discourse on AI shapes the norms for conflict -- the Aff
portrays AI technology as "inevitable" and a "competition." This shapes
our perceptions of other states.**

**Bode, 2021 - Professor of International Relations at the University of
Southern Denmark** \[Ingvild June 25, AutoNorms "Reflecting on the
Future Norms of Warfare"
https://www.autonorms.eu/reflecting-on-the-future-norms-of-warfare-2/
Acc. 5/27/22 TA\]

Further, [in terms of other factors that may continue to shape/disrupt
current norms of conflict and warfare, we should also consider the
arguments that states offer]{.underline}. [State discourse often points
to the inevitability of integrating more and more AI into weapon
systems. The reason for this is often two-fold: first, states map out
weaponised AI as part of a steady, irresistible process that they cannot
resist]{.underline}. [Second, we have also seen an emphasis on great
power competition resurfacing. From a US]{.underline} (and a UK)
[perspective, it is necessary to include ever more AI into weapon
systems because China]{.underline} ([or]{.underline} to a lesser extent
[Russia]{.underline}) [are doing it. China and Russia, in turn, speak of
the same necessity as a reaction to]{.underline} [US (and allied states)
moves]{.underline}. What gets lost in these dynamics is that [both
options represent particular]{.underline}, not exclusive [courses of
action that policymakers can or cannot engage in]{.underline}. In sum,
tracking [how norms of conflict and warfare change, especially
considering emerging technologies, requires going beyond a narrow and
toward a critical understanding of international law]{.underline}. I
argue that we can expect to see norms, when defined broadly as
understandings of appropriateness, emerging in practices of designing,
training for, and operating (novel) weapon systems. How these norms
relate to standards enshrined in international law is an active research
question.

**"Inevitability" arguments are deterministic -- they rely on the
assumption that technology shapes society.**

**Trabucco and Stanley-Lockman, 2022 -- prof of Political Science,
University of Copenhagen and prof of Defense and Strategic Studies,
Nanyang Technological University** \[Lena and Zoe, The Oxford Handbook
of AI Governance, March, "NATO's Role in Responsible AI Governance in
Military Affairs"
https://www.oxfordhandbooks.com/view/10.1093/oxfordhb/9780197579329.001.0001/oxfordhb-9780197579329-e-69
Acc 4/16/22 TA\]

However, this is complicated by the reality that [military organizations
that see technological superiority as a core element of deterrence and
defense, including NATO, engage in forms of technological
determinism]{.underline} that STS scholars squarely reject. Respective
views on [technological determinism]{.underline}---which [considers that
technology shapes society as a largely autonomous process with limited
human agency---thus creates a tension for governance
prospects.]{.underline}15 To spotlight the aspects of military
innovation related to governance, this section briefly expands on the
overlaps and tensions between STS and military innovation literature.

**Our rhetoric of an "Arms Race" is threat construction -- current AI
development is not a race.**

**Scharre, 2021 - Director of Studies at Center for New American
Security** \[Paul, Texas National Security Review Vol 4, Iss 3 Summer
"Debunking the AI Arms Race Theory"
https://tnsr.org/2021/06/debunking-the-ai-arms-race-theory/Artificial
Intelligence Acc 5/27/22 TA\]

[Current Military AI Competition Is Not an "Arms Race"]{.underline} [As
Heather Roff has written, the arms race framing "misrepresents the
competition going on among countries."5 To begin with, AI is not a
weapon. AI is a general-purpose enabling technology with myriad
applications.]{.underline} [It is not like a missile]{.underline} or a
tank. [It is more like electricity]{.underline}, the internal combustion
engine, or computer networks.6 General-purpose technologies like AI have
applications across a range of industries. Wired magazine co-founder
Kevin Kelly has argued that it "will enliven inert objects, much as
electricity did more than a century ago. Everything that we formerly
electrified we will now cognitize."7 [Nations may very well be in a
technology race to adopt AI across a range of industries]{.underline}.
AI will help to improve economic productivity and, by extension,
economic and military power. During the industrial revolution, early
adopters of industrial technology significantly increased their national
power. From 1830 to 1890, Britain and Germany, which were both early
industrializers, more than doubled their per capita gross national
product while Russia, which lagged in industrialization, increased its
per capita gross national product by a mere 7 percent over that 60-year
period.8 These technological advantages led to increased economic and
military power, most notably for Europe relative to the rest of the
world. In 1790, Europe (collectively), China, and India (including what
is now Pakistan and Bangladesh) held roughly the same shares of global
manufacturing output, with Europe and India each holding about
one-quarter of global manufacturing output and China holding roughly
one-third. They all had approximately equivalent levels of per capita
industrialization at that time. But the industrial revolution
skyrocketed European economic productivity. By 1900, Europe collectively
controlled 62 percent of global manufacturing output, while China held
only six percent and India less than two percent. These economic
advantages translated into military power. By 1914, Europeans occupied
or controlled over 80 percent of the world's land surface.9 Being ahead
of the curve in adopting AI is likely to lead to significant national
advantages. Although AI can increase military capabilities, the more
consequential advantages over the long term may come from non-military
AI applications across society. Long-term benefits from AI could include
increased productivity, improved healthcare outcomes, economic growth,
and other indicators of national well-being. Increasing productivity is
especially significant because it has a compounding effect on economic
growth. Over the long term, technological progress is the main driver of
economic growth.10 [The scale of military AI spending, at least at
present, is nowhere near large enough to warrant the title of "arms
race."]{.underline} Of course, AI can also be used for weapons.
Militaries around the world are actively working to adopt AI to improve
their military capabilities. Yet [the militarization of AI does not, at
present, meet the traditional definition of an arms race, despite the
rhetorical urgency of many national leaders]{.underline}. Michael D.
Wallace, in his 1979 article "Arms Races and Escalation," defined an
arms race as "involving simultaneous abnormal rates of growth in the
military outlays of two or more nations" resulting from "the competitive
pressure of the military itself, and not from domestic forces exogenous
to this rivalry." Wallace further stated that [the concept of an arms
race only applied "between nations whose foreign and defense policies
are heavily interdependent" and who have "roughly comparable"
capabilities.11 AI is being adopted by many countries around the
globe.]{.underline}12

**Threat Construction creates a security dilemma -- if we believe we are
insecure because we are losing "the arms race" we will create a
self-fulfilling prophecy where we race to catch up.**

**Horowitz and Scharre, 2021 - Senior Fellows at the Technology and
National Security Program at the Center for a New American Security**
\[Michael and Paul, Jan 12, "AI and International Stability: Risks and
Confidence-Building Measures"
[https://www.cnas.org/publications/reports/ai-and-international-stability-risks-and-confidence-building-measures
Acc
6/6/22](https://www.cnas.org/publications/reports/ai-and-international-stability-risks-and-confidence-building-measures%20Acc%206/6/22)
TA\]

[A]{.underline}n additional [challenge stems from security dilemma
dynamics. Competitive pressures could lead nations to shortcut test and
evaluation (T&E) in a desire to field new AI capabilities ahead of
adversaries.]{.underline} Similar [competitive pressures to beat others
to market appear to have played an exacerbating role in accident risk
relating to AI systems in self-driving cars]{.underline} and commercial
airplane autopilots.23 [Militaries evaluating an AI system of uncertain
reliability could]{.underline}, not unjustifiably, [feel pressure to
hasten deployment if they believe others are taking similar
measures]{.underline}. Historically, these pressures are highest
immediately before and during wars, where the risk/reward equation
surrounding new technologies can shift due to the very real lives on the
line. For example, competitive pressures may have spurred the faster
introduction of poison gas in World War I.24 Similarly, in World War II,
Germany diverted funds from proven technologies into jet engines,
ballistic missiles, and helicopters, even though none of the
technologies proved mature until after the war.25 [This dynamic risk
might spark a self-fulfilling prophecy in which countries accelerate
deployment of insufficiently tested AI systems out of the fear that
others will deploy first.26 The net effect is not an arms race but a
"race to the bottom" on safety, leading to the deployment of unsafe AI
systems and heightening the risk of accidents and
instability.]{.underline}

**"Arms Race" rhetoric increases the risk of international instability
because it pressures states to deploy untested and unsafe AI weapons
systems.**

**Scharre, 2021 - Director of Studies at Center for New American
Security** \[Paul, Texas National Security Review Vol 4, Iss 3 Summer
"Debunking the AI Arms Race Theory"
https://tnsr.org/2021/06/debunking-the-ai-arms-race-theory/Artificial
Intelligence Acc 5/27/22 TA\]

[A]{.underline} related [risk of a "racing" dynamic]{.underline} among
competitors [could come from an acceleration]{.underline}, not of the
pace of operations on the battlefield, but of the process [of fielding
new AI systems]{.underline}. [AI systems today have a host of safety and
security problems that can make them brittle, unreliable, and
insecure]{.underline}.29 Because machine learning in particular can
create new ways in which systems can fail, militaries face novel
challenges in adopting AI systems.30 [Militaries will have to adopt new
methods to test, evaluate, verify, and validate AI systems]{.underline}
(also known as TEVV).31 Such concerns related to autonomy are well known
in the U.S. defense community,32 although at present they have not been
solved to a satisfactory degree. Machine learning introduces additional
challenges with regard to testing, evaluation, verification, and
validation. [A rush to field AI systems before they are fully tested
could result in a "race to the bottom" on safety, with militaries
fielding accident-prone AI systems.]{.underline} There are strong
bureaucratic and institutional imperatives for militaries to field
systems that are robust and secure. Indeed, designing systems to
military specification standards often means making them more robust for
a wider range of environmental conditions and shocks than comparable
commercial systems, even at the expense of other aspects of performance,
such as size, weight, or usability. AI presents novel challenges,
however, in achieving the robustness needed for operating in the
complex, hazardous, and adversarial environments that often characterize
military operations. Certain AI methods today, such as deep learning,
remain relatively immature with significant reliability challenges. A
2017 Department of Defense report by the JASON scientific advisory group
explained that deep neural networks are immature as regards the
"illities", including reliability, maintainability, accountability,
validation and verification, debug-ability, evolvability, fragility,
attackability, and so forth. ... Further, it is not clear that the
existing AI paradigm is immediately amenable to any sort of software
engineering validation and verification. This is a serious issue, and is
a potential roadblock to DoD's \[Department of Defense's\] use of these
modern AI systems, especially when considering the liability and
accountability of using AI in lethal systems.33 [The Defense
Department's 2018 AI strategy calls for building AI systems that are
"resilient, robust, reliable, and secure."]{.underline}34 Yet, the
current state of technology makes achieving this goal particularly
difficult for AI systems that incorporate deep learning, a subfield of
AI that has seen significant growth and attention in recent years. While
there is active research underway to improve AI safety and security,
militaries will have to adapt to the technology as it currently is, at
least for the time being. [An ideal process would be for militaries
to]{.underline} engage in experimentation, prototyping, and concept
development, but also to [subject AI systems to rigorous TEVV under
realistic operational conditions before deployment. Taking shortcuts on
testing and evaluation and fielding a system before it is fully tested
could lead to accidents, which, in some settings, could undermine
international stability.]{.underline}
