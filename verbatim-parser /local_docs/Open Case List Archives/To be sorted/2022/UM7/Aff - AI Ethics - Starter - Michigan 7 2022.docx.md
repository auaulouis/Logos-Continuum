# Aff -- AI Ethics Aff -- UM22 Starter Packet

## Starter Notes

#### Brief Explanation

The militaries of the world are incorporating Artificial Intelligence
into their weapons, planning, operations, logistics -- pretty much
everywhere. Some countries are going all-in -- developing Lethal
Autonomous Weapons (LAWs - killer robots) and nano-drone swarms, known
charmingly as Genocide Swarms. Others are more cautious and are limiting
their AI use to wargame simulations, or targeting mods. Perhaps the
biggest issue in controversy is the level of Autonomy. Fully autonomous
weapons exclude humans from the kill chain -- search, target
acquisition, engagement, execution. Low Autonomy weapons might be an AI
adversary in a simulated attack against Russian forces in Ukraine. Very
few states have (allegedly) developed or deployed fully autonomous
weapons, but as the technology improves exponentially, that is a
possible future.

This affirmative plan calls for human control -- a "human in the loop"
who has final say over AI functions. It has the US engage NATO in
establishing ethical principles, known as RAI (responsible AI use), for
all NATO operations and weapons. There are three reasons for this:

Cohesion. If everyone in NATO is on the same page and coordinated on
technology and tactics, that is called Interoperability. The command,
political, and public will to get coordinated is called Cohesion.
Interoperability and Cohesion are the foundation of effective coalition
military forces and operations. AI threatens to disrupt that cohesion
and interoperability. If the US has Fully Autonomous weapons, and Italy
does not, then those weapon systems will have trouble interoperating
(not sure if that is a word). If the US is using mostly automated
systems, and France is sending mostly human troops, then France might
ask why they should risk French lives when the US is not. If the US
spends A Lot on Fully Autonomous weapons, but Lithuania doesn't have
that technology, then the US might feel that other nations are not
sharing the burden. If the British public opposes LAWs, and they are in
a conflict allied with the US, who is using LAWs, then the British
public might demand that their government stop supporting the operation.
Or the US might perceive that the UK would stop supporting it. Dialogue
to cooperate to establish norms for AI throughout the alliance would
improve cohesion and interoperability, which makes for a more effective
and credible NATO, which stops a nuclear war.

Crisis Instability. Many of the new AI weapons are "brittle" -- they
tend to react poorly when the situation goes outside of their programmed
parameters. Like if the Ukrainians put reflective tape on stop signs to
disorient the Russian drones piloted by AI. For example, without a human
in the loop, AI accelerates battle decisions to machine speed. If
something goes wrong, it goes Really Wrong Really Fast, without anyone
able to check the escalation to nuclear war. Back in the 80's, Russian
Early Warning Radars detected what it thought to be a US nuclear attack
and signaled that to Soviet Rocket Command. But a Lieutenant Colonel --
Petrov -- trusted his gut instinct and said "Maybe is birds, comrade."
Crisis averted. AI would have sent the nuclear weapons in a split
second. There are many crises in the future for Europe, and
miscalculation or instability in them could escalate a conflict
accidentally or mistakenly. Human control would provide a stop in the
escalation chain to prevent war.

Human Dignity. Autonomous AI weapons would make decisions about life and
death by algorithm. The soldiers who are killed would just be part of a
decision tree -- depersonalized and objectified. Civilian casualties
would become data points. This is unethical because it does not respect
human dignity. Now, to be clear -- all war is undignified. A human
soldier shooting you leaves you just as dead as an autonomous drone.
However, that soldier shares your humanity. They feel compassion, and
hope, and empathy, and regret. They know what it means to take a life
because they value their own. AI has none of that -- it cannot "know"
what it means to kill or die, because it cannot "feel" life. For better
or worse, if war involves lethal decisions, the choice to kill another
person should be made by a person. International and Human Rights laws
are founded on the principle of Human Dignity. Our actions, arguments,
motives and intentions must respect the Dignity of all persons. Death by
algorithm does not respect the dignity of individual people, because AI
cannot "know" what it means to be alive. (This is where the Cybernetics
K comes in...) This is a deontological argument, not a consequential or
utilitarian one. It says that before you decide if plan has good or bad
outcomes, you first have to determine whether it is Right or Wrong. If
AI weapons are unethical, you never even evaluate if they have good
consequences -- that is the Ethics First framework argument.

#### Some Comments

The 1AC is WAY too long to read all three advantages -- it is impossible
to run them all. DON'T destroy the quality of the evidence by trying to
highlight it down to read all three. Instead, choose two, or maybe even
one, advantage to read, and cut out cards before highlighting into
incoherence. For instance, if you want to run Human Dignity, that works
pretty well all by itself.

I know that there are three inherency cards and like 8 extension or AT
blocks on inherency, and that is too much. In fairness, this case is
probably not inherent, so "Be Prepared!"

A lot of the literature and international debate is about LAWs. It's
more exciting to talk about Killer Robots. However, that is only a small
fraction of the military uses for AI in NATO, so don't get hung up on
that. On the other hand, "human control" means that weapons are no
longer "autonomous" -- at least not fully autonomous. So the plan does
de facto ban LAWs.

## 1AC

### 1AC - Inherency

#### Contention One -- Inherency -- The Pentagon does not require Human Control for artificial intelligence in weapons.

**Allen, 2022 - Director, AI Governance Project, Strategic Technologies
Program at CSIS** \[Gregory C. June 6 "DOD Is Updating Its Decade-Old
Autonomous Weapons Policy, but Confusion Remains Widespread"
[https://www.csis.org/analysis/dod-updating-its-decade-old-autonomous-weapons-policy-confusion-remains-widespread
Acc
6/6/22](https://www.csis.org/analysis/dod-updating-its-decade-old-autonomous-weapons-policy-confusion-remains-widespread%20Acc%206/6/22)
TA\]

[In November 2012, the Department of Defense (DOD) released its policy
on autonomy in weapons systems: DOD Directive 3000.09]{.underline} (DODD
3000.09). [Despite being nearly 10 years old, the policy remains
frequently misunderstood, including by leaders in the U.S.
military]{.underline}. For example, in February 2021, [Colonel Marc E.
Pelini]{.underline}, who at the time was the division chief for
capabilities and requirements within the DOD's Joint Counter-Unmanned
Aircraft Systems Office, [said,]{.underline} "Right now we don\'t have
the authority to have a human out of the loop. [Based on the existing
Department of Defense policy, you have to have a human within the
decision cycle at some point to authorize the engagement.\" He is simply
wrong. No such requirement appears in DODD 3000.09, nor any other DOD
policy. Misconceptions]{.underline} about DODD 3000.09 [appear to extend
even to high-ranking flag officers]{.underline}. In April 2021, General
Mike Murray, the then-four-star commander of Army Futures Command, said,
"Where I draw the line---and this is, I think well within our current
policies---\[is\], if you're talking about a lethal effect against
another human, you have to have a human in that decision-making
process." [Breaking Defense]{.underline}, a news outlet that [reported
on Murray's remarks at the time, stated that the requirement to have a
human in the decisionmaking process is "official Defense Department
policy." It is not. DODD 3000.09 does not ban autonomous weapons or
establish a requirement than U.S. weapons have a "human in the
loop."]{.underline} In fact, that latter phrase never appears in DOD
policy. [Instead, DODD 3000.09 formally defines what an autonomous
weapon system is and requires any DOD organization proposing to develop
one to either go through an incredibly rigorous senior review process or
meet a qualifying exemption]{.underline}. Regarding the latter, cyber
weapons systems, for example, are exempted. [The DOD recently announced
that it is planning to update DODD 3000.09 this year. Michael Horowitz,
director of the DOD's Emerging Capabilities Policy Office]{.underline}
and the Pentagon official with responsibility for shepherding the
policy, praised DODD 3000.09 in a recent interview, stating that "the
fundamental approach in the directive remains sound, that the directive
laid out a very responsible approach to the incorporation of autonomy
and weapons systems." While not making any firm predictions, Horowitz
[suggested that major revisions to DODD 3000.09 were
unlikely]{.underline}.

#### Our military does not prioritize ethics in the deployment of AI.

**Lawrence and Cordey, 2020 -- researchers for The Cyber Project at the
Belfer Center for Science and International Affairs** \[Christie and
Sean, August, The Cyber Project Paper "The Case for Increased
Transatlantic Cooperation on Artificial Intelligence Edited by Lauren
Zabierek and Julia Voo
https://www.belfercenter.org/sites/default/files/2020-08/TransatlanticAI.pdf
Acc. 4/21/22 TA\]

In recent months, Head of Global Governance, Regulation, Innovation and
the Digital Economy at the Centre for European Policy Studies Andrea
Renda pointed out that interests and [political support for greater
transatlantic coordination and collaboration on AI seems to be
increasing]{.underline}. This is notably demonstrated by the visit in
early 2020 of high-level officials on both sides of the Atlantic. The
first, as already mentions in the Political collaboration section, was
the visit in January from former Director of [the JAIC Lt. Gen. Jack
Shanahan]{.underline} and then CTO Nand Mulchandani to Brussels. During
their visit to the NATO headquarters and with EU leaders, Lt. Gen.
Shanahan [underlined the need for greater collaboration for AI,
particularly within NATO, in order to counter revisionist states such as
China and Russia that promote digital authoritarianism]{.underline}. As
Acting Director of the JAIC Nand Mulchandani mentioned, Lt. [Gen.
Shanahan also underscored the need for the US and Europe to rally around
common values and not allow technicalities around ethics to block
collaboration]{.underline}. 279 The second high level visit was by a
delegation from the European Parliament's Civil Liberties Committee,
which visited Washington D.C and Boston in February 2020. The delegation
held meetings with US Congressional members and representatives across
the US Administration, including the DOS, DOJ, DHS, FTA, FBI, and PCLOB.
The Members of the European Parliament (MEPs) also held meetings with
representatives from industry, universities, think-tank, start-ups, and
NGOs. The discussions centered on a variety of AI topics, ranging from
potential future US federal legislation on personal data protection, the
use of artificial intelligence in law enforcement, and the visa waiver
program.280

#### NATO states have differing positions on responsible use for AI -- the potential exists for agreement, but states are implementing their own policies. 

**Stanley-Lockman, 2021 - Center for Security and Emerging Technology**
\[Zoe CSET Issue Brief August "Responsible and Ethical Military AI
Allies and Allied Perspectives"
https://cset.georgetown.edu/publication/responsible-and-ethical-military-ai/
Acc. 4/21/22 TA\]

[With more of a focus on norms and state responsibility, a broader
definition of responsibility beyond the DOD "responsible AI" principle
can also introduce new convergences for U.S]{.underline}. RAI
implementation. The DOD RAI Strategy and Implementation Pathway recently
tasked by [Deputy Secretary]{.underline} Kathleen [Hicks can incorporate
these views and help allies refine their approaches to AI ethics in
order to enable greater cooperation]{.underline} [and allied AI
adoption.]{.underline} [This is because many allies see firm approaches
to managing ethical risk as a prerequisite policy question before
investing in AI-enabled capability development]{.underline}---including
defensive systems and countermeasures. Overall, [international
engagement is mutually beneficial to responsible AI endeavors. The
United States should look at how other countries are implementing their
approaches, just as the United States can exert influence and maintain
its leadership role in responsible and ethical AI for defense by helping
its allies and partners form their own views]{.underline} in alignment
with one another. Conclusion [No single actor has a monopoly on the
answers to implementing responsible AI in]{.underline} any high-risk
area, let alone [defense. Cooperation is therefore important to
collectively navigate the difficulties of responsible governance of
emerging technologies]{.underline}. For DOD, the focus has been
predominantly on the transposition of safe and ethical AI principles
into action. [Rather than adopting principles for defense, some allies
are moving straight into implementation]{.underline}. Thus far, this is
borne out in tabletop exercises, outreach, ministerial committees,
ethical reviews, education and certifications, exercises and trials, and
defense programs of record. It is too early to judge these fledgling
efforts, but tracking their evolution may prove useful to broader AI
ethics implementation, be it for other defense ministries or even
civilian actors. While jumping straight to implementation can mean a
more pragmatic focus on tools, tracking how different AI stakeholders
use those tools may be more difficult. In this way, principles can be
seen as a helpful organizing force, as is the case for DOD. This said,
the scale of the U.S. military bureaucracy and national security
innovation base may require higher visibility relative to allied
counterparts. Still, another key difference is precisely this
visibility. The analysis here is based on information in the public
domain, which may also partially explain its transatlantic tilt. [The
U.S. approach to responsible and ethical AI for defense]{.underline}
also [differs from other countries in that the]{.underline} consultation
and [process]{.underline} that led to its principles [is far more
transparent than is true for most allies. A possible Catch-22 could be
at play here, with allies reticent to publicize approaches to such
controversial issues, despite the fact that offering such inroads can
build trust and confidence that governments are handling these
high-stakes questions responsibly.]{.underline}

### 1AC - Plan

#### Plan: The United States federal government should substantially increase its security cooperation with the North Atlantic Treaty Organization on ethical principles to ensure human control in artificial intelligence systems. 

### 1AC - Advantage -- Cohesion

#### Advantage -- NATO Cohesion

#### NATO does not have a coordinated position on the ethics of AI -- this undermines military and political cohesion, which undermines NATO militarily.

**Stanley-Lockman, 2021 - Center for Security and Emerging Technology**
\[Zoe CSET Issue Brief August "Responsible and Ethical Military AI
Allies and Allied Perspectives"
https://cset.georgetown.edu/publication/responsible-and-ethical-military-ai/
Acc. 4/21/22 TA\]

At the same time, [significant differences in ethical approaches to AI
in defense could imperil political cohesion and undermine coalition
success. Politically, alignment on ethics is important because shared
values are at the foundation of U.S. alliances]{.underline}.3 [This also
trickles down to the operational level]{.underline}, [where differing
views on ethics could mean that allies field their systems with
different]{.underline} legal authorizations and [rules of
engagement]{.underline}.4 [If coalition partners deem each others'
capabilities to be based on different legal, ethical, and doctrinal
assumptions, then forces may not be able to communicate and operate
togethe]{.underline}r.5 Further, [if different ethical bases for
capability development mean that some countries have higher thresholds
for what they develop]{.underline} and contribute to coalition
operations, [then others may perceive them as not equally sharing risks
to life]{.underline}.[6 As such, political cohesion and policy
considerations about ethics could directly influence operational
effectiveness]{.underline}. In other words, [failure to align allied
perspectives on AI ethics in defense will inevitably undermine the
ability of allied forces to understand each other and work
together]{.underline}.7

#### Diverse AI standards undermine military interoperability because it prevents different nations militaries from cooperating. NATO dialogue is key to coordinate ethical standards for military AI. Meaningful human control which respects human dignity is essential to political cohesion.

**van der Merwe, 2021 - Fellow Center for European Policy Analysis
Defense Tech Initiative** \[Joanna, Feb 17, "NATO Leadership on Ethical
AI is Key to Future Interoperability"
https://cepa.org/nato-leadership-on-ethical-ai-is-key-to-future-interoperability/
Acc 4/16/22 TA\]

In October 2020, [Deputy Secretary General of NATO Mircea Geoană
highlighted the benefits of establishing a "transatlantic community
cooperating on Artificial Intelligence (AI)."]{.underline} The Deputy
Head of NATO's Innovation Unit followed with a commitment to its
responsible use. [The US Department of Defense (DoD) adopted Ethical
Principles for AI in 2020 and has committed to bringing together NATO
member and partners to operationalize these principles. Despite these
statements and developments, more work is required to tackle the very
real challenge that ethical AI will pose to future interoperability
within NATO. Without a NATO-led initiative focused on aligning these
ethical principles across the Alliance, the interoperability risk of
nations fielding AI-based systems that hinder joint operations is
high]{.underline}. [As the foremost security framework for Europe and
North America]{.underline}, as well as the leading defense alliance for
promoting and protecting democratic values, [NATO is able to facilitate
alignment on this issue. As part of a broader strategy on emerging and
disruptive technologies, NATO must prioritize ethical AI if it wishes to
promote the shared values]{.underline} upon which it was founded, [play
a key role in facilitating innovation across the Atlantic, and
ultimately retain the ability of its members to undertake joint
operations]{.underline}. [Establishing NATO ethical AI principles is the
first step toward both technical and political alignment, in turn
enhancing and fostering interoperability, which is the foundation for
NATO to respond to emerging threats as an Alliance, in a flexible and
timely manner.]{.underline} A key challenge for NATO is raising
awareness that [the answers to ethical questions can no longer be left
to later stages of the development and procurement cycle. Decisions made
at the political and legal level will have a significant impact on the
engineering practices used to develop AI]{.underline}, as well as the
technical characteristics of the AI-based systems. [The answers to
questions such as respecting human dignity, human control, and
accountability will be the foundation upon which many technical elements
are programed]{.underline}. Systems developers need to make a number of
calls throughout the development cycle informed by the answers to key
questions, including: how to label data what data to use, and what is an
acceptable outcome? These answers will also impact how AI systems are
evaluated and ultimately deployed. [If individual nations or groups are
left to develop their own ethical principles without wider alignment to
NATO, the result will be a number of AI-based systems with varying
technical specifications based on the legal and policy decisions made by
individual governments when answering the key questions]{.underline}. As
has been demonstrated in areas such as facial recognition and policing
algorithms, the assumptions made by those developing the tools and
answering the key questions have a significant impact on the real-world
functioning of the tool and societal acceptance of its ethics. The risk
of tools failing to gain acceptance depends on the legal and ethical
decisions made by governments. For the military, this may mean one state
using an AI-based system that is seen as unacceptable by another, and in
a joint operation one state fielding a system that cannot be used by
another. Or worse yet, [this could render a joint operation impossible.
Without the ability to interoperate across NATO, the inability to
effectively and efficiently respond to future threats would undermine
the Alliance.]{.underline} The role of the private sector is another
aspect of ethical AI development that has proved a challenge to
governments and the transatlantic relationship. Within states,
governments have struggled to adequately regulate Big Tech firms, which
has led to these companies encroaching on government responsibilities to
protect and uphold the public interest. This encroachment permeates all
aspects of government, including defense and security. As Deputy
Secretary of Defense Kathleen Hicks discussed during her confirmation
hearings, the lack of competition is also a challenge to innovation in
the private defense industry. This, along with a lack of regulation,
feeds into the power imbalance between the sectors. Consequently,
private sector companies building the AI and AI systems that are or will
be deployed on the battlefield are deciding the ethics policies for
themselves. [The transatlantic partnership must focus on coordinating
these core principles and systematic governance to ensure AI systems
development aligns with the rule of law and democracy. In particular,
this must ensure answering questions about human dignity, human control,
and accountability.]{.underline} [NATO is the ideal defense and security
forum for this alignment. Given the US lead on adopting ethical
principles for the entire DoD and the EU's drive to assert checks and
balances for private-sector tech companies, NATO remains the
organization that can bring these two together and establishes the
ethical bottom line. These will then ensure the diverging legal and
ethical stances towards Big Tech do not lead to an interoperability
barrier in the future]{.underline}. If developments surrounding the
General Data Protection Regulation (GDPR) and the challenges it brought
for U.S.-based, data-driven companies are any indication, a strong
transatlantic led initiative is needed in order to ensure the same
challenges do not hinder NATO. [The solution to the challenge that
ethical AI poses for the future of interoperability within NATO is for
the Alliance to establish shared transatlantic ethical principles,
i]{.underline}nformed by the US DoD, the EU, and others. [Establishing
these principles will not only strengthen transatlantic political
relations; more technically, it will allow for the establishment of
standardization agreements and inform training and education initiatives
of the Alliance in the future.]{.underline}

#### The ethical use of AI is essential for political cohesion of the alliance because it is necessary to the Legitimacy of its military operations.

**Trabucco and Stanley-Lockman, 2022 -- prof of Political Science,
University of Copenhagen and prof of Defense and Strategic Studies,
Nanyang Technological University** \[Lena and Zoe, The Oxford Handbook
of AI Governance, March, "NATO's Role in Responsible AI Governance in
Military Affairs"
https://www.oxfordhandbooks.com/view/10.1093/oxfordhb/9780197579329.001.0001/oxfordhb-9780197579329-e-69
Acc 4/16/22 TA\]

[The political dimension of the Alliance rests on the bedrock of a
shared commitment to the "principles of democracy]{.underline},
individual liberty and the rule of law," as enshrined in the
foundational North Atlantic Treaty of 1949.60 [Shared values are
important for NATO operations because they help constitute their
legitimacy.]{.underline} In addition to the North Atlantic Council
exerting civilian oversight over NATO operations, [legitimacy also
includes respect for international legal principles including the core
principles of international humanitarian law]{.underline}, or the laws
of armed conflict, distinction, proportionality, and necessity.
[Without]{.underline} political oversight and [legitimacy, NATO's
military power would be less effective at shaping norms and promoting
stability in the international system. The introduction of AI means that
NATO has the moral and strategic imperative to adopt technologies that
confer legitimacy and responsible innovation]{.underline}.61 [Acting on
a shared commitment to democratic values is vital to the political
cohesion of the NATO Alliance]{.underline}, just as much as it is a
determinant of military effectiveness in a predictable security
environment. Put simply, [shared values are important to both political
and operational coherence between Allies]{.underline}. In its 2018
Framework for Future Alliance Operations, the strategic command Allied
Command Transformation urged discussion of the legal and ethical
dimensions of technological advancement to both know how it would impact
NATO decision-making and how the Alliance would be prepared to address
adversaries who do not share in that vision.62 As such, [NATO is
contending with the ways that ethical AI impacts its own cohesion
internally and how differences between allies may project outward in the
face of competitors whose ethical frameworks and commitment to the rule
of law differ.]{.underline} Internally, there is a strong national
government commitment to responsible AI. Recently, transatlantic
cooperation has initiated partnerships of largely NATO states committed
to advancing responsible AI with goals towards data sharing and future
interoperability.63 AI defense partnerships are not restricted to
military innovation but rather aim to facilitate civilian innovation
cooperation for defense purposes.

#### Cohesion is essential for an effective alliance -- AI poses the greatest threat to NATO interoperability and cohesion

**Dufour 2018 - Colonel in the Canadian Army, currently working with
NATO** \[Martin, NDC Policy Brief No. 6 December "Will artificial
intelligence challenge NATO interoperability?"
https://www.ndc.nato.int/news/news.php?icode=1239 Acc. 4/21/22 TA\]

[NATO has arguably been the most successful alliance of its kind, and
much of this success can be attributed to its cohesion]{.underline} in
the face of various threats. [At the heart of this cohesion lie two
important notions: burden sharing between members; and
interoperability.]{.underline} The Alliance's cohesion however has
increasingly come under pressure over the last two decades, and [there
are growing challenges with the level of interoperability between member
countries. While numerous technical and political factors influence
interoperability, the emergence of disruptive technologies]{.underline}
such as genetic engineering, nanotechnology, additive manufacturing and
robotics, [are likely to make this challenge more acute in the next two
decades.]{.underline} Of the many technologies rapidly emerging, [none
is likely to have as significant an impact as that of artificial
intelligence,]{.underline} which combines with other technologies and
multiply their effect by allowing the development of advanced autonomous
systems. And [while the latter holds]{.underline} the promise of
developing new classes of weapons with [great military potential, its
asymmetrical adoption among the various NATO allies could also lead to
significant interoperability problems.]{.underline}

#### An effective NATO is essential to the long term peace of Europe

**de Maizière and Mitchell, 2020 - Former German Defense Minister and
former U.S. Assistant Secretary of State** \[Thomas and Wess, November
25, "NATO 2030: United for a New Era Analysis and Recommendations of the
Reflection Group Appointed by the NATO Secretary General"
https://www.nato.int/nato_static_fl2014/assets/pdf/2020/12/pdf/201201-Reflection-Group-Final-Report-Uni.pdf
Acc. 4/12/22 TA\]

Conclusion: The Reflection Process concludes at an important inflection
point in world affairs and Euro-Atlantic relations, in which [the future
role of NATO is of growing importance to a stable and open international
order]{.underline}. The effects of Covid-19 will echo through the decade
ahead, exacerbating existing trends, potentially heightening
international competition, and causing long-term scarring to the global
economy. While [the historical record suggests room for optimism about
NATO's long-term future, it also cautions against
complacency]{.underline} and self-congratulation. [Political adaption is
in the lifeblood of NATO but it is also a baseline requirement of its
survival]{.underline}. In 1949, twelve countries established the
Alliance: binding post-War Europe to a Western trajectory and cementing
the transatlantic bond. Seventy-one years on, twelve have become thirty,
standing together to defend the security and prosperity of a billion
people. Throughout this time, NATO has been through phases of renewal
and reorientation, while always delivering its central mission and never
deviating from its founding principles. Throughout, the Alliance has
remained strong and resolute at each turn, always challenging itself to
be the best it can be. This ability to respond, adapt, and renew its
internal bonds has been NATO's hallmark over the last seven decades.
Throughout our consultations, there was a unanimous view that another
such moment is upon the Alliance today. Since 2014, supported by the
outstanding work of the Secretary General who was appointed that year,
NATO has implemented the biggest reinforcement of collective defence in
a generation. [The Alliance now needs a process of political adaptation
to match the progress made in the military sphere. The urgency of this
effort is driven by an evolving security environment which has become
more challenging and complex in recent years. Alongside the potent
threat from Russia, China requires particular attention as its influence
and presence grows.]{.underline} Terrorism in all its forms and
manifestations remains an immediate threat. More space is being
contested physically, as the line between peace and war continues to
blur, with disinformation and subversion posing serious challenges to
our democracies. [Hybrid attacks need new thinking about deterrence and
defence, driven in part by new and emerging technologies]{.underline}.
Agreeing a shared response to these challenges has at times tested NATO
unity, with Allies taking positions that reflect anxieties about their
long-term strategic futures. No single Ally can address these challenges
alone. So it is essential that all Allies recommit to the spirit as well
as the letter of the Washington Treaty, reaffirm their political
commitment to one another, sustain their commitment to democratic
values, and glean the benefits that come from the projection of
collective strength. As our report describes, [NATO needs to enhance its
ability to respond to both existing and new threats]{.underline}, from
both state and non-state actors, increase its range of political tools
to deter adversaries and defend the Alliance in the modern threat
environment. The Alliance will maintain the capacity for continual
adaptation to reflect changing strategic circumstances. 5 Conclusion In
our recommendations, [we have set out ways in which NATO could respond
to emerging technology and hybrid attacks, including by working more
effectively with partners]{.underline}. A balance also needs to be found
between the vital contribution North America continues to make to the
security of Europe, and the increasing share of the burden which
European Allies themselves will be taking in the years ahead. This
should come together under a new Strategic Concept, which recognises the
progress made and the new challenges since 2010; and equips the Alliance
to deal with those to come. In the face of attempts to divide, competing
priorities, criticism and intense scrutiny, Allies need to retain their
confidence in the durability and vitality of the Alliance, manage
differences, rise above disagreements and close their ranks against
threats affecting them, as they have for more than seventy years. [The
peace that most of Europe has enjoyed for the last seven decades is a
historical exception, not the rule. NATO remains the guardian of that
precious asset]{.underline}. As we submit our recommendations, we have
every confidence that NATO will move from reflection to further action,
so that it can continue to be the cornerstone of Allies' collective
defence and for the preservation of peace and security for decades to
come.

#### The Ukraine conflict could escalate to a global great power war. A cohesive and unified NATO is essential to contain the conflict -- disunity will be seen as weakness by Moscow.

**Graham, 2022 -- Fellow at the Center for Preventive Action and Europe
Program at the Council on Foreign Relations** \[Thomas, March 8
Contingency Planning Memorandum No. 38 "Preventing a Wider European
Conflict" [https://www.cfr.org/report/preventing-wider-european-conflict
Acc
6/11/22](https://www.cfr.org/report/preventing-wider-european-conflict%20Acc%206/11/22)
TA\]

[The large-scale Russian invasion of Ukraine now underway could quite
plausibly precipitate a wider conflict in Europe.]{.underline} The
United States is focused primarily on raising the costs to Russia with
punishing sanctions and reassuring North Atlantic Treaty Organization
(NATO) allies neighboring Russia of its commitment to collective
defense. Less attention has been given to containing the war to Ukraine
and preventing its escalation into a broader European conflict. [The
stakes are enormous. The ripple effects of a wider conflict in Europe
would spread across the globe, stressing the geopolitical, economic, and
institutional foundations of the international order]{.underline} the
United States has fashioned and underwritten since the end of the Second
World War. [It would test the resilience of the U.S. global system of
alliances]{.underline}, the international financial system, global
energy markets, arms control regimes, and global institutions [in the
face of ever more violent great power competition. No region of the
world would be spared]{.underline}, although developments on the
Eurasian supercontinent, the other locus of world power and economic
might outside North America, would bear the gravest consequences for
U.S. interests. NATO (North Atlantic Treaty Organization) The Russian
military intervention in Ukraine could easily escalate into a larger
conflict stretching from the Baltic to the Black Sea and further west
into Europe. Although Russia, wielding massive military superiority,
might overrun Ukrainian forces in a matter of weeks, stabilizing and
pacifying the country will likely prove to be a grueling and costly
affair. A significant Ukrainian resistance movement is almost certain to
emerge. With sustained Western support, it could prolong the warfare for
months, if not years. The first wave of sanctions that Washington has
levied on Moscow could be followed by others in a continuing effort to
raise the cost to Moscow and force it to yield. A negotiated end to the
conflict will not come easily, since Washington has framed it in
Manichean terms as a world historical struggle between the democratic
West and the aggressive, malevolent, and autocratic Russia. Anything
short of "victory" will be decried as surrender or appeasement in the
West, while Russia will not capitulate on a matter it considers vital to
its security and prosperity. [The stage is thus set for an escalating
cycle of violence,]{.underline} with Moscow seeking to stamp out a
Ukrainian insurgency and retaliate against Western efforts to stop
Russia's advance. [If the conflict wears on, Moscow could be
increasingly tempted to expand its military operations further into
Europe]{.underline} to achieve its goals. As a first option, [Russia
could intensify pressure on states neighboring Ukraine]{.underline}
(e.g., Hungary, Poland, Romania, and Slovakia) that could provide safe
havens for insurgents or the inevitable government-in-exile. It will
doubtless reinforce its military presence in Kaliningrad and elsewhere
in the Baltics and patrol the Baltic Sea more aggressively. [It could
deploy hybrid-war tactics---cyberattacks, disinformation campaigns, and
economic sabotage---to destabilize countries]{.underline} providing safe
havens. If those actions did not sufficiently degrade the resistance,
Moscow could even launch direct attacks on insurgents and their
supporters outside Ukraine, as well as attempt to assassinate leading
figures in the government-in-exile, akin to the attacks it has made on
Chechen rebels and Federal Security Service (FSB) defectors in Europe in
recent years. Such steps could, at a minimum, draw frontline NATO states
directly into the military conflict with Russia, obligating the United
States and other allies to come to their defense. To build up further
pressure, Moscow could also "weaponize" the inevitable refugee flows
into neighboring states. Refugees, who would likely number in the
millions, would move first into unoccupied Ukrainian territory but
eventually into adjacent European states, which have shown little
tolerance for outsiders. Moscow could use harsh military and police
tactics that would increase the number of refugees and seek to guide
them into countries where they would create the greatest socioeconomic
stress, such as Moldova. In addition, Moscow could increase the tension
by pushing Belarusian President Aleksandr Lukashenko to again seek to
push thousands of Middle Eastern migrants across the borders into Poland
and Lithuania. That could lead to border clashes, as it almost did on
occasion last fall, with Russia supporting its ally, Belarus, and NATO
states coming to the defense of allies under attack. A second option
Moscow could pursue is opening up a second front in the Balkans. In
recent years, Russia has taken a number of destabilizing actions in the
region, seeking to weaken Montenegro after its accession to NATO,
exacerbate tensions between Serbs and Bosniaks in Bosnia-Herzegovina,
and undermine relations between Serbia and Kosovo. As it fought in
Ukraine, Russia could encourage Republika Srpska leader Milorad Dodik to
press for separation from Bosnia, threatening to reignite the bitter
wars of the 1990s in the former Yugoslavia. A Balkans war would
complicate the security calculus of all countries in the region, as well
as that of Germany and France, which have significant interests there.
To quell the fighting, NATO countries could decide to use military force
against Bosnian Serb forces enjoying Russian support. If the conflict
wears on, Moscow could be increasingly tempted to expand its military
operations further into Europe to achieve its goals. A third, riskier,
option would be to directly attack the United States, the country that
Moscow believes is orchestrating a larger anti-Russia campaign. In
response to Western sanctions designed to crater Russia's financial
system and undermine critical industries, Moscow could launch major
cyberattacks against U.S. critical infrastructure. If a cyberattack were
to take down a major financial institution or corrupt its records, the
ensuing havoc in U.S. markets could prompt overwhelming public and
congressional pressure for a forceful response. [The U.S. and NATO
response to Russian actions will impact Moscow's decisions on the
conduct of the conflict]{.underline}. Both [a weak response]{.underline}
and an excessively harsh one [could lead to escalation.]{.underline} In
the first case, [Moscow could be tempted to press militarily even
further into Europe to enlarge its sphere of influence.]{.underline}
Vladimir Putin has demanded that NATO withdraw its forces back to the
lines they held in 1997, when the NATO-Russia Founding Act was signed
and the first wave of post−Cold War expansion remained in the future.
His remarks announcing the start of hostilities against Ukraine hinted
at a broader effort to restore Russia's control over all of the former
Soviet Union. That could include military action against the Baltic
states, especially Lithuania, through which Moscow could try to carve
out a land corridor to Kaliningrad, a Russian exclave on the Baltic Sea.
NATO would have little choice but to provide military aid to those
states if it did not want to forfeit its role as the central pillar of
European security. Crippling sanctions, meanwhile, could provoke Putin
to lash out with greater violence. If Putin felt cornered, he could
escalate the conflict either horizontally to other countries or
vertically to the nuclear level in a desperate effort to save himself,
his regime, and, in his mind, Russia itself. And he could find
considerable public support for such a reaction. Already, some Russians
believe that U.S. and EU sanctions are aimed not simply at the leaders
behind the war but, by cratering the economy, at all Russians. Warning
Indicators As is the case with the current crisis in Ukraine, Moscow's
intentions will remain ambiguous. The indicators of an approaching
escalation in the conflict beyond Ukraine are likely to fall into three
categories. The first indicators that political and military conditions
are increasing the risk of broader conflict include a breakdown in
channels of communication with Moscow. The absence of active diplomatic
ties would preclude a negotiated resolution of the conflict in Ukraine.
An end to U.S.-Russian military-to-military channels would undermine any
effort to avoid direct military conflict between the two countries.
Another indicator would be major insurgent successes that dramatically
increase Russian casualties. Moscow would be tempted to move more
aggressively against insurgent safe havens rather than capitulate on
what it considers to be its vital interest in Ukraine. A wider European
conflict would pose the stiffest challenge to the global standing of the
United States since the end of the Cold War and to the international
system it has built and underwritten for decades longer. Second are the
indicators that Moscow is preparing for a broader conflict, which it
would undoubtedly argue had been forced by Western actions. Such signs
include Kremlin efforts to prepare the Russian public for a wider
conflict, which could entail official statements, greater media focus on
escalating Western "aggression," an increased pace of civil defense
drills, and mobilization of reserves. Another indicator includes the
massing of Russian forces in the Baltic region. It could include such
moves as aggressive hybrid actions to destabilize Poland and the Baltic
states, coupled with efforts to rally indigenous ethnic Russian
communities against their governments. Third are the indicators that
Moscow is intentionally seeking to widen the conflict. This could
include greater support for Bosnian Serb leader Dodik, such as
diplomatic and financial backing, and provision of weapons. They could
also encourage Serb leaders to more assertively pursue their grievances
against Kosovo. Implications for the United States [A wider European
conflict would pose the stiffest challenge to th]{.underline}e global
standing of the United States since the end of the Cold War and to the
[international system]{.underline} it has built and underwritten for
decades longer. It would test the durability of its global system of
alliances and the efficacy of international regimes and institutions
[that have guarded world peace]{.underline}, security, and prosperity.
The challenge would come at a time when the United States itself is in
immense disarray, as a deeply polarized polity confronts massive
domestic problems---the pandemic, inflation, racial justice, and
cultural wars---that leave less time and fewer resources for foreign
matters. The United States will be tested to see whether it can muster
the will, energy, and creativity to execute an effective policy toward
the unfolding crisis in Europe. At home, public attention has been
focused on developments in and around Ukraine, but the Joe Biden
administration cannot ignore the home front. In response to U.S.-levied
sanctions, Russia can be expected to step up its cyber operations
against the United States. It will more actively sow disinformation,
seek to exacerbate domestic tensions, and paralyze critical
infrastructure. The severity of the attacks will likely rise in
proportion to the harshness of the sanctions Washington levies on
Moscow. Abroad, [the fate of the transatlantic community, a central
pillar of U.S. security and prosperity, would be a stake]{.underline}.
One of the Biden administration's priorities, as laid out in the Interim
National Security Strategy Guidance released in March 2021, is repairing
U.S. alliances---especially with Europe---after four disruptive years
under President Donald Trump. Although relations are more cordial,
significant substantive differences remain and the willingness of allies
to align behind a common purpose for the long haul remains questionable.
The United States' allies have rallied behind a harsh set of sanctions
in response to Russia's invasion of Ukraine, but [preserving unity as
the conflict drags on remains a challenge, especially if sacrifice is
spread unevenly across NATO]{.underline}, as will most likely be the
case. Putin will seek to exploit divisions through differentiated levels
of pressure on NATO members, targeted energy cutoffs, offers of
negotiation, and the like to advance two long-standing Russian goals:
the end of NATO as a collective defense organization and the erosion of
the foundations of the EU. Should he succeed, the new order that would
emerge in Europe is far from certain. But Russia would undoubtedly play
a central role in its formulation, and almost any conceivable new order
would diminish the power and role of the United States on the continent.
A similar situation obtains in the Indo-Pacific region. The Biden
administration spent 2021 bolstering relations with its allies and
partners---energizing the Quad (the United States, Australia, India, and
Japan), and cutting a submarine deal with the United Kingdom and
Australia---to meet the growing strategic challenge posed by China. A
major, prolonged European distraction could undo further efforts to
pivot to Asia, raise doubts among allies and partners about the
credibility of the U.S. commitment, and free China to pursue its
objectives with greater vigor. The United States could avoid this
outcome by pursuing lesser goals in Europe---leading to the quicker
development of a new order less favorable to American interests---or by
a massive buildup of its military capabilities that would enable it to
play a major, perhaps decisive, role in both regions. The latter would
have to come at the cost of the Biden administration's domestic
priorities. Whether the Biden administration could muster sufficient
domestic political support, if it decided to move in this direction, is
far from certain. The United States' allies have rallied behind a harsh
set of sanctions in response to Russia's invasion of Ukraine, but
preserving unity as the conflict drags on remains a challenge,
especially if sacrifice is spread unevenly across NATO. In addition to
regional challenges, [a major European conflict would also stress
critical international regimes]{.underline} and institutions. One of the
first victims would likely be the arms control regime that has served as
the foundation of strategic nuclear stability for the past fifty-plus
years. The United States withdrew from some central elements---including
the Anti-Ballistic Missiles (ABM) and the Intermediate-Range Nuclear
Forces (INF) treaties---but two critical elements have remained in
place: the New START treaty and the Nonproliferation Treaty (NPT). A
wider conflict in Europe would all but guarantee that the United States
and Russia could not agree to a follow-on treaty to the New START treaty
before it expires in 2026, and the NPT review conference tentatively
scheduled for August 2022 would fall by the wayside. As a consequence,
[the incipient arms race now underway, fueled by]{.underline} new
technologies---hypersonics, cyber tools, and [artificial
intelligence---would accelerate. A new wave of nuclear proliferation
could ensue]{.underline}, especially if U.S. allies and partners lose
faith in America's commitment to extended deterrence. Mutually assured
destruction, which for better or worse has anchored strategic stability
since the early 1970s, would be severely stressed in a multipolar
nuclear landscape with Russia and the United States fighting at least a
proxy war. Likewise, a broader conflict in Europe would stress, perhaps
to the breaking point, the United Nations and many of its auxiliary
organizations. Already stymied by a growing rift between the Western
permanent members and Russia and China, the Security Council would have
failed in its primary reason for being---to prevent the outbreak of a
major conflict in Europe. It could continue to exist as a forum for the
airing of grievances and acrimonious debate, but it would serve little
purpose as a platform for addressing major global issues. Finally, the
humanitarian costs of a wider conflict in Europe would be staggering,
particularly given the destructiveness of modern weapons. Beyond the
physical destruction and loss of life, untold numbers of refugees would
flow across borders not only into Central East Europe but perhaps
further West depending on the scale of the fighting. The strain on the
socioeconomic systems---coming on top of the stress of the two-year-old
pandemic, economic dislocation, and mounting inflation---could bring
some close to collapse. Preventive Options U.S. policy toward Russia has
traditionally been a combination of deterrence and diplomacy. The Biden
administration deployed both as it tried to dissuade Russia from
invading Ukraine. Both have a role to play in reducing the risk of a
wider European conflict, now that Russia has invaded. [Many of the steps
that the Biden administration is now taking to counter Russia could be
accelerated and expanded to deter it from expanding its military
operations beyond that country. They would likely prove more effective
due to NATO's]{.underline} Article 5 [collective defense
guarantee]{.underline}, which does not apply to Ukraine. The Biden
administration could: [With its NATO allies, accelerate and expand its
current augmentation of forces in vulnerable allies along the frontier
with Russia to reassure them---and convince Moscow---of the alliance's
commitment to collective defense]{.underline}. Step up its already
intensive schedule of consultations with allies to maintain alliance
unity in the face of a burgeoning Russian threat. Develop a long-term
plan to reduce Europe's dependence on imported Russian gas, building on
the stopgap measures it is already putting in place to deal with a
near-term decision by Moscow to stop flows of gas westward. Consider
cutting off energy imports from Russia, and asking the Europeans to do
the same, but only after it has prepared the American public for the
economic hardship (rising energy costs, inflation) such a step would
entail. Accelerate efforts to harden American and allied critical
infrastructure against cyber intrusions. The Biden administration could
also resume its diplomatic efforts to find a negotiated solution. To
that end, it could: Resist the temptation to cut off channels of
communication, as past administrations have done in reaction to Russian
aggression. White House−to-Kremlin and military-to-military channels
will be critical to reducing misunderstandings that could lead to direct
military confrontation between the two countries. In addition, a White
House−to-Kremlin link could provide a platform for negotiating an end to
the conflict before it spreads beyond Ukraine. Carefully recalibrate its
rhetoric to ensure that the confrontation does not turn into an
existential one, where victory, whatever that might mean, is the only
acceptable outcome. Such a posture would ignore the reality that Russia
is unlikely to capitulate in a matter of vital interest---and would
escalate rather than surrender. Talk of regime change and possible war
crimes charges would probably prove counterproductive and fuel public
support for escalation, especially at a moment when polls suggest the
war effort enjoys the backing of the vast majority of the Russian
population. Avoid appearances that the United States and NATO are waging
a conflict against the Russian people. Releasing constructive proposals
for resolving the conflict (including provisions for the lifting of
sanctions), and urging the Ukrainians to publish reasonable negotiating
terms, would be more likely than bellicose warnings to turn the Russian
elites and public against the war. Russians need to be persuaded that
the United States and Europe are not seeking a punitive peace but are
open to a renewal of relations should their country act to end the
conflict. Accelerate efforts to get information to the Russian people
that would give them a more accurate portrayal of the brutal,
unnecessary conflict their leaders are waging allegedly on their behalf.
Students and young professionals would be particularly receptive to such
information and inclined to protest. The mitigating options identified
below, with the exception of invoking Article 5, could also be taken now
to induce Russia to de-escalate and withdraw from Ukraine and to prevent
it from expanding its military operations beyond Ukraine. Mitigating
Options Should the conflict spread beyond Ukraine despite U.S. efforts,
the task will be to bring it to an end on terms favorable to the United
States as quickly as possible. Washington could consider diplomatic
initiatives, defensive steps, and sanctions. Diplomatically, Washington
could invoke Article 5 of the North Atlantic Treaty to make clear NATO's
determination to come to the aid of members under Russian attack. It
could call for an urgent session of the UN Security Council to focus on
the threat posed by Russia to international peace and security. The
debate would doubtlessly be acrimonious, but the United States needs to
make a concerted effort to shape public opinion and isolate Russia as
the aggressor. Washington could also propose a P5 (the five permanent
members of the UN Security Council: the United States, China, France,
Russia, and the United Kingdom) meeting to discuss steps to reduce the
risk of nuclear war. To avoid turning this into a two-bloc standoff
between Russia and China and the Western powers, India could be added to
the discussion. But New Delhi could resist being drawn into an East-West
conflict, as it has in the past. The Biden administration should take
care not to provoke severe Russian retaliation or produce spillover
effects that cause undue harm to its or its allies' interests. With
regard to defensive measures, Washington could enlarge NATO de facto to
coordinate strategy and tactics with Sweden and Finland, with an eye to
their de jure membership in the near future. It could also send a small
NATO contingent to the Balkans (Albania, Croatia, Montenegro, and North
Macedonia) to warn Serbia and Republika Srpska against aggressive
actions against Kosovo and Bosnia-Herzegovina. Concerning sanctions,
Washington could build on the sanctions it had already levied to raise
the costs further. However, the Biden administration should take care
not to provoke severe Russian retaliation or produce spillover effects
that cause undue harm to its or its allies' interests. Recommendations
The Biden administration is already taking steps to prevent the spread
of conflict in Europe and harden the resilience of allies and partners
in the face of Russia's invasion of Ukraine: further augmentation of
NATO forces, including a greater presence of American troops and
equipment, along the entire Russia/NATO frontier stretching from the
Baltic to the Black Sea; frequent consultations with allies and
partners; steps to handle the large-scale exodus of refugees from
Ukraine; organization of fuel shipments to Europe from various sources
to cover gaps in the event of a Russian cutoff of gas exports; measures
to harden U.S. and allied computer networks against attacks. The task is
to turn those expedient measures into strategies to fortify the
transatlantic community against a prolonged threat from the East, which
Russia will continue to pose even if the current crisis is somehow
defused in the near future. In particular, the United States needs to
work with its European allies to drastically reduce Europe's dependence
on Russian gas. The goal should be to cut that dependence in half by the
end of the decade by fully using the regasification facilities in place,
building more, and accelerating work on renewables. In addition, even as
the United States and European Union are dealing with the war in
Ukraine, they need to recommit themselves to sorting out the continuing
problems in Bosnia-Herzegovina and between Serbia and Kosovo to reduce
the opportunities for destabilizing Russian interference in the Balkans.
Finally, to ease the burden on states bordering Ukraine, the United
States should be working with the UNHCR and its allies to develop plans
for the long-term resettlement of Ukrainian refugees throughout Europe
in case of a long period of instability in Ukraine. The United States
should confront the urgent crisis in Europe without unduly sacrificing
focus on the strategic challenge in the Indo-Pacific. All these steps,
however, do not go far enough to deal with the enduring Russia
challenge. The Biden administration needs to do more, ideally as part of
a larger effort to reposition the United States strategically on the
global stage. Critically, the United States should confront the urgent
crisis in Europe without unduly sacrificing focus on the strategic
challenge in the Indo-Pacific, and to prepare for a major change in the
geopolitics of the Eurasian supercontinent. A tall order but not an
impossible task. There are three core elements to this task: rethinking
NATO, enhancing the U.S. presence in the Indo-Pacific, and creating a
security forum to enhance allies' support for U.S. policy across
Eurasia. Rethinking NATO. The strategic goal should be the achievement
of a near perfect overlap in NATO and EU membership among European
states. That would provide the foundation for the development of a
united European pillar inside NATO, in a sense resolving the tension
between NATO and the EU (if not necessarily between the United States
and Europe). The European pillar would assume ever greater
responsibility for the defense of the continent, backed up by the
American strategic deterrent, thus freeing up American forces to deal
with the growing challenges in the Indo-Pacific region. The alliance's
new strategic concept, to be adopted at the Madrid Summit this coming
June, provides an opportunity to articulate this goal, as well as to lay
out the full breadth and enduring nature of the Russia challenge. [The
United States should consider pressing for the following steps: Fortify
NATO's eastern border. The alliance should abandon the pledge of the
1997 NATO-Russia Founding Act not to deploy permanent substantial combat
forces to new members. It should augment its forces in vulnerable member
states]{.underline}, as long as there is no agreement between NATO and
Russia to mutually restrict force levels in border zones. Prepare for
the eventual membership of Finland and Sweden to reinforce the northern
flank. In the face of Russian conduct, the populations of these two
countries are reconsidering their long-standing traditions of
neutrality. While staying out of the domestic debate, the United States
and other allies should indicate that they would welcome the two
countries into the alliance and articulate clearly the changing nature
of the security environment in the Baltic region brought on by a more
aggressive Russia. Repair relations with Turkey. This is a matter
primarily for the United States, which has levied sanctions on its ally
for its purchase of S-400s, an advanced Russian air defense system. The
United States could take a first step by approving the sale to Turkey of
the F-16s it has requested. Washington should also look for an
opportunity amidst deteriorating relations with Moscow to persuade
Ankara to reconsider its purchase of S-400s. Forego expansion into the
former Soviet space for an extended period. No one believes that any
former Soviet state will be ready for membership for years to come.
Without necessarily abandoning the Open Door policy, the alliance should
make clear that it will not expand eastward while it focuses on its own
consolidation.

#### Ethical principles for AI improve NATO interoperability -- cooperation is essential because it builds on ethical bonds. 

**Gilli, 2020 - Senior Researcher at the NATO Defense College**
\[Andrea, NDC Research Paper No.15 -- December ""NATO-Mation":
Strategies for Leading in the Age of Artificial Intelligence"
https://www.ndc.nato.int/news/news.php?icode=1514 Acc 4/21/22 TA\]

Artificial Intelligence, machine learning and big data represent some,
but not the only, instances of technological progress. In contrast to
other realms of technology, however, [artificial intelligence bodes
far-reaching ramifications across all areas of society,]{.underline} the
economy, [and the military, on account of its ubiquitous nature that
enables pervasive diffusion]{.underline}. When major technological
changes occur, governments usually step in to update regulations or
introduce ethical rules to help align incentives among different actors.
Some measures include providing key complementary goods and services,
which the market may otherwise undersupply. Governments also draw
strategic directions. [The Atlantic Alliance is]{.underline}
encountering a major challenge; but is also [standing before a potent
opportunity to shape the future security environment, thereby
preserving]{.underline} the freedom and well-being of its Allies'
citizens, and maintaining [its technological leadership]{.underline}.
[One possible way forward consists of pursuing the NATO-mation
agenda]{.underline} described in this report: [a set]{.underline} of
individual and collective initiatives, [joint solutions and coordinated
actions spanning]{.underline} across several fields and domains and at
different levels. This Research Paper has highlighted 11 different areas
where action is possible and desirable. Challenges, dynamics, actors and
constraints differ in each area, but [NATO can play a significant role
as both as initiator and as coordinator. In the ethics domain, NATO has
an interest in upholding its founding values]{.underline}.
Innovation-wise, there is a strong rationale for the Alliance to
identify or even create an internal actor to champion AI as well as to
help Allies generate an innovative workforce and pioneer more modern
work environments. Change, however, is difficult. Innovation, if history
has any lesson to spare, is even more difficult. Launching pilot
projects can help subdue scepticism and temper the understandably
reluctant attitude of some, all the while fostering greater familiarity
with new technologies. While the transformation we are observing is
technology-driven, its main repercussions will be on human beings, their
ideas, their norms and their organizations. In addition to ushering in
an innovative workplace and an innovative workforce, [experimentation
will be necessary to]{.underline} accept, understand and [improve novel
technologies]{.underline}, [as well as to steer their evolution to
fulfil ethical considerations]{.underline}, alongside tactical,
operational and strategic necessities. Traditionally, [NATO has played a
significant part in this respect and, if anything, should continue and
probably expand this role further.]{.underline} NATO Allies have,
however, a broader opportunity to shape the evolution of AI-related
technologies both through increasing R&D spending and through targeted
investments. The bulk of AI research is driven by the private sector.
Most observers worry that this has deprived military establishments of
the control over new technologies. Private-sector driven research, at
least in the realm of AI, has several drawbacks, including a short-term
and narrow focus, and a reliance on economically, technologically and
environmentally unsustainable solutions. There is a role for NATO-wide
coordinated public action and investments, and eventual concertation
with the European Union, in tackling these issues. Similarly, Allies
will soon have to contemplate and prepare for the infrastructures on
which AI systems will run: quantum and cloud computing as well as 5G
networks warrant closer scrutiny. Whether NATO should provide cloud
computing the way it does through AWACS aircraft, airspace management,
is however, another issue. Historically, NATO has delivered collective
defence with a strong attention to arms control. Whilst not an easy
endeavour with immediate payoffs, the Alliance can contribute to ongoing
debates on how to reach the goal of preserving international stability.
[NATO armed forces, combined, are more than the sum of the single parts:
this is attributable to interoperability. In the NATO context,
standardization plays a critical role: coordination among Allies is
important,]{.underline} both within the Alliance and without, such as in
Standards Development Organizations (SDOs). [The Atlantic Alliance won
the Cold War]{.underline} and overcame the challenges it faced in the
ensuing decades [because of the bonds among the Allies. Such bonds
are]{.underline} not primarily political, military or diplomatic: they
are [cultural]{.underline}, ideological [and ethical]{.underline}, [and
are based on]{.underline} the founding principles on which the Alliance
was built. Democracy, rule of law, [human rights]{.underline}, and free
markets have guaranteed the longevity of transatlantic relations. AI
technologies, through deep-fakes, may undermine those values. This is an
insidious challenge which, however, may also one day prove existential.
This is why NATO must be poised to play a key role in this technological
realm.

#### Ethical principles for AI increase political support for NATO by calming public fears. 

**Valášek, 2017 - director of Carnegie Europe** \[Tomáš August 31, "How
Artificial Intelligence Could Disrupt Alliances"
https://carnegieeurope.eu/strategiceurope/72966 Acc 4/22/22 TA\]

This dilemma is not new. Even today, capitals delegate certain decisions
to commanders and assume the political risk if an operation goes badly.
But in such cases, responsibility can be drawn after the fact and the
guilty can be punished---there is no such recourse with AI. Also, [while
publics understand and make allowances for human fallibility, they feel
uncomfortable about machine-made mistakes. This puts democracies at a
distinct disadvantage. Undemocratic governments that are unconcerned
about public reaction will have fewer qualms about removing humans from
the loop. This strengthens the case for a broad international agreement
on offensive military uses of AI, to reassure potentially anxious
publics and, ideally, prevent the most egregious applications of
artificial intelligence in warfare. In short, it is time to put AI more
prominently on NATO's]{.underline} and the EU's [agenda.]{.underline}

#### Public Support is crucial for cohesion and interoperability -- it builds solutions to burden sharing problems and attracts a tech workforce.

**Lin-Greenberg, 2020 - member of the MIT Security Studies Program**
\[Erik Vol 3, Iss 2 Spring, Texas National Security Review "Allies and
Artificial Intelligence: Obstacles to Operations and Decision-Making"
http://dx.doi.org/10.26153/tsw/8866 Acc 4/22/22 TA\]

[Even if a state has the resources to develop AI capabilities, limited
public support for AI-enabled military systems can hamper such efforts.
Opposition can stem from the uncertainty surrounding AI's functionality,
or from moral and ethical objections]{.underline} to delegating
decisions on the use of force to computers. [One recent cross-national
survey,]{.underline} for instance, [finds significant public disapproval
of the use of lethal autonomous weapons]{.underline} among key U.S.
allies. To be sure, autonomous weapons and AI are distinct, but AI is
incorporated into the software architecture of most autonomous systems,
and pundits and the public often conflate the two.57 In South Korea and
Germany, 74 and 72 percent of the local populations, respectively,
oppose their use (compared to 52 percent opposition among the U.S.
public).58 These two countries are close U.S. allies that host dozens of
U.S. military installations and over 60,000 American troops.59 [Tepid
public support at home and abroad can stymie alliance military
operations in two ways. First, public opposition to the use of AI among
allied populations may lead policymakers to restrict the use of
AI-enabled technologies for military operations. In the event of future
hostilities]{.underline}, for example, the South Korean or German
[governments might oppose an ally's use of AI-enabled lethal weapon
systems on their territory]{.underline}.60 Indeed, advocacy from the
public and activist groups has led a growing number of states ---
including U.S. allies like Pakistan and Jordan --- to call for bans on
the use of lethal autonomous weapon systems.61 [Second, civilian
engineers and researchers that develop AI technology may refuse to work
on military AI contracts. Disruptions to AI development can hinder the
fielding of new capabilities and generate mistrust between the
government and civilian firms]{.underline}. [Google employees, for
instance, protested their involvement in Project Maven, a Defense
Department program that uses AI to analyze video collected by military
drones]{.underline}.62 In a letter to their CEO, the employees argued
that "Google should not be in the business of war," explaining that the
company should not "outsource the moral responsibility of \[its\]
technologies to third parties," and that work on Defense
Department-backed AI would "irreparably damage Google's brand."63 The
resistance ultimately led Google to terminate its involvement in the
contract and generated public criticism of the Defense Department's AI
efforts.64

### 1AC - Advantage - Crisis Instability

#### Advantage -- Crisis Instability

#### Europe faces a future of crises that require NATO to adapt. Disruptive technologies will exacerbate these crises. 

**de Maizière and Mitchell, 2020 - Former German Defense Minister and
former U.S. Assistant Secretary of State** \[Thomas and Wess, November
25, "NATO 2030: United for a New Era Analysis and Recommendations of the
Reflection Group Appointed by the NATO Secretary General"
https://www.nato.int/nato_static_fl2014/assets/pdf/2020/12/pdf/201201-Reflection-Group-Final-Report-Uni.pdf
Acc. 4/12/22 TA\]

2 Introduction and Main Findings "NATO stands as history's most
successful alliance." NATO enters the eighth decade of its existence
with both a longer record of success and a wider assortment of looming
challenges than its founders could have foreseen when they signed the
Washington Treaty in April 1949. In the thirty years since the collapse
of the Soviet threat that called NATO into existence, [the Western
Alliance has defied innumerable predictions of its imminent
demise]{.underline}. [It ended two wars and ethnic cleansing in the
Western Balkans]{.underline}, extended the hand of partnership to Russia
and other former adversaries, stepped up to the threat of terrorism
directed against NATO territory, engaged abroad including in
Afghanistan, [and responded with clarity, unity, and resolve to the
threat posed by Russian aggression in the Euro-Atlantic
region]{.underline}. Today, NATO stands as history's most successful
alliance, encompassing nearly a billion people and half of global GDP
across a space that stretches from the Pacific coast of North America to
the Black Sea. [Yet, future uncertainties demand that NATO continues to
adapt. The world of the next ten years will be very
different]{.underline} than the world that the Alliance inhabited either
during the Cold War or the decades that immediately followed. [It will
be a world]{.underline} of competing great powers[, in which assertive
authoritarian states with revisionist foreign policy agendas seek to
expand their power and influence]{.underline}, and in which NATO Allies
will once again face a systemic challenge cutting across the domains of
security and economics. Well-known threats like terrorism, in all its
forms and manifestations will persist, even as [new risks
loom]{.underline} from pandemics and climate change, and [as emerging
and disruptive technologies (EDTs) present both dangers and
opportunities for the Alliance]{.underline}. Against this changing
backdrop, NATO has experienced internal strains. Recent years have seen
Allies engaged in disputes that partly reflect anxieties about their
long-term strategic futures. Some Europeans worry that the United States
is turning inward---or that its commitment to their continent will
diminish as it increases focus on the Indo-Pacific. Some Americans worry
that Europeans will shirk their responsibilities for the common defence
-- or even pursue a path of autonomy in a way that splinters the
Alliance. Inside NATO, societal divisions have arisen and representative
democracy is being challenged. In many ways, the Alliance could be said
to be formidable in military strength; but it is far from invulnerable
to such political turbulence. In spite of these challenges[, NATO
remains indispensable. In fact, the fundamental purpose of NATO is more
demonstrably clear today than it has been for decades]{.underline}. NATO
has weathered stormy times before, surviving the Soviet threat, the Suez
Crisis, divisions among Allies over the Vietnam war, dictatorships in
its own ranks, the Euromissile debates, disagreements over enlargement,
and the Iraq War---just to name a few. Now, as then, Allies have
remained bound together by a combination of shared principles,
democratic institutions, and the benefit that all Allies derive from
collective security. Looking out to 2030, the need for a collective
defence Alliance to protect Europe and North America against threats to
their physical security and democratic way of life is as strong as ever.
A Strategic Anchor in Uncertain Times "The fundamental purpose of NATO
is more demonstrably clear today than it has been for decades." 6 [Yet
NATO will have to continue to adapt]{.underline}. In a world of systemic
challengers and proliferating threats, [the Alliance,]{.underline} in
complementarity with the comprehensive military adaptation it has
undergone[, must cement its ability to act as the principal political
forum for the strategic and geopolitical challenges]{.underline} facing
the transatlantic community. Fulfilling this role will require even
greater cohesion than NATO has possessed in recent years. As it has
since NATO's founding, cohesion resides in the ability and will to act
collectively against shared threats. This is the lifeblood that ensures
the vitality, credibility, and durability of the Alliance[;]{.underline}
it becomes all the more important in [a sharpened competitive
environment]{.underline} that [requires collaboration and effective
networks to deal with growing threats]{.underline}. In recent years,
[Allies]{.underline} have strengthened the military component of NATO
and should continue to do so. But in parallel, they [must move
decisively to bolster the political dimension of NATO, including its
foundations of shared democratic principles, mechanisms of consultation,
processes of decision-making, and political tools for responding to
current and emerging threats. If they do so, NATO will be in a strong
position to protect the]{.underline} freedom and [security of its
members and act as an essential pillar of an open and stable
international order.]{.underline}

#### A lack of human control increases crisis instability and escalation risks -- recent wargaming simulations prove that the threat is already here. 

**Sauer, 2021 - Senior Research Fellow at Bundeswehr University**
\[Frank serves on the International Panel on the Regulation of
Autonomous Weapons IRRC No. 913 March "Stepping back from the brink: Why
multilateral regulation of autonomy in weapons systems is difficult, yet
imperative and feasible"
https://international-review.icrc.org/articles/stepping-back-from-brink-regulation-of-autonomous-weapons-systems-913
Acc 4/5/22 TA\]

Escalation risks and crisis instability [Weapons systems operating
without human control generate]{.underline} [not only new
vulnerabilities but also unpredictability due to unforeseeable
interactions with their environment, in turn creating new risks of
unintended, unwanted escalation.]{.underline}66 In that regard, [the
interaction between two or more autonomous systems is to be considered
in particular]{.underline}. High-frequency trading67 provides a useful
analogue, because unforeseen and unwanted interaction processes between
two or more autonomously operating trading algorithms occur on a regular
basis, sometimes causing so-called "flash crashes" and resulting in
financial losses. This can be remedied with regulation of the financial
market to an extent, but [without internationally binding regulation of
autonomy on the battlefield, unforeseeable interactions of LAWS might
end in unintended use of force at machine speed, even accidental war
before humans can intervene]{.underline}.68 [This risk is not in some
distant future]{.underline}. At the Dubai Airshow in 2019, the chief of
staff of the US Air Force, General David Goldfein, presented the
simulated engagement of an enemy navy vessel with a next-to-fully
automated kill chain. The vessel was first picked up by a satellite,
then target data was relayed to airborne surveillance as well as command
and control assets. A US Navy destroyer was then tasked with firing a
missile, the only remaining point at which this targeting cycle involved
a human decision, with the rest of the "kill chain ... completed machine
to machine, at the speed of light".69 Any machine error in such a system
would, if left uncorrected by a human due to automation bias, propagate
quickly. It stands to reason that the error would propagate "at the
speed of light" as well, were the human to be removed. [A recent
wargaming exercise conducted by the RAND Corporation underlines the
risks of crisis instability and unintended escalation; in this exercise,
simulated forces were set "on 'full auto' to signal resolve ...\[,\] in
one case lead\[ing\] to inadvertent escalation.]{.underline} Systems set
to autonomous mode reacted with force to an unanticipated situation in
which the humans did not intend to use force."70

**A lack of human control threatens to escalate crises into wars due to
the speed of conflicts, the gap between human and AI solutions, and the
pressure for pre-delegation**

**Vestner, 2021 - Head of Security and Law Programme at Geneva Centre
for Security Policy** \[Tobias, July 8 "Warfare and Artificial
Intelligence" in Robin Geiß and Henning Lahmann (eds), Research Handbook
on Warfare and Artificial Intelligence -forthcoming
https://www.gcsp.ch/publications/military-operations-and-artificial-intelligenceGCSP
Acc 5/27/22 TA\]

Most importantly for military strategy, [AI applications may assist
decision-makers to monitor the battlefield and develop scenarios.
Indeed, AI could be developed to predict the behaviour and reactions of
foreign countries]{.underline} or generate simulations of the
progression of ongoing conflicts.32 AI may also be useful to assess
threats, provide risk analyses, and suggest courses of action,
ultimately guiding decision-makers on the best response to take.33 In
addition, AI may support the alignment of the armed forces' ways and
means with the given political and strategic objectives - a major
function of military strategy. [A consequence of such developments would
be an increased speed and quality of military processes.]{.underline}
While this would provide significant advantages to those states with the
most performant AI,34 [this may also pressure armed forces to
increasingly delegate the orchestration of military operations to AI
systems]{.underline}.35 Indeed, the use of AI for military strategy may
also lead to challenges. Reliable AI systems would need to be trained
with vast data sets.36 Furthermore, it has been warned that [AI may
exacerbate threats, transform their nature and characteristics, and
introduce new security threats]{.underline}.37 [A tabletop exercise on
the integration of AI into nuclear C2 systems showed that such systems
were 'vulnerable to malicious manipulation that can severely degrade
strategic stability']{.underline},38 for instance. [Such vulnerabilities
would derive mostly from the risk posed by third actors using techniques
to deceive, disrupt or impair C2 systems]{.underline},39 which indicates
the importance of system safety for AI to be used for military strategy.
[Another significant challenge is that AI may accelerate the speed of
warfare to the extent that humans cease to be able to follow the
developments, ultimately leading humans to lose control]{.underline}. 40
[This phenomenon has been termed battlefield 'singularity' or
'hyperwar'.41 This may lead to strategic errors and accidents, including
involuntary conflict escalation]{.underline}. Even if such risks can be
alleviated, [the increased reliance on AI would reduce the human element
of military strategy, in particular psychology and human judgment. It
has been argued that this could lead to a 'gap between how the AI solves
a problem framed by humans, and how those humans would solve it if they
possessed the AI's speed, precision, and brainpower']{.underline}.42 Yet
it has also been argued that strategy development would require the
understanding of values, the balance of costs, and the understanding of
the complex social system in which war operates, thereby significantly
limiting AI's use for military strategy.43 Yet it is also possible that
[when enemies possess]{.underline} high levels of rational prediction
power provided by [AI systems, the decisive factor in warfare will not
be the AI systems' capabilities but the human judgment,]{.underline} [in
particular concerning critical and difficult choices.44 This, however,
presumes a certain level of meaningful human involvement]{.underline}.
In sum, [AI may enhance military]{.underline} strategy development and
strategic [decision-making,]{.underline} notably if able to process more
data and make sense of complexity with more precision and [at a higher
speed than humans and simple computing.]{.underline} A likely result is
[an acceleration of military operations]{.underline}, which [may
increase pressure on armed forces to integrate AI and may marginalize
human judgment]{.underline}. States' recent adoption of defence
strategies on and related to AI indicate that states increasingly intend
to develop, acquire, and operationalize AI for military purposes. As
such, t[he possession and use of AI is a strategic objective itself. In
light of secrecy around]{.underline} the development of [new
technologies, states' investment in military AI can become a strategic
liability as it may increase the risk of destabilizing arms races,
misperceptions, and miscalculations.]{.underline}

**Crisis instability risks nuclear escalation -- the lack of human
control threatens the firebreak between conventional and nuclear
conflict.**

**Sauer, 2021 - Senior Research Fellow at Bundeswehr University**
\[Frank serves on the International Panel on the Regulation of
Autonomous Weapons IRRC No. 913 March "Stepping back from the brink: Why
multilateral regulation of autonomy in weapons systems is difficult, yet
imperative and feasible"
https://international-review.icrc.org/articles/stepping-back-from-brink-regulation-of-autonomous-weapons-systems-913
Acc 4/5/22 TA\]

Strategic instability It has recently been suggested that AI as a
decision-making aid to humans might help improve the performance of
nuclear early-warning and command and control systems, thus reducing the
risk of false alarms and inadvertent nuclear use.72 That said, calls for
complete automation in the nuclear realm -- that is, for handing over
the decision to use nuclear weapons from humans to machines -- are
practically non-existent.73 [But even with the proverbial push of the
button not yet delegated to algorithms, the rush to increase autonomy in
military applications and to automatize military processes increases the
risk of nuclear stability.]{.underline}74 For instance, [the increasing
capacities of conventional weapons systems -- including weapon autonomy
-- are beginning to affect the strategic level. This development has
been described as the increasing "entanglement" of the nuclear and the
conventional realm resulting, for example, from "non-nuclear threats to
nuclear weapons and their associated command, control, communication,
and information (C3I) systems]{.underline}".75 Simply put, [advanced
conventional capabilities increasingly allow for nuclear assets to be
put at risk. Autonomy in conventional weapons systems is one such
advanced capability, thus feeding into this increasing entanglement and,
in turn, deteriorating strategic stability.]{.underline} One specific
illustration of this dynamic is the deployment of stealthy unmanned
aerial vehicles and the use of "swarming". Perdix is a swarming test
program pursued by the US Air Force. In the future, drone swarms of this
type might facilitate the search for dispersed mobile missile launchers.
Another example is the use of maritime autonomous systems for hunting
nuclear-powered ballistic missile submarines, known as SSBNs. The
DARPA-funded Anti-Submarine Warfare Continuous Trail Unmanned Vessel is
a program that resulted in the development of an autonomous trimaran
called Sea Hunter, which is currently being tested by the US Navy. Its
ability to detect and pursue SSBNs could potentially limit the
second-strike capabilities of other nuclear powers. These capabilities
are just emerging, and neither Perdix nor Sea Hunter, nor their
successors, will single-handedly destabilize the global nuclear order.
Also, the hypothesis that systems such as Sea Hunter would render the
oceans "transparent",76 virtually nullifying the utility of sea-launched
nuclear weapons as a reliable second-strike asset, is hotly debated.
Nevertheless, [the mere perception of nuclear capabilities becoming
susceptible to new risks from the conventional realm is bound to sow
distrust between nuclear-armed adversaries.]{.underline} Furthermore, a
system like Sea Hunter demonstrates how [autonomous weapon technologies
are expediting the completion of the targeting cycle, thus putting the
adversary under additional pressure and potentially creating
"use-them-or-lose-them" scenarios with regard to executing a nuclear
second strike.]{.underline} The entanglement problem, which weapon
autonomy is feeding into, is further aggravated by an increasing
political willingness to use nuclear means to retaliate against
non-nuclear attacks on early-warning and control systems or the weapons
themselves. The Trump administration\'s nuclear posture review77 signals
that the United States may, from now on, respond with nuclear means to
significant, non-nuclear strategic attacks (moving away from a
"single-purpose" nuclear deterrence framing for nuclear weapons). Russia
has already held this position for some time due to the United States'
advantage in conventional weapons technology. This does not bode well
for stability between the two largest nuclear powers. To sum up this
section, [weapon autonomy]{.underline} not only promises military
benefits but also [creates new vulnerabilities and, more importantly,
contributes to an overall accumulation of strategic risk and
instability. Increasing operational speed beyond the capability of human
cognition removes humans as a valuable fail-safe against unwanted
escalation.]{.underline}

**Autonomous AI undermines NATO's collective defense by eroding nuclear
deterrence**

**Gilli, 2020 - Senior Researcher at the NATO Defense College**
\[Andrea, NDC Research Paper No.15 -- December ""NATO-Mation":
Strategies for Leading in the Age of Artificial Intelligence"
https://www.ndc.nato.int/news/news.php?icode=1514 Acc 4/21/22 TA\]

Arms control, technology regimes and confidence building [NATO's core
task of collective defence has been at the heart of the Atlantic
Alliance since its inception. In strategic terms, collective defence
translates into deterrence]{.underline} and is in fact fundamental to
ensure the security, wellbeing and freedom of Allies and their
citizens[. Collective defence is, however, just one side of the coin.
Historically, NATO Allies have also pursued arms control and
disarmament]{.underline}. Recent statements on the role of nuclear
weapons bear testament to this: [while NATO wishes for a world free of
nuclear weapons, nuclear deterrence remains a pillar of the
Alliance]{.underline} with a view to preventing intimidation, coercion
or attack from outside powers.263 [The emergence of AI, ML and BD has
generated an important debate on nuclear stability]{.underline}.264
[Many worry that, when integrated into strategic forces, AI may bring
about a set of nefarious consequences: the brittleness characterizing
algorithms may lead to inaccuracy, and thus to crises;265 machine speed
may lead to the removal of humans from the loop, thus tipping crises
into escalations]{.underline};266 last but not least, some still
emerging technologies such as [AI may undermine the survivability of
second-strike nuclear capabilities, thus eroding the foundations of
nuclear deterrence]{.underline}.267

#### Ethical principles and Responsible AI use prevents miscalculation and escalation

**International Panel on the Regulation of Autonomous Weapons, 2021**
\[(iPRAW) coordinated by: German Institute for International and
Security Affairs, July "Building Blocks for a Regulation on LAWS and
Human Control Updated Recommendations to the GGE on LAWS"
https://www.readkong.com/page/building-blocks-for-a-regulation-on-laws-and-human-control-8617434
Acc 2/27/22 TA\]

Albeit not being at the center of the CCW debate on LAWS, [military AI
and machine autonomy could also have manifold implications for
international security and strategic stability.]{.underline} A major
benefit of applying autonomous functions for military purposes is the
possibility of accelerating information processing, decision-making, and
command and control cycles. [A faster tempo of warfare]{.underline}
however also [runs risk of overwhelming human operators and undermining
human judgment, especially in crisis situations]{.underline}. [This
could be aggravated by the fact that AI-enabled systems are often not
entirely comprehensible for humans, especially those relying on machine
or deep learning]{.underline}. [Automation bias, meaning human
overreliance]{.underline} and over- trust in the effectiveness of
machine autonomy, [has already caused various accidents in the civilian
domain]{.underline} and could be particularly acute if human operators
were not aware of the limits of AI and autonomy. Therefore, [technical
errors, coupled with unpredictable, opaque systems and automation bias
could lead to a situation where humans might lose the ability to control
escalation and manage war termination]{.underline}. Furthermore[,
already existing threat perceptions and an increasing speed of warfare
could spur arms competition towards greater levels of autonomy that
again increases the speed of conflicts, leading to a vicious
circle]{.underline}. Similarly, while sophisticated AI- enabled systems
might not be easily built or acquired, rather [\"crude\" LAWS and their
components could diffuse rather rapidly, potentially falling into the
hands of illegitimate or irresponsible actors.]{.underline} Export
controls might be able to mitigate this issue to a certain extent[.
Whether AI-enabled systems and machine autonomy will strengthen or
undermine strategic stability will to a large extent depend
on]{.underline} their application and [the human role]{.underline}. In
many cases, [AI technologies could aid human operators and strengthen
strategic stability. For example]{.underline}, by enabling the
integration of heterogeneous data and rapid information processing, [AI
methods could improve situational awareness of human operators and
commanders.]{.underline} [This however presupposes that technical risks
and limitations]{.underline} as well as risks in relation to
human-machine interaction [are taken into account and that safety
measures and adequate training of human operators ensure]{.underline}
reliable systems and their [responsible use.]{.underline} Ultimately,
the focus should be on aiding rather than replacing the unique judgment
of humans.

#### Human control solves crisis management and prevents accidental nuclear escalation.

**Sauer, 2021 - Senior Research Fellow at Bundeswehr University**
\[Frank serves on the International Panel on the Regulation of
Autonomous Weapons IRRC No. 913 March "Stepping back from the brink: Why
multilateral regulation of autonomy in weapons systems is difficult, yet
imperative and feasible"
https://international-review.icrc.org/articles/stepping-back-from-brink-regulation-of-autonomous-weapons-systems-913
Acc 4/5/22 TA\]

[Humans are more resistant to mass error than machines. Also, humans,
despite being slower]{.underline} and sometimes making mistakes, [are
better managers than machines. They have the capacity to grasp an
unusual situation and understand its context as well as to reflect on a
decision]{.underline}, its genesis, its implications and the weight of
the responsibility that accompanies it[. In terms of crisis management,
all this makes humans superior to machines]{.underline}, which so far
are only capable of recognizing patterns and executing predefined
actions, and which reach superhuman performance only in those narrowly
defined scenarios for which they were specifically trained. [By removing
human control, the distinct role of humans as a versatile fail-safe
mechanism is lost. The prominent case of Lieutenant Colonel Stanislav
Petrov renders this evident. The 1983 NATO exercise Able Archer was
misunderstood by the Soviets as a cover for an attack with tactical
nuclear forces.]{.underline} During this time, a Soviet early-warning
satellite registered first one, then a couple more US nuclear
intercontinental ballistic missile launches. [Petrov,]{.underline} the
watch officer in charge at the time, [decided (correctly) that this had
to be a false alarm and gave the all-clear up the chain of command, thus
preventing further, potentially nuclear escalation]{.underline} in this
tense situation. [Petrov\'s decision could not have been made by a
completely automated system. He later testified that he had arrived at
his decision by following a gut feeling,]{.underline} by wondering about
the nature of the supposed strike, and by drawing on his past
experiences with the early-warning system that he deemed not fully
trustworthy.71 [If the human]{.underline} on the destroyer [in the
next-to-fully automated kill chain]{.underline} presented by General
[Goldfein were ever to be removed,]{.underline} fully actualizing the
key advantage of weapon autonomy that is fighting at machine speed, [the
"Petrov effect" would be lost]{.underline}. While, in that conventional
scenario, this would not mean the inadvertent use of nuclear weapons,
strategic stability is nevertheless already being affected by the effort
to increase autonomy in military systems.

### 1AC - Advantage - Human Dignity

#### Advantage -- Human Dignity 

#### Every person has inherent worth -- if we value ourselves, then we Must respect the value of others to be consistent. Sacrificing the dignity of some to protect others denies equal dignity and is morally unacceptable. Every person must be treated as an end in and of themselves.

**Applbaum, 1998 - Professor of Ethics and Public Policy at the Carr
Center for Human Rights Policy** \[Arthur \"Are Violations of Rights
Ever Right?\*.\" Ethics 108.2 (1998): 340-366 Acc 12/27/20 TA\]

[Since we think of ourselves as beings that matter, consistency demands
that we extend that status to others]{.underline}. Says Nagel, ''I
believe, as did Kant, that what drives us in the direction of universal-
izability is the difficulty each person has in regarding himself as
having value only for himself, but not in himself. [If people are not
ends in themselves]{.underline}---i.e. impersonally valuable---[then
they have a much lower order of worth.'' If one wishes to]{.underline}
view and [value oneself as]{.underline} a being that is [an end in
itself]{.underline}, and not as a means to be used for the ends of
others, [then the status of an end must be extended to
others]{.underline}. The violation of other persons---[using them as
means---therefore is an impersonal bad]{.underline}, something we all
have reason to avoid and prevent. But if such violation is bad, why
should we not seek to minimize violations, even if that sometimes
requires a lesser violation? Because [a violation-minimizing violation
uses one as a means for the ends of others and so fails to treat persons
as ends in themselves]{.underline}. If persons are to matter in the
highest possible way, then morality must value not only the absence of
violations of persons, but the treatment of persons as beings who have
the status of being inviolable---whose violation is not permissible.
''What actually happens to us is not the only thing we care about: What
may be done to us is also important, quite apart from whether or not it
is done to us---and the same is true of what we may do as opposed to
what we actually do.'' Since having the status of inviolability is of
great value, if morality permits violations so as to maximize the good
of not being violated, all persons cease to have a high degree of
inviolability, which is a great bad. [We all may be better off in a
world in which morality always treated us as ends, and so where it is
always morally impermissible to violate us]{.underline}, even though we
are thereby more likely to suffer violation at the hands of immoral
actors

#### Only respecting human dignity prevents treating humans as a means to an end. Utilitarianism is an instrumental rationality, which denies human dignity.

**Heyns, 2016 - Professor of Human Rights Law, University of Pretoria**
\[Christof, Human Rights Quarterly 38 (2016) 350--378 " Human Rights and
the use of Autonomous Weapons Systems (AWS) During Domestic Law
Enforcement"
<https://www.academia.edu/37475669/Human_Rights_and_the_use_of_Autonomous_Weapons_Systems_AWS_During_Domestic_Law_Enforcement>
Acc 12/27/20 TA\]

There are different conceptions of dignity. For example, it has a
religious connotation for some and is tied up with the belief that
humans are created in the image of God (imago Dei);87 others rely on the
Kantian notion that [each person should be treated as an end and not as
a means]{.underline}.88 [Dignity can]{.underline} also [be seen as
related to the capacity of someone to be a moral person and as moral
responsibility.]{.underline}89 Violations of dignity often take the form
of physical infringements of the person, but it is not confined to
that---not being able to act out one's moral choices could be another
form of indignity. [Underlying the concept of dignity is a strong
emphasis on the idea of the infinite or incommensurable value of each
person]{.underline}.90 The Kantian concept of dignity assigns this value
to each person as a separate and unique, or irreplaceable, individual.
Acts of indignity involve an unwarranted reduction of this worth. [Each
person has an inner core that may not be infringed upon, even if such
infringement would be beneficial to the common good because that would
mean they are used as a tool. Individual dignity cannot be accounted for
properly within schemes of instrumental rationality, where competing
claims are measured against each other based on their assigned weights,
because each human being is considered to be of infinite
value]{.underline}.91 Exactly whose dignity is potentially at stake? In
the first place, I would submit, the concern should be about the dignity
of those at the receiving end of the force used: those targeted as well
as those caught in the crossfire.92 Secondly, the dignity of those in
whose name force is used may also be at stake. The effect of AWS on the
dignity of these two different categories will now be considered in
turn. How does the use of AWS impact the dignity of those subjected to
the use of force? It has been argued that [to have the decision whether
one lives or dies being made by machines is the ultimate
indignity]{.underline} and similar arguments can be made about other
forms of force, especially if they are potentially lethal or can
seriously maim. 93 [A machine, bloodless and without morality or
mortality, cannot fathom the significance of using force against a human
being and cannot do justice to the gravity of the
decision]{.underline}.94 [Each instance where force is used against a
human being requires that another human being should decide]{.underline}
afresh [whether to cross that threshold]{.underline}. [The heuristic
argumentation used by computers fails to capture and do justice the
complexity and fullness of human life and decisions about
life.]{.underline} Robots cannot be preprogrammed to respond in an
appropriate way to the infinite number of scenarios that real life---and
real people---offer.95 [Death by algorithm means that people are treated
simply as targets and not as complete and unique human
beings]{.underline}, who may, by virtue of that status, meet a different
fate. They are placed in a position where an appeal to the humanity of
the person on the other side is not possible. Some have argued that
people in such situations are treated like pests or objects, as a
nuisance that must be gotten rid of, rather than as someone with
inherent dignity.96 [When someone comes into the sights of a computer,
that person is literally reduced to numbers: the zeros and the ones of
bits]{.underline}.

#### Even if AI saves lives, it destroys the Dignity that gives value to lives.

**Docherty, 2018 - senior researcher in the Arms Division of Human
Rights Watch** \[Bonnie August 21, "Heed the Call A Moral and Legal
Imperative to Ban Killer Robots"
[https://www.hrw.org/report/2018/08/21/heed-call/moral-and-legal-imperative-ban-killer-robots#](https://www.hrw.org/report/2018/08/21/heed-call/moral-and-legal-imperative-ban-killer-robots)
Acc 12/27/20 TA\]

[Showing respect for human life entails minimizing killing.]{.underline}
Legal and [ethical judgment helps humans]{.underline} weigh different
factors to [prevent]{.underline} arbitrary and [unjustified loss of life
in armed conflict]{.underline} and beyond. [It would be difficult to
recreate such judgment]{.underline}, developed over both human history
and an individual life, [in fully autonomous weapons]{.underline}, and
they could not be pre-programmed to deal with every possible scenario in
accordance with accepted legal and ethical norms. Furthermore, most
humans possess an innate resistance to killing that is based on their
understanding of the impact of loss of life, which fully autonomous
weapons, as inanimate machines, could not share. [Even if fully
autonomous weapons could adequately protect human life, they would be
incapable of respecting human dignity. Unlike humans, these robots would
be unable to appreciate fully the value of a human life and the
significance of its loss.]{.underline} They would make life-and-death
decisions based on algorithms, reducing their human targets to objects.
Fully [autonomous weapons would thus violate the principles of humanity
on all fronts]{.underline}.

#### Autonomous artificial intelligence does not respect human dignity -- it lacks empathy and judgement. Death by algorithm reduces human targets to data points, not people.

**Docherty, 2018 - senior researcher in the Arms Division of Human
Rights Watch** \[Bonnie August 21, "Heed the Call A Moral and Legal
Imperative to Ban Killer Robots"
[https://www.hrw.org/report/2018/08/21/heed-call/moral-and-legal-imperative-ban-killer-robots#](https://www.hrw.org/report/2018/08/21/heed-call/moral-and-legal-imperative-ban-killer-robots)
Acc 12/27/20 TA\]

[Due to their lack of emotion and legal and ethical judgment, fully
autonomous weapons would face significant obstacles in complying with
the principles of humanity. Those principles require the humane
treatment of others and respect for human life and human dignity. Humans
are motivated to treat each other humanely because they feel compassion
and empathy for their fellow humans]{.underline}. Legal and ethical
judgment gives people the means to minimize harm; it enables them to
make considered decisions based on an understanding of a particular
context. [As machines, fully autonomous weapons would not be sentient
beings capable of feeling compassion. Rather than exercising judgment,
such weapons systems would base their actions on pre-programmed
algorithms, which do not work well in complex and unpredictable
situations]{.underline}. [Showing respect for human life entails
minimizing killing]{.underline}. Legal and [ethical judgment helps
humans weigh different factors to prevent arbitrary and unjustified loss
of life]{.underline} in armed conflict and beyond. [It would be
difficult to recreate such judgment, developed over both human history
and an individual life, in fully autonomous weapons, and they could not
be pre-programmed to deal with every possible scenario]{.underline} in
accordance with accepted legal and ethical norms. Furthermore, [most
humans possess an innate resistance to killing that is based on their
understanding of the impact of loss of life, which fully autonomous
weapons, as inanimate machines, could not share. Even if fully
autonomous weapons could adequately protect human life, they would be
incapable of respecting human dignity. Unlike humans, these robots would
be unable to appreciate fully the value of a human life and the
significance of its loss. They would make life-and-death decisions based
on algorithms, reducing their human targets to objects. Fully autonomous
weapons would thus violate the principles of humanity on all
fronts.]{.underline}

#### Human Dignity requires that the decision to take human life be made by a human. Even if war is inevitable, inhumanity is not.

**Rosert, 2019 - Professor for International Relations at Universität
Hamburg** \[Elvira, with Frank Sauer Researcher at Bundeswehr, Global
Policy, July 5 "Prohibiting Autonomous Weapons: Put Human Dignity First"
https://doi.org/10.1111/1758-5899.12691 Acc 12/27/20 TA\]

In addition to its successful mobilization in stigmatization and
norm‐setting processes on anti‐personnel landmines and cluster
munitions, the principle of distinction as enshrined in International
Humanitarian Law also figures prominently in the debate on lethal
autonomous weapons systems (LAWS). Proponents of a ban on LAWS frame
these as indiscriminate, that is, unable to distinguish between
civilians and combatants, and thus as inherently unlawful. The flip side
of this particular legal argument is, however, that LAWS become
acceptable when considered capable of distinguishing between combatants
and civilians. We thus argue, first, that this particular legal basis
for the call for a ban on LAWS might be rendered obsolete by
technological progress increasing discriminatory weapon capabilities.
Second, we argue that the argument is normatively troubling as it
suggests that, as long as civilians remain unharmed, attacking
combatants with LAWS is acceptable. Consequently, we find that the legal
principle of distinction is not the overall strongest argument to
mobilize when trying to stigmatize and ban LAWS. [A]{.underline} more
[fundamental, ethical argument within the debate about LAWS]{.underline}
-- and one less susceptible to 'technological fixes' -- [should be
emphasized instead, namely that life and death decisions on the
battlefield should always and in principle be made by humans
only.]{.underline} Lethal autonomous weapons systems: a threat to human
dignity Numerous arguments motivate the current call for an
international, legally binding ban on so‐called lethal autonomous
weapons systems (LAWS).1 Strategic concerns include proliferation, arms
races and escalation risks (Altmann and Sauer, 2017; Rickli, 2018).
Military concerns include the incompatibility of LAWS with a traditional
chain of command or the potential for operational failures cascading at
machine speed (Bode and Huelss, 2018; Scharre, 2016). [Ethical concerns
include the fear that LAWS might further increase the dehumanization and
abstractness of war]{.underline} (and thus its propensity), [as well as
its cruelty if warfare is delegated to machines incapable of empathy or
of navigating in dilemmatic situations]{.underline} (Krishnan, 2009;
Sauer and Schörnig, 2012; Sparrow, 2015; Sparrow et al., 2019; Wagner,
2014). Legal concerns include difficulties of attribution,
accountability gaps, and limits to the fulfillment of obligatory
precautionary measures (Brehm, 2017; Chengeta, 2017; Docherty, 2015).
But the most prominent concern, focalizing some elements of the concerns
just mentioned, is the danger these weapons pose to civilians. This
argument\'s legal underpinning is the principle of distinction --
undoubtedly one of the central principles of International Humanitarian
Law (IHL), if not the central principle (Dill, 2015). As multifaceted
and complex as the debate on military applications of autonomy is now,
what has been articulated at its very beginning (Altmann and Gubrud,
2004; Sharkey, 2007) and consistently since then is that LAWS would
violate IHL due to their inability to distinguish between combatants and
civilians. This image of LAWS as a threat to civilians is echoed
routinely and placed first by all major ban supporters (we substantiate
this claim in the following section). That LAWS would be incapable of
making this crucial distinction -- and thus have to be considered
indiscriminate -- is assumed because 'civilian‐ness' is an
under‐defined, complex and heavily context‐dependent concept that is not
translatable into software (regardless of whether the software is based
on rules or on machine learning). Recognizing and applying this concept
on the battlefield not only requires value‐based judgments but also a
degree of situational awareness as well as an understanding of social
context that current and foreseeable computing technology does not
possess. We unequivocally share this view as well as these concerns. And
yet, in this article, we propose to de‐emphasize the indiscriminateness
frame in favor of a deeper ethical assertion, namely that [the use of
LAWS would infringe on human dignity. The minimum requirement for
upholding human dignity, even in conflicts, is that life and death
decisions on the battlefield should always and in principle be made by
humans]{.underline} (Asaro, 2012; Gubrud, 2012). Not the risk of
(potential) civilian harm, but rather [retaining meaningful human
control to preserve human dignity should be at the core of the message
against LAWS.2]{.underline}

#### The Process of taking a human life matters as much as the consequence -- if the process denies humanity, it undermines dignity. Autonomous AI denies dignity because machines cannot understand the value of human dignity, so the decision cannot take into account the unique individual to be killed. 

**Docherty, 2018 - senior researcher in the Arms Division of Human
Rights Watch** \[Bonnie August 21, "Heed the Call A Moral and Legal
Imperative to Ban Killer Robots"
[https://www.hrw.org/report/2018/08/21/heed-call/moral-and-legal-imperative-ban-killer-robots#](https://www.hrw.org/report/2018/08/21/heed-call/moral-and-legal-imperative-ban-killer-robots)
Acc 12/27/20 TA\]

Respect for Human Life and Dignity Definition [A second principle of
humanity requires actors to respect both human life and human
dignity]{.underline}. Christof Heyns, former special rapporteur on
extrajudicial, summary or arbitrary executions, highlighted these
related but distinct concepts when he posed two questions regarding
fully autonomous weapons: "\[C\]an \[they\] do or enable proper
targeting?" and ["Even if they can do proper targeting, should machines
hold the power of life and death over humans?"]{.underline}\[68\] The
first considers whether a weapon can comply with international law's
rules on protecting life. The second addresses the "manner of targeting"
and whether it respects human dignity.\[69\] I[n order to respect human
life, actors must take steps to minimize killing]{.underline}.\[70\] The
right to life states that "\[n\]o one shall be arbitrarily deprived of
his life."\[71\] It limits the use of lethal force to circumstances in
which it is absolutely necessary to protect human life, constitutes a
last resort, and is applied in a manner proportionate to the
threat.\[72\] Codified in Article 6 of the International Covenant on
Civil and Political Rights, the right to life has been recognized as the
"supreme right" of international human rights law, which applies under
all circumstances. During times of armed conflict, international
humanitarian law determines what constitutes arbitrary or unjustified
deprivation of life. It requires that actors comply with the rules of
distinction, proportionality, and military necessity in situations of
armed conflict.\[73\] Judgment and emotion promote respect for life
because they can serve as checks on killing. The ability to make legal
and ethical judgments can help an actor determine which course of action
will best protect human life in the infinite number of potential
unforeseen scenarios. An instinctive resistance to killing provides a
psychological motivation to comply with, and sometimes go beyond, the
rules of international law in order to minimize casualties. Under the
principles of humanity, [actors must also respect the dignity of all
human beings. This obligation is premised on the recognition that every
human being has inherent worth that is both universal and
inviolable]{.underline}.\[74\] Numerous international
instruments---including the Universal Declaration of Human Rights, the
International Covenant on Civil and Political Rights, the Vienna
Declaration and Programme of Action adopted at the 1993 World Human
Rights Conference, and regional treaties---enshrine the importance of
dignity as a foundational principle of human rights law.\[75\] The
Africa Charter on Human and Peoples' Rights explicitly states that
individuals have "the right to the respect of the dignity inherent in a
human being."\[76\] [While respect for human life involves minimizing
the number of deaths]{.underline} and avoiding arbitrary or unjustified
ones, [respect for human dignity requires an appreciation of the gravity
of a decision to kill]{.underline}.\[77\] The ICRC explained that [it
matters "not just if a person is killed or injured but how they are
killed or injured, including the process by which these decisions are
made]{.underline}."\[78\] [Before taking a life, an actor must truly
understand the value of a human life and the significance of its loss.
Humans should be recognized as unique individuals and not reduced to
objects with merely instrumental or no value.]{.underline}\[79\] [If an
actor kills without taking into account the worth of the individual
victim, the killing undermines the fundamental notion of human
dignity]{.underline} and violates this principle of humanity.
Application to Fully Autonomous Weapons [It is highly unlikely that
fully autonomous weapons would be able to respect human life and
dignity. Their lack of legal and ethical judgment would interfere with
their capacity to respect human life.]{.underline} For example,
international humanitarian law's proportionality test requires
commanders to determine whether anticipated military advantage outweighs
expected civilian harm on a case-by-case basis. Given the infinite
number of contingencies that may arise on the battlefield, fully
autonomous weapons could not be preprogrammed to make such
determinations. The generally accepted standard for assessing
proportionality is whether a "reasonable military commander" would have
launched a particular attack,\[80\] and reasonableness requires making
decisions based on ethical as well as legal considerations.\[81\] Unable
to apply this standard to the proportionality balancing test, fully
autonomous weapons would likely endanger civilians and potentially
violate international humanitarian law.\[82\] [Fully autonomous weapons
would also lack the instinctual human resistance to killing]{.underline}
that can protect human life beyond the minimum requirements of the
law.\[83\] An inclination to avoid killing comes naturally to most
people because they have an innate appreciation for the inherent value
of human life. Empirical research demonstrates the reluctance of human
beings to take the lives of other humans. For example, a retired US Army
Ranger who conducted extensive research on killing during armed conflict
found that "there is within man an intense resistance to killing their
fellow man. A resistance so strong that, in many circumstances, soldiers
on the battlefield will die before they can overcome it."\[84\] As
inanimate objects, fully autonomous weapons could not lose their own
life or understand the emotions associated with the loss of the life of
a loved one. It is doubtful that a programmer could replicate in a robot
a human's natural inclination to avoid killing and to protect life with
the complexity and nuance that would mirror human decision making.
[Fully autonomous weapons could not respect human dignity, which relates
to the process behind, rather the consequences of, the use of
force]{.underline}.\[85\] [As machines, they could truly comprehend
neither the value of individual life nor the significance of its loss.
They would base decisions to kill on algorithms without considering the
humanity of a specific victim]{.underline}.\[86\] Moreover, these
weapons would be programmed in advance of a scenario and could not
account for the necessity of lethal force in a specific situation. In a
CCW presentation as special rapporteur, Christof Heyns explained that:
[to allow machines to determine when and where to use force against
humans is to reduce those humans to objects; they are treated as mere
targets. They become zeros and ones in the digital scopes of weapons
which are programmed in advance]{.underline} to release force without
the ability to consider whether there is no other way out, without a
sufficient level of deliberate human choice about the matter.\[87\]
Mines Action Canada similarly concluded that ["\[d\]eploying \[fully
autonomous weapons\] in combat displays the belief that any human
targeted in this way does not warrant the consideration of a live
operator, thereby robbing that human life of its right to
dignity.]{.underline}"\[88\] [Allowing a robot to take a life when it
cannot understand the inherent worth of that life or the necessity of
taking it disrespects and demeans the person whose life is taken. It is
thus irreconcilable with the principles of humanity]{.underline}
enshrined in the Martens Clause.

#### Human dignity demands that humans remain in the loop for artificial intelligence weapon systems.

**International Panel on the Regulation of Autonomous Weapons, 2021**
\[(iPRAW) coordinated by: German Institute for International and
Security Affairs, July "Building Blocks for a Regulation on LAWS and
Human Control Updated Recommendations to the GGE on LAWS"
https://www.readkong.com/page/building-blocks-for-a-regulation-on-laws-and-human-control-8617434
Acc 2/27/22 TA\]

The precautionary principle: [The obligation to take precautions is not
limited to attacks but the principle applies]{.underline} -- at least to
a certain extent -- [to military operations in general]{.underline}. The
notion of "military operations" has a broader meaning encompassing "any
movements, maneuvers and other activities whatsoever carried out by the
armed forces with a view to combat." Parties to a conflict have to take
all feasible measures to verify the target and to avoid excessive damage
to civilians. [Weapon systems based on computational methods could be a
helpful instrument to support humans]{.underline} in their
decision-making processes, especially with regard to target
verification. However, in case LAWS are deployed, it would be
questionable whether such weapon systems are able to obtain reliable
information to distinguish adequately between legitimate and
illegitimate targets. As adumbrated above, the distinction between
combatants and civilians and between military and civilian objects
respectively is challenging, often requiring the cognitive capacity to
detect slight nuances in human behavior and changes of circumstances.
Furthermore, the principle of precautions in attack requires parties to
a conflict to take feasible measures to avoid excessive harm to
civilians. This entails the obligation to deploy weapons with
predictable effects which, in turn, necessitates an adequate
understanding of the weapon system and its operating principles by the
human operators and the military commanders. Furthermore, [the use of
weapon systems based on computational methods may oblige parties to a
conflict to apply even higher standards of precaution]{.underline}. It
is also important to bear in mind that the principle of precautions
requires parties to a conflict to abort a mission in case the military
objective turns out to be civilian. A LAWS would have to be able to
autonomously abort a mission in case of doubt regarding the respective
target. One way to address the risk of legal violations posed by the
employment of LAWS is to regulate how these systems are used, i.e. with
human control, rather than to regulate the systems, that is, their
numbers or capabilities, themselves. Human Control as a Consequence of
IHL? [The obligation to maintain human control arguably derives from
IHL]{.underline}, at least by implication. However, IHL does not
indicate how human control should be operationalized. It is arguable
that the principles of distinction, proportionality, and precautions in
attack imply a requirement for the user to have sufficient situational
understanding but also options for intervention. Accordingly, the
operator/commander must be able to review legal assessments and
translate human decision-making into the system's action during attack
prior to the actual engagement. One option to exert control is to impose
operational constraints on LAWS, for example, by not using them in
anti-personnel mode or against military objects by use. Even if some of
those legal issues were be solved by technological achievements in the
future[, certain ethical challenges would still call for human control
in the use of force,]{.underline} especially if used against human
targets. Therefore[, iPRAW's ethical considerations focus on the concept
of human dignity]{.underline}. Based on the work of Peter Asaro and
Christof Heyns, [we defined a set of three minimum requirements for
human dignity in the use of force as the ability to: Recognize a human
being as a human,]{.underline} not just distinguish it from other types
of objects and things but [as a being with rights that deserve
respect]{.underline}; [Understand the value of life and the significance
of its loss; and Reflect upon the reasons for taking lif]{.underline}e
and reach a rational conclusion that killing is justified in a
particular situation. Depending on the moral position, one would assume
or deny that autonomous functions in weapon systems break the link to
moral agency. In the first case, [it would be necessary to safeguard
moral agency through human control]{.underline}, in the latter case one
would want to safeguard the ability to use a weapon system lawfully at
the current state of technology. In consequence, [both positions would
require human control in both the design of the system and in its
use]{.underline}. Inherent in both views is an acknowledgment -- tacit
or explicit -- of the principle of human control.

### 1AC - Solvency 

#### Contention Two -- Solvency - The US is key to promote AI principles in NATO because of its scientific and military leadership and its ability to coordinate nations. 

**Kahn and Horowitz, 2021 -- Research and Senior Fellows at the Council
on Foreign Relations** \[Lauren and Michael, The Washington Quarterly
44:4 "Leading in Artificial Intelligence through Confidence Building
Measures" [https://doi.org/10.1080/0163660X.2021.2018794 Acc
6/6/22](https://doi.org/10.1080/0163660X.2021.2018794%20Acc%206/6/22)
TA\]

Leveraging US Leadership Advances in artificial intelligence, driven by
machine learning methods and related approaches, are already reshaping
international politics. Economics, societies, and now militaries are
adapting, with various degrees of speed. As it is early in the age of
AI, there is still significant uncertainty about the specific ways that
AI and machine learning will impact military behavior and the future of
war. [One significant concern involves the potential for AI-enabled
military applications to increase the risk of accidents, unintentional
conflict, and inadvertent escalation. Poorly programmed, trained, or
deployed algorithms could be subject to accidents and their uses could
be misinterpreted by adversaries. Even if algorithms work as intended
and give militaries an advantage, the increase in the speed of warfare
from their use could create pressure for escalation in a crisis or early
in a conflict. In the midst of competition with China and Russia, the
United States can simultaneously benefit and be at risk from military
applications of AI.]{.underline} [Given its economic, military, and
scientific leadership, the United States has a unique opportunity to
shape the global AI landscape through the promotion of norms and CBMs
that could decrease the risk of unintentional conflict and
escalation]{.underline}. [Only the United States has the convening power
to bring allies and adversaries to the table]{.underline}, whether
bilaterally or multilaterally, [for dialogue around areas of shared
interests---the areas most likely to be building blocks for
cooperatio]{.underline}n. [Key areas for potential cooperation include
AI safety standards,]{.underline} dialogue on AI and strategic
stability, [commitments to keep humans in the loop]{.underline} for the
use of nuclear weapons, and an Autonomous Incidents Agreement. All
require further conversation. Through proposals that involve shared
commitments to standards and policies that the United States would be
willing to pursue unilaterally, [the United States can increase global
AI safety without revealing information that would compromise US
capabilities or undermine US military adoption of AI. The United States
can therefore leverage AI to ensure future military superiority and
simultaneously decrease the risk that military uses of AI will have
disastrous unintended consequences.]{.underline}

#### NATO is key to ethical principles for AI weapons due to its democratic principles and its existing agencies with experience in AI

**Stanley-Lockman, 2021 - Center for Security and Emerging Technology**
\[Zoe CSET Issue Brief August "Responsible and Ethical Military AI
Allies and Allied Perspectives"
https://cset.georgetown.edu/publication/responsible-and-ethical-military-ai/
Acc. 4/21/22 TA\]

Multilateral Institutions Focusing on Ethical and Responsible AI in
Defense Understanding ethics and legality as part of the adoption of
emerging technologies is not only a priority for democratic countries,
but also a topic of interest for multilateral institutions that are part
of the security and defense architecture. Autonomy in weapons has been
on the agenda at the UN level for longer than the countries highlighted
here have spent time bridging technical and policy approaches to
responsible AI in defense. Select [multilateral institutions are also
critical for consultations and alignment about these
issues]{.underline}. Beyond those mentioned here, several smaller allies
may be waiting for multilateral views to then drive their own approaches
to RAI, rather than dedicating resources to first issuing national views
that will later have to align with broader multilateral structures.
[NATO is an obvious player in this domain]{.underline}, for reasons that
are described below. [The emerging PfD is important as an AI-specific
multilateral format for like-minded countries---including non-treaty
partners---to coalesce on this policy area]{.underline}. North Atlantic
Treaty Organization (NATO) [NATO is an important actor because it can
help coordinate and facilitate consultations between allies to come to
agreement on how ethical and responsible AI developments impact
interoperability, cohesion, and operations]{.underline}. [Further, the
NATO Defence Planning Process is the primary defense planning tool for
many Allies.142 The focus on principles for "responsible use" is
consistent with NATO's added value without dwelling too much on
development]{.underline}---which happens primarily at the national (or
bi- /multilateral levels outside of NATO).143 Here, responsibility
refers both to best practices in engineering (e.g., ethical design) and
responsible state behavior.144 The North Atlantic Council and Military
Committee---the senior civilian and military decisionmaking bodies in
NATO---began contending with EDTs, including AI, in 2018.145 [This
high-level interest built on several years of military and scientific
experience at the working levels, as well as the introduction of
conceptual and operational considerations in the workstream
of]{.underline} Allied Command Transformation and [the NATO Science &
Technology Organization]{.underline} in the 2010s. When presenting on
EDTs to the senior civilian and military leadership at NATO in 2018,
then-Supreme Allied Commander Transformation General Denis Mercier
stressed that legal, ethical, and political differences between Allies
could "endanger our capacity to operate together."146 He also focused on
considerations around the "level of confidence in new technologies" as
an adoption factor that is not purely technical.147 This set the tone
for political alignment on ethical concerns across the
Alliance---already building on the foundations of the shared value
embodied in the North Atlantic Charter and legal framework in which NATO
operates. Subsequently, in October 2019, the Allies agreed to an EDT
Roadmap that cited "legal and ethical norms" and "arms control aspects"
as key technology areas among Alliance priorities to consider.148 [The
political will to cooperate on technologies was solidified in February
2021, when NATO Defence Ministers endorsed an EDT
Strategy]{.underline}.149

#### NATO serves as The global model for ethical AI -- acting now can incentivize other nations to follow along.

**Stanley-Lockman and Trabucco, 2022 -- prof of Defense and Strategic
Studies, Nanyang Technological University and prof of Political Science,
University of Copenhagen** \[Zoe and Lena, The Oxford Handbook of AI
Governance, March, "NATO's Role in Responsible AI Governance in Military
Affairs"
https://www.oxfordhandbooks.com/view/10.1093/oxfordhb/9780197579329.001.0001/oxfordhb-9780197579329-e-69
Acc 4/16/22 TA\]

Ethics and values One of the vital aspects of AI which has garnered
significant global attention is [the ethical implications of artificial
intelligence as a military technology---an issue that has divided much
of the global community, including NATO member states.]{.underline} As a
starting point, researchers and analysts have considered the
implications of emerging military technology in terms of ethical
responsibility and regulation, especially as states and organizations
continue to release AI ethical principles, guidelines, and standards.55
We explore how [NATO can operationalize the debate around ethics and
values of military AI to garner coordination and continue progress of
EDT harmonization among partners]{.underline}. Building on the
theoretical discussion from STS and military innovation literature
above[, the adoption of technologies that reinforce values serves the
strategic interest of NATO to shape technological innovation against
current waves of illiberalism.]{.underline} Additionally, [infusing AI
development with certain ethical principles and values can have
operational advantages and benefits, and NATO can, in particular,
promote the ethical principles as operational standards for the
Allies]{.underline}. A common critique within the ethics debate is that
approaching new technology with an ethical or democratic values-driven
perspective translates into comparative military disadvantage.
Essentially, if your adversary develops technology without the
constraints of ethical principles then there will be diminished
effectiveness on the battlefield.56 We find this critique unfounded
because it assumes there is a false trade-off between ethics and
effectiveness; instead, we argue ethical foundations are built into the
architecture of modern warfare.57 As such, ethics is a background
condition for battlefield effectiveness, which is already infused in
military decision-making and helping to guide the boundaries of
international humanitarian law. As such, ethical guidelines do not have
to detract from a military's capacity or competency to devise means and
methods of warfare that will serve their national or coalition
interest.58 If anything, [a first-mover advantage can incentivize an
ethical and values-driven AI to establish the threshold of technological
standards globally]{.underline}.59

#### A statement of ethical principles is a Confidence Building Measure, which helps reduce the risk of AI instability

**Horowitz and Scharre, 2021 - Senior Fellows at the Technology and
National Security Program at the Center for a New American Security**
\[Michael and Paul, Jan 12, "AI and International Stability: Risks and
Confidence-Building Measures"
[https://www.cnas.org/publications/reports/ai-and-international-stability-risks-and-confidence-building-measures
Acc
6/6/22](https://www.cnas.org/publications/reports/ai-and-international-stability-risks-and-confidence-building-measures%20Acc%206/6/22)
TA\]

[CBMs may be a useful tool for managing risks relating to military AI
applications.]{.underline} There are a number of possible CBMs that
states could adopt that may help mitigate the various AI-related risks
previously outlined. [These include]{.underline} broad CBMs applicable
to AI as a category, [CBMs designed to address some of the limitations
of AI]{.underline}, and CBMs focused on specific missions for which
militaries might use AI.49 Broad CBMs These CBMs focus broadly on
mechanisms for dialogue and agreement surrounding military uses of AI,
rather than the specific content of agreements. Given that a key goal of
CBMs is to enhance trust, mechanisms that serve as a building block for
more substantive dialogue and agreement can, in some cases, be an end in
themselves and not just a means to an end.50 [These could include
promoting international norms for how nations develop and use military
AI systems]{.underline}, Track II academic-to-academic exchanges, direct
military-to-military dialogues, [and agreements between states regarding
military AI, such as a]{.underline} code of conduct or [mutual statement
of principles]{.underline}. Promoting Norms In 2019, the U.S. Defense
Innovation Board proposed a set of AI principles for the U.S. Defense
Department, which DoD subsequently adopted in early 2020. While these
principles no doubt have domestic audiences in the U.S. defense
community and tech sector, they also serve as an early example of a
state promulgating norms about appropriate use of AI in military
applications. The DoD AI principles included a requirement that DoD AI
systems be responsible, equitable, traceable, reliable, and
governable.51 (The full set of DoD AI principles is included in the
Appendix). Similarly, the DoD's unclassified summary of its AI strategy,
released in 2019, called for building AI systems that were "resilient,
robust, reliable, and secure."52 A focus of the strategy was "leading in
military ethics and AI safety."53 There is value in states promoting
norms for responsible use of AI, including adopting and employing
technology in a way that reflects an understanding of the technical
risks associated with AI systems. While stating such principles is not
the same as putting in place effective bureaucratic processes to ensure
their compliance, [there is]{.underline} nevertheless [value in states
publicly signaling to others]{.underline} (and to their own
bureaucracies) [the importance of using AI responsibly in military
applications]{.underline}. While these norms are at a high level, they
nevertheless signal some degree of attention by senior military and
civilian defense officials to some of the risks of AI systems, including
issues surrounding safety, security, responsibility, and
controllability. These signals may aid internal bureaucratic efforts to
mitigate various AI-related risks, as bureaucratic actors can point to
these official documents for support. Additionally, [to the extent that
other nations find these statements credible, they may help signal to
other nations]{.underline} at least some degree of awareness and
[attention to these risks, helping to incentivize others to do the
same.]{.underline}

#### Even if dialogue does not produce a viable plan, the process of discussion promotes cohesion through shared values.

**Hill and Marsan, 2018 - Director and Senior Assistant, NATO Office of
Legal Affairs** \[Steven and Nadia, 7-18-18 "Artificial Intelligence and
Accountability: A Multinational Legal Perspective"
https://www.sto.nato.int/publications/STO%20Meeting%20Proceedings/STO-MP-IST-160/MP-IST-160-PP-4.pdf
Acc 4/21/22 TA\]

In the 2016 Warsaw Summit Communiqué, NATO Heads of State and Government
recognized that "the changed and evolving security environment demands
the ability to meet challenges and threats of any kind and from any
direction".2 As an Alliance of 29 Nations focused on collective defense
as one of its core tasks, [NATO must be prepared to address emerging
threats and security challenges arising from the development of new
technologies.]{.underline} Among these, [the development and use of
Artificial Intelligence (AI) presents both opportunities and challenges
to the North Atlantic security landscape.]{.underline} On 22 March 2018,
Allied Command Transformation, NATO's adaptation hub located in Norfolk,
Virginia in the United States of America, organized an informal workshop
with NATO Ambassadors and Military Representatives to discuss the impact
of the development of disruptive technologies on the Alliance. [One
question raised concerned the interoperability challenges NATO could
face as Allies develop disruptive technology with differing
capabilities.]{.underline} A take-away from the discussion is that
[Nations may wish to discuss some of the legal implications of this
emerging technology in]{.underline} a multilaterally forum such as
[NATO.]{.underline} [We have already witnessed the effect that the
development and use of AI is having on Allied security]{.underline}: the
recent revelations regarding the use of Facebook data by a consulting
firm for political leverage show that automated data processes providing
sophisticated profiling of individuals can be used to manipulate the
fundamental elements of democracy, with potentially profound effects on
security. [How Allies respond to this changed security environment, both
individually and collectively, raises a number of legal questions. The
cross-cutting nature and widespread impact of AI-enabled technologies
necessitates multinational discussion and debate]{.underline}. [For now,
these debates are fueled by the absence of State practice and
jurisprudence on the development and use of AI-enabled
technology]{.underline}. Multinational organizations such as [NATO can
provide a venue for discussion amongst Allies, providing a forum where
States can express their views on these issues from a military and
security perspective. As of yet, there are no NATO policies providing
guidance on how to address the development and use of AI
technology]{.underline}. Consequently, this paper provides some
preliminary observations from the personal perspective of NATO legal
advisers who have begun to encounter these issues rather than on the
basis of agreed NATO positions or doctrine. It is therefore intended to
introduce rather than resolve some of the key legal issues that are
likely to arise in future AI-related discussions within NATO. This paper
begins by presenting the concept of "legal interoperability," one of the
tools that legal advisers working in NATO seek to promote. It then
introduces some of the current legal issues and debates surrounding the
development and use of AI, including the difficulty in defining key
concepts arising out of the increased use of AI such as "autonomy," and
questions pertaining to accountability.3 Finally, this paper examines
how [further dialogue among Allies and with NATO partners can contribute
to the development of a reliable approach on accountability issues
related to AI-enabled technology]{.underline}. The paper argues [that
given the rapidly evolving technology and the asymmetric approach and
capabilities of nations on the matter, efforts within the Alliance
should focus on ensuring NATO's "legal preparedness" so that collective
action is not thwarted by legal hurdles and mismatched legal
approaches.]{.underline}

## 

## Case Extensions

### Inherency -- No Human Control

#### The US military does not require human control for AI, despite common misperceptions

**Freedberg, 2019 -- deputy editor for Breaking Defense** \[Sydney J
"The frontline of a new age in defense Artificial Intelligence"
https://cdn2.hubspot.net/hubfs/2097098/MCM120_BreakingDefense_AI_ebookR1%20(1).pdf
Acc 5/25/22 TA\]

The April 2018 Army released its ATLAS artificial intelligence targeting
program solicitation, inspiring runaway headlines about "AI-powered
killing machines." Why did this happen? The answer lies in a strange mix
of misperceptions and some very real loopholes in the Pentagon's policy
on lethal AI. ["The US Defense Department policy on autonomy in weapons
doesn't say that the DoD has to keep the human in the loop," Army Ranger
turned technologist Paul Scharre said. "It doesn't say that. That's a
common misconception."]{.underline} Buzzwords & Firestorms ATLAS came to
public attention in about the worst way possible: an unheralded
announcement on a federal contracting website (fbo.gov) on February 19th
2019, an indigestible bolus of buzzwords that meant one thing to
insiders but something very different to everyone else --- not just the
general public but even civilian experts in AI. The name itself is
ominous: ATLAS stands for Advanced Targeting and Lethality Automated
System. The wording on the website made it worse, soliciting white
papers on "autonomous target acquisition technology, that will be
integrated with fire control technology, aimed at providing ground
combat vehicles with the capability to acquire, identify, and engage
targets at least 3X faster than the current manual process." "The LA in
ATLAS stands for Lethality Automated," pointed out an appalled Stuart
Russell, an AI scientist at Berkeley who's campaigned for a global ban
on lethal autonomous weapons. ["'Acquire, identify, and engage targets'
is essentially the UN definition of lethal autonomy." But it's not the
military definition, which is where the problem starts. The military has
long applied the loaded word "lethality" to anything that could make
weapons more effective, not just the weapons themselves]{.underline}.
Adding new infrared targeting sensors to tanks, for example, is
officially a "lethality" upgrade. Networking Navy ships so they can
share targeting data is called "distributed lethality." Then came
Defense Secretary Jim Mattis, a retired Marine Corps four- star who
liked the word "lethal" so much that underlings plastered it on
everything they were trying to sell him on, from high-tech weapons to
new training techniques. What about "engagement"? In plain English, a
"military engagement" means people are trying to kill each other
(lethally)." But in the military, "engagement" can mean anything from
"destroy" to "consider" to "talk to." A Key Leader Engagement (KLE) in
Iraq meant soldiers talking with a tribal elder, sheikh, or other
influential person over tea. [So in military language]{.underline} ---
at once abstrusely technical and sloppy --- [an artificial intelligence
can increase "lethality" and "engage" a potential target by helping a
human soldier spot it and aim at it, without the AI having any control
over the trigger.]{.underline}

#### DOD policies do not require human control for AI due to unclear definitions

**Freedberg, 2019 -- deputy editor for Breaking Defense** \[Sydney J
"The frontline of a new age in defense Artificial Intelligence"
https://cdn2.hubspot.net/hubfs/2097098/MCM120_BreakingDefense_AI_ebookR1%20(1).pdf
Acc 5/25/22 TA\]

• [DoD 3000.09, Section 4.c(2), covers "human- supervised autonomous
weapons systems"]{.underline} --- since a human overseer can turn it off
at any time , like Aegis-- and specifically limits them to defensive
purposes, explicitly banning the "selecting of humans as targets." •
Section 4.c(3) allows computer-controlled non- lethal systems, such as
radar jammers. (Automated cybersecurity software is permitted
elsewhere). • Section 4.c(1) allows the use of lethal force by "semi-
autonomous weapons systems" (emphasis added), which aren't fully
computer-controlled. But even those must "not autonomously select and
engage individual targets or specific target groups that have not been
previously selected by an authorized human operator." Such strictly
regulated systems are a far cry from the Terminator, or even Stuart
Russell's more realistic nightmare scenario of swarming mini-drones. But
[while Section 4.c is the heart of the Pentagon policy on autonomous
weapons, it's immediately followed by a loophole:]{.underline} • Section
4.d states that "Autonomous or semi- autonomous weapon systems intended
to be used in a manner that falls outside the policies in subparagraphs
4.c.(1) through 4.c.(3) must be approved" before development can
proceed. Who approves? Two deputy secretaries of defense (policy and
technology) and the Chairman of the Joint Chiefs. Getting three such
high-level officials to sign on is a daunting challenge for any
bureaucrat, but it's hardly impossible. • Even after the three officials
approve an exception, [the system must follow a long list of safety and
testing guidelines and ensure]{.underline} "commanders and operators
\[can\] exercise [appropriate levels of human judgment]{.underline} in
the use of force." [But "appropriate" is left undefined.]{.underline}
What's more, [if all three officials agree, they can ask the Deputy
Secretary of Defense to waive all of those restrictions]{.underline},
"with the exception of the requirement for a legal review, in cases of
urgent military operational need" --- again, left undefined. [Nowhere in
this document, incidentally, will you find the comforting but imprecise
phrase "human in the loop]{.underline}." In fact, when I used it in a
query to the Pentagon, I got gentle [chiding from DoD spokesperson
Elissa Smith: "The Directive does not use the phrase 'human in the
loop,' so we recommend not indicating that DoD has established
requirements using that term."]{.underline}

#### UN principles did not encourage Human Control.

**Bolton, 2021 - professor of political science at Pace University**
\[Matthew with Matilda Byrne, Ryan Gariepy, Emilia Javorsky, Volker
Lehmann, and Laura Nolan, January "Addressing The Threat Of Autonomous
Weapons Maintaining Meaningful Human Control"
http://library.fes.de/pdf-files/iez/17215.pdf Acc 5/27/22 TA\]

MEANINGFUL POSITIVE OBLIGATIONS Many States have coalesced around the
concept of meaningful human control as the basis for new international
law on LAWS. However, [meaningful human control is not explicitly
referenced in the 11 Guiding Principles. States opted for less clear
terminology like »human responsibility« and »human-machine interaction«
in paragraphs b) and c). There is concern that such language, without
elaboration and clarification, would add little meaningful constraint on
State behavior.]{.underline} To be meaningful, certain operational and
technical requirements for human control need to be met. It must be
active and involved (so-called »in-the-loop«) not passive (»on-theloop «
or »out-of-the-loop«). Human control needs to be maintained from the
activation of a weapon until an attack is completed, aborted or
terminated. This requires real-time human supervision at the level of
the attack, including full knowledge of the weapon system's actions and
a reliable communications link between the weapon and its operator. It
also requires allowing sufficient time in decision-making for a human
operator to make meaningful decisions about targeting and a capacity to
intervene and deactivate the system. A positive obligation to maintain a
ratio of human operators/weapon systems greater than or equal to 1:1 may
help in this regard.

#### The US minimizes regulations on military AI

**Lawrence and Cordey, 2020 -- researchers for The Cyber Project at the
Belfer Center for Science and International Affairs** \[Christie and
Sean, August, The Cyber Project Paper "The Case for Increased
Transatlantic Cooperation on Artificial Intelligence Edited by Lauren
Zabierek and Julia Voo
https://www.belfercenter.org/sites/default/files/2020-08/TransatlanticAI.pdf
Acc. 4/21/22 TA\]

AI Regulation and Principles [Although the US has made significant
progress in detailing its research priorities and developing interagency
groups responsible for coordinating research efforts, the US federal
government still lacks a mature, overarching regulatory framework or
governance structure to guide AI innovation and deployment. The US
government's approach, spearheaded by the Trump administration, has been
to provide the minimal level of regulation necessary to appease
industry's desire for guidance while avoiding burdensome
regulatio]{.underline}n that the Administration and industry fears would
limit innovation. In line with the American AI Initiative, the Office of
Management and Budget (OMB) published its January 2020 draft memo with
guidance intended to aid Federal agencies as they develop regulatory and
non-regulatory oversight approaches to non-government applications of
"weak"140 AI.141 [The memo highlighted the need "to avoid regulatory and
non-regulatory actions that needlessly hamper AI innovation]{.underline}
and growth."142 It detailed ten principles to guide oversight (see Table
2 below). The memo urges departments and agencies to consider
non-regulatory approaches to address risks posed by AI applications,
including providing exemptions from regulations, pursuing safe harbor
pilot programs, increasing public access to government data, and
developing voluntary consensus standards. Furthermore, Agencies should
engage in international dialogues to promote consistent regulation while
protecting American AI interests and democratic values.144 Despite this
progress[, it is not clear how this policy will be executed as none of
the guidance has become law]{.underline}.145

### Inherency - Cooperation

#### NATO cooperation on AI is fragmented and underfunded now.

**Lawrence and Cordey, 2020 -- researchers for The Cyber Project at the
Belfer Center for Science and International Affairs** \[Christie and
Sean, August, The Cyber Project Paper "The Case for Increased
Transatlantic Cooperation on Artificial Intelligence Edited by Lauren
Zabierek and Julia Voo
https://www.belfercenter.org/sites/default/files/2020-08/TransatlanticAI.pdf
Acc. 4/21/22 TA\]

[Transatlantic collaboration for AI-related research is taking place at
varying levels although these projects are relatively ad hoc and
materialize within existing scientific and technological research
agreements and roadmaps]{.underline}. For instance, the current Roadmap
for US-EU Science & Technology prioritizes four areas for transatlantic
cooperation, most of which leverage AI (e.g., health, transportation,
bioeconomy, marine and arctic research) or promote institutions that do
(e.g., European Organization for Nuclear Research or CERN).35, 36 These
collaborative links are supported and promoted through a variety of
arrangements and initiatives, such as BILAT 4.0, EURAXES37 or the
European Network of Research and Innovation Centers and Hubs (ENRICH).
In general, and despite challenges to systematically integrating US
entities into European research programs, the US remains the leading
non-EU ("third country") participant in Horizon 2020,38 with over 60
participations and 1,200 partnerships.39 [US funding]{.underline}
contributions to Horizon 2020 [and participation in AI-related projects,
however, is meager]{.underline} than its broader research involvement in
Horizon 2020. For instance, US collaborative links with Horizon 2020
projects can only be found in 2% of AI-related projects, 12% of deep
learning projects, and 4% of machine learning-related projects.40
[Accordingly, there is still plenty of room for
improvement.41]{.underline}

#### US NATO AI coordination is insufficient now -- adversarial AI proves we are vulnerable

**Lawrence and Cordey, 2020 -- researchers for The Cyber Project at the
Belfer Center for Science and International Affairs** \[Christie and
Sean, August, The Cyber Project Paper "The Case for Increased
Transatlantic Cooperation on Artificial Intelligence Edited by Lauren
Zabierek and Julia Voo
https://www.belfercenter.org/sites/default/files/2020-08/TransatlanticAI.pdf
Acc. 4/21/22 TA\]

After being told in the wake of 9/11 that European and NATO allies
pledged to fight Al Qaida alongside American troops, the then United
States National Security Advisor Condoleezza Rice said "it was good to
have friends in the world at a time like this."1 Nicholas Burns, the
then US Ambassador to NATO, has since reflected on the importance of the
transatlantic alliance[. Losing the relationship with NATO]{.underline}
and members of the European Union, he believes, [would lead the US to
"lose our strongest anchor in a dangerous and complex
world]{.underline}."2 The world has changed a lot since September 2001,
however these relationships are no less important. Global terrorism is
still a threat, but the rise of China and technological advancements
have converged to create both new opportunities and new challenges.
[Artificial intelligence (AI) promises to help]{.underline} the world
find a vaccine for Covid-19, add up to \$15.7 trillion to the global
economy, and [improve militaries]{.underline}' ability to detect,
defend, and deter against cyberattacks.3 [However, AI technologies could
also provide adversaries and authoritarian governments with tools to
increase censorship, automate disinformation. and engage in constant
cyber or kinetic conflict]{.underline}.4 Despite all of these changes,
the importance of a strong relationship between the United States and
the European Union has been a constant. The transatlantic disagreements
that have characterized the past few years---and have hampered a united
front on emerging technologies like 5G and AI5---are not the first time
US-EU relations have suffered, but they should not further divide allies
that share common values.6 [Deepened US-EU cooperation across the entire
AI ecosystem7 is necessary to advance a more secure, safe, and
prosperous world, but to do this the current level of AI-related
coordination and partnership needs to be increased]{.underline}. This
report's purpose is twofold: first, to inform policymakers and
researchers about the current state of transatlantic AI efforts; and
second, to recommend specific areas where transatlantic AI collaboration
should be strengthened. Based on a comprehensive study of over 260
documents and reports covering the period from December 1997 to June
2020, we proposes more than 16 recommendations to increase US-EU AI
collaboration across the entire AI ecosystem, as well as 9
recommendations for AI cooperation in the healthcare, environmental
sciences, and defense sectors. [Greater transatlantic efforts are needed
to prevent the advancement of an AI vision that is adversarial and
harmful to the wellbeing of the United States,]{.underline} the European
Union, [and allies.]{.underline}

### 

### Harms -- Cohesion 

#### Different ethical perspectives can undermine political cohesion -- different ethical perspectives can increase burden sharing tensions

**Stanley-Lockman, 2021 - Center for Security and Emerging Technology**
\[Zoe CSET Issue Brief August "Responsible and Ethical Military AI
Allies and Allied Perspectives"
https://cset.georgetown.edu/publication/responsible-and-ethical-military-ai/
Acc. 4/21/22 TA\]

This philosophical debate is implicit in the Defence Ethics Committee's
opinions, and is relevant to adjacent questions about AI for two
practical reasons. First is that French military officers see the need
for guiding principles for technology that increases operational
distance or can be applied to grey-zone activity. They note that [ethics
and law are well structured in rules of engagement]{.underline},
commander's intent, and compliance with legal frameworks for conflict.
[But because these often only apply above a certain threshold of
hostility, guidance that allows the armed forces to maintain "ethics and
moral strength" for other types of military activity is
lacking]{.underline}.69 [This includes governance for technology that
increases the distance between operators and operations, including in
the information domain]{.underline}. As technology broadens and
accelerates changes to the operating environment, taking these ethical
and moral considerations into account could necessitate new
"deontological principles" on duty and obligation for the French Armed
Forces.70 The concept of distance from operations enters into advisory
opinions, both in relation to automation bias and psychological effects.
But even with the advisory opinions of the ministerial committee, such a
framework is still missing.71 Second is that [different allied
perspectives on distance from operations could affect coalition
operations if political differences widen. If guiding principles on
distance from operations enter into rules of engagement]{.underline} or
prompt questions about commanders' intent and responsibility, [then
different ethical bases for the use of new technologies in warfare could
create political tensions. More specifically, if the more
technologically advanced allies send more AI-enabled support and fewer
troops as their contributions to coalition operations, some allies may
perceive that others are not willing to equally share the burden of
risks to life.]{.underline}72 [If not managed]{.underline}, [sensitive
issues that stem from different ethical risk calculations could decrease
political cohesion.]{.underline}

#### AI challenges alliance cohesion because uncoordinated capabilities destroy interoperability.

**Lin-Greenberg, 2020 - member of the MIT Security Studies Program**
\[Erik Vol 3, Iss 2 Spring, Texas National Security Review "Allies and
Artificial Intelligence: Obstacles to Operations and Decision-Making"
http://dx.doi.org/10.26153/tsw/8866 Acc 4/22/22 TA\]

[Artificial intelligence]{.underline} (AI) promises to increase military
efficiency, but also [poses unique challenges to multinational military
operations]{.underline} and decision-making that scholars and
policymakers have yet to explore. [The data- and resource-intensive
nature of AI development creates barriers to]{.underline} burden-sharing
and [interoperability that can hamper multinational operations. By
accelerating the speed of combat and providing adversaries with a tool
to heighten mistrust between allies, AI can also strain the complex
processes that allies and security partners use to make decisions. To
overcome these challenges and prepare for AI-enabled warfare,
policymakers need to]{.underline} develop institutional, procedural, and
technical solutions that streamline decision-making and [enhance
interoperability]{.underline}. In June 2019, the United States announced
a new artificial intelligence (AI) partnership with Singapore that calls
for collaboration on the development and use of AI technologies in the
national security domain.1 Is this type of cooperation a harbinger of
things to come? The burgeoning military use of AI --- technology that
carries out tasks that normally require human intelligence --- has the
potential to alter how states carry out military operations. [AI-enabled
technologies --- like autonomous drone swarms]{.underline} and
algorithms that quickly sift through massive amounts of information ---
[can increase the speed and efficiency of warfare, but they may also
exacerbate the coordination and decision-making challenges frequently
associated with multinational military operations carried out by
allies]{.underline} and security partners. Policymakers and experts in
the United States and other countries have urged international
cooperation on the development and use of AI, but this guidance
overlooks important questions about the challenges of AI collaboration
in the security domain. President Donald Trump's executive order on AI
directs "enhance\[ed\] international and industry collaboration with
foreign partners and allies" to maintain "American leadership in AI."2
Similarly, the congressionally chartered [National Security Commission
on Artificial Intelligence warns, "If the United States and its allies
do not coordinate early and often on AI-enabled capabilities, the
effectiveness of our military coalitions will suffer."3 Several of
Washington's allies have echoed these calls for
collaboration]{.underline}. Germany's 2019 National AI Strategy
advocates for "work\[ing\] with the nations leading in this field ... to
conduct joint bilateral and/or multilateral R&D activities on the
development and use of AI."4 While cooperation is important, what
challenges might allies and partners encounter as they work together to
develop and deploy AI in the military domain? And what steps might
states take to overcome these obstacles?

#### Interoperability is key to cohesion -- it enables communication, intelligence and planning.

**Dufour 2018 - Colonel in the Canadian Army, currently working with
NATO** \[Martin, NDC Policy Brief No. 6 December "Will artificial
intelligence challenge NATO interoperability?"
https://www.ndc.nato.int/news/news.php?icode=1239 Acc. 4/21/22 TA\]

What interoperability means [Cohesion,]{.underline} often mentioned as
the Alliance's center of gravity, [lies at the heart of NATO's
success]{.underline}. [Underwriting this cohesion is the ability of
member states to]{.underline} share the burden of producing military
capabilities to service the whole, and the burden of
[conduct]{.underline}ing [operational missions]{.underline}. [To do this
successfully, members have to be able to undertake military actions in
concert with each other in a fully coordinated, and as much as possible
integrated manner]{.underline}. This is referred to as interoperability,
defined as "the ability of systems, units or forces to provide services
to, and accept services from other systems, units or forces and to the
use the services so exchanged to enable them to operate effectively
together".1 This definition implies that there are several layers to
interoperability which need to be addressed to ensure forces are able to
operate together effectively in a military context. These include
technical features permitting systems to physically connect to one
another and exchange information; and the alignment of procedures and
processes to allow military personnel to function within the same space
and achieve common goals without fratricide. It also implies that this
takes place at various levels of operation, from tactical to strategic.
[A high level of interoperability allows allies to effectively exchange
intelligence and information, cooperatively plan complex military
operations, and conduct integrated missions with fully exchangeable
force packages.]{.underline}

#### AI swarms will undermine human control, which destroys allied support for military operations.

**Valášek, 2017 - director of Carnegie Europe** \[Tomáš August 31, "How
Artificial Intelligence Could Disrupt Alliances"
https://carnegieeurope.eu/strategiceurope/72966 Acc 4/22/22 TA\]

But [the deployment of AI by allies carries its own political
risk.]{.underline} While its use to defend networks or information
integrity is widely accepted, [AI's military applications will introduce
new tensions to alliances. The sense of equality and codecision among
members could be at risk because of worries about accountability. When
countries fight as a group they want to have a say in how that alliance
prosecutes the war. But that becomes impossible if the fighting in the
future is done by machines. While for now the United States has a policy
of keeping a human "in the loop" on decisions to use lethal force,
military tactics and technology keep evolving]{.underline}. As
artificial intelligence becomes capable of tackling more complex tasks,
the [killing in the future may]{.underline} [not be done by a
single]{.underline} missile-armed unmanned aerial vehicle ([UAV)
but]{.underline}, in the medium term, [by swarms of UAVs that use
AI]{.underline} to constantly adopt tactics and targeting in real time,
[leaving little time and space for human interference. This development
might make allies more reluctant to join the fight in the first place.
They would worry that if AI-directed weapons kill innocent
civilians]{.underline} by mistake or inflict disproportional carnage,
governments will be blamed despite having no control over the action
itself.

### Harms - Interoperability

#### Uncoordinated adoption of AI systems undermines NATO interoperability, making miscalculation and accidents more likely.

**Trabucco and Stanley-Lockman, 2022 -- prof of Political Science,
University of Copenhagen and prof of Defense and Strategic Studies,
Nanyang Technological University** \[Lena and Zoe, The Oxford Handbook
of AI Governance, March, "NATO's Role in Responsible AI Governance in
Military Affairs"
https://www.oxfordhandbooks.com/view/10.1093/oxfordhb/9780197579329.001.0001/oxfordhb-9780197579329-e-69
Acc 4/16/22 TA\]

[Even without being attacked, governability of AI in a NATO context also
means understanding how AI-enabled and autonomous systems developed by
the 30 Allies]{.underline}---and other partners---[will interact with
one another]{.underline}. NATO has expressed interest in governability
as a principle of AI "to disengage or deactivate in case of unintended
behavior,"85 which echoes the U.S. Department of Defense definition of
governable AI.86 Disengaging adversaries is important to maintain
de-escalation measures in conflict. [For NATO, interoperability between
systems also relates to governable AI because allies must also consider
how the interactions between the 30 Allies' own AI-enabled and
autonomous systems may result in unintended or emergent
behavior.]{.underline}87 This means that [NATO has a responsibility to
coordinate activities]{.underline}---be they technical exchanges,
standardization efforts, or training and exercises---[to build
confidence that the systems perform as humans intend.88 Without this
coordination, the lack of interoperability of allied systems could lead
to accidents, and separately, the potential loss of operational
effectiveness also presents vulnerabilities for adversaries to
exploit.]{.underline} In addition to governability, [NATO and its Allies
are assessing the risks that bias, attacks, and lack of interpretability
can introduce in relation to the anticipated uses of a given AI
system]{.underline}.89 In security and defense, new and heightened risks
include poisoning of the information environment, deception systems and
techniques, uncertainty about the performance of systems in new and
unknown environments, and the possibility that tensions or accidents
escalate at a faster tempo than humans and institutions can process,
among others. [These risks can manifest either in motivated attacks or
unintentional failure modes.]{.underline}90 In both cases, [assuring and
certifying that military assets are safe and secure is important given
the inherently high risk in operational environments]{.underline}. These
operational environments include the presumption that an adversary is
disrupting one's own systems, be it by directly attacking the AI systems
themselves, or disrupting the broader command, control, and
communications systems under which the AI systems are operating.91
Mitigating these types of risks is typically done in testing,
evaluation, validation and verification (TEVV) and in experimentation
activities.92 Yet AI cannot be validated and verified the way
traditional software systems are because there is no guarantee that an
AI system will perform in the real world as it does in a testing
environment, and because lifelong-learning systems will perform
differently over their lifecycle. Having robust assurance and TEVV
processes in place are also important for operators to build trust in
the systems they are meant to use, as well as for citizenries and
coalition partners at large to see that accountability procedures still
apply. As such, [building institutional procedures to govern AI safety
and security is necessary to build trust in the use of the
technology---as well as to develop countermeasures and defensive systems
that protect against adversarial threats]{.underline}. N[ATO thus has an
institutional responsibility to prevent and mitigate these intentional
and unintentional failures if using AI]{.underline} in operations and
mission support.93 As Table 69.1 shows, [the Alliance also has a range
of relevant entities to coordinate national approaches to AI safety and
security]{.underline}, as well as facilitate safety measures as part of
responsible use in the Alliance-wide ecosystem.

#### Without coordination, adoption of AI by NATO will undermine interoperability as it becomes a zero sum competition between allies

**Gilli, 2020 - Senior Researcher at the NATO Defense College**
\[Andrea, NDC Research Paper No.15 -- December ""NATO-Mation":
Strategies for Leading in the Age of Artificial Intelligence"
https://www.ndc.nato.int/news/news.php?icode=1514 Acc 4/21/22 TA\]

What types of problem could emerge? [Technology-generated efficiency
gains in production lead to lower prices. However, lower prices lead to
increases in demand]{.underline} -- because the relative price of
substitute goods (rivals) increases.76 Over the past decade, AI -- and,
in particular, ML -- has made a particular activity, prediction,
cheaper: it is reasonable to forecast that this trend will continue in
the future.77 [As AI becomes cheaper, however, the demand for AI-related
services will increase, thus leading to more demand for]{.underline}
related necessities like [AI specialists]{.underline}, AI infrastructure
(and 5G networks) and AI components (processors). This in turn might
well lead to scarcity and higher prices, thus pitching actors against
one another -- not unlike the early stages of the COVID-19 crisis, when
individual self-interested actions led to collectively bad outcomes.78
[In the context of a military alliance]{.underline}, the problem goes
much deeper, as [it can generate a beggar-thy-neighbour effect with
allies competing for the same scarce resources]{.underline}.79
[Moreover, without consultation and cooperation, Allies could end up
developing different technological solutions, with the risk of
undermining compatibility and interoperability.]{.underline} Similarly,
they could end up prioritizing some problems over others, with the risk
of developing multiple, different and redundant solutions while
neglecting other points in need of attention.[80 However, through
intra-alliance coordination and cooperation, as well as dialogue and
consultation, secondary market mechanisms and other approaches, NATO
could provide an important contribution to identify and address this
type of problems.81]{.underline}

#### Asymmetric AI development undermines NATO interoperability

**Dufour 2018 - Colonel in the Canadian Army, currently working with
NATO** \[Martin, NDC Policy Brief No. 6 December "Will artificial
intelligence challenge NATO interoperability?"
https://www.ndc.nato.int/news/news.php?icode=1239 Acc. 4/21/22 TA\]

Overcoming the interoperability gap It is evident that [of the emerging
disruptive technologies, none is likely to have as significant an impact
on warfare as that of artificial intelligence]{.underline}. The effect
of artificial intelligence is already being felt in numerous fields, and
its further development and combination with other technologies will
compound this, allowing the development of advanced autonomous systems.
[While the latter holds the promise of creating new classes of weapons
with great military potential, disproportional growth among the various
NATO allies could lead to complex interoperability problems, further
widening the existing interoperability gap]{.underline} between member
nations. [Over the next two decades Alliance members will be
increasingly challenged by the rapid evolution of artificial
intelligence and risk becoming unable to operate together should the
asymmetric adoption of emerging technologies not be carefully managed.
It is therefore paramount that countries begin preparing for the future
impact of artificial intelligence in the military realm,]{.underline}
and start adopting the technology in niche areas to ensure they do not
fall too far behind.

#### Rapid US adoption of AI widens the NATO interoperability gap

**Dufour 2018 - Colonel in the Canadian Army, currently working with
NATO** \[Martin, NDC Policy Brief No. 6 December "Will artificial
intelligence challenge NATO interoperability?"
https://www.ndc.nato.int/news/news.php?icode=1239 Acc. 4/21/22 TA\]

The true power of artificial intelligence however emerges when it is
combined with other technologies to enable fully autonomous operations.
Definitions of autonomous systems are varied, but all revolve around the
concept of systems able to make autonomous decisions, independent of
human operators, based on a self-constructed understanding of a
situation, and with the ability to adapt to the changing environment at
a speed unmatched by humans. This ability to learn and adapt without
human input is the key distinction between today's task-specific
automated systems and fully autonomous systems. Once coupled to weapons
systems such as uninhabited vehicles, or command and control apparatus,
such [autonomous systems could then herald a new era of
warfare]{.underline} where the time required to collect and analyze
large amounts of data, and make quick decisions in the face of rapidly
evolving situations is greatly reduced, thereby providing a significant
competitive advantage. This new era was called "hyper war" or "war at
the speed of light" in a 2017 report on NATO Adaptation. 6 This report
reveals that [the United States]{.underline}, witnessing its
technological advantages rapidly eroding, [has embarked on a Third
Offset Strategy, which heavily relies on the adoption of emerging
disruptive technologies, and is already well on its way to adopting a
range of capabilities including "robotics, system autonomy,
miniaturization, scaling big data, artificial intelligence and deep
learning".7 The rapid development and adoption of such technologies has
the potential to further widen the interoperability gap between NATO
members.]{.underline}

### Harms -- Crisis Instability

**AI weapons systems will increase the risk of accidents and
miscalculation during a crisis because autonomous machine speed is too
fast for humans to control**

**Scharre, 2021 - Director of Studies at Center for New American
Security** \[Paul, Texas National Security Review Vol 4, Iss 3 Summer
"Debunking the AI Arms Race Theory"
https://tnsr.org/2021/06/debunking-the-ai-arms-race-theory/Artificial
Intelligence Acc 5/27/22 TA\]

An Accelerating Tempo of Warfare One possibility for how [AI could alter
warfare in a manner that would leave all states worse off would be if it
accelerated the tempo of war past the point of human control, making
warfare faster, more violent, and less controllable.]{.underline} There
are advantages to adding intelligence into machines, but given the
limitations of AI systems today, the optimal model for achieving the
highest quality decision-making would be a joint human-machine
architecture that combines human and machine decision-making. [One way
in which machines outperform humans]{.underline}, however, [is in
speed]{.underline}. It is possible to envision a competitive dynamic in
which countries feel compelled to automate increasing amounts of their
military operations in order to keep pace with adversaries. Then-Deputy
Secretary of Defense Robert O. Work summed up the dilemma when he asked,
"If our competitors go to Terminators and we are still operating where
the machines are helping the humans and it turns out the Terminators are
able to make decisions faster, even if they're bad, how would we
respond?"22 [This is a classic security dilemma. One state's pursuit
of]{.underline} greater automation [and faster reaction times undermines
other states' security and leads them to similarly pursue more
automation just to keep up]{.underline}. If states fall victim to this
trap, [it could lead to all states being less secure, since the pursuit
of greater automation would not merely be an evolution in weapons and
countermeasures that simply leads to the creation of new weapons in the
future.]{.underline} At some point, [warfare could shift to a
qualitatively different regime in which humans have less control over
lethal force as decisions become more automated and the accelerating
tempo of operations pushes humans "out of the loop" of
decision-making.]{.underline} Some Chinese scholars have hypothesized
about a battlefield "singularity," in which the pace of combat eclipses
human decision-making.23 U.S. scholars have used the term "hyperwar" to
refer to a similar scenario.24 While the speed of engagement
necessitates automation in some limited areas today, such as immediate
localized defense of ships, bases, and vehicles from rocket and missile
attack, expanding this zone of machine control into broader areas of war
would be a significant development. [Less human control over warfare
could lead to wars that are less controllable and that escalate more
quickly or more widely than humans intend.]{.underline} Similarly,
[limiting escalation or terminating conflicts could be more challenging
if the pace of operations on the battlefield exceeds human
decision-making. Political leaders would have a command-and-control
problem in which their military forces are operating]{.underline}
"inside" (i.e., [faster than) their own decision cycle]{.underline}. The
net effect of the quite rational desire for nations to gain an edge in
speed could lead to an outcome that is worse for all. Yet, competitive
dynamics could nevertheless drive such a result.

**Autonomous AI decision making creates a "Black Box" -- military
strategies become Unexplainable, which makes them impossible to
understand or study for humans and prevents us from repeating
mistakes.**

**Lin-Greenberg, 2020 - member of the MIT Security Studies Program**
\[Erik Vol 3, Iss 2 Spring, Texas National Security Review "Allies and
Artificial Intelligence: Obstacles to Operations and Decision-Making"
http://dx.doi.org/10.26153/tsw/8866 Acc 4/22/22 TA\]

Uncertainty Surrounding AI Technology [AI can also strain alliance
decision-making by fueling uncertainty about information and military
actions]{.underline}. [Unlike human analysts]{.underline} [or military
personnel]{.underline} [who can be asked to explain and justify
their]{.underline} findings or [decisions]{.underline}, [AI generally
operates in a "black box."]{.underline} 97 [The neural networks that
underpin many cutting-edge AI systems are opaque and offer little
insight into how they arrive at their conclusion]{.underline}s.98 These
networks rely on deep learning, a process that passes information from
large data sets through a hierarchy of digital nodes that analyze data
inputs and make predictions using mathematical rules. As data flows
through the neural network, the net makes internal adjustments to refine
the quality of outputs. Researchers are often unable to explain how
neural nets make these internal adjustments. [Because of this lack of
"explainability," users of AI systems may have difficulty understanding
failures and correcting errors.]{.underline}99 Policymakers have called
for the development of more transparent AI systems, and researchers are
working to develop explainable AI tools that peer inside the AI black
box.100 Yet, [many decision-makers remain uncomfortable with the
uncertainty surrounding AI-enabled systems]{.underline}. The commander
of the U.S. Air Force's Air Combat Command, for instance, publicly
explained that he was not yet willing to rely on AI programs to analyze
the full-motion video collected by reconnaissance drones. He argued that
although systems are improving, they are still unable to consistently
provide accurate analysis.101 So long as the decisions and analysis of
AI systems remain opaque, military commanders may be reluctant to trust
AI-enabled systems. And if used, AI may contribute to the fog of war,
rather than reduce it, making it difficult to make decisions using
information delivered by AI technologies. [The operational implications
associated with uncertainty and lack of trust in AI would likely be
exacerbated in multinational alliance contexts. There is significant
cross-national variation in trust in AI technologies, even among close
allies]{.underline}. One 2018 survey, for instance, found that just 13
percent of respondents in Japan and 17 percent of respondents in South
Korea trust artificial intelligence, compared to 25 percent of
respondents in the United States. Similar [disparities exist between the
United States and many of its NATO allies]{.underline}. In Spain, 34
percent of respondents trust artificial intelligence, compared to 21
percent in Canada, 40 percent in Poland, and 43 percent in Turkey.102
Given this variation, policymakers and commanders from some states may
be more reluctant to use AI-enabled systems or trust the information
they deliver than leaders from other states during multinational
operations.

**Autonomous AI undermines nuclear security during a crisis -- it makes
systems vulnerable to hacking and disruption.**

**Shah, 2019 - Research Assistant at the Center for International
Strategic Studies** \[Syed Sadam CISS Insight Vol.VII, No.2 "The Perils
of AI for Nuclear Deterrence"
https://journal.ciss.org.pk/index.php/ciss-insight/article/download/10/9
Acc 5/25/22 TA\]

AI cyber threats and nuclear deterrence [Spoofing is useful to fool a
target by pretending to be the original source. In nuclear matters, it
can be used]{.underline} to access top-secret information or [for
ordering a false nuclear launch]{.underline} or to expose the system's
vulnerabilities. 26 However, [with the inception of AI and the use of
speech synthesis, and faking voice commands, spoofing has been
revolutionized in recent years. An AI hacker can lure in more targets
quantitatively, and qualitatively than humans]{.underline}. Zero Fox, an
IT security company conducted research to compare the efficiency of
artificial and natural intelligence by sending different users a hacking
malicious link. Artificial hacker (Ah) was taught to design and
implement its own phishing bait, unlike Mr. Thomas Fox-Brewster who
participated in the experiment. Ah succeeded in luring 275 victims at
the rate of 6.75 tweets per minute, and Thomas could only target 49
users and pump out 1.075 tweets per minute.27 28 [Malicious use of AI in
spoofing makes nuclear decision-making and communication systems quite
vulnerable. If several fake early- warnings appear to be real, will the
commander-in-charge of the nuclear weapons order the
launch?]{.underline} Similarly, individuals working on sensitive
information may mistakenly provide secret information to an AI hacker.
[The threats of cyber-spoofing are not recent. Emanating from threats,
the scale and scope of attacks and methods are not only diverse but also
real.]{.underline} It happened [in 1983]{.underline} when [the algorithm
of Early Warning System]{.underline} (EWs) [wrongly sensed incoming
missiles]{.underline}, a warning sounded multiple times; [however, the
officer-in-charge Stanislav Petrov prevented the imminent crisis by
trusting his instincts]{.underline} and not the alarm bell. 29 [The rise
in the use of autonomous weapons and systems will also drive inferior AI
adversaries' interest in using cyber countermeasures.]{.underline} 30
Every software run machine is vulnerable to attack, and particularly
zero-day attacks come out even with all the defensive measures.

#### A lack of human control undermines accountability, which entangles NATO in conflicts initiated by AI. 

**Hill and Marsan, 2018 - Director and Senior Assistant, NATO Office of
Legal Affairs** \[Steven and Nadia, 7-18-18 "Artificial Intelligence and
Accountability: A Multinational Legal Perspective"
https://www.sto.nato.int/publications/STO%20Meeting%20Proceedings/STO-MP-IST-160/MP-IST-160-PP-4.pdf
Acc 4/21/22 TA\]

4.0 ACCOUNTABILITY OF AI-ENABLED TECHNOLOGY [International and domestic
law offers some tools to cope with the effects of AI-enabled weapons
from a multinational collective defense perspective]{.underline}. New
technology does not necessarily require new laws and it is not the aim
of this paper to propose the creation of new legal frameworks. The
applicable legal framework will depend on the technology used and on the
actual and potential effects caused by that technology and can include
various national laws, human rights law, the law of state
responsibility, international humanitarian law and the law of armed
conflict. [With machines taking on the qualities of human intelligence,
including perception, cognition and action, the issue of accountability
for illegal acts performed by autonomous systems has dominated the
debate. The issue is]{.underline} not so much what legal framework
applies, [but who is legally responsible for the effects of AI-enabled
weapons.]{.underline} Accountability and Responsibility [AI-enabled
action that constitutes an "armed attack"]{.underline} within the sense
of Article 51 of the UN Charter, [could potentially set off the
invocation of Article 5 of the Washington Treaty]{.underline}. Setting
aside the issue of the threshold to be applied to determine whether an
AI-enabled attack would constitute an "armed attack", [the preliminary
question that would arise is that of attribution. After all, assigning
responsibility for an armed attack is a necessary precondition to the
use of self-defence measures. AI-enabled weapons challenge our
traditional notions of responsibility: who can be held accountable
for]{.underline} the effects of this [technology,]{.underline}
especially [when]{.underline} [fully autonomous]{.underline}? Can we
even talk of a "legal personality" of AI-enabled systems? In this area,
[States have been rather clear in expressing the need for responsibility
to be attributed to human beings]{.underline}.18 The international
debates and discussions on the legal approach to take with respect to
lethal autonomous weapons systems (LAWS) provides a useful reference on
the concept of "human control" where the substance of the debate has
been how to regulate human-machine interaction, often referred to as
"human-machine teaming". The human role in independent machine
decision-making can vary, as exemplified through the OODA loop of
decision-making (Observe, Orient, Decide and Act).19 An "in the loop"
system requires human intervention for its operation, an "on the loop"
system provides for human intervention if needed, and an "out of the
loop" system does not require human intervention at all, a prospect that
is criticized by observers who have supported the concept of "minimum
human control." Privileging human judgment and accountability above
mechanical efficiency, [the United States]{.underline}, for example,
[has taken a clear stance towards asserting that a human being should
always be kept in the decision-making loop for the use of
LAWS,]{.underline} as the ultimate decider on the use of lethal force in
the battlefield.

### Extend -- Harms - Human Dignity 

#### 1. [Inhumanity]{.underline} -- AI cannot respect human dignity because it cannot know the value of life because it is not alive.

**Docherty, 2014 - senior researcher in the Arms Division of Human
Rights Watch** \[Bonnie "Shaking the Foundations The Human Rights
Implications of Killer Robots" Human Rights Watch
http://www.hrw.org/sites/default/files/reports/ arms0514_ForUpload_0.pdf
Acc 12/27/20 TA\]

[Fully autonomous weapons would possess the power to kill people yet be
unable to respect their dignity. As inanimate machines, they could truly
comprehend neither the value of individual life nor the significance of
its loss. Allowing them to make determinations to take life away would
thus conflict with the principle of dignity.]{.underline}82 Critics of
fully autonomous weapons have expressed serious moral concerns related
to these shortcomings. In his 2013 report to the Human Rights Council,
Christof Heyns, the special rapporteur on extrajudicial killing, wrote,
\[A\] human being somewhere has to take the decision to initiate lethal
force and as a result internalize (or assume responsibility for) the
cost of each life lost in hostilities, as part of a deliberative process
of human interaction.... [Delegating this process dehumanizes armed
conflict even further and precludes a moment of deliberation in those
cases where it may be feasible. Machines lack morality and mortality,
and should as a result not have life and death powers over
humans.83]{.underline} Heyns described this issue as an "overriding
consideration" and declared that [if fully autonomous weapons are found
morally unacceptable, "no other consideration can justify the deployment
of \[fully autonomous weapons\], no matter the level of technical
competence at which they operate]{.underline}."84

#### 2. [Hope]{.underline} -- Autonomous AI weapons deny dignity because algorithmic decisions deny Hope, which is essential to dignity.

**Heyns, 2016 - Professor of Human Rights Law, University of Pretoria**
\[Christof, Human Rights Quarterly 38 (2016) 350--378 " Human Rights and
the use of Autonomous Weapons Systems (AWS) During Domestic Law
Enforcement"
<https://www.academia.edu/37475669/Human_Rights_and_the_use_of_Autonomous_Weapons_Systems_AWS_During_Domestic_Law_Enforcement>
Acc 12/27/20 TA\]

The interrelated considerations of time and hope play a role in this
context. [One of the problems presented by computer algorithms that
determine when AWS will be allowed to release force is that they do so
in advance, on the basis of hypotheticals]{.underline}, while there is
no true and pressing emergency rendering such a far-reaching decision
unavoidable. Even if it may be permissible in a real emergency to take
far-reaching measures, it does not follow that such decisions can be
taken in the abstract. Decisionmaking by politicians and scientists
about the life and limb of people based on theoretical possibilities
contemplated in the halls of the legislature or laboratories risks
trivializing the issues at stake. Hypotheticals such as the ticking-bomb
scenario present the same kind of problem. It makes crossing the
threshold of using force against another human being too easy. The
statute authorizing the shooting down of the plane, struck down in the
German Air Security case cited above, had a similar quality---it
authorized in advance, in the abstract, the ending of the lives of
imaginary passengers. This seems to fly in the face of the requirement
that the use of force against a human being should be a measure of last
resort.97 [Hope, often against the odds, is an important part of our
psychological makeup]{.underline} and dealing with the harshness of
reality.98 A sentence of life without parole, for example, like the
death penalty, can be seen as a violation of dignity because it means
"writing off" the person, not leaving open the possibility of hope. The
possibility of a deliberative process somewhere down the line, where a
change of mind and fate is possible, is ruled out in advance by the
introduction of AWS if human control is sacrificed in the process [A
world in which death comes with the certainty of science, with no
prospect]{.underline}, however farfetched, [of a last minute change of
plan, or the possibly of human compassion]{.underline}---what may be
called humanitarian override---[offers little room for hope. Hope in
this sense---and thus dignity---may be one of the casualties of
AWS.]{.underline}

#### 3. [Objectification]{.underline} -- Autonomous weapons systems violate human dignity because they reduce humans to objects -- to data points. 

**Sauer, 2021 - Senior Research Fellow at Bundeswehr University**
\[Frank serves on the International Panel on the Regulation of
Autonomous Weapons IRRC No. 913 March "Stepping back from the brink: Why
multilateral regulation of autonomy in weapons systems is difficult, yet
imperative and feasible"
https://international-review.icrc.org/articles/stepping-back-from-brink-regulation-of-autonomous-weapons-systems-913
Acc 4/5/22 TA\]

[The key ethical implication of weapon autonomy in a weapons system\'s
critical functions is thus that allowing algorithms to make kill
decisions violates human dignity because the victim is reduced to an
object, a mere data point fed to an automated, indifferent killing
machine]{.underline}. It is worth spelling out that [this objection is
valid even if civilians]{.underline} (or other non-combatants) [remain
unharmed]{.underline}. After all, narrowing the focus solely to the
possibility that LAWS might not be able to make proper -- or even better
-- distinctions between combatants and civilians, a cornerstone of the
legal case against LAWS discussed in the CCW, loses sight of the fact
that [combatants, too, are imbued with human dignity]{.underline}. In
other words, weapon autonomy raises a more fundamental concern than the
legal strand of the LAWS debate suggests, because "successfully
discerning combatants from noncombatants is far from the only issue".89
As a general rule, [the use of LAWS against humans can be deemed an
unacceptable infringement on human dignity because delegating the
decision to kill to an algorithm devalues human life.]{.underline}90
Exceptions from this rule would only be conceivable if they were
explicitly not made on the basis of weighing bare lives against each
other and then deliberately opting for algorithmic killing. An example
for such a boundary case could be a sailor\'s reliance on weapon
autonomy in a narrowly bound scenario of desperate self-defence. If the
aforementioned navy frigate91 were to be under a saturation attack by
anti-ship missiles and, potentially, also manned aircraft, then
inadvertently endangering human life by relying on autonomous defensive
fire for the survival of the ship and its crew could be considered
acceptable ex post. Generally speaking again, [being killed as the
result of algorithmic decision-making matters for the person dying
because a machine taking a human life has no conception of what its
action means: "In the absence of an intentional and meaningful decision
to use violence, the resulting deaths are meaningless and
arbitrary."]{.underline}92 In other words, [the least we can do when
killing another human being in war is to recognize this death of a
fellow member of our species and put the weight accompanying this
decision onto our conscience. The mindlessness of machines killing
humans based on software outputs strips the latter of their right to be
recognized as humans in the moment of death]{.underline}. This also
matters for society at large. Modern warfare, especially in democracies,
already decouples societies from warfighting in terms of political and
financial costs.93 [A society outsourcing moral costs by no longer even
concerning itself with the act of killing,]{.underline} with no
individual combatants' psyches burdened by the accompanying
responsibility, c[rosses a moral line. It risks losing touch
with]{.underline} fundamental humanitarian values such as [the right to
a dignified life and respect towards fellow human beings]{.underline}.94
To sum up this section, while the legal verdict on weapon autonomy
increasing IHL compliance is still out and will be for some time,
[a]{.underline} more [fundamental objection against LAWS based on
deontological limits is valid]{.underline} today.

#### 4. [Compassion]{.underline} -- AI systems cannot uphold dignity because they cannot feel compassion for others.

**Docherty, 2018 - senior researcher in the Arms Division of Human
Rights Watch** \[Bonnie August 21, "Heed the Call A Moral and Legal
Imperative to Ban Killer Robots"
[https://www.hrw.org/report/2018/08/21/heed-call/moral-and-legal-imperative-ban-killer-robots#](https://www.hrw.org/report/2018/08/21/heed-call/moral-and-legal-imperative-ban-killer-robots)
Acc 12/27/20 TA\]

[In order to treat other human beings humanely, one must exercise
compassion and make legal and ethical judgments.\[52\]
Compassion]{.underline}, according to the ICRC's fundamental principles,
[is the "stirring of the soul which makes one responsive to the distress
of others]{.underline}."\[53\] [To show compassion, an actor must be
able to experience empathy---that is, to understand and share the
feelings of another---and be compelled to act in
response]{.underline}.\[54\] [This emotional capacity is vital in
situations when determinations about the use of force are
made.]{.underline}\[55\] It drives actors to make conscious efforts to
minimize the physical or psychological harm they inflict on human
beings. Acting with compassion builds on the premise that "capture is
preferable to wounding an enemy, and wounding him better than killing
him; that non-combatants shall be spared as far as possible; that wounds
inflicted be light as possible, so that the injured can be treated and
cured; and that the wounds cause the least possible pain."\[56\] While
compassion provides a motivation to act humanely, legal and ethical
judgment provides a means to do so. [To act humanely, an actor must make
considered decisions as to how to minimize harm]{.underline}.\[57\] Such
decisions are based on the ability to perceive and understand one's
environment and to apply "common sense and world knowledge" to a
specific circumstance.\[58\] Philosophy professor James Moor notes that
[actors must possess the capacity to "identify and process ethical
information about a variety of situations]{.underline} and make
sensitive determinations about what should be done in those
situations."\[59\] In this way, legal and ethical judgment helps an
actor weigh relevant factors to ensure treatment meets the standards
demanded by compassion. Judgment is vital to minimizing suffering: one
can only refrain from harming humans if one both recognizes the possible
harms and knows how to respond.\[60\] Application to Fully Autonomous
Weapons [Fully autonomous weapons would face significant challenges in
complying with the principle of humane treatment because compassion and
legal and ethical judgment are human characteristics. Empathy, and the
compassion for others that it engenders, come naturally to human
beings]{.underline}. Most humans have experienced physical or
psychological pain, which drives them to avoid inflicting unnecessary
suffering on others. Their feelings transcend national and other
divides. As the ICRC notes, "feelings and gestures of solidarity,
compassion, and selflessness are to be found in all cultures."\[61\]
People's shared understanding of pain and suffering leads them to show
compassion towards fellow human beings and inspires reciprocity that is,
in the words of the ICRC, "perfectly natural."\[62\]

#### 5. [Inherent Worth]{.underline} -- AI denies dignity because it cannot respect the Inherent Worth of individuals -- it can only mimic morality.

**Johnson and Axinn, 2013 - Prof of Philosophy at the Univ of South
Florida and PhD Candidate in Engineering at Penn** \[Aaron and Sidney,
Journal of Military Ethics, Volume 12, Issue 2, August "The Morality of
Autonomous Robots" www.tandfonline.com/10.1080/15027570.2013.818399 Acc
12/27/20 TA\]

Morality [Humans give themselves moral commands, programs are given the
commands they are to follow. A machine can replicate an action that has
been called moral, but morality does not come from following someone
else.]{.underline} In this way [a machine can act morally]{.underline},
by mimicking its programmer, [but it cannot be moral.]{.underline}
Similarly when playing back a video of a moral act one would not say
that the video was moral, it is simply replaying the moral act of its
subject. An autonomous robot is clearly more complicated than a video,
and can more directly interact with the world and respond to feedback,
however it is still just playing back moral actions, and is not truly
making the decisions for itself. Moral commands are based on values, and
values are produced and indicated by sacrifices (Axinn 2010, also see
section on Military Honor, below). While robots are aware of following
orders, they are not aware of making sacrifices. Artificial Intelligence
still has no real notion of sacrifice (and Artificial Morality is still
just a phrase and not a developed subject). Therefore robots have no
values of their own, although they are following the values of their
programmers. A distinctive human characteristic therefore is the ability
to think morally based on one's values, and to give oneself the moral
commands. [Morality requires]{.underline}, according to Kant, [that the
principle of one's action must be one that is capable of being followed
universally]{.underline}. Again, [not the specific act, but the
principle of the act must be one that everyone can follow. The ability
to judge and extract the principles of an action cannot be]{.underline}
codified or [programmed]{.underline}. As Kant put it, 'judgment...cannot
be taught.' (1965: p. A133). Furthermore, Kant insists, 'general logic
can supply no rules for judgment...' (1965: p. A135). [Without this
ability we find once again that a robot can only mimic
morality]{.underline} by replicating actions as commanded.

### Harms - Framework -- Ethics First

#### NATO has a moral obligation to adopt ethical principles for AI weapons before they are used

**Gilli, 2020 - Senior Researcher at the NATO Defense College**
\[Andrea, NDC Research Paper No.15 -- December ""NATO-Mation":
Strategies for Leading in the Age of Artificial Intelligence"
https://www.ndc.nato.int/news/news.php?icode=1514 Acc 4/21/22 TA\]

Ethical purpose: ethics and values (with Zoe Stanley-Lockman) [NATO
has]{.underline} an interest, and [a moral obligation, to promote the
adoption of its values in the realm of AI]{.underline}. Democratic
values inform the Alliance's goals, thus giving meaning to its material
capabilities -- including its military power. At the same time[, the
integration of AI into the fields of security and defence poses unique
moral, ethical, legal, and safety-related questions]{.underline}.85 [It
is thus imperative that the Alliance actively considers and
operationalizes AI ethics, regardless of the degree and scope of AI
integration within NATO and its Allies]{.underline}. The common
principles and values pronounced in the Atlantic Treaty represent the
foundations on which NATO was built. Such principles and values --
democracy, freedom, rule of law, individual rights, free markets -- are
the bond underlying the transatlantic community, which in fact predates
the Atlantic Alliance.86 [Norms and values can strongly shape the
international system]{.underline}.87 Given the intense international
competition in the technological, military, economic, and normative
domains, [embedding democratic values into AI is as much a strategic
imperative for the Alliance as it is a functional one.88 In addition to
signalling to domestic populations that NATO and its Allies follow
through on their commitments to uphold values as the basis of the
political and military Alliance, incorporating AI ethics into the
"NATO-mation" agenda also serves as a bulwark against the incursion of
unwelcome illiberal values in the course of future technological
development.89]{.underline}

#### Morality must come first because the weapons don't exist yet, and the ethical framework will Decide how they are treated. 

**Johnson and Axinn, 2013 - Prof of Philosophy at the Univ of South
Florida and PhD Candidate in Engineering at Penn** \[Aaron and Sidney,
Journal of Military Ethics, Volume 12, Issue 2, August "The Morality of
Autonomous Robots" www.tandfonline.com/10.1080/15027570.2013.818399 Acc
12/27/20 TA\]

Conclusion For the reasons presented above, reasons both military and
humanitarian, we propose that autonomous robots, carrying lethal weapons
and operated by computer programs alone, be treated on the same basis as
the United States now treats chemical weapons (gas warfare among them).
As noted earlier, the U.S. and all other nations should agree not use
such weapons. A second Ottawa-style conference would be an appropriate
mechanism to accomplish this moral goal internationally. [Nuclear
weapons are an example of technology that was brought into use before
civilization and the laws of war could react to them. We need to act now
to establish the moral and legal standing of automatic robots before
they enter into common usage]{.underline}. The Ottawa conference
prohibiting anti-personnel mines (August 2007), as mentioned, would be
an excellent model for what should be done about automatic robots. A
nation that relies on such weapons ignores the humanitarian basis for
the laws of war, and when there is an international convention banning
them, such a nation will be acting dishonorably. [As technology
continues to progress there will certainly be borderline questions11,
but the central notion cannot be abandoned, that a lethality decision is
to be made only by a human and not a machine. That should remain the key
focus of debate and be the guiding moral principle.]{.underline}

#### The Moral questions about AI are the most important ones -- prefer my contextual evidence

**Johnson and Axinn, 2013 - Prof of Philosophy at the Univ of South
Florida and PhD Candidate in Engineering at Penn** \[Aaron and Sidney,
Journal of Military Ethics, Volume 12, Issue 2, August "The Morality of
Autonomous Robots" www.tandfonline.com/10.1080/15027570.2013.818399 Acc
12/27/20 TA\]

The Major Question [All of these questions ignore the bigger, more
important moral question: should we relinquish the decision to kill a
human to a non-human machine? In this paper we argue primarily that the
discussion needs to focus on this most important question, and not on
the details of the technology or its efficacy.]{.underline} We now
further argue that [the decision to take a human life must be an
inherently human decision and]{.underline} that [it would be unethical
to allow a machine to make such a critical choice]{.underline}. [The
concept of what a rational human is, a being that can give itself the
moral law, is essential to considering this matter.]{.underline}

### Harms - Framework - Human Dignity

#### Dignity is the most important value because it is a precondition for all other human values

**Heyns, 2016 - Professor of Human Rights Law, University of Pretoria**
\[Christof, Human Rights Quarterly 38 (2016) 350--378 " Human Rights and
the use of Autonomous Weapons Systems (AWS) During Domestic Law
Enforcement"
<https://www.academia.edu/37475669/Human_Rights_and_the_use_of_Autonomous_Weapons_Systems_AWS_During_Domestic_Law_Enforcement>
Acc 12/27/20 TA\]

However, [there is]{.underline} also [the]{.underline} further [question
of whether machines should be given this power.]{.underline} This raises
the question of whether the rights to bodily integrity do not require
whatever force is used against a human being to be authored by a human
being as opposed to a robot. For example, [is it not inherently
arbitrary for a machine to take decisions about life and death over
human beings?]{.underline} The question whether machines should have
such power also raises questions about the right to human dignity. B.
The Right to Human Dignity The preamble of the Charter of the United
Nations reaffirms "faith in fundamental human rights \[and\] in the
dignity and worth of the human person."74 Article 1 of the Universal
Declaration of Human Rights provides that "\[a\]ll human beings are born
free and equal in dignity and rights."75 The International Covenant on
Civil and Political Rights refers to dignity in its preamble but does
not list it as a substantive right in the rest of the text, though it is
intertwined with other rights.76 [The protection of human dignity is the
common aim of international human rights law]{.underline} as well as
IHL, and this commonality underlies their complementarity.77 [Dignity
has been called the "mother"]{.underline}78 [of all human
rights]{.underline} and is recognized to underlie much of IHL.79 [Is it
inherently a violation of the right to dignity if the decision to use
force against a human being is made by a computer as opposed to another
human?]{.underline}

#### Focusing on dignity is essential to make the ban successful -- it resonates with the public.

**Rosert, 2019 - Professor for International Relations at Universität
Hamburg** \[Elvira, with Frank Sauer Researcher at Bundeswehr, Global
Policy, July 5 "Prohibiting Autonomous Weapons: Put Human Dignity First"
https://doi.org/10.1111/1758-5899.12691 Acc 12/27/20 TA\]

The discourse about LAWS is not and does not have to be confined to the
CCW discussions. While [human dignity]{.underline} is admittedly not a
crucial point of reference within this particular forum, it [is a
universal and ubiquitous concept that does resonate with
States]{.underline} Parties in other UN fora (such as the General
Assembly, for instance). In addition, as of 2019, it is in fact quite
likely that the CCW will yet again end up being merely an incubator for
regulative action. Should the process surrounding LAWS leave the CCW,
[proponents of a ban on LAWS must be ready to refocus on human
dignity]{.underline}. After all, in the US, 55 per cent of the public is
opposed to LAWS (Carpenter, 2013); in Germany, 71 per cent of the
population is against handing weapons control in warfare over to AI
(YouGov 2018). And international opinion polls conducted online by the
Open Roboethics Initiative (2015) as well as by IPSOS (CSKR 2019b; Roff,
2017) indicate not only a similar picture but even growing resistance
against LAWS at the global level. [Since the public\'s reaction to LAWS
is mostly visceral, rather than based on legal considerations, putting
human dignity first will gain traction. This point is granted even by
skeptics of our line of argument: 'There could be some campaigning
advantages. Saying that something is against human dignity evokes a
strong visceral response']{.underline} (Sharkey, 2018, p. 9).

#### Dignity is the most important value because it is key to our international human rights obligations

**Docherty, 2014 - senior researcher in the Arms Division of Human
Rights Watch** \[Bonnie "Shaking the Foundations The Human Rights
Implications of Killer Robots" Human Rights Watch
http://www.hrw.org/sites/default/files/reports/ arms0514_ForUpload_0.pdf
Acc 12/27/20 TA\]

IV\. Human Dignity [The concept of human dignity lies at the heart of
international human rights law.]{.underline} The opening words of the
UDHR assert that ["recognition of the inherent dignity and of the equal
and inalienable rights of all members of the human family is the
foundation of freedom, justice and peace in the world."]{.underline}78
In ascribing inherent dignity to all human beings, the UDHR implies that
[everyone has worth that deserves respect]{.underline}.79 The ICCPR
establishes the inextricable link between dignity and human rights,
stating in its preamble that the rights it enumerates "derive from the
inherent dignity of the human person."80 Regional treaties echo this
position, and the Vienna Declaration of the 1993 World Human Rights
Conference affirms that "all human rights derive from the dignity and
worth inherent in the human person."81

### 

### Solvency - Ethics

#### NATO principles for Responsible AI use allow allies to align with the broader agenda.

**Stanley-Lockman, 2021 - Center for Security and Emerging Technology**
\[Zoe CSET Issue Brief August "Responsible and Ethical Military AI
Allies and Allied Perspectives"
https://cset.georgetown.edu/publication/responsible-and-ethical-military-ai/
Acc. 4/21/22 TA\]

As part of the implementation of the EDT agenda[, the NATO AI Strategy
is expected to pick up on this theme in "guidance on both principles for
responsible use of AI-enabled platforms and export control
mechanisms]{.underline}."150 Accountability and transparency---for
weapon systems with varying levels of autonomy, as well as AI enabled
systems---and rules for industry may feature in this approach.151
Already, the [NATO]{.underline} Science & Technology Organization [has
identified a "strong emphasis on explainability, trust and human-AI
collaboration"]{.underline} as well as "processes and standards for
verification, validation and accreditation" [as areas of interest for
NATO, though they are not yet formalized in publicly pronounced
principles]{.underline}.152 Overall, [the issuance of
principles]{.underline} at the NATO level will be helpful to reflect
priorities of multiple Allies, [permit more Allies to align nascent or
ad hoc initiatives to the broader agenda with one another, and
potentially help bridge responsible AI with best practices or
standardization that industry can follow.153]{.underline}

#### Agreeing to ethical principles is essential to responsible use of AI, which includes standardization and interoperability

**Christie, 2020 - Deputy Head of the Innovation Unit, NATO Emerging
Security Challenges Division** \[Edward Hunter, 24 November NATO Review,
"Artificial Intelligence at NATO: dynamic adoption, responsible use"
https://www.nato.int/docu/review/articles/2020/11/24/artificial-intelligence-at-nato-dynamic-adoption-responsible-use/index.html
Acc. 4/16/22 TA\]

Committing to responsible use [The Alliance's success with AI will also
depend on new and well-designed principles and practices relating to
good governance and responsible use]{.underline}. Certain Allied
governments have already made certain public commitments in the area of
responsible use, addressing concepts such as lawfulness, responsibility,
reliability, and governability, among others. In parallel, Allies have
taken part in the Group of Governmental Experts on Lethal Autonomous
Weapon Systems under the auspices of the United Nations, leading to the
formulation of 11 guiding principles. Importantly, [there is a good case
for viewing work on adopting AI and work on principles of responsible
use as complementary and synergistic. In effect, there are certain
essential principles or goals that will underpin and facilitate both
engineering good practice, as well as responsible state
behaviour.]{.underline} Certain national principles imply a need for
specific design requirements. For example, a principle of governability
may be linked to technical abilities to detect and avoid unintended
consequences, and to disengage or deactivate in case of unintended
behaviour. The technical characteristics required to ensure that these
and other objectives are met will necessarily be part of the design and
testing phases of relevant systems. In turn, the relevant engineering
work will be an opportunity to refine understanding, leading to more
granular and more mature principles. Further work in the area of
Testing, Evaluating, Verifying and Validating (TEVV) will be essential,
as will support from relevant Modelling and Simulation efforts. [NATO's
well-established strengths in the area of standardization will help
frame these lines of effort, while also ensuring interoperability
between Allied forces.]{.underline}

#### The US must coordinate with Allies on ethical principles to ensure the responsible use of AI

**Schmidt and Work, 2021 -- Chairs of The National Security Commission
on Artificial Intelligence** \[Eric and Robert, "National Security
Commission on Artificial Intelligence Final Report Executive Summary"
<https://www.nscai.gov/2021-final-report/> Acc 6/7/22 TA\]

Manage risks associated with [AI-enabled and autonomous
weapons]{.underline}. AI will enable new levels of performance and
autonomy for weapon systems. But it [also raises important legal,
ethical, and strategic questions surrounding the use of lethal force.
Provided their use is authorized by a human commander or
operator]{.underline}, properly designed and tested AI enabled and
autonomous [weapon systems can be used in ways that are consistent with
international humanitarian law]{.underline}. DoD's
[rigorous,]{.underline} existing weapons review and targeting
procedures, including its dedicated [protocols for autonomous weapon
systems and commitment to strong AI ethical principles, are capable of
ensuring that the United States will field safe and reliable AI-enabled
and autonomous weapon systems and use them in a lawful
manner]{.underline}. While it is neither feasible nor currently in the
interests of the United States to pursue a global prohibition of
AI-enabled and autonomous weapon systems, the global, unchecked use of
such systems could increase risks of unintended conflict escalation and
crisis instability. [To reduce the risks, the United States
should]{.underline} (1) clearly and publicly affirm existing U.S. policy
that only human beings can authorize employment of nuclear weapons and
seek similar commitments from Russia and China; (2) [establish venues to
discuss AI's impact on crisis stability with competitors;
and]{.underline} (3) [develop international standards of practice for
the development, testing, and use of AI-enabled and autonomous weapon
systems.]{.underline}

### Solvency -- Human Control

#### Human control is the essential common goal that allows allies to develop AI standards and norms.

**Stanley-Lockman, 2021 - Center for Security and Emerging Technology**
\[Zoe CSET Issue Brief August "Responsible and Ethical Military AI
Allies and Allied Perspectives"
https://cset.georgetown.edu/publication/responsible-and-ethical-military-ai/
Acc. 4/21/22 TA\]

In this light, this issue brief seeks to provide policymakers and
analysts with one view on how [similarities between allied perspectives
on ethical AI for defense create opportunity for increased
collaboration,]{.underline} and how [the differences that are beginning
to take shape can undermine said collaboration]{.underline}. [Alignment
and collaboration start with an understanding of variations in
definitions of terms]{.underline} like trustworthy AI, ethical AI, and
responsible AI in the defense context. [These definitions are often
fluid, depending on the legal, ethical, and cultural traditions of
different countries. But different conceptions of responsible military
AI nevertheless share foundations]{.underline} that help frame the
analysis here. Broadly speaking, this issue brief focuses on how
[defense stakeholders steward AI innovation and integration in ways
that]{.underline}: (1) respect the moral and ethical reasoning that
underpins the responsible use of force; (2) [meet and enhance compliance
with law, which is based on ethics and translates reasoning into
concrete obligations;]{.underline} and (3) [minimize risks and
unintended consequences for a safer and more secure international
security environment. To uphold ethical]{.underline}, legal, and safe
[foundations of AI development]{.underline} and deployment in defense,
[allies coalesce around]{.underline} two [shared]{.underline} themes in
their [approaches to responsible military AI]{.underline}. First is that
decisions around the design, development, deployment, and diffusion of
AI do not enter into a vacuum, but rather into an existing,
multi-layered legal framework. It is not controversial for democratic
countries to declare their shared obligation to respect law in order to
remain accountable.8 This accountability is owed to domestic
citizenries, to the armed forces themselves, and to allies and partners
in coalition settings, as well as to adversaries and the international
community at large. As such, for some allies, emerging conceptions of
responsible AI are closely interlinked with responsible state behavior,
with continued legal compliance as the minimum requirement.9 The second
commonality in all of the frameworks examined here is [a shared focus on
human centricity]{.underline}. There are several definitions of
human-centric AI, but for the purpose of this analysis, it can be
understood as the idea that AI is designed to meet human needs and
improve upon the role of the human.10 Not all frameworks use the term
itself, but all stress the central role of humans in that machines
should not replace humans and that [humans remain responsible and
accountable for decisions.]{.underline} By extension, this means a
common approach to designing AI systems in such a way that the human
user is not expected to adjust her or his own decision-making capacities
to conform to the technology.11 The inverse would place the machine at
the center of decision-making systems. Meanwhile, [countries consider
humans to be central to defense planning and operations, and
stress]{.underline} in their positions [that they do not think it moral
or lawful to delegate human responsibility to machines.]{.underline}

#### Human control is key to for responsible AI use

**Sauer, 2021 - Senior Research Fellow at Bundeswehr University**
\[Frank serves on the International Panel on the Regulation of
Autonomous Weapons IRRC No. 913 March "Stepping back from the brink: Why
multilateral regulation of autonomy in weapons systems is difficult, yet
imperative and feasible"
https://international-review.icrc.org/articles/stepping-back-from-brink-regulation-of-autonomous-weapons-systems-913
Acc 4/5/22 TA\]

Luckily, the process of conceptualizing the issue and translating it
into diplomatic language has begun, and has already made some progress.
After almost six years, [the codification of a positive obligation of
human control over weapons systems is establishing itself more and more
at the heart of the debate]{.underline}. This general notion, gaining
prominence in the wake of the call for ["meaningful human
control"]{.underline} originally introduced by the NGO Article 36,15 is
being embraced by civil society as well as a consistently growing number
of CCW States Parties. Accordingly, the conceptualization is now finding
broad acceptance in both academic literature and the diplomatic debate
-- not least because the United States and the International Committee
of the Red Cross (ICRC) have adopted it. This [is not some sort of
categorical definition]{.underline} of LAWS (versus non-LAWS) via a list
of criteria. Instead, [it is a functional understanding of the
phenomenon]{.underline}.16 [From a functionalist point of view, the LAWS
issue is best understood as one of autonomy in a weapons
system]{.underline} -- that is, of the machine rather than a human
performing a certain function (or certain functions) during the
system\'s operation.17 Every military operation concluding with an
attack on a target can be systematized along discrete steps of a kill
chain or targeting cycle.18 This includes finding, fixing, tracking,
selecting and engaging the target (as well as assessing the effects
afterwards). Many weapons systems are already capable of performing some
of the targeting cycle functions without human input or supervision --
for example, a drone navigating from one waypoint to the next via
satellite navigation and thus performing a part of the "finding"
function without having to be remotely controlled. [An autonomous
weapon]{.underline}, however, [completes]{.underline} the entire
targeting cycle -- including [the final stages of selecting and engaging
the target with force]{.underline} -- without human intervention. [In
the debate about LAWS, the focus rests mainly on those last two
functions]{.underline} ([which the ICRC calls "critical"]{.underline}19)
because [most of the effects of weapon autonomy currently under
discussion derive from giving up human control]{.underline} over them
and handing the decision to use force over to a machine.20

#### Human control is key -- it is the most pressing issue to resolve in military AI

**Vestner, 2021 - Head of Security and Law Programme at Geneva Centre
for Security Policy** \[Tobias, July 8 "Warfare and Artificial
Intelligence" in Robin Geiß and Henning Lahmann (eds), Research Handbook
on Warfare and Artificial Intelligence -forthcoming
https://www.gcsp.ch/publications/military-operations-and-artificial-intelligenceGCSP
Acc 5/27/22 TA\]

At this stage, [AI technologies and their military applications, as well
as respective policies, are only emerging. Yet the integration of AI
into armed forces will most likely transform the preparation and
execution of military operations]{.underline}. This paper has analysed
how AI systems will most likely affect and be affected by principal
instruments for preparing and conducting military operations. Overall,
the introduction of AI for military operations leads to a tension
between AI influencing these instruments and these instruments serving
to properly manage military AI. With regard to strategy, it can be
expected that AI will be used for developing strategies, similar to
planning activities. The introduction of AI applications throughout
armed forces will likely also need to be considered by military
strategists, as the speed and complexities of military operations may
increase. Doctrine is an appropriate tool to define armed forces'
perception of AI as well as humans' role regarding the use and control
of AI, thereby serving as a hook for institutional ethics, values, and
identity. Given doctrine's purpose, AI will likely not have a major role
in determining doctrine. Yet the planning process will likely be heavily
supported by AI systems, which may lead to higher quality and speed of
planning processes, eventually improving military decision-making. While
AI can well support the management of ROE, these are instruments that
can serve to guide the concrete authority attributed to AI systems and
define human-machine teaming for specific missions in line with superior
guidelines such as doctrine, (other) directives, and plans. Orders,
however, are likely to become irrelevant for the interaction between AI
systems and human-system interaction. Overall, a recurring theme is the
interaction between AI systems and commanders, operators, and soldiers.
Indeed, [human control is a requisite for the purposeful use of AI in a
human world. Yet, human-machine teaming remains subject to
challenges]{.underline}. Many processes related to military operations
will still require human input. Moreover, it seems crucial that military
staff will be enabled to follow, understand, and keep appropriate
control of AI systems. [This is not only an ethical]{.underline} and
legal challenge [but a requisite to achieve effective enhancement of
military operations through the introduction of AI.]{.underline} Further
reflections and research on AI and military operations in general as
well as on AI and strategy, doctrine, plans, ROE, and orders, in
particular, should therefore focus [on the human-machine interaction, as
this remains the most pressing challenge of AI-enabled
warfare]{.underline}. This may serve to find and define an adequate
balance between AI influencing instruments for preparing and conducting
military operations and these instruments serving to properly manage
military AI.

### Solvency - Cohesion

#### Cooperation is essential for cohesion. Ethical AI principles are key to interoperability and doctrine.

**Stanley-Lockman, 2021 - Center for Security and Emerging Technology**
\[Zoe CSET Issue Brief August "Responsible and Ethical Military AI
Allies and Allied Perspectives"
https://cset.georgetown.edu/publication/responsible-and-ethical-military-ai/
Acc. 4/21/22 TA\]

[Responsible and ethical military AI between allies is important because
policy alignment can improve interoperability in doctrine, procedures,
legal frameworks, and technical implementation measures. Agreeing not
only on human centricity for militaries adopting technology, but also on
the ways that accountability and ethical principles enter into the
design, development, deployment, and diffusion of AI helps reinforce
strategic democratic advantages]{.underline}. [Conversely, ethical gaps
between allied militaries could have dangerous consequences that imperil
both political cohesion and coalition success]{.underline}. More
specifically, if allies do not agree on their responsibilities and risk
analyses around military AI, then gaps could emerge in political
willingness to share risk in coalition operations and authorization to
operate alongside one another. Even though the United States is the only
country to have adopted ethical principles for defense, key allies are
formulating their own frameworks to account for ethical risks along the
AI lifecycle. This report explores these various documents, which have
thus far been understudied, at least in tandem. Overall, [the analysis
highlights both convergences in ethical approaches to military AI and
burgeoning differences that could turn into political or operational
liabilities.]{.underline}

#### Cooperation is key to standardize NATO's approach to AI, which is key to avoiding technology gaps.

**Michelson, 2021 - Senior Fellow at the Center for European Policy
Analysis's Transatlantic Defense Tech Initiative** \[Colonel (Retired)
Brian M. February 23 "Why NATO Needs Lethal Autonomous Weapon Standards"
https://cepa.org/why-nato-needs-lethal-autonomous-weapon-standards/ Acc.
4/5/22 TA\]

[Lethal autonomous weapon systems will come to dominate
warfare]{.underline} in the coming years. [NATO needs to harmonize its
approach to their development and use, or risk being left
behind.]{.underline} The rapid weaponization of artificial intelligence,
"big data," social media, robotics, and a host of other technologies
presents a clear competitive challenge to NATO, an alliance with members
that exist on a wide spectrum of military-technological capabilities.
[The future effectiveness of NATO will be driven in large part by how it
handles these challenges from hobbling its ability both to act in unison
and to prevail in a contest of wills]{.underline}. While there are
numerous potential [technology gaps,]{.underline} one that [will likely
only increase is partner nations' ability and willingness to employ
lethal autonomous weapon systems]{.underline}. These systems will
inevitably grow more capable, and more necessary, in the coming decade.
[Technological gaps are inevitable considering the disparities in GDP
and military budgets.]{.underline} The United States accounts for over
70 percent of NATO's overall military spending, while the next three
largest contributors (the United Kingdom, France, and Germany) provide
approximately half of the remaining 30 percent. And with most NATO
nations continuing to fund their militaries at under the 2 percent GDP
goal, technological gaps will continue to grow. For perspective, the
2021 United States Department of Defense research and development budget
is approximately equal to the entire defense outlay of France and
Germany combined. With such a large differential, what can be done to
help enable effective investments in autonomous weapons by smaller
nations? Even more specifically, [how can smaller nations provide
capabilities that can integrate into, and contribute to the alliance? To
better invest limited funds, now is the time to look at a NATO standard
for lethal autonomous weapons and their ethical use]{.underline}. While
there is no agreed-upon international definition of lethal autonomous
weapons systems, the U.S. Department of Defense defines them as "weapon
system\[s\] that, once activated, can select and engage targets without
further intervention by a human operator." While these are not
Schwarzenegger-style Terminators and still have a degree of human
control over them, the technology enabling these systems is maturing
rapidly, and military necessity will increasingly demand that these
systems gain broader parameters of autonomous action. Yet
[despite]{.underline} the complexity of these systems and [the
inevitability of their proliferation, NATO does not currently have a
common standard for their use or development. In fact, some NATO
countries even have opposing views of how to handle them.]{.underline}

### Solvency -- Interoperability

#### Addressing ethical concerns is key to interoperability -- differing ethical views cause different technological approaches, which destroys NATO's ability to coordinate between different militaries.

**Michelson, 2021 - Senior Fellow at the Center for European Policy
Analysis's Transatlantic Defense Tech Initiative** \[Colonel (Retired)
Brian M. February 23 "Why NATO Needs Lethal Autonomous Weapon Standards"
https://cepa.org/why-nato-needs-lethal-autonomous-weapon-standards/ Acc.
4/5/22 TA\]

[NATO standards are designed to ensure compatibility among weapon
systems, communication architecture]{.underline}, and a host of other
warfighting systems. The 7.62mm small arms round is a good example of
this. [But what is the]{.underline} 7.62mm equivalent [standard for the
development and employment of autonomous weapon systems]{.underline}?
This opens a host of related questions regarding the employment of these
systems: [What Identification -- Friend -- Foe (IFF) capability should
ground and air units require to prevent fratricide? What degree of
certainty does a lethal autonomous weapon system require before final
engagement? What level of collateral damage is acceptable? What degree
of compatibility between systems is required? Should all these
parameters (and others) be adjustable, and if so, at what command level?
The attendant ethics]{.underline} also [need to be
addressed]{.underline}. NATO's experience in Afghanistan was a case
study in the challenges of coalition warfare. [Differing risk
tolerances, legal requirements, ethical views, domestic political
concerns, and at times simply combat capability, all combined into to
complex policy cocktail that impeded the effectiveness of combat
operations. While modern militaries have accountability, legal, and
ethical systems incorporated into their command structures, they are not
uniform and leaders in differing militaries have varying degrees of
authority.]{.underline} [The key questions hinge on two issues: Who gets
to decide to employ an autonomous weapon, and who is responsible should
things go wrong]{.underline}? The Kunduz hospital strike in October of
2015 was driven primarily by human error. Responsibility was fixed on
the chain of command and 16 leaders were disciplined. Who will be
responsible if a member nation conducts a NATO-authorized strike and it
goes terribly wrong? If this framework is not thoroughly established
ahead of time, not only is it likely that commanders may hesitate to use
this capability, the risk-aversion inherent in bureaucracies may limit
the development of autonomous weapons that will be needed in future
conflicts. [In the emerging field of lethal autonomous weapons,
establishing a common NATO standard for the development and use of
autonomous weapons will help address the gap in capabilities among NATO
member nations. By establishing these standards, nations can ensure that
their defense expenditures on autonomous weapons will create systems
that are interoperable, able to contribute to NATO's capability, and can
be employed within defensible ethical guidelines.]{.underline}

#### Collaboration is key to avoid lopsided adoption of AI systems, which would undermine joint operations. 

**Lawrence and Cordey, 2020 -- researchers for The Cyber Project at the
Belfer Center for Science and International Affairs** \[Christie and
Sean, August, The Cyber Project Paper "The Case for Increased
Transatlantic Cooperation on Artificial Intelligence Edited by Lauren
Zabierek and Julia Voo
https://www.belfercenter.org/sites/default/files/2020-08/TransatlanticAI.pdf
Acc. 4/21/22 TA\]

Joint US-EU defense-related AI efforts appear sparse,349 however there
is a growing push particularly from the US and NATO to increase
transatlantic defense cooperation on AI. The aforementioned DOD
Artificial Intelligence Strategy stressed the importance the US and
allied partners to "maintain its strategic position to prevail in future
battlefields and safeguard a free and open international order."350
Growing Chinese and Russia aggression and weaponization of new
technologies like AI is a central motivation for the US government's
push to strengthen the international AI alliance.351 [Acting Director of
JAIC Nand Mulchandani explains that collaboration with European partners
on AI is necessary for three main reasons]{.underline}. [First, joint AI
R&D and adoption efforts are important for developing strong
capabilities and bolstering transatlantic hard power. Second, leading AI
capabilities\--among the US and its allies\--acts as a deterrent for
conflict. Third, if deterrence fails, AI-related interoperability and
capabilities are necessary if the US and its allies must go to war. A
military alliance cannot be lopsided and unequal AI adoption may hinder
the tactical aspects of conflict]{.underline}.352 During the already
mentioned recent visit to NATO headquarters in Brussels, Former Director
of the JAIC Lt. Gen. [Shanahan had]{.underline} [collaborative
engagements with]{.underline} European allies and [NATO around the
importance of AI joint efforts, ensuring military interoperability, and
the convergence of Europe's AI ethical principles]{.underline} with the
DOD's five principles of AI ethics.353 The visit appeared to increase
NATO's focus on devising an AI strategy for the Alliance.354 Despite
these positive steps, there are still concerns about potential obstacles
to increased defense collaboration, particularly around IP ownership and
funding restrictions that may exclude US involvement in EDF and
PESCO.355, 356

#### Ethical principles are essential for [Legal interoperability]{.underline} within NATO military operations 

**Trabucco and Stanley-Lockman, 2022 -- prof of Political Science,
University of Copenhagen and prof of Defense and Strategic Studies,
Nanyang Technological University** \[Lena and Zoe, The Oxford Handbook
of AI Governance, March, "NATO's Role in Responsible AI Governance in
Military Affairs"
https://www.oxfordhandbooks.com/view/10.1093/oxfordhb/9780197579329.001.0001/oxfordhb-9780197579329-e-69
Acc 4/16/22 TA\]

NATO and Technological Change: Three Pillars of AI Governance This
section considers three pillars where [NATO has procedures and
competency to operationalize AI governance through both mechanisms of
policy alignment and standards, and enhance security in the
international environment]{.underline}. The pillars reflect foundational
issue areas constitutive of governance but are also issue areas where
previous scholars have cautioned as particularly challenging in the AI
governance space. The three pillars---(1) [ethics and
values]{.underline}, (2) [legal norms, and]{.underline} (3) safety and
[security---are meant to illustrate three conditions for NATO to
facilitate policy and standards harmonization]{.underline}. Importantly,
these pillars are not exhaustive areas in which NATO will need to
consider governance structures to responsibly implement AI technology,
but rather highlight particular issues that researchers and analysts
acknowledge as significant hurdles in navigating AI governance (see
Table 69.2).54 Table 69.2: Cross-tabulating NATO's governance mechanisms
with pillars of AI governance Ethics and Values Legal Norms Safety and
Security NATO Policy and Strategic Planning Core, shared values at
foundation of Alliance's political cohesion that informs civilian
oversight of operations and overall institutional effectiveness;
included in principles [Alignment between differing legal
interpretations between Allies, particularly as affects the ability of
forces to]{.underline} communicate and [interoperate in dynamic
contexts]{.underline}; constant calibration of policies based on legal
interpretations Strategic planning for maintaining integrity of
information in military operations and transparency measures that
reinforce democratic accountability; [setting priorities on defensive
systems and countermeasures to protect from motivated attacks and
intentional failure modes of AI-enabled weapon systems]{.underline}

#### Legal interoperability is key to NATO cohesion and operational interoperability - different legal viewpoints make it impossible to coordinate militarily.

**Hill and Marsan, 2018 - Director and Senior Assistant, NATO Office of
Legal Affairs** \[Steven and Nadia, 7-18-18 "Artificial Intelligence and
Accountability: A Multinational Legal Perspective"
https://www.sto.nato.int/publications/STO%20Meeting%20Proceedings/STO-MP-IST-160/MP-IST-160-PP-4.pdf
Acc 4/21/22 TA\]

1.0 THE NATO CONTEXT: LEGAL INTEROPERABILITY NATO is an Alliance of
values, and one of the core values that the Alliance defends is the rule
of law. The commitment to respect the rule of law is enshrined in the
preamble to the 1949 North Atlantic Treaty and is reaffirmed in
declarations by NATO Heads of State and Government at their regular
Summit meetings. It is a key element of NATO and NATO-led operations and
is necessary to ensuring legitimacy and securing public support.
Compliance with international law is an essential component of NATO's
success in all of its endeavours and activities. Correspondingly, the
demand for legal advice from over 70 legal offices that make up the NATO
legal community has never been higher. The scope of issues on which the
typical NATO legal adviser is called to advise on can be daunting and
guidance on Allied views is not always available, especially in an
easy-to-access consolidated format. This is especially true in the
context of emerging technologies such as the security impact of
AI-enabled tools. Under the auspices of the NATO Office of Legal
Affairs, [NATO has launched and reinvigorated a number of legal
dialogues]{.underline}, including with Allies, partner nations and
international organisations. [Such legal dialogues are essential to
understanding Allies' potentially conflicting legal
postures]{.underline}. [NATO Allies each have their sovereign domestic
legal systems and are bound by different international legal
obligations]{.underline}. Allies also often have their own understanding
on what international law obligations are applicable to them and under
what conditions. As an alliance of 29 nations, NATO takes all decisions
by consensus and [the main challenge]{.underline} in such a multilateral
environment [is enabling Nations to act together in line with the
individual legal obligations of each, taking account of the differences
in applicable legal parameters. A key technique that legal advisers use
to help achieve consensus despite these legal differences is "legal
interoperability". "Legal interoperability" is derived from the military
concept of interoperability of forces, whereby the military forces of
the 29 Allies are able to work together to achieve common
objectives]{.underline}. NATO doctrine defines the interoperability of
forces as "\[t\]he ability of the forces of two or more nations to
train, exercise and operate effectively together in the execution of
assigned missions and tasks."4 In the Warsaw Summit Communiqué, Allied
Heads of State and Government referred to the need for interoperability
to accomplish NATO's goals. Within NATO, there is often a diversity of
legal views on core issues of international law. Let us take, for
example, the laws of war, where 27 of NATO's 29 Allies are party to the
Additional Protocols to the Geneva Conventions. This challenge is also
reflected in International Criminal Law as the Rome Statute establishing
the International Criminal Court is likewise adopted only by 27 Allies.
As a military Alliance founded on the rule of law, these sometimes
different legal obligations can be challenging. The challenges are
especially daunting when there is a need for a rapid decision by Allies
based on collective consensus and requiring legal justification. NATO'
raison d'être is collective defence, and legal interoperability is one
of the key enablers for the Alliance. From a legal perspective,
interoperability refers to the need for Nations to work together in a
variety of contexts despite the application of differing legal
frameworks and obligations. As one observer of NATO operational law
defined it, "'\[l\]egal interoperability' is understood ... as the
ability of the forces of two or more nations to operate effectively
together in the execution of assigned missions and tasks and with full
respect for their legal obligations, notwithstanding the fact that
nations concerned have varying legal obligations and varying
interpretations of these obligations." 5 Legal interoperability relies
on a broad understanding of those areas of legal divergence amongst
Allies, which requires careful analysis of the legal positions expressed
by the relevant Nations[. In the case of new technologies, this
understanding is difficult to acquire, in part because Nations have not
had the opportunity to set forth their legal views or have refrained
from doing so in order to prevent the loss of a potential technological
edge. Within NATO, legal interoperability has proven to be indispensable
to rapid decision-making and is facilitated by Allies' pragmatism when
faced with questions of collective security, emphasizing and relying on
those elements that are common to all rather than on those elements of
divergence.]{.underline}

### Solvency - Cooperation

#### NATO sets the Standards for AI Ethics - cooperation is key to setting Operational Norms for responsible AI. 

**Trabucco and Stanley-Lockman, 2022 -- prof of Political Science,
University of Copenhagen and prof of Defense and Strategic Studies,
Nanyang Technological University** \[Lena and Zoe, The Oxford Handbook
of AI Governance, March, "NATO's Role in Responsible AI Governance in
Military Affairs"
https://www.oxfordhandbooks.com/view/10.1093/oxfordhb/9780197579329.001.0001/oxfordhb-9780197579329-e-69
Acc 4/16/22 TA\]

NATO's Mechanisms to Govern AI [NATO's increasing interest in EDTs
introduces the need to consider how governance priorities can help
reinforce the Alliance's influence.]{.underline} The STS and military
innovation literature provide the theoretical foundations for NATO's
stewardship of AI as they place attention on "the role that institutions
play in shaping technological trajectories."45 [As AI development
continues, the actions that NATO and its members take will have
important implications for their capacity to adopt, respond to, and
shape their future operating environment. Particularly for democracies,
this confers to military stakeholders a dual responsibility to prevent
and manage risks, as well as to proactively shape their approach to
technological development anchored in democratic values and security. As
a multinational alliance with an incentive to drive cooperation and
alignment, NATO is situated to define and operationalize norms, as well
as promote standards that help shape the contours of future military
effectiveness and technological competition.]{.underline} In a RRI
framework, not only is this an institutional role, but it also becomes
an institutional responsibility. To apply this responsibility to NATO's
stewardship of AI, the institutional interplay between technology,
structure, and concepts is a form of socio-technical system with
important implications for AI governance because they link the ways that
an institution uses its power to adopt and shape AI trajectory to its
respective ends. Already, several mechanisms are built into military
bureaucracies to ensure that technology is adopted in alignment with
responsible engineering practices and responsible state behavior.46 [The
Alliance is organized to harmonize between Allies so that their
contributions enhance military effectiveness and political cohesion
between like-minded democracies.]{.underline} We argue [that these
effectiveness-centric mechanisms likewise empower NATO to exert its
influence in technology governance]{.underline}. More specifically,
[this entails the Alliance helping steward technological development for
a more predictable strategic environment and enhanced democratic clout
around the exploitation of technology reinforcing rule of
law]{.underline}. For NATO, we focus on strategic and policy planning,
as well as standards and certification because they reflect the
Alliance's particular strengths and interests in S&T. These practices
are relevant to governance insofar as they exemplify an institution's
power to shape the trajectory of technological development---but this
selection is by no means exhaustive.47

#### Empirically -- NATO cooperation can overcome the challenges AI poses to the alliance.

**Lin-Greenberg, 2020 - member of the MIT Security Studies Program**
\[Erik Vol 3, Iss 2 Spring, Texas National Security Review "Allies and
Artificial Intelligence: Obstacles to Operations and Decision-Making"
http://dx.doi.org/10.26153/tsw/8866 Acc 4/22/22 TA\]

[States are racing to achieve superiority in the AI domain, and AI
research and development is flourishing: In early 2019, the U.S.
Department of Defense unveiled its AI strategy]{.underline}.5
[Meanwhile, China has pledged to develop a \$150 billion AI
sector]{.underline} by 2030,6 [and Russian President Vladimir Putin
famously asserted, "whoever becomes the leader in \[AI\] will become the
ruler of the world."]{.underline}7 AI development promises to bring
enhanced accuracy and efficiency to complex and dangerous tasks, but
policymakers and scholars have yet to fully explore how these benefits
compare with potential risks --- particularly in the context of
multinational military operations.8 To be sure, decision-makers have
expressed concerns about the reliability of AI technologies and the
ethical implications of delegating military operations to computers.9
These [AI-specific challenges]{.underline}, however, [may magnify the
coordination and commitment challenges that frequently plague military
operations conducted by multinational alliances]{.underline} and
coalitions. Drawing from theories of alliance politics and analysis of
emerging AI technologies, I map out two areas where AI could hamper
multinational military operations. [First, AI could pose challenges to
operational coordination by complicating]{.underline} burden-sharing and
[the interoperability of multinational forces. Not all alliance or
coalition members will possess AI capabilities, raising barriers to
military cooperation]{.underline} as AI-enabled warfare becomes
increasingly common. States with AI technologies will also need to
overcome political barriers to sharing the sensitive data required to
develop and operate AI-enabled systems. At the same time, rivals can
stymie multinational coordination by using AI to launch deception
campaigns aimed at interfering with an alliance's military
command-and-control processes. [Second, AI could hamper
alliance]{.underline} and coalition [decision-making by straining the
processes]{.underline} and relationships [that undergird decisions on
the use of force. By increasing the speed of warfare, AI could decrease
the time leaders,]{.underline} from the tactical to strategic levels,
[have to]{.underline} debate policies and [make decisions. These
compressed timelines may not allow for the complex negotiations and
compromises that are defining characteristics of alliance
politics]{.underline}.10 [Decision-making may be further hampered if the
"black box" and unexplainable nature of AI causes leaders to lack
confidence in AI-enabled system]{.underline}s. And, just as adversaries
could use AI to interfere with command and control, they could also use
AI to launch misinformation campaigns that sow discord among allies and
heighten fears that allies will renege on their commitments. To be sure,
barriers to multinational military cooperation are not new, but AI may
intensify these difficulties.11 [To help overcome these
obstacles]{.underline} to coordination and decision-making challenges,
[alliance]{.underline} and coalition [leaders can draw lessons from past
cases of successful cooperation]{.underline} and a growing corpus of
national-level AI strategies [to develop]{.underline} international
agreements and [standards that streamline the integration of AI into
multinational operations]{.underline}.

#### NATO cooperation is necessary for an ethical response -- coordination builds public support and addresses important details.

**Gilli, 2020 - Senior Researcher at the NATO Defense College**
\[Andrea, NDC Research Paper No.15 -- December ""NATO-Mation":
Strategies for Leading in the Age of Artificial Intelligence"
https://www.ndc.nato.int/news/news.php?icode=1514 Acc 4/21/22 TA\]

It is worth noting that NATO's role in the AI ethics sphere differs from
that of many other organizations, such as national governments and the
European Union, because NATO is not a regulatory body. NATO complies
with existing laws and regulations, including the Laws of War, which
nations and the international community created. This means that
regulatory and normative questions such as the development and
deployment of autonomous weapons systems will not be determined at NATO
level. Nevertheless, [there is still room for NATO to play a clear
role.]{.underline} Indeed, [given doubts and worries about the adoption
of AI for military purposes, NATO can help generate more public support
and engagement by clearly defining ethical boundaries and moral
guidelines.]{.underline} The uses of AI in military operations --
ranging from logistics to maintenance, from recruitment to retainment,
from intelligence, surveillance and reconnaissance to medical tests and
medical evacuation, and more90 -- go beyond the discussions on lethal
autonomous weapons systems that have dominated European debate about AI
in military affairs. Accordingly, [the range of ethical questions
relevant to NATO extend beyond focusing on the tip of the spear.
Seemingly mundane uses of AI, such as in human resources or decision
support, can still pose distinct ethical questions the Alliance should
be prepared to handle]{.underline}. Addressing security risks,91
minimizing bias in systems,92 developing trust,93 and respecting privacy
are fundamental tasks for the Alliance to ensure the future
effectiveness of AI, whether in battle or in other functions. [The age
of intelligent machines requires the Alliance to reiterate its
commitment to values as new moral and ethical questions emerge, because
algorithms do not have a conscience, personal preferences or moral
agency]{.underline}. These statistical machines have no understanding of
good and bad, or fair and unjust.94 All an algorithm can do is achieve
its human-defined reward function, not provide any context or
information on whether the right question is being asked.95 Instead of
giving moral agency to algorithms, humans and organizations can view AI
as a "moral entity". This means that we humans are dutybound to adhere
to our moral code of conduct when interacting with the systems, rather
than shirking human responsibility to computers.96 At the organizational
level, [this means that the design, development and deployment of AI
should be "ethically aligned" with the Alliance's values and
goals.97]{.underline}

### Solvency -- Public Support

#### NATO discussions are key to overcome lack of public knowledge about AI policies -- this addresses concerns about AI weapons.

**Stanley-Lockman, 2021 - Center for Security and Emerging Technology**
\[Zoe CSET Issue Brief August "Responsible and Ethical Military AI
Allies and Allied Perspectives"
https://cset.georgetown.edu/publication/responsible-and-ethical-military-ai/
Acc. 4/21/22 TA\]

[A related caveat is that some allies are either in the midst of
constructing, or choosing to not publicize, their approaches to ethical
and responsible AI.]{.underline} As such, allies' formulations of
responsible military AI should be seen as evolving processes. [The
narrow availability of information on AI ethics beyond autonomy in
weapons may reflect political sensitivities, including cultural
differences around how transparent defense ministries and armed forces
are.]{.underline} As such, [the hope is to fill this gap by addressing
how allies conceive of AI ethics for defense.]{.underline}

#### Collaboration builds trust which prevents a political backlash that threatens NATO operations.

**van Weel, 2021 -- Assistant Secretary General for Emerging Security
Challenges, NATO** \[David, Dec 7, "Artificial intelligence: Can we go
from chaos to cooperation?" AEI Panel Discussion - Moderator: Elisabeth
Braw
https://www.aei.org/events/artificial-intelligence-can-we-go-fromchaos-to-cooperation/
Acc 5/11/22 TA\]

Now, in doing so, [we have to bridge a gap of distrust. So, I've been on
panels quite a lot where people say, "Well, please, I don't trust the
defense use of artificial intelligence." And that's something we need to
address. We are a trusted user. We, NATO, all the 30 allies, we all
subscribe to the democratic values, and we all subscribe to the same
values that our societies are built upon. And we're there to protect it.
So in using AI, in developing AI, we have to ensure and make clear to
both the private sector and to our societies that we are a trusted user
of artificial intelligence, that we intend to use AI by the standards
that we adhere to --- all our weapons systems and]{.underline} the use
of force in defending ourselves. And that's why the AI strategy that we
just devised came up. And that's why we made it public to a large
extent. We are not known at NATO for publishing a lot. We try to keep
secrets a lot. In this case, I really advocated that that is kind of
against the whole purpose of having this AI strategy. [The purpose of
this AI strategy is to gain a better understanding of]{.underline}
what's out there, to prove that we are willing to engage with [this
innovative new ecosystems]{.underline}, in order to look at the defense
purposes of AI. [Also, look at the threats of the misuse of AI by
adversaries, making sure that certain technology doesn't leak away to
those who might not adhere to the same standards that we
have]{.underline}. And then [developing this all in a responsible
way.]{.underline}

### Solvency - CBMs

#### A statement of ethical principles is a Confidence Building Measure for AI -- it signals our allies and values, which helps improve understanding

**Horowitz and Scharre, 2021 - Senior Fellows at the Technology and
National Security Program at the Center for a New American Security**
\[Michael and Paul, Jan 12, "AI and International Stability: Risks and
Confidence-Building Measures"
[https://www.cnas.org/publications/reports/ai-and-international-stability-risks-and-confidence-building-measures
Acc
6/6/22](https://www.cnas.org/publications/reports/ai-and-international-stability-risks-and-confidence-building-measures%20Acc%206/6/22)
TA\]

Code of Conduct [Nations could agree to a written set of rules or
principles for how they adopt AI into military systems. These rules and
principles,]{.underline} even if not legally binding, [could
nevertheless serve a valuable signaling and coordination function to
avoid some of the risks in AI adoption]{.underline}. A code of conduct,
statement of principles, or other agreement could include a wide range
of both general and specific statements, including potentially on any or
all of the confidence-building measures listed above. Even if countries
cannot agree on specific details beyond promoting safe and responsible
military use of AI, more [general statements could nevertheless be
valuable in signaling to other nations some degree of mutual
understanding about responsible use of military AI and help create
positive norms of behavior]{.underline}. [Ideally, a code of conduct
would have support from a wide range of countries and major military
powers]{.underline}. However, if this were not possible, then a
multilateral statement of principles from like-minded countries could
still have some value in increasing transparency and promulgating norms
of responsible state behavior.

#### CBMs promote the responsible use of AI -- they create international norms, gain the attention of leaders, and promote bureaucratic change.

**Horowitz and Scharre, 2021 - Senior Fellows at the Technology and
National Security Program at the Center for a New American Security**
\[Michael and Paul, Jan 12, "AI and International Stability: Risks and
Confidence-Building Measures"
[https://www.cnas.org/publications/reports/ai-and-international-stability-risks-and-confidence-building-measures
Acc
6/6/22](https://www.cnas.org/publications/reports/ai-and-international-stability-risks-and-confidence-building-measures%20Acc%206/6/22)
TA\]

[CBMs may be a useful tool for managing risks relating to military AI
applications.]{.underline} There are a number of possible CBMs that
states could adopt that may help mitigate the various AI-related risks
previously outlined. [These include]{.underline} broad CBMs applicable
to AI as a category, [CBMs designed to address some of the limitations
of AI]{.underline}, and CBMs focused on specific missions for which
militaries might use AI.49 Broad CBMs These CBMs focus broadly on
mechanisms for dialogue and agreement surrounding military uses of AI,
rather than the specific content of agreements. Given that a key goal of
CBMs is to enhance trust, mechanisms that serve as a building block for
more substantive dialogue and agreement can, in some cases, be an end in
themselves and not just a means to an end.50 [These could include
promoting international norms for how nations develop]{.underline} and
use [military AI systems]{.underline}, Track II academic-to-academic
exchanges, direct military-to-military dialogues, and agreements between
states regarding military AI, such as a code of conduct or mutual
statement of principles. Promoting Norms In 2019, the U.S. Defense
Innovation Board proposed a set of AI principles for the U.S. Defense
Department, which DoD subsequently adopted in early 2020. While these
principles no doubt have domestic audiences in the U.S. defense
community and tech sector, they also serve as an early example of a
state promulgating norms about appropriate use of AI in military
applications. The DoD AI principles included a requirement that DoD AI
systems be responsible, equitable, traceable, reliable, and
governable.51 (The full set of DoD AI principles is included in the
Appendix). Similarly, the DoD's unclassified summary of its AI strategy,
released in 2019, called for building AI systems that were "resilient,
robust, reliable, and secure."52 A focus of the strategy was "leading in
military ethics and AI safety."53 [There is value in states promoting
norms for responsible use of AI, including adopting and employing
technology in a way that reflects an understanding of the technical
risks associated with AI systems]{.underline}. While stating such
principles is not the same as putting in place effective bureaucratic
processes to ensure their compliance, [there is]{.underline}
nevertheless [value in states publicly signaling to others]{.underline}
([and to their own bureaucracies) the importance of using AI responsibly
in military applications]{.underline}. While these norms are at a high
level, [they]{.underline} nevertheless [signal some degree of attention
by senior military and civilian defense officials to some of the risks
of AI systems, including issues surrounding safety, security,
responsibility, and controllability]{.underline}. [These signals may aid
internal bureaucratic efforts to mitigate various AI-related risks, as
bureaucratic actors can point to these official documents for
support.]{.underline} Additionally, to the extent that other nations
find these statements credible, they may help signal to other nations at
least some degree of awareness and attention to these risks, helping to
incentivize others to do the same.

#### CBMs prevent accidental lunch during a crisis through information sharing.

**Horowitz and Scharre, 2021 - Senior Fellows at the Technology and
National Security Program at the Center for a New American Security**
\[Michael and Paul, Jan 12, "AI and International Stability: Risks and
Confidence-Building Measures"
[https://www.cnas.org/publications/reports/ai-and-international-stability-risks-and-confidence-building-measures
Acc
6/6/22](https://www.cnas.org/publications/reports/ai-and-international-stability-risks-and-confidence-building-measures%20Acc%206/6/22)
TA\]

Executive Summary Militaries around the world believe that the
integration of machine learning methods throughout their forces could
improve their effectiveness. From algorithms to aid in recruiting and
promotion, to those designed for surveillance and early warning, to
those used directly on the battlefield, [applications of artificial
intelligence (AI) could shape the future character of warfare. These
uses could also generate significant risks for international stability.
These risks relate to broad facets of AI that could shape warfare,
limits to machine learning methods that could increase the risks of
inadvertent conflict,]{.underline} [and]{.underline} specific mission
areas, such as [nuclear operations, where the use of AI could be
dangerous. To reduce these risks and promote international stability, we
explore the potential use of confidence-building measures
(CBMs),]{.underline} constructed around the shared interests that all
countries have in preventing inadvertent war. Though not a panacea,
[CBMs could create standards for information-sharing and notifications
about AI-enabled systems that make inadvertent conflict less
likely]{.underline}.

### Solvency -- Global Model

#### Without cooperative solutions, NATO will lose its leverage to create global norms for other countries to follow. 

**Gilli, 2020 - Senior Researcher at the NATO Defense College**
\[Andrea, NDC Research Paper No.15 -- December ""NATO-Mation":
Strategies for Leading in the Age of Artificial Intelligence"
https://www.ndc.nato.int/news/news.php?icode=1514 Acc 4/21/22 TA\]

[The increasing power of processors]{.underline}, accuracy of
algorithms, and availability of digital data [are driving
the]{.underline} dramatic artificial intelligence ([AI)-centred
technological transformation]{.underline} now in progress. These changes
have already turned companies, industries and markets upside down, and
we are also starting to see their effects on the battlefield. The
employment of unmanned vehicles, reliance on big data for target
detection, identification and acquisition, as well as the potentials of
machine learning in other critical functions such as logistics and
maintenance are only some of the possible examples of how warfare will
evolve in the near future. The North Atlantic Treaty Organization
[(NATO) and its Allies cannot be bystanders during this technological
transition]{.underline}. [Some countries and organizations have already
taken important steps. Others are more hesitant.]{.underline} [The
Atlantic Alliance has a moral obligation to act, both to preserve and
extend its military leadership]{.underline} -- and thus the wealth and
security of its citizens -- [and to shape this process in keeping with
its democratic principles,]{.underline} freedom-inspired values [and
commitment to fundamental human rights]{.underline}. As NATO works on
its Artificial Intelligence Strategy, which could be published in 2021,
this Research Paper aims at contributing to both the policy debate and
the public discussion about AI and its implications for the Alliance.
The paper offers a series of analyses, lessons learned, proposals, and
recommendations, that build on best practices and solutions adopted in
the civilian and military fields, on perspectives drawn from the
academic literature as well as on ideas generated in the broader AI
community. The various parts of the paper are all linked to the single
overarching concept of ["NATO-mation", or the idea that]{.underline}
[NATO has an important role to play so as to prepare for and to shape
this technological transformation. NATO Allies need to be proactive:
without common, coordinated, cooperative or joint solutions, they will
not be able to achieve all their goals effectively and
efficiently]{.underline}. The paper thus elaborates on the concept of
"NATO-mation" in 11 different building blocks, as summarized below: •
[Ethical purpose: first and foremost, NATO's strength comes from its
values, which give meaning to its military capabilities and represent
the bond between the Allies.]{.underline} As technological progress
demands answers to major ethical questions regarding the development,
employment and purpose of intelligent machines, [NATO and its Allies
have the opportunity to shape international norms and behaviour in this
respect]{.underline} while simultaneously strengthening their commitment
to the Alliance's founding principles. Setting up an ethical board of
experts and adopting some common ethical principles are two starting
points.

#### Ethical principles are key to global modeling -- moral claims shape the international environment.

**Gilli, 2020 - Senior Researcher at the NATO Defense College**
\[Andrea, NDC Research Paper No.15 -- December ""NATO-Mation":
Strategies for Leading in the Age of Artificial Intelligence"
https://www.ndc.nato.int/news/news.php?icode=1514 Acc 4/21/22 TA\]

Lead and shape with ethical principles. The proposed Ethics Board could
start its work from scratch, defining a new set of guiding ethical
principles for NATO and its Allies, or it could take a pragmatic
approach and borrow from the important work already done by several
institutions, including the OECD and the European Union.109 As recalled
above, work has already been done by international organizations such as
the European Union and the OECD, national governments, such as the US
Department of Defense and the French Ministry of the Armed Forces,
private companies like Google and IBM, as well as non-governmental
institutions like the Institute for Electrical and Electronics
Engineering.110 [Having clear and simple ethical principles is
important, both internally, as actors at different levels need to be
able to make choices, and externally, as NATO and its Allies may want
not only to signal their ethical and moral commitments to their own
citizenries]{.underline}, including developers and civil society, [but
also to shape the international environment]{.underline}. [The priority
is to ensure that NATO Allies adopt clear ethical guidelines which
reflect their values]{.underline} -- [such as]{.underline} democratic
representation, [civilian control of the armed forces]{.underline},
individual responsibility, [the centrality of human life, and compliance
with the Laws of War -- and that such values inform international norms,
practices, agreements and possible future treaties]{.underline}. Leading
in this realm means moulding the future security environment, in
particular by embedding democratic values into this pervasive
technology.111 What ethical principles should NATO adopt when it comes
to AI? This will be up to the Ethics Board, if established, or to other
authorities. As recalled, there is, however, a general agreement on some
principles:112

#### Allied approaches to AI Ethics foster global modelling by creating a Responsible AI Ecosystem.

**Stanley-Lockman, 2021 - Center for Security and Emerging Technology**
\[Zoe CSET Issue Brief August "Responsible and Ethical Military AI
Allies and Allied Perspectives"
https://cset.georgetown.edu/publication/responsible-and-ethical-military-ai/
Acc. 4/21/22 TA\]

[To facilitate comparisons with allies' approaches to AI ethics for
defense, two taskings]{.underline} of the RAI Working Council
[are]{.underline} particularly [relevant. The first is the broad,
overall aim of leading responsible AI globally. To this end, creating a
"Responsible AI Ecosystem" is one of the foundational tenets of RAI
implementation]{.underline}.32 Here, [international components of this
ecosystem include allies and partners to enable better multi-stakeholder
collaboration and to also advance norm development "grounded in shared
values."]{.underline}33 [With the overall aim of leading responsible AI
globally, this foundational tenet]{.underline} is the only one that
[explicitly names international engagement.]{.underline} Yet other
foundational tenets are also relevant to assess convergences and
divergences, including: governance structures for accountability; trust
based on testing, evaluation, validation, and verification (TEVV); and a
whole-of lifecycle approach to risk management.34

#### The DOD must lead our allies on AI policy to raise awareness and perception of ethical issues.

**Stanley-Lockman, 2021 - Center for Security and Emerging Technology**
\[Zoe CSET Issue Brief August "Responsible and Ethical Military AI
Allies and Allied Perspectives"
https://cset.georgetown.edu/publication/responsible-and-ethical-military-ai/
Acc. 4/21/22 TA\]

The key takeaways are as follows: • [DOD remains the leader in
developing an approach to ethical AI for defense. This first-mover
position situates the JAIC well to lead international engagements on
responsible military AI.]{.underline} • Allies fall on a spectrum from
articulated (France, Australia), to emerging (the U.K., Canada), to
nascent (Germany, the Netherlands) views on ethical and responsible AI
in defense. These are flexible categories that reflect the availability
of public documents. • [Multilateral institutions]{.underline} also
[influence how countries perceive and implement AI ethics in defense.
NATO and JAIC's AI Partnership for Defense (PfD) are important venues
pursuing responsible military AI agendas,]{.underline} while the
European Union and Five Eyes have relevant, but relatively less defined,
roles. • [Areas of convergence among allies' views of ethics in military
AI include the need to comply with existing ethical and legal
frameworks, maintain human centricity]{.underline}, identify ethical
risks in the design phase, and implement technical measures over the
course of the AI lifecycle to mitigate that risk. • There are fewer
areas of divergence, which primarily pertain to the ways that allies
import select civilian components of AI accountability and trust into
their defense frameworks. These should be tracked to ensure they do not
imperil future political cohesion and coalition success. • [Pathways for
leveraging shared views and minimizing the possibility that divergence
will cause problems include using multilateral formats to align views on
ethics, safety, security]{.underline}, and normative aspects.

### Solvency - Dialogue

#### Dialogue with allies can overcome differences over the ethical use of AI in defense -- discussion moves principles into practical implementation. 

**Stanley-Lockman, 2021 - Center for Security and Emerging Technology**
\[Zoe CSET Issue Brief August "Responsible and Ethical Military AI
Allies and Allied Perspectives"
https://cset.georgetown.edu/publication/responsible-and-ethical-military-ai/
Acc. 4/21/22 TA\]

Additionally, differences between allies' views on responsible and
ethical AI in defense may also stem from the extent to which other
countries apply civilian AI policy and regulation frameworks to their
own defense approaches. Although overviews of government principles and
policies for ethical civilian AI are not discussed at length here, they
become more visible in other countries, as well as the EU. Some of these
aspects are encouraging---for example, German industry's voluntary
compliance with trustworthy AI principles, to the extent they overlap
with defense, due in part to the fact that there is no equivalent
defense framework they can follow and implement themselves. Other entry
points of civilian concepts into the defense realm include the
Australian concept of contestability, Canadian DND subjectivity to the
algorithmic impact assessment, and the choice of some EU countries to
apply the General Data Protection Regulation to their defense sectors.
These are both related to accountability as well as privacy, which are
key differences that should be tracked even though [allies
overwhelmingly agree on the importance of ethics and safety for AI in
defense.]{.underline} Still, [similarities between defense stakeholders
include the view that militaries could not only inject new risks into
their operating environments, but also expose their own organizations to
risk if they leave ethical and legal questions unaddressed. Countries
may have different ways to define and measure these ethical
risks]{.underline}, as Table 1 implies, and as is detailed in the
appendices. [Overall, though, there is an implicitly common approach
which recognizes that they must contend with the associated technical,
legal, political, and moral risks from the front end of AI
development]{.underline}. [More concretely, they also agree that the way
to implement responsible AI involves]{.underline} technical measures
tied to safety and security, as well as [procedures that make the legal
context by which they abide as clear as possible]{.underline}. Overall,
[pathways for leveraging shared views to advance the implementation of
responsible AI should include learning from allies whose views emphasize
both human responsibility and responsible state behavior]{.underline}
extending beyond minimum legal obligations. Addressing these issues is
not only a question of good engineering practices, but is also an
exercise of responsible state behavior.188 In a narrow sense,
responsible AI can refer to ensuring that AI systems enter into
human-centric frameworks that are defined by humans to maintain human
agency and responsibility. More broadly, though, it is notable that some
allies see preserving freedom of action as part of a vision of
responsible AI that encompasses responsible state behavior. Having a
legitimate basis for military action is a feature of responsible state
behavior, with civilian government oversight of militaries at its core
to maintain accountability at home and abroad. This enters into the
language of responsibility because operating in coalitions under
multinational mandates can also confer international political
legitimacy to operations.189

#### Dialogue is key to collaboration on AI -- it is the first step to cooperation because it builds on common grounds. 

**Fu, 2021 -- Professor of Technology and International Development,
University of Oxford** \[Xiaolan, Dec 7, "Artificial intelligence: Can
we go from chaos to cooperation?" AEI Panel Discussion - Moderator:
Elisabeth Braw
https://www.aei.org/events/artificial-intelligence-can-we-go-fromchaos-to-cooperation/
Acc 5/11/22 TA\]

And [another very important challenge]{.underline} that we have [to
address]{.underline}, I mean, I think David will mention, [is the AI
application in defense]{.underline}. And [we]{.underline} really
[need]{.underline} some, you know, global treaty [to make sure we
demilitarize and de-weaponize the application of artificial
intelligence]{.underline}. So, [there are a lot of common grounds
between countries]{.underline}. [Europe]{.underline} --- Jonathan will
talk about [North America]{.underline}. And also, they [are
the]{.underline} three [major leaders in the world in artificial
intelligence.]{.underline} So, [based on this common
interest]{.underline} and the common grounds, I think [to
collaborate]{.underline}, work together. Elisabeth, I think today's AI
initiative is so great. I really liked the title, you know,
["collaboration," "cooperation"]{.underline} put in the title. So work
together [to make sure AI used for good]{.underline}, to make good for
the society and the community. And we need to take action. [Dialogue is
the first step and to set up the mechanisms to facilitate the
dialogue]{.underline}. And even debates would be very important. Let me
finish here. And so, yeah, that's all from me. Thank you.

#### NATO dialogue on AI improves its credibility -- this is crucial for harmonization and interoperability

**Trabucco and Stanley-Lockman, 2022 -- prof of Political Science,
University of Copenhagen and prof of Defense and Strategic Studies,
Nanyang Technological University** \[Lena and Zoe, The Oxford Handbook
of AI Governance, March, "NATO's Role in Responsible AI Governance in
Military Affairs"
https://www.oxfordhandbooks.com/view/10.1093/oxfordhb/9780197579329.001.0001/oxfordhb-9780197579329-e-69
Acc 4/16/22 TA\]

Standards and certification [To maintain its relevance in a security
architecture increasingly concerned with the way that technology shifts
power dynamics and scales threats to international security, NATO has an
incentive to foster cooperation, promote standards of practice, and
incentivize Allied AI harmonization. It is strategically salient to
facilitate a dialogue and engagement among Allies on AI, but it is
practically important to use NATO's position to facilitate Allied
cooperation regarding standards to project the Alliance's ability to
interoperate in future operations. NATO standards aim to enhance
interoperability among partners and successful implementation of
strategy.]{.underline} More specifically, [standards and certification
are used to establish and implement requirements aligned with safe
development and responsible use of technology.]{.underline} In addition
to purely technical standards, NATO has operational standards that
specify "conceptual, organizational or methodological requirements to
enable materiel, installations, organizations or forces to fulfil their
functions or missions."51 In line with the definitions from STS and
military innovation scholarship, standards can thus be seen as a
mechanism to translate responsibility-derived state and organizational
AI policy into actionable functions. In fact, [NATO has set certain
standards for the Allies and these standards subsequently become the
norm]{.underline}.

#### Dialogue clarifies Legal Interoperability which is key to cohesive action

**Hill and Marsan, 2018 - Director and Senior Assistant, NATO Office of
Legal Affairs** \[Steven and Nadia, 7-18-18 "Artificial Intelligence and
Accountability: A Multinational Legal Perspective"
https://www.sto.nato.int/publications/STO%20Meeting%20Proceedings/STO-MP-IST-160/MP-IST-160-PP-4.pdf
Acc 4/21/22 TA\]

ABSTRACT This paper supports the specialist meeting on Big Data and
Artificial Intelligence for Military Decision Making by exploring the
legal implications of new technology on NATO decision making. The paper
begins by presenting [the concept of "legal interoperability," one of
the tools that legal advisers working in NATO seek to
promote]{.underline}. It then introduces some of the current legal
issues and debates surrounding the development and use of AI, including
the difficulty in defining key concepts arising out of the increased use
of AI such as "autonomy," and questions pertaining to accountability.
Finally, this paper examines how [further dialogue among Allies and with
NATO partners can contribute to the development of a reliable approach
on accountability issues related to AI-enabled technology]{.underline}.
The paper argues that [given the rapidly evolving technology and the
asymmetric approach and capabilities of nations, efforts within the
Alliance should focus on ensuring NATO's "legal preparedness" so that
collective action is not thwarted by legal hurdles and mismatched legal
approaches amongst Allies.]{.underline}

### NATO says Yes

#### Allies will follow US leadership on AI -- they want defense cooperation

**Kahn and Horowitz, 2021 -- Research and Senior Fellows at the Council
on Foreign Relations** \[Lauren and Michael, The Washington Quarterly
44:4 "Leading in Artificial Intelligence through Confidence Building
Measures" [https://doi.org/10.1080/0163660X.2021.2018794 Acc
6/6/22](https://doi.org/10.1080/0163660X.2021.2018794%20Acc%206/6/22)
TA\]

[A]{.underline}n informal [multilateral agreement could be proposed and
opened for signature to all nations. If the United States leads,
American allies and partners would be likely to sign on, both due to the
impact of American leadership in shaping attitudes and the likely
perception that following US-led principles would facilitate defense
cooperation in this area. One might argue that]{.underline} US
leadership in multilateral [AI standards]{.underline}
[could]{.underline} create risks for the United States if it involves
commitments that [prevent the US deployment]{.underline} and use [of
militarily important AI-enabled systems. These risks, however, are
minimal]{.underline}. [The standards the United States would promote
involve commitments to]{.underline} international humanitarian law and
[responsible behavior that the United States already follows when it
comes to the development and use of military systems. Thus, it would not
require changes in US behavior that might slow down responsible military
AI adoption.]{.underline}

#### NATO would support the plan -- they are looking for common ground on ethical principles

**Sprenger, 2021 - Europe editor for Defense News** \[Sebastian, Apr 27,
"NATO tees up negotiations on artificial intelligence in weapons"
https://www.c4isrnet.com/artificial-intelligence/2021/04/27/nato-tees-up-negotiations-on-artificial-intelligence-in-weapons/
Acc 4/22/22 TA\]

[NATO is seeking common ground on artificial intelligence in defense
applications]{.underline} ahead of a strategy document this summer.
(MF3d/Getty Images) COLOGNE, Germany --- NATO officials are kicking
around a new set of questions for member states on artificial
intelligence in defense applications, as the alliance seeks common
ground ahead of a strategy document planned for this summer. [The move
comes amid a grand effort to sharpen NATO's edge in what officials call
emerging and disruptive technologies]{.underline}, or EDT. [Autonomous
and artificial intelligence-enabled weaponry is a key element in that
push, aimed at ensuring tech leadership on a global scale.]{.underline}
Exactly where the alliance falls on the spectrum between permitting
AI-powered defense technology in some applications and disavowing it in
others is expected to be a hotly debated topic in the run-up to the June
14 NATO summit. ["We have agreed that we need principles of responsible
use, but we're also in the process of delineating specific
technologies," David van Weel, the alliance's assistant
secretary-general for emerging security challenges, said]{.underline} at
a web event earlier this month organized by the Estonian Defence
Ministry. [Different rules could apply to different systems depending on
their intended use and the level of autonomy involved,]{.underline} he
said. For example, an algorithm sifting through data as part of a
back-office operation at NATO headquarters in Brussels would be
subjected to a different level of scrutiny than an autonomous weapon. In
addition, rules are in the works for industry to understand the
requirements involved in making systems adhere to a future NATO policy
on artificial intelligence. The idea is to present a menu of
quantifiable principles for companies to determine what their products
can live up to, van Weel said. F[or now, alliance officials are teeing
up questions to guide the upcoming discussion]{.underline}, he added.
[Those range from basic introspections about whether AI-enabled systems
fall under NATO's "legal mandates," van Weel explained, to whether a
given system is free of bias,]{.underline} meaning if its
decision-making tilts in a particular direction.

#### NATO support is increasing -- recent visits prove.

**Lawrence and Cordey, 2020 -- researchers for The Cyber Project at the
Belfer Center for Science and International Affairs** \[Christie and
Sean, August, The Cyber Project Paper "The Case for Increased
Transatlantic Cooperation on Artificial Intelligence Edited by Lauren
Zabierek and Julia Voo
https://www.belfercenter.org/sites/default/files/2020-08/TransatlanticAI.pdf
Acc. 4/21/22 TA\]

[In recent months, however, interests and political support for greater
transatlantic coordination on AI seems to be increasing. This trend was
notably demonstrated by a visit from]{.underline} Lt. Gen. Jack
Shanahan---then Director of [the US Department of Defense's Joint
Artificial Intelligence Center]{.underline} (JAIC)---to Brussels in
January 2020 and a visit by the European Parliament's delegation to
Washington D.C in February 2020[. Both visits included discussions on AI
with a variety of key stakeholders, such as NATO]{.underline},
representatives from the US Congress, State Department, Federal Transit
Administration (FTA), Federal Bureau of Investigation (FBI), and Privacy
and Civil Liberties Oversight Board (PCLOB).34

#### Europe supports cooperation on ethics -- EU actions prove

**Lawrence and Cordey, 2020 -- researchers for The Cyber Project at the
Belfer Center for Science and International Affairs** \[Christie and
Sean, August, The Cyber Project Paper "The Case for Increased
Transatlantic Cooperation on Artificial Intelligence Edited by Lauren
Zabierek and Julia Voo
https://www.belfercenter.org/sites/default/files/2020-08/TransatlanticAI.pdf
Acc. 4/21/22 TA\]

European Union: The European Union, like the United States, intends to
leverage AI's potential as a strategic and transformative technology. 17
However[, the EU has positioned itself as a leader in trustworthy,
human-centric, ethical, and values-based AI,18 in comparison to the US
government's emphasis on the need for AI innovation to protect American
values]{.underline}, civil liberties, and privacy. The EU recognizes
that it trails behind the US and China in terms of volume of investment
and maturity of its tech industry.19 Nonetheless, [the EU believes it
can capitalize on its underlying structural strengths]{.underline}
(e.g., academic and innovation record) [and on its values to compete
globally and reaffirm its]{.underline} digital and [technological
sovereignty]{.underline}. 20 Starting with its 2018 Communication:
Artificial Intelligence for Europe,21, 22 [the European
Commission]{.underline} (EC) [has launched a coordinated effort
promoting AI]{.underline}.23 Policies include increasing public and
private investments from \$5.6 billion to \$22 billion annually;24
coordinating research and innovation across Europe; [devising ethical
guidelines]{.underline}; fostering digital skills in its workforce; and
promoting public and private sector adoption of AI.25 To support and
counsel these efforts, the EC has established the High-Level Expert
Group on AI (AI HLEG) comprising 52 experts who advise the Commission on
policy and regulatory changes.

### NATO is Key to Solvency

#### 1. [Consultation]{.underline} - NATO is key because it has unique consultation mechanisms that include industry partners.

**Allison, 2020 - has a degree in Cyber Security from Glasgow Caledonian
University** \[George, Nov 2 UK Defence Journal "Cooperation on AI will
'boost security' say NATO"
https://ukdefencejournal.org.uk/cooperation-on-ai-will-boost-security-say-nato/
Acc 4/22/22 TA\]

["There are considerable benefits of setting up a transatlantic digital
community cooperating on Artificial Intelligence (AI) and emerging and
disruptive technologies, where NATO can play a key role as a facilitator
for innovation and exchange", said NATO Deputy Secretary General Mircea
Geoană.]{.underline} On Wednesday he took part in a high-level virtual
discussion on transatlantic cooperation in the era of AI, organised by
the Atlantic Council's Future Europe Initiative and GeoTech Center.
Mircea Geoana, NATO DSG participated in a webstreamed meeting on
artificial intelligence AI NATO say that Mr. Geoană engaged in this
conversation alongside the Chair and Vice Chair of the National Security
Commission on Artificial Intelligence (NSCAI), Dr. Eric Schmidt and
Secretary Robert O. Work, and the Head of Cabinet of European Commission
Executive Vice-President Margrethe Vestager, Ambassador Kim Jørgensen.
They discussed what modern technologies mean for European and American
defence and security stakeholders, why the United States and the
European Union should cooperate on AI, and how best to promote shared
values in the field. ["NATO is a natural platform for transatlantic
cooperation of AI," the Deputy Secretary General underlined. "NATO
offers its consultative mechanisms and unique networks for collaboration
on defence and security questions. Bringing together Allies and
partners, public and private sector, innovators and industry. We have
great communities in areas like military capability development, science
and technology, standardisation -- and of course our Command Structure
and military exercises.]{.underline} We also have new cross-cutting
policy teams on Innovation Policy, who cover AI, and on Data Policy," he
pointed out.

#### NATO is key because it is best at facilitating coordination -- that is its core function. 

**Trabucco and Stanley-Lockman, 2022 -- prof of Political Science,
University of Copenhagen and prof of Defense and Strategic Studies,
Nanyang Technological University** \[Lena and Zoe, The Oxford Handbook
of AI Governance, March, "NATO's Role in Responsible AI Governance in
Military Affairs"
https://www.oxfordhandbooks.com/view/10.1093/oxfordhb/9780197579329.001.0001/oxfordhb-9780197579329-e-69
Acc 4/16/22 TA\]

Strategic and policy planning

NATO structures around strategic and policy planning both set Allied
ambitions and priorities and have the competency to implement them
through its many consultative bodies, coordination formats, and albeit
to a lesser extent, technology foresight capacities. [NATO has
facilitative power among Allies, both for defense planning and for the
conduct of operations]{.underline}. A cornerstone in modern architecture
of international security is coalition warfare---or, more broadly, joint
operations. [Working with military partners has become a critical
feature of modern security policy, where there is more power in
enhancing numbers, but also in having allies that lend political and
practical legitimacy to deterrence and operations.]{.underline}49 [NATO
is vital to that effort for many reasons, but also because NATO's
facilitative power is significant to promote coordination and
cooperation]{.underline}. Simply put, partners and allies are a
necessary feature of modern military behavior, and [strategic and policy
planning are necessary functions to encourage and underpin cohesion in
alliance settings. This is important for AI governance because the
nature of AI poses new strategic challenges and will require
multilateral approaches and]{.underline} some degree of
[cohesion]{.underline} to effectively incorporate RRI frameworks in
policy planning. As such, [the necessity of working with security
partners extends to the AI-policy frontier.]{.underline}

#### 2. [Rules of Engagement]{.underline} - NATO is Key because it establishes Rules of Engagement which are essential for ethical AI use.

**Vestner, 2021 - Head of Security and Law Programme at Geneva Centre
for Security Policy** \[Tobias, July 8 "Warfare and Artificial
Intelligence" in Robin Geiß and Henning Lahmann (eds), Research Handbook
on Warfare and Artificial Intelligence -forthcoming
https://www.gcsp.ch/publications/military-operations-and-artificial-intelligenceGCSP
Acc 5/27/22 TA\]

[ROE are appropriate tools to determine how to use AI under which
conditions for specific contexts and missions]{.underline}.
[ROE]{.underline} -- or related rules of behaviour -- [may set the
parameters for diverse military applications of AI, thereby translating
given political, military, legal, and ethical considerations and
limitations of documents at a higher echelon, such as
doctrine]{.underline} or international legal obligations, [into concrete
instructions. This can represent a framework for action to be programmed
into the AI system]{.underline}. For example, ROE could determine a
geographical zone or a certain list of potential tasks for which systems
are authorized to take action. Outside those limits, they would not act
on the processed information. Time checks or other limits, such as
pre-set permission to (not) engage specific targets, may also be fixed
by ROE.100 Similarly, ROE can foresee that a system needs to flag
unexpected events or issues. In this context, some have suggested that
AI may be able to choose which ROE to apply based on the environment or
its programmed mission.101 ROE can also define the interaction between
humans and AI systems for specific missions. In particular, ROE can
establish how a commander or operator needs to monitor and control the
system during deployment. As the need for human control may vary
according to the specific task attributed to an AI system and the
respective context and operation, ROE for AI can define the level of
autonomy for certain types of operations or phases thereof.102 ROE can
further address or refer to other sources, such as manuals and
directives, on how to implement various forms of human control, such as
direct, share, or supervisory control.103 [Importantly, ROE may limit
commanders' or operators' authority, which may force them to refer up in
the chain of command. This can be a significant role of ROE for the
human-machine teaming in military operations, notably when confronted
with unanticipated situations]{.underline} or issues for which the
system or its use had not been previously authorized. [ROE are
particularly relevant when AI is used for or in relation to targeting as
this implies harming persons and objects]{.underline}. Notably if
considered that [AI cannot incorporate ethical or contextual assessments
into its decision process,104 human control and judgment should be
meaningful]{.underline} in the context of decisions regarding the use of
lethal force.105 While most publicly available [policies establish this
principle]{.underline}, as described above, [they do not specify its
precise meaning. ROE and directives can fill this gap]{.underline}. To
this end, a code of conduct for operators of AI systems related to
targeting or a model of ROE for such systems could be established.106

#### 3. [Military Doctrine]{.underline} - NATO is key because it establishes Military Doctrine, which is essential for AI standards.

**Vestner, 2021 - Head of Security and Law Programme at Geneva Centre
for Security Policy** \[Tobias, July 8 "Warfare and Artificial
Intelligence" in Robin Geiß and Henning Lahmann (eds), Research Handbook
on Warfare and Artificial Intelligence -forthcoming
https://www.gcsp.ch/publications/military-operations-and-artificial-intelligenceGCSP
Acc 5/27/22 TA\]

To conclude, it is unlikely that AI will have a substantial function for
establishing military doctrine since it serves to define and regulate
military organizational issues and aspects of military operations which
strongly relate to beliefs, values, and identity. Yet because of this
function, [doctrine has an important role to define armed forces'
fundamental relation to AI.]{.underline} In particular, [doctrine is
appropriate for establishing in general terms for what tasks AI will
(not) be used, how AI will (not) be used, and how the
organization]{.underline} and its members [perceive and value
AI.]{.underline} Most importantly [given AI's characteristics, doctrine
can establish how humans can and should interact with AI]{.underline}
and what organizational culture should reign in this regard. This can
set the normative framework for further military directives and military
procedures. [States' emerging ethical guidelines may serve as a basis
and be incorporated into military doctrines.]{.underline} Produced [in
accordance with the respective military doctrine, military operation or
action plans are concepts]{.underline} and instructions [to achieve
military objectives in line with the available means]{.underline}. Plans
reflect the commander's intent and oftentimes include different courses
of action (COA). [A variety of military planning and decision-making
models exist. NATO's Comprehensive Operations Planning
Directive]{.underline} (COPD) [provides a good overview]{.underline}
[and synthesis]{.underline} of various Western models.69 The Canadian
Armed Forces, for instance, follow six steps, namely initiation,
orientation, concept development, decision plan development, and plan
review.70 According to a general description, planning consists of
'\[p\]lanning and scheduling the detailed tasks required to accomplish
the specified COA; \[a\]llocating tasks to the diverse forces \[...\];
\[a\]ssigning suitable locations and routes; \[e\]stimating friendly and
enemy battle losses (attrition); \[and p\]redicting enemy actions or
reactions.'71

#### Military Doctrine is essential for AI standards -- it is necessary to operationalize human control.

**Vestner, 2021 - Head of Security and Law Programme at Geneva Centre
for Security Policy** \[Tobias, July 8 "Warfare and Artificial
Intelligence" in Robin Geiß and Henning Lahmann (eds), Research Handbook
on Warfare and Artificial Intelligence -forthcoming
https://www.gcsp.ch/publications/military-operations-and-artificial-intelligenceGCSP
Acc 5/27/22 TA\]

Similarly, [doctrine is the appropriate tool to define ethical standards
for the development, acquisition, and use of AI systems]{.underline}.
[As military doctrines]{.underline} are drafted in accordance with
international law and generally [call upon members of armed forces to
respect international law, doctrine can also define the modalities for
AI systems and operators' compliance with international]{.underline}
[law]{.underline}. [As such, doctrine is an important tool to impose
constraints regarding AI and human-machine teaming which apply across
services to all members of the armed forces.]{.underline} [This can
imply the general need for meaningful human control of AI
systems]{.underline} or the prohibition of the delegation of certain
functions to AI systems. More specifically[, doctrine can set the
principles and parameters for the integration of AI into organizational
processes.]{.underline} For example, AI systems working on the
consolidation, prioritization, and framing of data are likely to require
revised military doctrine and guidelines on armed forces' use and
collection of information.52 While systems whose tasks are limited to
observation would require limited doctrinal adjustments, systems that
have more 'active' tasks will likely necessitate more specific
guidelines on elements such as safeguards, degree of autonomy, and
communication with the operator as well as on their interaction with
human forces, including human-machine teaming.53 Furthermore, it has
been argued that [tactical applications generally make rule-based
decisions, whereas operational and strategic decisions are often
value-based. In this case]{.underline}, what type of decision-making
process is preferred at each level, and [whether it should be
standardized among all systems are questions that should be explored at
the doctrinal level.]{.underline}54

#### 4. [Existing Agreements]{.underline} - NATO is key to harmonization because of existing standardization agreements

**Stanley-Lockman, 2021 - Center for Security and Emerging Technology**
\[Zoe CSET Issue Brief August "Responsible and Ethical Military AI
Allies and Allied Perspectives"
https://cset.georgetown.edu/publication/responsible-and-ethical-military-ai/
Acc. 4/21/22 TA\]

While the issuance of principles and commitment to uphold shared values
through responsible use are undoubtedly important, it is worth noting
that [NATO does have different considerations from nations (and
militaries) as a supranational, non-regulatory body. Its more unique
contribution could be standardization, as already incorporated into
standardization agreements]{.underline} [and training publications---the
latter of which could focus on responsible AI in training.]{.underline}
In anticipation of forthcoming guidelines or standards for industry, it
is worth noting that NATO operational standardization also encompasses
work implementing the Law of Armed Conflict into operational
practice.160 [For NATO]{.underline}, building on General Mercier's
remarks that ethics and interoperability are linked, [standardization on
safe and ethical AI can]{.underline} likewise [link to interoperability
at the technical and procedural levels.161]{.underline}

#### NATO is key -- it best translates principles into action

**Gilli, 2020 - Senior Researcher at the NATO Defense College**
\[Andrea, NDC Research Paper No.15 -- December ""NATO-Mation":
Strategies for Leading in the Age of Artificial Intelligence"
https://www.ndc.nato.int/news/news.php?icode=1514 Acc 4/21/22 TA\]

Implementation and execution. [The transition from principles to action
is critical in any activity. Ethics is no exception, and while issuing
comprehensive principles is difficult]{.underline} in any organisation,
[ensuring their implementation is an even greater challenge]{.underline}
so as to strike the fine line between ethics washing and ethics
"bashing".113 Indeed, governance is an important factor in ensuring that
technology is used responsibly and in line with the Alliance's desired
outcomes.114 [In this respect, NATO may have a relevant role in
supporting its Allies. A dedicated organization]{.underline}, such as an
Artificial Intelligence Integration & Implementation-Enabling Centre
(next chapter), [could prove particularly useful, not least because it
could help Allies share]{.underline} concerns, considerations[,
solutions and best practices as well as support them with]{.underline}
ad hoc [training]{.underline} activities.115 Similarly, [aligned
principles may eventually be translated into standards -- and related
measures such as benchmarks]{.underline} and annotations -- an aspect
which will be discussed towards the end of this Research Paper.

#### 5. [Transparency]{.underline} - NATO is best positioned to encourage cooperation on ethical principles because of its transparency and accountability .

**Trabucco and Stanley-Lockman, 2022 -- prof of Political Science,
University of Copenhagen and prof of Defense and Strategic Studies,
Nanyang Technological University** \[Lena and Zoe, The Oxford Handbook
of AI Governance, March, "NATO's Role in Responsible AI Governance in
Military Affairs"
https://www.oxfordhandbooks.com/view/10.1093/oxfordhb/9780197579329.001.0001/oxfordhb-9780197579329-e-69
Acc 4/16/22 TA\]

[NATO's influence in the functioning of joint operations and
multinational military operations situates the Alliance to coordinate
between how Allies implement ethical principles in their own national AI
development. Specifically, NATO is well-situated to advocate for
transparency, accountability, and data governance, which are also
adoption factors that can translate into operational
benefits]{.underline}, among other values.69 For example, [these factors
can promote coordination among Allies on ethical guidelines on the
development and use of AI]{.underline}, as this will be a necessary
foundation in any future joint operation that uses this technology.
"[The transatlantic partnership must focus on coordinating these core
principles and systematic governance to ensure AI systems development
aligns with the rule of law and democracy]{.underline}. In particular,
[this must ensure answering questions about human dignity, human
control, and accountability ... NATO remains the organization that can
bring these two (U.S. and EU) together and establishes the ethical
bottom line]{.underline}."70 The issues of transparency and
accountability will define the scope of future implementation.

#### 6. [Legal Interoperability]{.underline} - NATO is key to solvency because only it can ensure that military operations follow all member states laws.

**Trabucco and Stanley-Lockman, 2022 -- prof of Political Science,
University of Copenhagen and prof of Defense and Strategic Studies,
Nanyang Technological University** \[Lena and Zoe, The Oxford Handbook
of AI Governance, March, "NATO's Role in Responsible AI Governance in
Military Affairs"
https://www.oxfordhandbooks.com/view/10.1093/oxfordhb/9780197579329.001.0001/oxfordhb-9780197579329-e-69
Acc 4/16/22 TA\]

[One vital and unique contribution for NATO is facilitating legal
interoperability among the Allies to resolve some of the most pressing
legal barriers for AI implementation in future Allied
operations]{.underline}. Legal interoperability, a subset of larger
coalition interoperability, refers to the operational coordination
around partner legal obligations and interpretations.75 It ensures "that
within a military alliance, military operations can be conducted
effectively consistent with the legal obligations of each nation."76
[Legal interoperability is a critical component of multilateral
operations that has thus far been under-examined, despite its centrality
to successful military operations.]{.underline} This is largely because
"legal factors have a bearing on everything in alliances and coalition
operations---from determining basic 'troop-to-task' considerations to
decisions regarding the targets to be engaged---and the types of
ordinances that may be used."77 [To enhance legal interoperability, NATO
can exert its influence on how Allies can develop and deploy AI
consistent with their legal obligations through its unique
standardization capacities]{.underline}. Historically, NATO has taken
significant steps to bridge the legal gap between Allies on critical
procedures that bridge responsible state behavior with such
"troop-to-task" considerations. One instructive example from past
operations is detention policies in non-international armed conflicts.78
The promulgation of detention standards illustrates the operational
significance of NATO's common legal procedures, even for coalitions of
the willing that formally operate outside NATO structures. By way of
background, the U.S.-led coalition in Afghanistan had internal debates
regarding the 96-hour security detention time period.79 The United
States advocated extending the 96-hour rule, where coalition partners
insisted adhering to the NATO standard, even though it was not a NATO
operation.80 Generally the detention example illustrates NATO legal
standards providing clarity to non-NATO operations; in some cases,
Allies adopt NATO standards as accepted thresholds that continue to
inform coalition policies beyond NATO structures and operations.
[Implementing AI in future military operations will almost certainly
complicate legal interoperability as there is a lack of uniform
standards,]{.underline} as in the detention example. [Even some of the
more basic implementation measures will garner legal uncertainty and
Allies will inevitably navigate with minimal legal clarity and no
standard procedures]{.underline}. Despite the roots of the legal debate
stemming from the question of lethality, the most pressing (and urgent)
legal issues will address the integration of necessary AI-enablers, such
as data gathering and sharing.

## 

## On Case Responses

### AT DOD Solving now

#### Current DOD policy does Not require human control -- that is a common misperception

**Freedberg, 2019 -- deputy editor for Breaking Defense** \[Sydney J
"The frontline of a new age in defense Artificial Intelligence"
https://cdn2.hubspot.net/hubfs/2097098/MCM120_BreakingDefense_AI_ebookR1%20(1).pdf
Acc 5/25/22 TA\]

The revised solicitation for ATLAS adds a paragraph emphasizing the
system will be "consistent with DoD legal and ethical standards,"
especially [Department of Defense Instruction 3000]{.underline}.09 o[n
"Autonomy in Weapon Systems." The final decision to fire will always be
a human being's job, the Army insists, in keeping with Pentagon policy.
But policy is not law, and the Pentagon leadership can change it
unilaterally. What's more, even though the military's AI policy is
usually described as requiring a "human in the loop," there's actually
an enormous loophole. "It authorizes the development of weapons that use
autonomy\...for defensive purposes like in Aegis or Active Protection
Systems," Scharre said.]{.underline} "For anything else, it creates a
review process for senior leaders to make a determination." ["It's not a
red light," Scharre told me. It's a stop sign: You halt, you check out
the situation --- and then you can go.]{.underline}

#### US policy only considers LAWs, not the whole range of Military AI

**Stanley-Lockman, 2021 - Center for Security and Emerging Technology**
\[Zoe CSET Issue Brief August "Responsible and Ethical Military AI
Allies and Allied Perspectives"
https://cset.georgetown.edu/publication/responsible-and-ethical-military-ai/
Acc. 4/21/22 TA\]

Executive Summary [Since the U.S. Department of Defense adopted its five
safe and ethical principles for AI in February 2020, the focus has
shifted toward operationalizing them. Notably, implementation efforts
led by the Joint Artificial Intelligence Center (JAIC) coalesce around
"responsible AI"]{.underline} (RAI) [as the framework for DOD, including
for collaboration efforts with allies and partners.]{.underline}1 [With
a DOD RAI Strategy and Implementation Pathway in the making, the first
step to leading global RAI in the military domain is understanding how
other countries address such issues themselves.]{.underline} This report
examines how key U.S. allies perceive AI ethics for defense. Defense
collaboration in AI builds on the broader U.S. strategic consensus that
allies and partners offer comparative advantages relative to China and
Russia, which often act alone, and that securing AI leadership is
critical to maintaining the U.S. strategic position and technological
edge. Partnering with other democratic countries therefore has
implications for successfully achieving these strategic goals.
[Yet]{.underline} [the military aspects of responsible AI that go beyond
debates on autonomous weapons systems are currently
under-discussed.]{.underline}

#### The DOD has not backed up its principles with policy action

**Freedberg, 2020 - deputy editor for Breaking Defense** \[Sydney, Sept
16 Breaking Defense "Military AI Coalition Of 13 Countries Meets On
Ethics"
https://breakingdefense.com/2020/09/13-nations-meet-on-ethics-for-military-ai/
Acc. 4/22/22 TA\]

[Defense Secretary Mark Esper officially adopted a set of AI ethics
principles in February, although implementation is still
nascent.]{.underline} He has also repeatedly denounced China and Russia
for developing, deploying, and in some cases exporting AI systems that
disrespected human rights or human control of lethal force.

#### Actions must back up words

**Horowitz and Scharre, 2021 - Senior Fellows at the Technology and
National Security Program at the Center for a New American Security**
\[Michael and Paul, Jan 12, "AI and International Stability: Risks and
Confidence-Building Measures"
[https://www.cnas.org/publications/reports/ai-and-international-stability-risks-and-confidence-building-measures
Acc
6/6/22](https://www.cnas.org/publications/reports/ai-and-international-stability-risks-and-confidence-building-measures%20Acc%206/6/22)
TA\]

[One risk to such statements is that if they appear manifestly at odds
with a state's actions, they can ring hollow, undermine a state's
credibility, or undermine the norm itself.]{.underline} For example,
[loudly proclaiming the importance of AI ethics while using AI systems
in a clearly unethical manner]{.underline}, such as for internal
repression or without regard for civilian casualties, [could not only
undermine a state's credibility but also undermine the value of the norm
overall, especially if other states fail to highlight the
disconnect]{.underline}. [Following through with meaningful actions to
show how a state puts these norms into practice is essential for them to
have real value.]{.underline}

### AT JAIC Solves Now

#### The JAIC conference did not establish policies -- the US is still out of step with NATO allies

**Trabucco, 2020 - Research Assistant at the Centre for Military Studies
at the University of Copenhagen** \[Lena, May 10 "AI Partnership for
Defense is a Step in the Right Direction -- But Will Face Challenges"
http://opiniojuris.org/2020/10/05/ai-partnership-for-defense-is-a-step-in-the-right-direction-but-will-face-challenges/
Acc 4/17/22 TA\]

Which brings me to the third challenge. [The AI partnership symposium
did not offer a coherent strategy for the partnership beyond advancing
core values the participating nations find important to the AI
pipeline]{.underline}. [Peter Singer, New America Foundation
fellow]{.underline} and strategist, [noted that the US has not yet
offered a coherent strategy to contrast its "near peers."
In]{.underline} one article, Singer said, "China has a fairly clear and
robust vision of this \[AI and its applications\] and it is actively
exporting that vision. [There is absolutely no way the US can compete
without offering a different and compelling vision and one that involves
our friends and allies."]{.underline}

#### The JAIC strategy lacks funding.

**Freedberg, 2019 -- deputy editor for Breaking Defense** \[Sydney J
"The frontline of a new age in defense Artificial Intelligence"
https://cdn2.hubspot.net/hubfs/2097098/MCM120_BreakingDefense_AI_ebookR1%20(1).pdf
Acc 5/25/22 TA\]

Well, [the Pentagon has released its AI strategy. There are virtually no
mentions of increased funding in it. But it does make the stakes clear:
"Failure to adopt AI will result in legacy systems irrelevant to the
defense of our people, eroding cohesion among allies and
partner]{.underline}s, reduced access to markets that will contribute to
a decline in our prosperity and standard of living, and growing
challenges to societies that have been built upon individual freedoms."
[It identifies the center of gravity for AI work in the military, the
Joint Artificial Intelligence Center (JAIC).]{.underline} And it sets
some priorities: "We will launch a set of initiatives to incorporate AI
rapidly, iteratively, and responsibly to enhance military
decision-making and operations across key mission areas. Examples
include improving situational awareness and decision-making, increasing
the safety of operating equipment, implementing predictive maintenance
and supply, and streamlining business processes. We will prioritize the
fielding of AI systems that augment the capabilities of our personnel by
offloading tedious cognitive or physical tasks and introducing new ways
of working."

### AT Partnership for Defense Solves Now

#### The PfD did not resolve AI disagreements between participants and did not include some key nations.

**Trabucco, 2020 - Research Assistant at the Centre for Military Studies
at the University of Copenhagen** \[Lena, May 10 "AI Partnership for
Defense is a Step in the Right Direction -- But Will Face Challenges"
http://opiniojuris.org/2020/10/05/ai-partnership-for-defense-is-a-step-in-the-right-direction-but-will-face-challenges/
Acc 4/17/22 TA\]

Despite the many benefits and opportunities, the partnership will face
challenges as it defines the boundaries and practices of the
collaboration. I believe three challenges ought to be raised at the
outset. DoD officials have discussed interoperability as a an explicit
goal of the partnership -- interoperability being an umbrella term
referring to group integration in order to operate cohesively and share
information effectively. [Interoperability is a necessary yet difficult
achievement in any kind of partnership, but the AI partnership has a
unique challenge. Namely, one of legal interoperability.]{.underline}
Legal interoperability is one subset of the broader partnership
interoperability and refers to the pursuit of the partnership's goals
within the participating state's diverse legal obligations and
interpretations. The partnership must find a way to achieve their
ultimate goal in a manner consistent with domestic and international
legal obligations of the participating states. [Differing regulatory
frameworks]{.underline} and data strategies among participants [could
lead to inadvertent challenges to the AI partnership legal
interoperability. This is particularly relevant for the European
partners, which currently constitute over half of the AI partnership.
The European Union has deliberately distinguished the European approach
to AI from China and the US]{.underline}, instead situating itself as
the global leader advancing responsible and trustworthy AI. As part of
this strategy, the EU submitted substantial proposals for data
regulation and restrictions. Public and private organizations in these
states are subject to European data regulation and restriction. [It is
not yet clear how these differences may affect the AI partnership;
largely because]{.underline} it is still in its infancy and [the DoD has
not offered many specifics about the partnership in practice. But the
issue could become a major one as the partnership moves
forward]{.underline}. [If the US hopes to expand the partnership to
include more European partners, then the different approaches to data
sharing could become a legal hurdle that hinders the legal
interoperability]{.underline} -- and thus partnership interoperability
-- potentially requiring special agreements. [The second challenge is
about which states are -- and which states are not -- included in the
partnership (so far).]{.underline} The larger trans-Atlantic
implications of the partnership should not be neglected as the
collaboration moves forward. [Crucial European partners are missing,
such as Germany and the Netherlands.]{.underline} Germany is the biggest
economy in Europe and a readily acknowledges the economic possibilities
of AI development. The Netherlands has yet to reach the AI development
of some of its European counterparts, but the Dutch government has been
explicit about investing into research initiatives to bolster the
Netherlands in the AI landscape. Mark Beall, JAIC Chief of Strategy,
expressed his hope that more states will join the AI partnership in the
future; but [the absence of some of Europe's heavy hitters could signal
a bigger issue for the partnership. Commentators have expressed concern
of a trans-Atlantic divide, and an intra-European divide, on AI in the
military domain.]{.underline} See, for example, here and here.

### AT CCW / UN Solve Now

#### CCW solutions are too slow due to Russian obstruction.

**Sauer, 2021 - Senior Research Fellow at Bundeswehr University**
\[Frank serves on the International Panel on the Regulation of
Autonomous Weapons IRRC No. 913 March "Stepping back from the brink: Why
multilateral regulation of autonomy in weapons systems is difficult, yet
imperative and feasible"
https://international-review.icrc.org/articles/stepping-back-from-brink-regulation-of-autonomous-weapons-systems-913
Acc 4/5/22 TA\]

The United Nations (UN) Convention on Certain Conventional Weapons (CCW)
is the epicentre of the global debate on autonomy in weapons systems.
The CCW\'s purpose "is to ban or restrict the use of specific types of
weapons that are considered to cause unnecessary or unjustifiable
suffering to combatants or to affect civilians indiscriminately".1 In
CCW parlance, the weapon autonomy issue is called "emerging technologies
in the area of lethal autonomous weapons systems" (LAWS). In November
2019, CCW States Parties decided to, once again, continue their
deliberations on LAWS. For the first time, however, these talks, which
had previously been conducted between 2014 and 2016 in informal meetings
and since 2017 within the framework of an expert subsidiary body called
a Group of Governmental Experts (GGE), were mandated to produce a
specific outcome. For ten days in 2020 and for an as-yet unknown number
of days in 2021 (when the CCW\'s next Review Conference is due), [the
GGE was and is tasked with debating and fleshing out "aspects of the
normative and operational framework"]{.underline} on LAWS.2 In addition,
in Annex III of their 2019 report, [States Parties adopted eleven
guiding principles to take into account going forward]{.underline}.3
After the first five-day meeting of 2020 was postponed and then
conducted in a hybrid format due to the current global COVID-19
pandemic, the second meeting had to be shelved, and [it is currently
unclear when and how the talks can resume]{.underline}. While [some
States -- most prominently Russia -- have displayed no interest in
producing new international law in the CCW]{.underline}, arguing that
"concerns regarding LAWS can be addressed through faithful
implementation of the existing international legal norms",4 others --
such as Germany -- claim that nothing short of "an important milestone"
has already been reached with the 2019 report cited above, even
describing the adopted eleven guiding principles as a "politically
binding regulation".5 Meanwhile, [the international Campaign to Stop
Killer Robots]{.underline} (Killer Robots Campaign, KRC) [is criticizing
CCW diplomacy as "moving forward at a snail\'s pace", with low ambitions
and negligible outcomes despite widespread public opposition to LAWS and
some thirty countries]{.underline} (twenty-six of which are CCW States
Parties) [calling for the immediate negotiation of a new, binding legal
instrument rather than continuing talks on frameworks and principles,
which the KRC tends to consider vague and redundant
respectively]{.underline}.6

#### UN Solutions will fail because they only focus on LAWs, not other applications of AI -- inflexibility limits potential solutions. 

**Kahn and Horowitz, 2021 -- Research and Senior Fellows at the Council
on Foreign Relations** \[Lauren and Michael, The Washington Quarterly
44:4 "Leading in Artificial Intelligence through Confidence Building
Measures" [https://doi.org/10.1080/0163660X.2021.2018794 Acc
6/6/22](https://doi.org/10.1080/0163660X.2021.2018794%20Acc%206/6/22)
TA\]

One might argue that the United States should let others lead on AI,
focusing instead on developing AI-enabled capabilities and not
concerning itself with how other countries behave. But there is no
substitute for American leadership and its ability to rally countries
around the world to support shared standards. If promoting norms of
responsible behavior with AI encourages other states to use military
applications of AI in more responsible ways, it will create a more
ethical and predictable security environment, likely benefiting the
United States. Additionally[, current international dialogue about
military uses of AI focuses almost exclusively on]{.underline} lethal
autonomous weapon systems ([LAWS]{.underline}), [the subject of a Group
of Governmental Experts in the Convention on Certain Conventional
Weapons]{.underline}.39 Currently, the international conversation has
been largely been driven by NGOs such as the Campaign to Stop Killer
Robots.40 [While such conversations help bring attention to some of
these issues, they oversimplify the risks and fixate on worst-case
scenarios that are more likely outcomes of artificial general
intelligence or human level machine intelligence rather than technology
today]{.underline}. [LAWS represent only a small fraction of the
universe of potential issues surrounding military applications of AI.
Broadening the international conversation about military uses of AI to
incorporate the full scope of potential applications would generate
better dialogue because it would include more of the real-world AI
scenarios likely to confront militaries. Expanding the discussion would
also allow states to pursue levels of control and regulation other than
an all-or nothing ban and create a more calibrated and flexible range of
approaches]{.underline} to different technologies with various levels
and types of associated risks.

#### The CCW will not produce a treaty -- it requires a consensus that the US and Russia are blocking.

**Miller, 2021 - Flight Chief, US Air Force** \[Amanda, Dec 14 Air Force
Magazine "UN Addresses Lethal Autonomous Weapons---aka 'Killer
Robots'---Amid Calls for a Treaty"
https://www.airforcemag.com/un-addresses-lethal-autonomous-weapons-aka-killer-robots-amid-calls-for-a-treaty/
Acc. 4/5/22 TA\]

[The United Nations' secretary-general advocated for new restrictions on
autonomous weapons as a U.N. group that negotiates weapons protocols
s]{.underline}tarted a week of meetings, in part, to discuss the matter.
Secretary-General Antonio Guterres addressed the Review Conference of
the U.N.'s Convention on Certain Conventional Weapons. Taking place in
Geneva, Switzerland, the Review Conference happens every five years.
Guterres preceded the weeklong meeting with a Dec. 13 message
encouraging conference members "to agree on an ambitious plan for the
future to establish restrictions on the use of certain types of
autonomous weapons." He described autonomous weapons as those "that can
choose targets and kill people without human interference." The
conference has identified artificial intelligence, for one, as an
"increasingly autonomous" technology. The Air Force has experimented
with autonomous weapons such as the Air Force Research Laboratory's
Golden Horde, which did not become a program of record but did succeed
in getting Small Diameter Bombs to collaborate with each other after
receiving and interpreting commands mid-flight. [The experimental Perdix
micro-drones, under the DOD's Strategic Capabilities Office, rely on
AI]{.underline}. And although the Defense Advanced Research Project
Agency's Gremlins drones don't rely on AI yet, they're designed to
accommodate that level of computing. Some countries and international
rights groups want the convention to negotiate a treaty that would ban
what the U.N. calls lethal autonomous weapons systems---and what others
call "killer robots"---[but diplomats told Reuters that's not likely to
happen]{.underline} this week. [It would require a consensus, and the
U.S., for one, has already rejected the idea. Russia was expected to do
the same.]{.underline}

#### CCW progress is limited by definitional debates

**Sauer, 2021 - Senior Research Fellow at Bundeswehr University**
\[Frank serves on the International Panel on the Regulation of
Autonomous Weapons IRRC No. 913 March "Stepping back from the brink: Why
multilateral regulation of autonomy in weapons systems is difficult, yet
imperative and feasible"
https://international-review.icrc.org/articles/stepping-back-from-brink-regulation-of-autonomous-weapons-systems-913
Acc 4/5/22 TA\]

Why regulating weapon autonomy is difficult: Conceptual pitfalls and
power politics [From UN Secretary-General António Guterres to prominent
members of the artificial intelligence]{.underline} (AI) and tech
[communities]{.underline}7 to most States Parties of the CCW, [there is
near unanimity that LAWS raise various legal, strategic and ethical
questions and concerns]{.underline}.8 Even so, [within the CCW States
Parties, a consensus on new, binding international law is still a long
way off. Regulating weapon autonomy through this multilateral forum is a
particularly tough nut to crack.]{.underline} As I will argue in this
section, this is due to two reasons[. First, weapon autonomy as an issue
is comparatively elusive and hard to conceptualize. Second, its
perceived military value is exceptionally high, and the current
geopolitical landscape is not conducive to new arms control
breakthroughs.]{.underline} Any discussion of the conceptual challenges
regarding weapon autonomy has to begin with pointing out a common
misunderstanding: the lack of progress in the CCW cannot be attributed
to States Parties not having arrived at a shared definition of LAWS
yet.9 Quite to the contrary, it has much more to do with the fact that
[the attempt to define LAWS was misconceived from the very
beginning]{.underline}. This warrants further elaboration. [The first
two to three years of the CCW process on LAWS were indeed plagued by
confusion and definitional struggles]{.underline}. Considerable effort
was required to delineate the LAWS debate from the disputes surrounding
remotely piloted aerial vehicles (drones) as well as to avoid
anthropomorphizing LAWS as a one-to-one replacement for human
soldiers.10 All stakeholders were seeking -- and quite a few lamenting
the lack of -- a "possible definition of LAWS", sometimes deliberately
so in order to justify political heel-dragging. The underlying rationale
was that arms control always requires a precise categorization of the
object in question, such as a landmine, before any regulative action can
be taken.

### AT EU Solves Now

#### The EU is not implementing military AI policies now -- they are focused on regulating civilian technology

**Stanley-Lockman, 2021 - Center for Security and Emerging Technology**
\[Zoe CSET Issue Brief August "Responsible and Ethical Military AI
Allies and Allied Perspectives"
https://cset.georgetown.edu/publication/responsible-and-ethical-military-ai/
Acc. 4/21/22 TA\]

[While the EU has adopted a bullish approach to trustworthy AI in the
civilian realm, European institutions have been slower to define the
implications for safe and ethical AI beyond the tip of the
spear]{.underline}. Key civilian policies and regulations, like the
General Data Protection Regulation and more recent legislation
instituting the European approach to "trustworthy" AI, have clear
carveouts for public safety, security, and defense. Still, the EU
approach to civilian AI policy is relevant to transatlantic defense
because the dual-use, general-purpose nature of AI means that military
adoption of AI will depend on the ethical frameworks that dominate
civilian development, regardless of carveouts. More directly, some
European countries also choose to apply EU legislation like the General
Data Protection Regulation to their own defense sectors, even though
they are not required to do so.167 With this overlap in mind, Appendix
VIII overviews the applicability of the EU trustworthy AI principles for
the defense realm. In addition to examples such as Airbus' application
of the ALTAI methodology to FCAS, it is notable that several European
defense efforts mention the European Commission-supported guidelines for
trustworthy AI as a positive step toward ensuring military uses of AI
adhere to ethical standards. For example, in their co-authored
food-for-thought paper on AI in defense, Finland, Estonia, France,
Germany, and the Netherlands made explicit reference to the Trustworthy
AI Principles, recognizing that the EU could leverage its normative
power because of the centrality of ethical standards in AI for
defense.168 The focus on safety and security in EU AI policy also
promotes "convergence between the AI community and the security
community" to enhance robustness.169 In sum, the emphasis on safety,
security, and risk in EU AI policy is not only a natural overlap, but
also one that European defense stakeholders are seeking out. However,
[it remains to be seen which EU body will take control of the
responsible and ethical military AI agenda]{.underline}. There are
various actors within the EU institutions that are largely beyond the
scope of this paper.170 Instead, [there are only inklings of how the EU
will approach responsible and ethical military AI at
present]{.underline}. In the future, this topic could also feature in
EU-U.S. security and defense dialogues. For now, it is the European
Parliament that plays the most visible role advancing ethics in European
military R&D funding. This was seen in mid-2018, in its attempt to ban
all military AI research using EU funds because of concerns about LAWS.
The agreed upon final version explicitly prohibits funding for LAWS at
the European level---a deal-breaker without which the Parliament would
never have agreed to allow for any defense funding.171 But [the final
result was narrower because the EU does not have jurisdiction over its
member states' armaments development unless they use EU
funds]{.underline}. As such, while important, especially for dualuse and
open-source systems, its jurisdiction on mandating ethical reviews is
still limited and likely to not affect the majority of national,
bilateral, or minilateral capability development programs. More
recently, the Parliament-issued Guidelines for military and non-military
use of Artificial Intelligence in January 2021 could indicate a stronger
ethical bent than seen in the other institutions.172

#### Trade and policy disagreements block EU / US cooperation on AI now.

**Lawrence and Cordey, 2020 -- researchers for The Cyber Project at the
Belfer Center for Science and International Affairs** \[Christie and
Sean, August, The Cyber Project Paper "The Case for Increased
Transatlantic Cooperation on Artificial Intelligence Edited by Lauren
Zabierek and Julia Voo
https://www.belfercenter.org/sites/default/files/2020-08/TransatlanticAI.pdf
Acc. 4/21/22 TA\]

Transatlantic Cooperation: [Despite over 40 years of scientific
relationships and projects between the United States and the European
Union]{.underline}, [AI-specific collaboration has been fraught with
varying degrees of political and academic skepticism on both side of the
Atlantic, notably within the European Commission and the governments of
some Member States]{.underline} (e.g., France and Germany).31 [Such a
dynamic is aggravated, in part, by the ever-deteriorating transatlantic
relationship spurred by policy and trade disagreements, public spats,
and increasing American isolationism]{.underline}. Despite such explicit
omissions and stand-offs at the highest levels, transatlantic
collaboration for AI does happen, most notably in various multilateral
forums working on standards (e.g., ISO, IEC, IEEE, G7, G20) or on ethics
and norms (e.g., OECD, GPAI32).33

#### Too many obstacles to US / EU cooperation on AI

**Lawrence and Cordey, 2020 -- researchers for The Cyber Project at the
Belfer Center for Science and International Affairs** \[Christie and
Sean, August, The Cyber Project Paper "The Case for Increased
Transatlantic Cooperation on Artificial Intelligence Edited by Lauren
Zabierek and Julia Voo
https://www.belfercenter.org/sites/default/files/2020-08/TransatlanticAI.pdf
Acc. 4/21/22 TA\]

Challenges to Collaboration & Recommendations [Full US-EU collaboration
faces five distinct, but interconnected obstacles]{.underline} (see
Figure 1 below). [At the highest level, the United States and European
Union have some diverging geopolitical interests (section A) illustrated
by: America's increasing isolationism, the European Union's rebalancing
to become a third power, the European Union's resistance to adversarial
discourse about China, and domestic political demands]{.underline} to
focus resources on COVID-19 responses. Flowing out of the geopolitical
landscape and political interests are three overarching considerations
that are bolstered by [differing beliefs about the role and size of
government and can fuel US-EU disagreements around AI.]{.underline}
These US national interests and EU common priorities are (section B):
AI's impact on national security and economic interests, as well as the
ethics and values that guide AI's development and use. [Finally, aspects
of the AI operating environment]{.underline} (sections C, D, and E[),
such as regulation and governance]{.underline} (including standards and
operationalizing principles), [funding, data spaces, hardware, and
computing resources, provide tactical areas for disagreement or
misalignment.]{.underline}

#### The EU is not cooperating with the US on AI now.

**Lawrence and Cordey, 2020 -- researchers for The Cyber Project at the
Belfer Center for Science and International Affairs** \[Christie and
Sean, August, The Cyber Project Paper "The Case for Increased
Transatlantic Cooperation on Artificial Intelligence Edited by Lauren
Zabierek and Julia Voo
https://www.belfercenter.org/sites/default/files/2020-08/TransatlanticAI.pdf
Acc. 4/21/22 TA\]

[Although the US consistently sounds the alarm bells around China's
AI]{.underline} aspirations and the EU urges international efforts
against AI that violates fundamental rights, increasingly noting China's
actions with concern,8 [little concrete international action has taken
place.]{.underline} The United States and the European Union's ongoing
reassessment of their respective AI strategies and legislation9 provides
a window of opportunity to align and collaborate. Transatlantic AI
cooperation is at a critical juncture and the United States and the
European Union should seize this opportunity to take concrete actions.
[The Current State The United States and the European Union are
separately assessing and updating their AI strategies]{.underline}.
However, it is a myth to assume they are not collaborating at all to
advance their AI-related goals. [Transatlantic cooperation on AI norms,
standards, research and development, and data sharing should
increase,]{.underline} but the United States and the European Union can
build upon an existing foundation for a stronger alliance.

### AT NATO solves now

#### NATO has not implemented its AI principles yet -- they need to move from words to actions

**van Weel, 2021 -- Assistant Secretary General for Emerging Security
Challenges, NATO** \[David, Dec 7, "Artificial intelligence: Can we go
from chaos to cooperation?" AEI Panel Discussion - Moderator: Elisabeth
Braw
https://www.aei.org/events/artificial-intelligence-can-we-go-fromchaos-to-cooperation/
Acc 5/11/22 TA\]

So [we've adopted six principles of responsible use]{.underline}. I'll
just read them out here. [They're lawfulness, responsibility and
accountability, "explainability" and traceability, reliability,
governability, and bias mitigation]{.underline}. So [all 30 NATO nations
have signed up]{.underline} --- including the US, I would say to
Jonathan --- [to having any use of AI in the defense or security purpose
adhere to these six principles.]{.underline} And [principles are nice,
but they need to be verifiable as well, and they need to be baked in
from the]{.underline} moment of the [first conception]{.underline} of an
idea up [until the delivery. And that's where we will have a new
initiative launched, where we have test centers across the
alliance]{.underline}, based alongside universities --- existing test
centers with knowledge, where allies that are thinking about
codeveloping AI for use in the defense sector can come in and verify
with protocols, with certain standards that we're setting, that this AI
is actually verified. [It's not real standard yet. But if the 30
nations]{.underline}, Western democracies, [start out by shaping
industry to adhere by these standards, then I feel that we are making an
impact]{.underline} --- at least [in the development of AI]{.underline},
and hopefully, also, in the larger world --- setting standards. So
[we're trying to contribute to a better world, but we need to be part of
the game in order to be able to do so.]{.underline}

#### NATO's AI strategy is fragmented now.

**Warrell, 2021 - Assistant Editor Financial Times** \[Helen, June 7,
Financial Times "Nato allies need to speed up AI defence co-operation"
[https://www.ft.com/content/61c1945c-d153-4d58-b9c5-dffd99a6919e Acc
6/4/22](https://www.ft.com/content/61c1945c-d153-4d58-b9c5-dffd99a6919e%20Acc%206/4/22)
TA\]

[Part of the problem is that western defence institutions have been slow
to recognise the potential of innovation beyond their own
industry]{.underline}. [For decades, a lot of technological development
would happen within the defence sector . . . and then shared with the
civilian sector. Now, it goes the other way around]{.underline} "For
decades, a lot of technological development would happen within the
defence sector --- the internet, nuclear, GPS, all of that was developed
by the defence industry and then shared with the civilian sector,"
Stoltenberg said. "Now, it goes the other way around. [It's a civilian
sector which is leading in the development of artificial
intelligence,]{.underline} quantum computing, and many of the new
disruptive technologies." [Some Nato members are ahead of others. The US
and France have published military AI strategies,]{.underline} while the
UK announced this year that it is to establish a centre for defence AI.
For the first time, Britain's intelligence agency, MI6, is recruiting
from the private sector for a new head of its "Q" branch --- the
technical lab made famous in the James Bond films.

### 

### AT No Definition of AI

#### Even if there is no Legal definition of AI, existing ones can provide guidance to policy makers

**Hill and Marsan, 2018 - Director and Senior Assistant, NATO Office of
Legal Affairs** \[Steven and Nadia, 7-18-18 "Artificial Intelligence and
Accountability: A Multinational Legal Perspective"
https://www.sto.nato.int/publications/STO%20Meeting%20Proceedings/STO-MP-IST-160/MP-IST-160-PP-4.pdf
Acc 4/21/22 TA\]

2.0 DEFINITIONS The first issue that arises in the context of AI which
highlights the difficulty in setting a clear conceptual scope and
applicable legal parameters, is a definitional one. There are no
internationally agreed legal definitions for the core concept of AI. The
lack of agreed legal definitions can hinder and stall discussions in
part because Nations may rightfully be hesitant to commit to the
regulation of a new technology when the scope and the evolution of that
technology is not clear. [While there might not be agreed legal
definitions of AI, broader discussions on AI have identified a number of
elements that can nevertheless provide some preliminary guidance. The
first suggested definition of AI is, "the capability of a computer
system to perform tasks that normally require human intelligence, such
as visual perception, speech recognition and decision-making."6 The
second definition of AI which is useful here is, "technologies that
enable machine learning, natural language processing, deduction through
vast data-computational power, and ultimately, automated
decision-making]{.underline} in robotics or software that can substitute
for tasks once performed exclusively by human action and judgement". 7
Another term that is important to define is "autonomy". The concept of
autonomy is key to AI because it is precisely the technological edge
provided by AI that enables autonomy or, expressed another way, enables
the independent action of a machine. Autonomy itself can be defined as
"the ability of a system, platform or software to complete a task
without human intervention, using behaviours resulting from the
interaction of computer programming with the external environment".8
Although beyond the scope of this paper, other definitions incorporate
key concepts in the military use and development of AI.9 These
preliminary definitions suggest that AI is about replicating human
perception, cognition and decision-making as well as introducing a
certain element of independence to these systems. Although the modelling
of human intelligence provides opportunities from a security
perspective, these are also fraught with challenges that are only
beginning to be understood: "there is now a broad consensus that AI
research is progressing steadily, and that its impact on society is
likely to increase. The potential benefits are huge, since everything
that civilization has to offer is a product of human intelligence; we
cannot predict what we might achieve when this intelligence is magnified
by the tools AI may provide, but the eradication of disease and poverty
are not unfathomable. Because of the great potential of AI, it is
important to research how to reap its benefits while avoiding potential
pitfalls". 10 3.0 LEGAL IMPLICATIONS ARISING FROM SPECIFIC USES OF AI
[There are already applications of AI that are of interest to a
multilateral security organisation such as NATO. Acknowledging the
absence of a clear consensus on the legal definition of AI, it is useful
to highlight some concrete areas where AI has been and is currently
being used with effects]{.underline}, both positive and negative, on
Allied security.11 Three areas of AI-enabled technologies are worth
mentioning here: intelligence, surveillance and reconnaissance; the
manipulation of personal data; and, disinformation.

#### Focusing on Human Control avoids the trap of trying to precisely define weapons

**Sprenger, 2021 - Europe editor for Defense News** \[Sebastian, Apr 27,
"NATO tees up negotiations on artificial intelligence in weapons"
https://www.c4isrnet.com/artificial-intelligence/2021/04/27/nato-tees-up-negotiations-on-artificial-intelligence-in-weapons/
Acc 4/22/22 TA\]

[Accountability and transparency are two more buzzwords expected to loom
large in the debate]{.underline}. Accidents with autonomous vehicles,
for example, will the raise the question of who is responsible ---
manufacturers or operators. The level of visibility into of how systems
make decisions also will be crucial, according to van Weel. "Can you
explain to me as an operator what your autonomous vehicle does, and why
it does certain things? And if it does things that we didn't expect, can
we then turn it off?" he asked. [NATO's effort to hammer out common
ground on artificial intelligence follows a push by the European Union
to do the same, albeit without considering military applications. In
addition, the United Nations has long been a forum for discussing the
implications of weaponizing AI.]{.underline} Some of [those
organizations have essentially reinvented the wheel every time,
according to Frank Sauer, a researcher at the Bundeswehr
University]{.underline} in Munich. [Regulators tend to focus too much on
slicing and dicing through various definitions of autonomy]{.underline}
and pairing them with potential use cases, he said. ["You have to think
about this in a technology-agnostic way," Sauer argued, suggesting that
officials place greater emphasis on the precise mechanics of human
control]{.underline}. "Let's just assume the machine can do everything
it wants --- what role are humans supposed to play?"

#### Even without a universal definition, there are many useful ones.

**Shah, 2019 - Research Assistant at the Center for International
Strategic Studies** \[Syed Sadam CISS Insight Vol.VII, No.2 "The Perils
of AI for Nuclear Deterrence"
https://journal.ciss.org.pk/index.php/ciss-insight/article/download/10/9
Acc 5/25/22 TA\]

Defining artificial intelligence (AI) [There are many definitions of AI
given by researchers. However, it is essential first to differentiate
between three overlapping concepts Machine Learning (ML), Deep Learning
(DL), and Artificial Intelligence (AI).]{.underline} Although these
concepts overlap in some domains, they are not the same. DL is a subset
of ML, and ML is a subset of AI.3 ML can simply be defined as the
algorithms that empower computers to learn by themselves based on the
available data. DL is the next level of development of ML. It works like
a human brain. Deep learning algorithms can be taught to do the same
tasks for computers, which the human brain does for humans.4 However,
[the goal of AI is to make computers and machines learn from experience,
think like a human brain and reason on their own]{.underline}. For this
purpose, artificial neural networks use math and algorithms (computer
programs) to impersonate the processes of the human brain to reason
independently.5 [There is not a single agreed definition of AI among
researchers, but few definitions offer a better explanation. For
instance, the US companion bill defined AI as "Any artificial system
that performs tasks under varying and unpredictable circumstances,
without significant human oversight, or that can learn from their
experience and improve their performance\..."6]{.underline} William A.
Carter from CSIS defines Machine Intelligence (MI) as "MI refers to
machines' ability to perform tasks that would normally require human
intelligence. Computer scientists and mathematicians develop MI systems
by imparting the ability to find patterns in large data sets to
computers (machine learning)."7 [Stanford's Researcher, John McCarthy
defines AI, as "Artificial Intelligence is the science and engineering
of making intelligent machines, especially intelligent computer
programs. Artificial Intelligence is related to the task of using
computers to understand human intelligence, but AI does not have to
confine itself to methods that are biologically
observable."]{.underline}8

#### Dialogue can clarify the definition of AI which facilitates solutions

**Allen, 2022 - Director, AI Governance Project, Strategic Technologies
Program at CSIS** \[Gregory C. June 6 "DOD Is Updating Its Decade-Old
Autonomous Weapons Policy, but Confusion Remains Widespread"
[https://www.csis.org/analysis/dod-updating-its-decade-old-autonomous-weapons-policy-confusion-remains-widespread
Acc
6/6/22](https://www.csis.org/analysis/dod-updating-its-decade-old-autonomous-weapons-policy-confusion-remains-widespread%20Acc%206/6/22)
TA\]

[As it updates the policy]{.underline}, [there are]{.underline} at least
four [key issues that DOD needs to address]{.underline}. The first two
will require actual changes to the policy, while the latter two can be
addressed simply by providing additional clarification and guidance,
perhaps through publication of a policy handbook. [Define how a system's
"AI-enabled" status does or does not affect the policy's
requirements.]{.underline} Define how retraining machine learning models
will be handled in the senior review process. [Clarify the specific
features that formally define an autonomous weapon system]{.underline}.
Clarify the types of weapons that are and are not required to go through
the autonomous weapon senior review process. While the final word on
these issues must come directly from DOD, the sections below attempt to
shed some initial light. Define how a system's "AI-enabled" status does
or does not affect the policy's requirements. The debate over what
constitutes AI as opposed to mere mechanical computation and automation
goes back at least as far as the 1830s, when Ada Lovelace worked with
Charles Babbage to design and program the first mechanical computers. In
recent decades, computer systems that in their heyday were routinely
called "AI," such as IBM's chess-playing Deep Blue system in 1997, have
higher-performing successors today that are merely called "software" or
"apps." Machine learning, a subfield of AI that has been responsible for
extraordinary research and commercialization progress since 2012, is
more often than not used as a synonym for AI---so much so that some
argue that any system that does not use machine learning should not be
referred to as AI. [It is important to know whether a system claiming to
be AI-enabled is using machine learning. Machine learning works
differently from traditional softwar]{.underline}e. It is better at some
things, worse at others, requires different factors to enable success,
and has different failure modes and risks that need to be addressed.
[When the U.S. military says that a given system is AI-enabled or that a
given project is an "AI project," it almost always means machine
learning. While describing a military system as AI-enabled or machine
learning-enabled provides useful information, it often remains unclear
just what functionality is being provided by AI and how central AI is to
the system overall]{.underline}. Suppose, hypothetically, that the F-35
Joint Strike Fighter Program Office decided that it should use machine
learning to improve the performance on one of the aircraft's many
different types of sensors. Does that mean that the entire F-35 program,
with its more than \$8.5 billion in annual spending, is building
AI-enabled weapon systems? The DODD 3000.09 update is a good opportunity
to formally define "AI-enabled" in DOD policy and to specify how using
machine learning does or does not affect the autonomous weapon senior
review process. [Confusingly, common usage of these terms differs
significantly in other countries]{.underline}. For example, Russian
weapons manufacturers routinely refer to their automated and robotic
military systems as using AI even if the system does not use machine
learning, and they rarely make it clear when it does. [Varied
definitions complicate international diplomacy on these
subjects.]{.underline}

### AT No Definition of Human Dignity

#### Being "Hard to Define" does not make human dignity irrelevant -- most international norms are operationally defined

**Sauer, 2021 - Senior Research Fellow at Bundeswehr University**
\[Frank serves on the International Panel on the Regulation of
Autonomous Weapons IRRC No. 913 March "Stepping back from the brink: Why
multilateral regulation of autonomy in weapons systems is difficult, yet
imperative and feasible"
https://international-review.icrc.org/articles/stepping-back-from-brink-regulation-of-autonomous-weapons-systems-913
Acc 4/5/22 TA\]

Universal human dignity That the use of LAWS would be a violation of
human dignity has been argued by various scholars of moral philosophy
and technology.82 The notion was picked up by the KRC83 and lately has
also been reiterated by the ICRC.84 [Opposing weapon autonomy on grounds
of human dignity has drawn some scrutiny]{.underline},85 and the
supposed "awkwardness"86 of this stance is commonly substantiated [by
pointing out that several meanings of dignity exist and that there is no
commonly agreed-upon definition of dignity. However, being hard to
define but relevant and even crucially important is a characteristic of
many normative concepts, including many legally codified
ones.]{.underline} [Cornerstones of IHL such as civilian-ness, which is
defined only ex negativo, or proportionality, which is not
quantifiable]{.underline} and is assessable only on a case-by-case
basis, [are examples]{.underline}.87 [Human dignity, too, is contained
in various international legal documents]{.underline}. The Universal
Declaration of Human Rights refers to it in its preamble, as does the UN
Charter. It is also invoked in national bodies of law, as well as court
decisions. The key example here is Germany\'s basic law Article 1(1),
which states human dignity\'s inviolability and prohibits the treatment
of humans as objects or means to an end, being referenced in a 2006
landmark decision by the German Constitutional Court. The judges struck
down a federal law that would have allowed the German air force to shoot
down a hijacked aeroplane that the hijackers may have intended to use as
a weapon to kill people on the ground. The Court deemed it
unconstitutional to use the aeroplane passengers as mere instruments to
try to achieve another, albeit worthy, goal.88

#### Non--Unique - Most legal and philosophical values lack agreed upon definitions. 

**Rosert, 2019 - Professor for International Relations at Universität
Hamburg** \[Elvira, with Frank Sauer Researcher at Bundeswehr, Global
Policy, July 5 "Prohibiting Autonomous Weapons: Put Human Dignity First"
https://doi.org/10.1111/1758-5899.12691 Acc 12/27/20 TA\]

From a strategic communication point of view, adjusting the message
toward the infringement on human dignity would have the general benefit
of dampening the overall level of contention. After all, while the
suggestion to rest the case against LAWS more firmly on human dignity
has drawn some scrutiny itself (see the overview in Sharkey, 2018), the
supposed 'awkwardness' (Baker, 2018) of [this proposal is commonly
substantiated by pointing out that several meanings of dignity exist and
that there is no agreed‐upon definition of dignity. Yet, being vague but
relevant and even crucially important is a characteristic of many
normative and even legally codified concepts]{.underline}. The concepts
of civilian‐ness, of proportionality, or of unnecessary suffering --
cornerstones of IHL despite all their ambiguities -- are just three
examples. Moreover, what we argue specifically is that mobilizing human
dignity would strengthen the stance against LAWS by making it more
resilient against consequentialist challenges, at least when compared to
legal claims. After all, the legal claim that LAWS are indiscriminate
weapons violating the principle of distinction might, in fact, prove
vulnerable due to (unlikely but not impossible) technological progress
that increases their discriminatory capabilities and even equips them
with the (equivalent of) 'common sense' and battlefield awareness that
human commanders possess (Amoroso et al., 2018, p. 33). In fact, this
exact point is already being invoked by opponents of a prohibition on
LAWS, and it keeps forcing its proponents into (rather pointless)
hypothetic legal and technological debates. In addition, the emphasis on
the protection of civilians from LAWS might jeopardize the call for a
comprehensive ban and instead end in mere restrictions on the use of
LAWS (e.g. in pre‐specified 'kill boxes' or domains like the high seas
where the presence of civilians is considered unlikely) (Anderson and
Waxman, 2013; Schmitt and Thurnher, 2013; Schmitt, 2013; HRW, 2016).

#### It's not impossible - There are minimum standards of human dignity defined by the Universal Declaration of Human Rights. 

**Johnson and Axinn, 2013 - Prof of Philosophy at the Univ of South
Florida and PhD Candidate in Engineering at Penn** \[Aaron and Sidney,
Journal of Military Ethics, Volume 12, Issue 2, August "The Morality of
Autonomous Robots" www.tandfonline.com/10.1080/15027570.2013.818399 Acc
12/27/20 TA\]

[The concept of human dignity is complicated and explained variously. As
a minimum, although not without controversy, we may take the statement
of the Universal Declaration of Human Rights (1949), "All human beings
are born free and equal in dignity and rights"]{.underline} (article 1).
[Dignity is often taken as the property that makes humans eligible
for]{.underline} the [human rights]{.underline} listed in the Universal
Declaration. Put another way, in Kant's phrase, [dignity means that the
individual has "an intrinsic worth," and has "no
equivalent."]{.underline} (1959: p. 435). [This is to say that each
human must be respected for his or her unique inherent or intrinsic
value.]{.underline} It is widely accepted that all humans have a certain
kind of equality, they are judged morally by the same rule. The American
Declaration of Independence puts the attitude toward all humans in a
vivid way. Near the end we find the statement, we hold our British
brethren 'as we hold the rest of mankind, Enemies in War, in Peace
Friends.' All humans are potentially our friends, and they all deserve
to have us respect their inherent dignity. Is there a loss of dignity
when a human fights with a machine, compared to fighting with another
human? As a non-lethal game it is acceptable,9 but in a fight to the
death the matter is different and far from trivial. [To give a
programmed machine the ability to 'decide' to kill a human is to abandon
the concept of human dignity.]{.underline} Humans are sometimes
accidentally killed by machines, but [for an autonomous robot/drone to
be programmed to kill a human is to treat a rational being as if it were
merely an object.]{.underline}

### AT No Definition of Human Control

#### Human control can be defined as "*situational understanding and options for intervention enabled both by design and in use*."

**International Panel on the Regulation of Autonomous Weapons, 2021**
\[(iPRAW) coordinated by: German Institute for International and
Security Affairs, July "Building Blocks for a Regulation on LAWS and
Human Control Updated Recommendations to the GGE on LAWS"
https://www.readkong.com/page/building-blocks-for-a-regulation-on-laws-and-human-control-8617434
Acc 2/27/22 TA\]

Block II -- Human Control: [The challenges]{.underline} discussed above
[are mostly caused by a lack of human control]{.underline} in the use of
force. Accordingly, a regulation of LAWS should focus on that[. iPRAW
defines human control as situational understanding and options for
intervention enabled both by design and in use. To account for the
context- dependency of human control, a future regulation of
LAWS]{.underline} (e.g. a CCW Protocol) [will probably have to consist
of rather abstract stipulations regarding the concept of human
control.]{.underline} The supplementary adoption of further agreements
-- legally or politically binding -- could be useful to delineate human
control in further detail.

#### Human Control has to be operationally defined in the context of specific applications.

**International Panel on the Regulation of Autonomous Weapons, 2021**
\[(iPRAW) coordinated by: German Institute for International and
Security Affairs, July "Building Blocks for a Regulation on LAWS and
Human Control Updated Recommendations to the GGE on LAWS"
https://www.readkong.com/page/building-blocks-for-a-regulation-on-laws-and-human-control-8617434
Acc 2/27/22 TA\]

[While it is possible to develop abstract minimum requirements for human
control in the use of force, the appropriate level or implementation of
human control depends on the details of the operational context. A
'one-size- fits-all' control solution that addresses all concerns raised
by the use of LAWS will most likely not be achievable because it cannot
account for the multitude of combinations of environmental
factors,]{.underline} operational requirements, and weapons
capabilities. [Instead a regulation would be more useful if it included
general approximations to be specified in each case]{.underline} along
the lines of existing IHL considerations. iPRAW encourages CCW States
Parties to develop and share specific examples of how control by design
and control in use can be implemented in weapon systems used in
different operational contexts.

#### Abstract requirements for human control will be specified in specific contexts when implemented.

**International Panel on the Regulation of Autonomous Weapons, 2021**
\[(iPRAW) coordinated by: German Institute for International and
Security Affairs, July "Building Blocks for a Regulation on LAWS and
Human Control Updated Recommendations to the GGE on LAWS"
https://www.readkong.com/page/building-blocks-for-a-regulation-on-laws-and-human-control-8617434
Acc 2/27/22 TA\]

Implications for a regulation: [The requirements presented above remain
quite abstract. The exact implementation of these factors depends
primarily on the specific context of a military operation. Translated
into a regulation, this calls for rather wide- ranging rules addressing
human control in more general terms, ideally supplemented by a set of
more specific documents]{.underline} to elucidate and further expound
the concept of human control and to operationalize it.

### 

### AT Countries won't agree on Specifics

#### Ethical principles for human control will leave room for flexibility on implementation by different nations

**Stanley-Lockman, 2021 - Center for Security and Emerging Technology**
\[Zoe CSET Issue Brief August "Responsible and Ethical Military AI
Allies and Allied Perspectives"
https://cset.georgetown.edu/publication/responsible-and-ethical-military-ai/
Acc. 4/21/22 TA\]

[This]{.underline} legal and [human-centric framing informs allies'
views on key questions related not only to responsibility, but
also]{.underline} explainability, [trust]{.underline}, and related
concepts. These [commonalities should be seen as a baseline for
responsible democratic governance of military AI, which leaves room for
nuance in how each country interprets and prioritizes these types of
principles. These nuances are important because defense stakeholders in
allied countries do not necessarily emphasize the same principles in
their evolving approaches to military AI.]{.underline}

#### NATO will work hard to get allies to adapt to emerging security concerns.

**Trabucco and Stanley-Lockman, 2022 -- prof of Political Science,
University of Copenhagen and prof of Defense and Strategic Studies,
Nanyang Technological University** \[Lena and Zoe, The Oxford Handbook
of AI Governance, March, "NATO's Role in Responsible AI Governance in
Military Affairs"
https://www.oxfordhandbooks.com/view/10.1093/oxfordhb/9780197579329.001.0001/oxfordhb-9780197579329-e-69
Acc 4/16/22 TA\]

In this chapter, we explore a role for the North Atlantic Treaty
Organization (NATO) in the emerging military AI governance architecture.
NATO (or the Alliance) is a military and political alliance among 30
contributing member states that are committed to collective security.
[Much of NATO's original purpose and current core tasks arguably leave
the Alliance's role uncertain in international governance regimes
contending with the impact of emerging technology on international
politics.]{.underline}1 As global powers compete for the economic and
military capabilities that AI can offer[, the Alliance has the
enormously challenging task of navigating varying political realities
and capabilities of Allies]{.underline}, all while effectively
recalibrating strategic relationships in the coming years. Recognizing
technological change as a key variable, [NATO has begun to adapt its
organizational composition and strategic footing to increase the
Alliance's capacity to meet emerging security challenges for military
capability development trends of both its own members and those of
competitors or adversaries.]{.underline}

### AT Europe Blocks Plan

#### European nations agree on the minimal principle of Human Control.

**Nadibaidze, 2021 - Ph.D. Student at the University of Southern
Denmark** \[Anna Entry submitted for the Second OSCE-IFSH Essay
Competition "Commitment to Control over Weaponised Artificial
Intelligence: A Step Forward for the OSCE and European Security"
https://www.osce.org/files/f/documents/8/3/507341.pdf Acc. 4/21/22 TA\]

Second, [the discourses surrounding weaponised AI -- the ways
that]{.underline} OSCE participating [States talk about LAWS -- also
have considerable impact on European security]{.underline}. Both a
common definition of LAWS and [an agreement on the appropriate level of
human control over weapons systems are lacking, which gives way for
misinterpretation and increases security risks]{.underline}. Let us
examine the discourses of three [major players in European security:
France,]{.underline} the Russian Federation, [and the United
Kingdom]{.underline}. Their [official positions converge on the
importance of retaining human control]{.underline}. The Minister of the
French Armed Forces, Florence Parly, said that "France refuses to
entrust the decision of life or death to a machine that would act in a
completely autonomous manner and would be beyond any human control"
(Ministère des Armées 2019). Russia said it "is committed to the need to
maintain human control over LAWS, no matter how 'advanced' these systems
may be" (Russian Federation 2021, 3). The UK Ministry of Defence noted
in January 2021, "the operation of our weapon systems will always be
under human control and no UK weapons systems will be capable of
attacking targets without this" (Ministry of Defence 2021).
[Nevertheless, autonomy and the concept of appropriate human control
over weapons systems are perceived differently by these
States.]{.underline} The Russian side remains opposed to a legally
binding treaty that would ban LAWS, arguing that the definition of LAWS
should "strike a balance between humanitarian concerns and legitimate
defence interests of States" (Russian Federation 2021, 3). [France has
suggested a division between "fully" and "partially" lethal autonomous
weapon systems]{.underline} and adopting different types of measures for
these two categories. In the French perspective, only the "fully"
autonomous weapons should be prohibited (French Republic 2021).
Meanwhile, [the UK]{.underline} has stated that "an autonomous system is
capable of understanding higher-level intent and direction", a
definition that is more precise and constraining than those of other
States (Ministry of Defence 2017, 13). A UK House of Lords Select
Committee report said this [definition is "clearly out of step with the
definitions used by most other governments]{.underline}" and also
"hamstrings attempts to arrive at an internationally agreed
definition"(Select Committee on Artificial Intelligence 2018, 105). [As
a common denominator, these States agree on the principle that weapons
systems should not function completely autonomously. However, the
differences in their perceptions hinder the progress on understanding
and preventing the security risks related to the use of weaponised
AI]{.underline}. They create misperceptions about the uses of AI,
specifically between the leaders in this sphere, who are all carefully
watching each other's technological developments. The NSCAI, for
instance, warned the US government that "competitors are actively
developing AI concepts and technologies for military use," specifically
focusing on China and Russia (2021, 22). The discrepancies in
definitions and discourses create risks of misunderstanding when, for
instance, one participating State is developing a certain weapons system
considered to be LAWS by another State. [Such communication issues can
lead to a security dilemma in which "one state's pursuit of greater
automation and faster reaction times undermines other states' security
and leads them to similarly pursue more automation just to keep up," and
encourgages experts to speak of an 'AI arms race']{.underline} (Scharre
2021).

#### US and European leaders have converged on AI ethical principles -- current guidelines prove.

**Lawrence and Cordey, 2020 -- researchers for The Cyber Project at the
Belfer Center for Science and International Affairs** \[Christie and
Sean, August, The Cyber Project Paper "The Case for Increased
Transatlantic Cooperation on Artificial Intelligence Edited by Lauren
Zabierek and Julia Voo
https://www.belfercenter.org/sites/default/files/2020-08/TransatlanticAI.pdf
Acc. 4/21/22 TA\]

Advancing AI Founded on Shared Values The US and EU should collaborate
to ensure their shared values set AI norms that ripple throughout the
global AI ecosystem. [Although ethics is mentioned as a potential source
of divergence, we believe]{.underline}, [as do]{.underline} Nand
Mulchandani, Acting Director of [the U.S. Department of Defense Joint
Artificial Intelligence Center]{.underline} (JAIC) and Ryan Budish,
Assistant Research Director at the Berkman Klein Center for Internet and
Society at Harvard University, [that]{.underline} US and EU
[policymakers,]{.underline} academics, and industry experts [are
fundamentally aligned on ethical priorities and the
importance]{.underline} of privacy, [human rights and the rule of
law.]{.underline}105 [AI ethical principles and draft regulatory
guidelines published by the European Commission, US Office of Management
and Budget, and US Department of Defense all recognize the need for: AI
systems to protect human rights]{.underline} and privacy; algorithms to
be fair, transparent, safe, secure, and governable; [and policymakers
and AI developers to be responsible and accountable to the
technologies]{.underline} (See Figure 2 below for an overview of common
language found in US and EU policy documents). Furthermore, according to
Acting Director Mulchandani and Andrea Renda, Head of Global Governance,
Regulation, Innovation and the Digital Economy at the Centre for
European Policy Studies[, US policymakers have recognized the importance
of an ethical, human-centered approach to AI for their European
counterparts and endeavored to communicate alignment on AI
principles]{.underline}.106 Differences could manifest in the
implementation and operationalization of these values, but these
discrepancies can be mitigated. The EU has positioned itself as a leader
in trustworthy and human-centric AI107 while the US108 highlights the
need for AI innovation to protect American values, civil liberties, and
privacy.

### AT France blocks the plan

#### France has released an AI strategy which supports human responsibility for action

**Stanley-Lockman, 2021 - Center for Security and Emerging Technology**
\[Zoe CSET Issue Brief August "Responsible and Ethical Military AI
Allies and Allied Perspectives"
https://cset.georgetown.edu/publication/responsible-and-ethical-military-ai/
Acc. 4/21/22 TA\]

[The French Ministry of Armed Forces has been studying AI associated
risks since 2019]{.underline}.37 That September, [France became the
first]{.underline} (and, still at the time of writing, only) [European
ally to publicly issue a dedicated military AI strategy.38 Ethics and
responsibility appear throughout the strategy, most notably as aspects
of "controlled AI"]{.underline} and in the announcement to establish a
ministerial Defence Ethics Committee.39 Subsequently, as described
below, the advisory opinions of the Defence Ethics Committee also lend
insights into French views on responsibility and other concepts. Each of
these building blocks is indicative of French thought and implementation
pathways for ethical and responsible military AI, even if not in the
form of adopted principles. The French military AI strategy, called
Artificial Intelligence in Support of Defence, describes "controlled AI"
as the overarching framework for Ministry guidelines on AI adoption,
including aspects that relate to ethics.40 Notably, control (maîtrise)
in this context refers to harnessing and governing AI, and is not
synonymous with human control (see Appendix II).41 [In addition to
adoption priorities like the imperative to maintain freedom of action
and interoperability with allies, the guidelines see "trustworthy,
controlled, and responsible AI" as interlinked concepts]{.underline}
under the headline "guidelines for controlled defence AI." These three
concepts come to light in [the need for the Ministry to "have robust and
secure systems which can be trusted]{.underline} to assist service
personnel and commanders, [dispelling any 'black-box' effect, while
retaining human responsibility for action."]{.underline}42 The rest of
the section unpacks what trustworthiness, control, and responsibility
mean, as gleaned from both the French military AI strategy and the
Defence Ethics Committee advisory opinions on other technologies.43

#### Collaboration creates the environment to overcome obstacles - the US will compromise to accommodate digital sovereignty and supply chain concerns

**Stanley-Lockman, 2021 - Center for Security and Emerging Technology**
\[Zoe CSET Issue Brief August "Responsible and Ethical Military AI
Allies and Allied Perspectives"
https://cset.georgetown.edu/publication/responsible-and-ethical-military-ai/
Acc. 4/21/22 TA\]

Other principles and equivalent topics, such [as controlled AI and
feedback mechanisms for societal input, are]{.underline} also [more
explicitly laid out in allies' approaches to AI ethics]{.underline}.
Here again, [this is not to say that the United States does not share
sovereignty concerns, but rather that these concerns are not as explicit
in DOD's ethical AI principles as in allies' documents. Seeing national
sovereignty as part of responsibility could come to be in tension with
cooperation---]{.underline}as well as procurement decisions that breed
dependence on the United States. It is notable that [DOD is just
beginning to insert supply chain considerations into]{.underline} its
publicly available [documentation on RAI. Meanwhile, it has been part of
the French approach since they began considering ethical risks of AI in
defense,]{.underline} and is also included in the Australian Method. [As
countries navigate this nexus, the extent to which sovereignty concerns
fuel tensions between democratic allies will depend on other forms of
cooperatio]{.underline}n.186 Nevertheless, because of the overlap
between security and assurance of control over the lifecycle of an AI
system, [responsible AI implementation pathways in the United States may
come to incorporate supply chain risks]{.underline}.187 In this way, it
would be [similar to]{.underline} traceability and [auditability
concerns that countries such as France]{.underline} and Australia
[mention in their approaches to sovereignty in AI.]{.underline}

### AT Britain blocks the plan

#### Britain would support the plan -- their Integrated Review Command Paper proves they support ethics regulations for AI

**Stanley-Lockman, 2021 - Center for Security and Emerging Technology**
\[Zoe CSET Issue Brief August "Responsible and Ethical Military AI
Allies and Allied Perspectives"
https://cset.georgetown.edu/publication/responsible-and-ethical-military-ai/
Acc. 4/21/22 TA\]

[In the U.K., ethical and normative aspects of AI feature in recent
strategic documents, including the government's Integrated Review of
national security and international policy,]{.underline} [and in the
Ministry of Defence's accompanying Command Paper]{.underline} published
a week later in March 2021. [The Integrated Review names "supporting the
effective and ethical adoption of AI and data technologies" and
"identifying international opportunities to collaborate on AI R&D,
ethics and regulation" as aspects that can help build public trust and
early adoption of military AI]{.underline}.94 This is consistent with
the Ministry of Defence's contributions to achieving the British
strategic interest of "the ethical development and deployment of
technology based on democratic values," as reaffirmed in the Command
Paper.95 One area of daylight between the two documents, however, is the
Integrated Review's concern about the gap between the pace of global
governance and the development of standards and norms, in contrast to
the Command Paper's stated need for "standards and norms for the
responsible and ethical adoption of these new technologies."96 How
exactly the U.K. Ministry of Defence will approach these interrelated
military governance challenges is due to become clearer in the near
future. More specifically, [the U.K. plans to establish a new Defence AI
Centre in order to centralize its AI developments]{.underline}.97
Further, [the U.K. Ministry of Defence is planning to publish a Defence
AI Strategy that will incorporate ethical adoption
considerations.]{.underline}98 A ministerial AI ethics committee is also
currently analyzing AI in defense, including issues related to trust.99
In terms of oversight, both the new Defence AI Centre and this committee
are important developments to bridge ethical AI endeavors at the working
level with a higher degree of political and strategic attention. [The
U.K. approach to military AI adoption includes a process for developing
guidelines on ethical AI]{.underline}, which includes public-facing
aspects led by the Defence Science and Technology Laboratory (Dstl).100
Dstl established an AI Lab in 2018, which has made it the natural home
for technical questions related to ethics, risk, and safety concerns.101
While few details of the ministerial AI ethics committee are available
at the time of writing, Dstl's activities advancing AI ethics in defense
provide an indication of the U.K. approach. For instance, Dstl sponsors
an ethics fellow at the Turing Institute to focus on "improving
robustness, resilience, and responses of systems that support
logistical, tactical and strategic operations, as well as wider
applications in urban analytics, cybersecurity and social data
science."102 Furthermore, in 2020, they also hosted a conference that
focused on safety, robustness, trustworthiness---which is part of the
process on creating ethical guidelines for military adoption of AI.103

### 

### AT Lack of Trained Workforce

#### NATO collaboration will increase recruitment of an AI workforce because it pools resources

**Gilli, 2020 - Senior Researcher at the NATO Defense College**
\[Andrea, NDC Research Paper No.15 -- December ""NATO-Mation":
Strategies for Leading in the Age of Artificial Intelligence"
https://www.ndc.nato.int/news/news.php?icode=1514 Acc 4/21/22 TA\]

[Access to talent and recruitment. As "NATO-mation" accelerates, and
NATO Allies integrate enterprise-level AI into their armed forces,
recruitment challenges will grow more acute]{.underline}: the faster the
pace, the more acute the challenge. [The essence of
strategy]{.underline}, however, [is to dictate the pace of
change]{.underline} so as not to have it determined by material
constraints [Alliance coordination]{.underline}, as discussed above,
[can prove an important enabler in this respect. On the one hand, the
Alliance can design pooling or scaling mechanisms for some types of AI
tasks]{.underline} or missions: for instance, 80 to 90 percent of data
science will entail manual vetting of data, including labelling and
structuring. There is enormous potential for plugging in different
contributions to the overall effort from different corners of the
Alliance, and thus reducing individual Allies' manpower needs.
[Similarly, by coordinating and cooperating on the development of common
solutions, Allies can share their talent pool and thus more easily
achieve their end goals]{.underline}. The economics of software affords
an advantage in this respect. In terms of human capital, the entry
barriers for software development are high: it takes time, resources and
institution to develop a talented software workforce. However once
developed, software can be reproduced at basically no cost. [NATO Allies
thus have an incentive to work together with joint teams for the
development of solutions]{.underline}. Finally, through an organization
like a potential A3IC, [NATO could provide an important source of
support for individual Allies: for instance, it could assist the
creation of digital corps or digital reserves as well as in revisiting
recruitment procedures for machine learning exerts]{.underline}.190

#### NATO Cooperation is key to luring AI Talent

**Stanley-Lockman, 2021 - Center for Security and Emerging Technology**
\[Zoe CSET Issue Brief August "Responsible and Ethical Military AI
Allies and Allied Perspectives"
https://cset.georgetown.edu/publication/responsible-and-ethical-military-ai/
Acc. 4/21/22 TA\]

Executive Summary Since the U.S. Department of Defense adopted its five
safe and ethical principles for AI in February 2020, the focus has
shifted toward operationalizing them. Notably, implementation efforts
led by the Joint Artificial Intelligence Center (JAIC) coalesce around
"responsible AI" (RAI) as the framework for DOD, including for
collaboration efforts with allies and partners.1 With a DOD RAI Strategy
and Implementation Pathway in the making, the first step to leading
global RAI in the military domain is understanding how other countries
address such issues themselves. This report examines how key U.S. allies
perceive AI ethics for defense. [Defense collaboration in AI builds on
the broader U.S. strategic consensus that allies and partners offer
comparative advantages relative to China and Russia]{.underline}, which
often act alone, and that [securing AI leadership is critical to
maintaining the U.S. strategic position and technological edge.
Partnering with other democratic countries therefore has implications
for successfully achieving these strategic goals]{.underline}. Yet the
military aspects of responsible AI that go beyond debates on autonomous
weapons systems are currently under-discussed.

### AT Russia and China won't Follow

#### Plan increases Russia and China's incentive to follow ethical norms, and increases pressure on them if they do not follow

**Kahn and Horowitz, 2021 -- Research and Senior Fellows at the Council
on Foreign Relations** \[Lauren and Michael, The Washington Quarterly
44:4 "Leading in Artificial Intelligence through Confidence Building
Measures" [https://doi.org/10.1080/0163660X.2021.2018794 Acc
6/6/22](https://doi.org/10.1080/0163660X.2021.2018794%20Acc%206/6/22)
TA\]

[China and Russia might also have good reasons to sign onto standards
that would commit to enhanced military AI safety. Neither country would
like to be perceived as not following international standards on
military AI safety, because it could undermine support within their AI
research communities and ability to keep those researchers.]{.underline}
Moreover, [if China and Russia did not sign, it would help the United
States build international credibility as a responsible AI actor,
increasing its attractiveness in the global competition for AI
talent]{.underline}.24 [Thus, whether China or Russia sign or not, the
United States would benefit.]{.underline} Overall, [standardization
would help to promote best practices concerning safety and ethics in the
development and adoption of these technologies, and would help to
alleviate some pressure and remove some sources of error during
use]{.underline}.

#### Ethical principles will lure AI developers from Russia and China -- innovators would prefer to work on responsible AI

**Trabucco, 2020 - Research Assistant at the Centre for Military Studies
at the University of Copenhagen** \[Lena, May 10 "AI Partnership for
Defense is a Step in the Right Direction -- But Will Face Challenges"
http://opiniojuris.org/2020/10/05/ai-partnership-for-defense-is-a-step-in-the-right-direction-but-will-face-challenges/
Acc 4/17/22 TA\]

In February, [we became the first military in the world to adopt ethical
principles for the use of AI]{.underline}, based on core values of
transparency, reliability, and governability. [These principles make
clear to]{.underline} the American people -- and [the world -- that the
United States will]{.underline} once again [lead the way in the
responsible development and application of emerging technologies,
reinforcing our role as the global security partner of
choice]{.underline}. [The US created the partnership in an effort to
maintain healthy lead and competitive advantage over China and
Russia]{.underline}, or "near-peer rivals," [in AI military innovation
and development.]{.underline} The DoD hopes the AI partnership will
maintain that competitive edge by offering opportunities for players in
the AI space to engage with security partners committed to ethical and
responsible AI development and application. The idea is [the partnership
could attract developers and innovators with a reliable alternative to
Chinese and Russian working relationships.]{.underline}

### AT Reclassification Circumvention

#### A lack of international norms legitimizes hiding autonomous weapons -- the plan establishes those legal and ethical norms. 

**Nadibaidze, 2021 - Ph.D. Student at the University of Southern
Denmark** \[Anna Entry submitted for the Second OSCE-IFSH Essay
Competition "Commitment to Control over Weaponised Artificial
Intelligence: A Step Forward for the OSCE and European Security"
https://www.osce.org/files/f/documents/8/3/507341.pdf Acc. 4/21/22 TA\]

[The 2020 Nagorno-Karabakh conflict demonstrates the types of risks
coming from the uses of weaponised AI. Unmanned aerial vehicles (UAVs)
were used by both Azerbaijan and Armenia]{.underline}, and several IHL
violations were recorded on both sides (Kozyulin 2021). While these
weapons systems are not officially classified as LAWS, their use has
been deemed an efficient way of conducting warfare and could even
contribute to other States' pursuit of unmanned vehicles (Cooper 2021).
[The issue is that there is no way of verifying the level of human
control over these weapons systems, which allows for operational
practices to continue silently changing norms of war and legitimize the
use of weaponised AI]{.underline}. In a possible future armed conflict
in Eurasia, there is potential for more IHL violations and further
diminishing role of human control over warfare. In other words, ["the
operational trend towards developing AI-enabled weapons systems
continues and is on track to becoming established as 'the new normal' in
warfare"]{.underline} (Bode and Huelss 2021, 224). [While there are no
legal norms of a responsible use of weaponised AI, the ways that States
use this technology will continue to shape the way that warfare is
conducted, while increasing risks to European security and
stability.]{.underline}

### AT Industry Circumvention

#### Industries want to follow ethical principles -- they have asked governments for clear guidelines

**Bolton, 2021 - professor of political science at Pace University**
\[Matthew with Matilda Byrne, Ryan Gariepy, Emilia Javorsky, Volker
Lehmann, and Laura Nolan, January "Addressing The Threat Of Autonomous
Weapons Maintaining Meaningful Human Control"
http://library.fes.de/pdf-files/iez/17215.pdf Acc 5/27/22 TA\]

[Further progress toward stigmatization of LAWS will require engaging
with]{.underline} multiple stakeholders, including
[industry]{.underline}, academia and civil society. However, inclusion
of a broad range of stakeholders should not distract from the onus of
responsibility for action remaining on states. [Expecting the private
sector to establish and maintain voluntary guidelines or codes of
conduct on meaningful human control is unrealistic,]{.underline} given
that [states are the customers]{.underline} of weapons contracts [and
stipulate their expectations to the private sector]{.underline}. [In
fact, technology companies themselves have stressed the need for clear
guidelines from states to help engineers]{.underline}, designers and
technology workers [make moral, ethical, and legal judgements about the
systems they build]{.underline}.11

#### Establishing Responsible AI norms signals to private industry that ethical concerns are important.

**Stanley-Lockman, 2021 - Center for Security and Emerging Technology**
\[Zoe CSET Issue Brief August "Responsible and Ethical Military AI
Allies and Allied Perspectives"
https://cset.georgetown.edu/publication/responsible-and-ethical-military-ai/
Acc. 4/21/22 TA\]

This is important not only for accountability, including to citizenries,
but also because [dedicating attention to responsible AI is a critical
way to signal to industry, civil society, academia, and the research
community that appropriate measures are not just boxes to tick, but are
fundamentally embedded in the development of systems. In other words,
responsible AI is important not just for public opinion, but also to
strengthen relationships with the expert community that is rightfully
concerned about the ethical implications of current AI
advancement.]{.underline}

### 

## Off Case Responses

### AT NATO Military Readiness DA

#### [Turn -- Reputation]{.underline} -- ethical norms attract top AI developers and improves the reliability of the weapons -- this improves NATO's technological superiority. It also prevents accidental wars. 

**Kahn and Horowitz, 2021 -- Research and Senior Fellows at the Council
on Foreign Relations** \[Lauren and Michael, The Washington Quarterly
44:4 "Leading in Artificial Intelligence through Confidence Building
Measures" [https://doi.org/10.1080/0163660X.2021.2018794 Acc
6/6/22](https://doi.org/10.1080/0163660X.2021.2018794%20Acc%206/6/22)
TA\]

[Leadership in standard-setting and confidence-building measures will
also enhance US military capability, rather than constraining the US
military]{.underline}, assisting the United States in strategic
competition. Unsafe AI systems do not just risk unintentional conflict
and inadvertent escalation---they are less likely to be effective
systems. [Committing to lead the world in AI safety could create a
ripple effect in the US defense enterprise and the private sector about
putting a premium on safe and ethical AI, in turn making it more likely
that military (and civilian) uses of algorithms are reliable, improving
their utility for the military]{.underline}. [Such signaling will also
likely improve the reputation of the US military with Silicon Valley and
AI/ML researchers, whose concerns about military uses of AI have loomed
large]{.underline} since a protest halted Google's renewal of its
Project Maven contract in 2018. While concerns about Silicon Valley's
opposition to the Department of Defense are overstated according to some
survey research,37 other surveys of AI/ML professionals show that
increasing the emphasis on safety is a high priority.38 [A public
commitment to safety will thus help the Department of Defense improve at
attracting top STEM talent, including AI/ ML talent, increasing its
ability to keep the American military ahead]{.underline}. One might
argue that the United States should let others lead on AI, focusing
instead on developing AI-enabled capabilities and not concerning itself
with how other countries behave. But there is no substitute for American
leadership and its ability to rally countries around the world to
support shared standards. [If promoting norms of responsible behavior
with AI encourages other states to use military applications of AI in
more responsible ways, it will create a more]{.underline} ethical and
[predictable security environment, likely benefiting the United
States]{.underline}. Additionally, current international dialogue about
military uses of AI focuses almost exclusively on lethal autonomous
weapon systems (LAWS), the subject of a Group of Governmental Experts in
the Convention on Certain Conventional Weapons.39 Currently, the
international conversation has been largely been driven by NGOs such as
the Campaign to Stop Killer Robots.40 While such conversations help
bring attention to some of these issues, they oversimplify the risks and
fixate on worst-case scenarios that are more likely outcomes of
artificial general intelligence or human level machine intelligence
rather than technology today. LAWS represent only a small fraction of
the universe of potential issues surrounding military applications of
AI. Broadening the international conversation about military uses of AI
to incorporate the full scope of potential applications would generate
better dialogue because it would include more of the real-world AI
scenarios likely to confront militaries. Expanding the discussion would
also allow states to pursue levels of control and regulation other than
an all-or nothing ban and create a more calibrated and flexible range of
approaches to different technologies with various levels and types of
associated risks. Finally, [one might argue that due to the military
importance of integrating advances in AI, the United States should not
do anything that could limit American behavior or flexibility with AI.
However]{.underline}, [the strategy]{.underline} above, [in addition to
decreasing the risk of unintentional conflict, would not constrain the
United States in developing or integrating advances in AI for military
purposes.]{.underline}

#### [No Link - Plan does not undermine NATO's military advantage]{.underline} - Military ethics and military effectiveness do not trade off, because ethics are a precondition for battlefield effectiveness.

**Trabucco and Stanley-Lockman, 2022 -- prof of Political Science,
University of Copenhagen and prof of Defense and Strategic Studies,
Nanyang Technological University** \[Lena and Zoe, The Oxford Handbook
of AI Governance, March, "NATO's Role in Responsible AI Governance in
Military Affairs"
https://www.oxfordhandbooks.com/view/10.1093/oxfordhb/9780197579329.001.0001/oxfordhb-9780197579329-e-69
Acc 4/16/22 TA\]

Additionally, [infusing AI development with certain ethical principles
and values can have operational advantages and benefits, and NATO can,
in particular, promote the ethical principles as operational standards
for the Allies. A common critique within the ethics debate is that
approaching new technology with an ethical]{.underline} or democratic
values-driven [perspective translates into comparative military
disadvantage]{.underline}. Essentially, if your adversary develops
technology without the constraints of ethical principles then there will
be diminished effectiveness on the battlefield.56 We find [this
critique]{.underline} unfounded because it [assumes there is a false
trade-off between ethics and effectiveness; instead]{.underline}, we
argue [ethical foundations are built into the architecture of modern
warfare]{.underline}.57 [As such, ethics is a background condition for
battlefield effectiveness, which is already infused in military
decision-making and helping to guide the boundaries of international
humanitarian law.]{.underline} As such, [ethical guidelines do not have
to detract from a military's capacity]{.underline} or competency to
devise means and methods of warfare that will serve their national or
coalition interest.[58 If anything, a first-mover advantage can
incentivize an ethical and values-driven AI to establish the threshold
of technological standards globally.]{.underline}59

#### [Turn -- Operator Trust]{.underline} -- ethical use policies increase a soldier's Trust in their weapons, which is essential to their effective use. It doesn't matter how good your AI weapons are if the military won't use them. 

**Gilli, 2020 - Senior Researcher at the NATO Defense College**
\[Andrea, NDC Research Paper No.15 -- December ""NATO-Mation":
Strategies for Leading in the Age of Artificial Intelligence"
https://www.ndc.nato.int/news/news.php?icode=1514 Acc 4/21/22 TA\]

[This is important because AI is data-intensive, unpredictable and
brittle.98 While a system may work well in the context in which it was
trained, it may break in an unfamiliar setting.]{.underline} The
reliance on BD in the current second wave of ML also creates
fallibilities,99 given that the [algorithms can scale up harm if the
data over- or under-represents certain grou]{.underline}ps.100 [For
neural networks in particular, it may be impossible to explain or
interpret results. Some refer to this as the "black-box" problem,
meaning that the outcomes of these complex AI systems are opaque to
humans that either want to reproduce the good, or prevent the bad from
recurring.]{.underline} [While traditional software can be debugged to
solve a performance issue, the lack of linear causality between a
programmer's inputs and the AI system's outputs means that it is
difficult to track bias and reliability]{.underline}. [Creating
organizational processes to minimize these concerns across the AI
lifecycle is critical to responsible use of the technology.]{.underline}
Considering [AI ethics also means developing trust in
systems]{.underline}. Organizations developing AI applications should
think deeply about user expectations related to transparency and
disclosure. Ultimately, these norms will change the distinction between
human, AI-assisted and AI interactions.101 "Calibrating" trust is
especially important for AI-assisted decision making,102 a concept whose
value will only increase given the emphasis on human-machine teaming in
Allied militaries.103 [In many cases, the]{.underline} unethical or
[unacceptable outcomes of AI systems pertain]{.underline} not to moral
dilemmas, but [to the reliability and robustness of the systems at hand.
As such, building trust is fundamentally tied to building safe and
secure systems]{.underline}.104 The problem of bias in AI illustrates
the overlap between AI safety and AI ethics: bias is morally
problematic, because it can unfairly harm or systematically discriminate
against specific groups105 -- and it can also be seen as a failure mode
that reduces the reliability of a given algorithm in a given context.106
[For NATO, this relates to the safety of enterprise tools and weapons
systems alike. In deployments, these safety measures would be critical
to ensure that AI-enabled weapons systems are used in a manner
consistent with the principles of international humanitarian
law.]{.underline} As explained in greater detail below, this may also
feed into the eventual standardization process.

#### [Solvency Outweighs their Link]{.underline} - Irresponsible spread of autonomous AI weapons causes international instability which is more important than the impact plan has on deployment

**Sauer, 2021 - Senior Research Fellow at Bundeswehr University**
\[Frank serves on the International Panel on the Regulation of
Autonomous Weapons IRRC No. 913 March "Stepping back from the brink: Why
multilateral regulation of autonomy in weapons systems is difficult, yet
imperative and feasible"
https://international-review.icrc.org/articles/stepping-back-from-brink-regulation-of-autonomous-weapons-systems-913
Acc 4/5/22 TA\]

Nevertheless, [regulating weapon autonomy in a manner that curbs
autonomy in the critical functions and keeps them under human control is
sorely needed.]{.underline} After all, [the consequences of inaction
would be dire]{.underline} [because the mid- and long-term
strategic]{.underline} and ethical [risks of unshackled weapon autonomy
far outweigh the desired short-term military gains]{.underline}
highlighted above. I will argue this in two steps below, by first
focusing on a number of operational and strategic implications and
subsequently evaluating the ethical implications of weapon autonomy in
regard to human dignity. Why regulating weapon autonomy is imperative:
Strategic implications [The potential impact of unregulated weapon
autonomy on military operations, as well as on global peace and
strategic stability as a whole, has drawn]{.underline} scholarly
[attention]{.underline} for quite a while.38 [This body of literature
suggests that the implications of regulatory inaction and an ensuing
rapid diffusion of weaponized autonomy-enabling technology range from
new military vulnerabilities to increased risks of instability and
escalation at both the operational and the strategic
level]{.underline}.39 Hence it is in fact especially [the great
powers]{.underline} that [should see it as being]{.underline} not only
their responsibility but also [in their genuine
self-interest]{.underline}40 [to curb this destabilizing chain of
effects]{.underline}.

#### [Turn -]{.underline} [Global Relevance]{.underline} - NATO standards for AI help improve the international image of NATO as a credible alliance

**Trabucco and Stanley-Lockman, 2022 -- prof of Political Science,
University of Copenhagen and prof of Defense and Strategic Studies,
Nanyang Technological University** \[Lena and Zoe, The Oxford Handbook
of AI Governance, March, "NATO's Role in Responsible AI Governance in
Military Affairs"
https://www.oxfordhandbooks.com/view/10.1093/oxfordhb/9780197579329.001.0001/oxfordhb-9780197579329-e-69
Acc 4/16/22 TA\]

New power distributions around AI and adjacent dual-use technologies are
among the motivating factors causing the Alliance to reconsider whether
its technological superiority may be threatened in the years ahead, as
reflected in the 2019 Emerging and Disruptive Technologies (EDTs)
Roadmap2 and, more recently, the NATO 2030 process.3 NATO navigates
these changes and then approaches AI-accelerated changes to the
international security environment in a highly political context.
Notably, in 2019, French President Emmanuel Macron surprised many
European counterparts by declaring NATO "brain-dead," a warning wrapped
in an even larger warning of trans-Atlantic security divisions.4 The
critique that NATO is a "brain-dead" or "irrelevant" institution has
existed in some form since the end of the Cold War.5 [As NATO combats
global perceptions of organizational irrelevance, there is a reason to
push for bureaucratic adaptation to better manage technology-driven
changes in the future. As such,]{.underline} despite some warnings to
the contrary, [Allies have an incentive to keep NATO a relevant military
institution and ensure that it adapts to emerging threats and for future
military contexts.]{.underline} The comment from President Macron helped
prompt the NATO 2030 agenda, which is currently taking shape to increase
the Alliance's role as a political actor and as an organization with a
greater focus on EDTs.6 As NATO bodies and Allies prepare for the impact
of AI on future military operations, [the Alliance has its own
responsibility to steward AI in ways that, inter alia, promote cohesion
between democratic countries, prevent risks, shore up interoperability,
project deterrence, and ensure stability.7 To achieve these aims,
cooperation and alignment are critical for the Alliance to maintain a
competitive edge and promote further innovation in alignment with shared
values]{.underline}.

#### [Case Outweighs]{.underline} - Military effectiveness does not justify the use of immoral weapons

**Johnson and Axinn, 2013 - Prof of Philosophy at the Univ of South
Florida and PhD Candidate in Engineering at Penn** \[Aaron and Sidney,
Journal of Military Ethics, Volume 12, Issue 2, August "The Morality of
Autonomous Robots" www.tandfonline.com/10.1080/15027570.2013.818399 Acc
12/27/20 TA\]

[Proponents of autonomous technology cite their potential tactical
benefits and argue that they reduce the risk of human harm]{.underline}
(at least on one side of the battle), [and we are therefore morally
compelled to use them if available and efficacious]{.underline}
(Strawser 2010). There is little doubt that an autonomous robot could
help an army, just as in the past nations have sometimes used other
contentious weapons to their advantage. [Any time a new weapon is
developed that allows you to more easily kill or hurt your enemy without
being killed yourself there is a tactical benefit to using the
weapon]{.underline} -- for example the U.S. deployment of atomic weapons
at the end of WWII had tactical benefits that is said to have led to the
Japanese surrender. [But as clearly established in the precedents of
atomic/nuclear weapons, chemical weapons, anti-personnel land mines,
barbed spears, etc, the efficacy of a weapon is not justification for
its use.]{.underline}

#### [Turn -- Friendly Fire]{.underline} - Safety and Reliability are key parts of ethical principles -- accountability is necessary to maintain public support by reducing friendly fire.

**Trabucco and Stanley-Lockman, 2022 -- prof of Political Science,
University of Copenhagen and prof of Defense and Strategic Studies,
Nanyang Technological University** \[Lena and Zoe, The Oxford Handbook
of AI Governance, March, "NATO's Role in Responsible AI Governance in
Military Affairs"
https://www.oxfordhandbooks.com/view/10.1093/oxfordhb/9780197579329.001.0001/oxfordhb-9780197579329-e-69
Acc 4/16/22 TA\]

Safety and security For humans to meet ethical and legal commitments
when developing and deploying AI, the systems themselves must be safe,
secure, and reliable. More simply put, [if humans and institutions
interacting with AI do not have confidence that the systems will perform
as expected, then they cannot assure that its development and deployment
are responsible. This makes safety and security a key pillar of
responsible AI governance]{.underline} for any actor.82 As this section
explores [for NATO]{.underline} in particular, [safety and security are
indispensable to the Alliance's stated goals to focus its approach to
EDTs in the areas of "deterrence and defense, capability development,
legal and ethical norms, and arms control aspects]{.underline}."83
[Politically, democratic militaries using AI cannot be accountable to
their citizenries nor their coalition partners if they lack mechanisms
to trace and explain how their systems are reliable]{.underline}.
Accidents and interference with AI systems could likewise create
political risks for the Alliance. For example, if deepfakes and
micro-targeted information attacks compromise confidence in the
integrity of information used to build a common operating picture, then
the operational difficulties could also erode political trust between
Allies in a few key ways. [In the North Atlantic Council, disagreement
about the integrity of information could slow the decision-making body's
ability to react to fast-changing operational realities]{.underline}.84
Further, [compromised AI systems may not only make it harder for forces
to prevent harm to non-combatants, but also to prevent friendly
fire.]{.underline} In this way, [coalition forces arguably face even
higher obligations to coordinate on the reliability of their
systems,]{.underline} relative to adversaries and near-peer competitors
that tend to operate alone. As such, responsible AI governance is not
purely technical; policy alignment and strategic planning are likewise
necessary to draw attention to risk management above the tactical level.

#### NATO cooperation on ethical principles will not undermine our technological superiority

**Trabucco and Stanley-Lockman, 2022 -- prof of Political Science,
University of Copenhagen and prof of Defense and Strategic Studies,
Nanyang Technological University** \[Lena and Zoe, The Oxford Handbook
of AI Governance, March, "NATO's Role in Responsible AI Governance in
Military Affairs"
https://www.oxfordhandbooks.com/view/10.1093/oxfordhb/9780197579329.001.0001/oxfordhb-9780197579329-e-69
Acc 4/16/22 TA\]

This chapter explores a role for the North Atlantic Treaty Organization
(NATO) in the emerging military artificial intelligence (AI) governance
architecture. As global powers compete for capabilities that AI can
offer, NATO has the challenging task of recalibrating strategic
relationships in the coming years. NATO has begun to recognize
technological change as a necessary variable and, in turn, adapt its
organizational composition and strategy to increase the Alliance's
capacity to meet emerging security challenges. [As NATO bodies and
Allies prepare for the impact of AI on future military operations, NATO
has its own responsibility to steward AI in ways that promote
harmonization among Allies and advance the NATO mission]{.underline}.
Toward this effort, the chapter highlights two governance mechanisms
within NATO's competency---strategic and policy planning, and standards
and certification---as practices that exemplify NATO's power to shape
the trajectory of technological development. [We operationalize these
governance tools by examining the three pillars that are particularly
challenging for AI governance: ethics and values, legal norms, and
safety and security]{.underline}. Within each pillar, we examine NATO's
facilitation of strategic policy planning and standards and
certification to emerge as a leader in establishing responsible
technological development and, ultimately, a more secure international
security environment. This chapter finds [there is space for NATO to
pursue its agenda to maintain technological superiority not just to
protect and defend its way of life, but to build on AI governance
pillars to steward military innovation on a responsible
trajectory.]{.underline}

#### NATO leadership on responsible AI use will allow it to set global norms and maintain our military advantage

**Trabucco and Stanley-Lockman, 2022 -- prof of Political Science,
University of Copenhagen and prof of Defense and Strategic Studies,
Nanyang Technological University** \[Lena and Zoe, The Oxford Handbook
of AI Governance, March, "NATO's Role in Responsible AI Governance in
Military Affairs"
https://www.oxfordhandbooks.com/view/10.1093/oxfordhb/9780197579329.001.0001/oxfordhb-9780197579329-e-69
Acc 4/16/22 TA\]

At the core, this chapter argues [that NATO is well positioned to
steward the development of military AI and institute governance
mechanisms towards coalition inclusion of responsible AI while
simultaneously maintaining incentives for comparative
advantage]{.underline}. Using the three pillars---ethics and values,
legal norms, and safety and security---as issue areas which present AI
governance challenges, we show that [NATO has space to emerge as a
leader in AI governance and contribute to responsible adoption of EDTs
in the international security environment]{.underline}. This builds on
foundations that derive NATO's responsibilities to govern AI according
to its values, legal obligations, and institutional interests. These
foundations from both STS and military innovation studies offer ways
that [the Alliance can activate its existing governance mechanisms to
exert influence in new ways. Not only is this influence important for
the Alliance to bolster its institutional relevance in an evolving
international security architecture, but it also dovetails with its
capacity to shore up military effectiveness and interoperability as
Allies modernize their arsenals and associated concepts into the
frontier]{.underline} [of AI.]{.underline}

### AT Industry Innovation DA

#### Turn - NATO collaboration will increase recruitment of an AI workforce because it pools resources

**Gilli, 2020 - Senior Researcher at the NATO Defense College**
\[Andrea, NDC Research Paper No.15 -- December ""NATO-Mation":
Strategies for Leading in the Age of Artificial Intelligence"
https://www.ndc.nato.int/news/news.php?icode=1514 Acc 4/21/22 TA\]

[Access to talent and recruitment. As "NATO-mation" accelerates, and
NATO Allies integrate enterprise-level AI into their armed forces,
recruitment challenges will grow more acute]{.underline}: the faster the
pace, the more acute the challenge. [The essence of
strategy]{.underline}, however, [is to dictate the pace of
change]{.underline} so as not to have it determined by material
constraints [Alliance coordination]{.underline}, as discussed above,
[can prove an important enabler in this respect. On the one hand, the
Alliance can design pooling or scaling mechanisms for some types of AI
tasks]{.underline} or missions: for instance, 80 to 90 percent of data
science will entail manual vetting of data, including labelling and
structuring. There is enormous potential for plugging in different
contributions to the overall effort from different corners of the
Alliance, and thus reducing individual Allies' manpower needs.
[Similarly, by coordinating and cooperating on the development of common
solutions, Allies can share their talent pool and thus more easily
achieve their end goals]{.underline}. The economics of software affords
an advantage in this respect. In terms of human capital, the entry
barriers for software development are high: it takes time, resources and
institution to develop a talented software workforce. However once
developed, software can be reproduced at basically no cost. [NATO Allies
thus have an incentive to work together with joint teams for the
development of solutions]{.underline}. Finally, through an organization
like a potential A3IC, [NATO could provide an important source of
support for individual Allies: for instance, it could assist the
creation of digital corps or digital reserves as well as in revisiting
recruitment procedures for machine learning exerts]{.underline}.190

#### AI Regulation will not undermine industry innovation -- empirical examples of technology prove.

**Heikkilä, 2021 - Politico's AI Correspondent in London** \[Melissa,
Politico March 31 "AI Decoded: NATO on AI warfare --- AI treaty
consultation --- Unions call for more AI protections"
https://www.politico.eu/newsletter/ai-decoded/politico-ai-decoded-nato-on-ai-warfare-ai-treaty-consultation-unions-call-for-more-ai-protections/
Acc 4/9/22 TA\]

AI LAWS WHY [REGULATION WON'T KILL INNOVATION]{.underline}: The Council
of Europe, the Strasbourg-based human rights organization, is hard at
work on a draft proposal on artificial intelligence. If ratified, the
treaty could become national law in the group's 47 member countries. AI:
Decoded rang up Gregor Strojin, the chair of the group's committee,
called CAHAI, that's drawing up the rules to hear the latest
developments. The goal: [CAHAI is working on an AI treaty with
additional rules for specific sectors,]{.underline} such as social
affairs and justice systems, and problems, like the discrimination of
minorities. The plan is to have a draft ready by the end of they year,
after which the Committee of Ministers --- national representatives to
the Council of Europe --- will debate it. "In the past, there have been
examples of treaties that were adopted in a matter of months. But
sometimes it can take years. Sometimes it can be never. But I think
we're making a pretty strong point that in this case, delaying it could
lead to irreparable damage," [Strojin said. "We are seeing increasing
use of technologies that's being imported from other parts of the world,
without any risk assessment, without any impact assessment in a way
that's actually dumping the technology on certain countries," he
continued.]{.underline} Been there done that: [Strojin also pushed back
against industry jeremiads that regulation would "hamper innovation." He
said that AI regulators can actually learn a lot from past negotiations
around pharmaceuticals and bioethics as an example of how regulation can
help innovation.]{.underline} In 1964, the Council of Europe adopted the
European Pharmacopoeia, which is the official standard and scientific
basis for the quality control of pharmaceuticals. ["Before that medical
products and also services were like snake oil. There was no common
assessment of]{.underline} what are the ingredients \[or\] [what are the
side effects of drugs," Strojin]{.underline} [said]{.underline}. (Sound
familiar?) ["I don't think we have a problem with the lack of innovation
in the pharmaceutical sector at this point," Strojin said.]{.underline}

#### Turn -- High Technology businesses are Opposed to autonomous AI systems -- many refuse to make them.

**Docherty, 2018 - senior researcher in the Arms Division of Human
Rights Watch** \[Bonnie August 21, "Heed the Call A Moral and Legal
Imperative to Ban Killer Robots"
[https://www.hrw.org/report/2018/08/21/heed-call/moral-and-legal-imperative-ban-killer-robots#](https://www.hrw.org/report/2018/08/21/heed-call/moral-and-legal-imperative-ban-killer-robots)
Acc 12/27/20 TA\]

Industry [High-profile technology companies and their representatives
have criticized fully autonomous weapons on various grounds. A Canadian
robotics manufacturer, Clearpath Robotics, became the first company
publicly to refuse to manufacture "weaponized robots that remove humans
from the loop."]{.underline}\[132\] In 2014, it pledged to "value ethics
over potential future revenue."\[133\] In a letter to the public, the
company stated that it was motivated by its belief that "that the
development of killer robots is unwise, unethical, and should be banned
on an international scale." Clearpath continued: \[W\]ould a robot have
the morality, sense, or emotional understanding to intervene against
orders that are wrong or inhumane? No. Would computers be able to make
the kinds of subjective decisions required for checking the legitimacy
of targets and ensuring the proportionate use of force in the
foreseeable future? No. Could this technology lead those who possess it
to value human life less? Quite frankly, we believe this will be the
case.\[134\] The letter shows that fully autonomous weapons raise
problems under both the principles of humanity and dictates of public
conscience. [In August 2017, the founders and chief executive officers
(CEOs) of 116 AI and robotics companies published a letter calling for
CCW states parties to take action on autonomous
weapons.]{.underline}\[135\] The letter opens by stating, "As companies
building the technologies in Artificial Intelligence and Robotics that
may be repurposed to develop autonomous weapons, we feel especially
responsible in raising this alarm."\[136\] The letter goes on to
highlight the dangers to civilians, risk of an arms race, and
possibility of destabilizing effects. It warns that "\[o\]nce this
Pandora's box is opened, it will be hard to close."\[137\] In a similar
vein in 2018, Scott Phoenix, CEO of Vicarious, a prominent AI
development company, described developing autonomous weapons as among
the "world's worst ideas" because of the likelihood of defects in their
codes and vulnerability to hacking.\[138\]

#### Industries want to follow ethical principles -- they have asked governments for clear guidelines

**Bolton, 2021 - professor of political science at Pace University**
\[Matthew with Matilda Byrne, Ryan Gariepy, Emilia Javorsky, Volker
Lehmann, and Laura Nolan, January "Addressing The Threat Of Autonomous
Weapons Maintaining Meaningful Human Control"
http://library.fes.de/pdf-files/iez/17215.pdf Acc 5/27/22 TA\]

[Further progress toward stigmatization of LAWS will require engaging
with]{.underline} multiple stakeholders, including
[industry]{.underline}, academia and civil society. However, inclusion
of a broad range of stakeholders should not distract from the onus of
responsibility for action remaining on states. [Expecting the private
sector to establish and maintain voluntary guidelines or codes of
conduct on meaningful human control is unrealistic,]{.underline} given
that [states are the customers]{.underline} of weapons contracts [and
stipulate their expectations to the private sector]{.underline}. [In
fact, technology companies themselves have stressed the need for clear
guidelines from states to help engineers]{.underline}, designers and
technology workers [make moral, ethical, and legal judgements about the
systems they build]{.underline}.11

#### No Link -- Human control will not slow technological development

**Bolton, 2021 - professor of political science at Pace University**
\[Matthew with Matilda Byrne, Ryan Gariepy, Emilia Javorsky, Volker
Lehmann, and Laura Nolan, January "Addressing The Threat Of Autonomous
Weapons Maintaining Meaningful Human Control"
http://library.fes.de/pdf-files/iez/17215.pdf Acc 5/27/22 TA\]

Improving communication between policymakers and scientific and
technical experts is crucial given the dual-use nature of artificial
intelligence. Indeed, [a prohibition on weapons outside of human control
would not be counterproductive to technological development. Rather,
there is a need to fully harness technological progress while
maintaining]{.underline} and advancing international law that safeguards
[humanitarian protections, human rights and international peace and
security.]{.underline}

Therefore, in September and October 2020, the Friedrich-

**[AT Soldiers Lives DA]{.underline}**

#### Removing human emotion from killing Increases the violence of war -- we become detached from it.

**Johnson and Axinn, 2013 - Prof of Philosophy at the Univ of South
Florida and PhD Candidate in Engineering at Penn** \[Aaron and Sidney,
Journal of Military Ethics, Volume 12, Issue 2, August "The Morality of
Autonomous Robots" www.tandfonline.com/10.1080/15027570.2013.818399 Acc
12/27/20 TA\]

Healthy Emotions [Arguments have been offered holding that robots/drones
may do a better job than humans in making target decisions because they
have no revenge motive]{.underline} (Arkin 2010). [They are not enraged,
as humans may be,]{.underline} by the killing of their buddies. [But
having no emotions, they do not have the attitude toward people that
'healthy' humans are expected to have. They do not realize the enormity
of an error in killing the 'wrong' person. Why is killing with emotions
morally superior to killing without emotions?]{.underline} As noted
above, honor requires the willingness to risk sacrifice, which in turn
requires intention and feeling---emotion. Since morality requires
respect for duty rather than following selfish goals, that respect also
requires a certain intention or feeling/emotion. The need for human
emotion in warfare is cited by at least some authors writing about
drones in the general press, for example John Sifton says, '[The unique
technology allows the mundane and regular violence of military force to
be separated further from human emotion. Drones foreshadow the idea that
brutality could become detached from humanity---and yield violence that
is, as it were, unconscious]{.underline}.' (Sifton 2012).

#### An Arms Race forces AI to be deployed before it is safe.

**Docherty, 2014 - senior researcher in the Arms Division of Human
Rights Watch** \[Bonnie "Shaking the Foundations The Human Rights
Implications of Killer Robots" Human Rights Watch
http://www.hrw.org/sites/default/files/reports/ arms0514_ForUpload_0.pdf
Acc 12/27/20 TA\]

The human rights implications of fully autonomous weapons compound the
many other concerns about use of the weapons. As Human Rights Watch and
IHRC have detailed in other documents, [the weapons would face
difficulties in meeting the requirements of international humanitarian
law, such as upholding the principles of distinction and
proportionality,]{.underline} in situations of armed conflict. In
addition, [even if technological hurdles could be overcome in the
future, failure to prohibit them now could lead to the deployment of
models before their artificial intelligence was perfected and spark an
international robotic arms race.]{.underline} Finally, many critics of
fully autonomous weapons have expressed moral outrage at the prospect of
humans ceding to machines control over decisions to use lethal force. In
this context, the human rights concerns bolster the argument for an
international ban on fully autonomous weapons.

#### AI cannot deescalate situations -- opponents don't know How to surrender to a robot, and the robot will see that as threatening.

**Docherty, 2014 - senior researcher in the Arms Division of Human
Rights Watch** \[Bonnie "Shaking the Foundations The Human Rights
Implications of Killer Robots" Human Rights Watch
http://www.hrw.org/sites/default/files/reports/ arms0514_ForUpload_0.pdf
Acc 12/27/20 TA\]

In addition, [the deployment of fully autonomous weapons]{.underline} in
law enforcement [situations could affect the actions of the individual
posing a potential threat. He or she might not know how to behave when
confronted with a machine rather than a human law enforcement officer.
The individual might respond differently to a robot]{.underline} than to
a human [and]{.underline} as a result [unintentionally appear
threatening. A robot's misinterpretation of the necessity of force could
trigger an arbitrary killing]{.underline} in violation of the right to
life.

**[AT Civilian Deaths DA]{.underline}**

#### AI is not more accurate in Combat Zones -- AWS only make better decisions than humans under ideal conditions. Combat is too complex and unpredictable for programming

**Docherty, 2018 - senior researcher in the Arms Division of Human
Rights Watch** \[Bonnie August 21, "Heed the Call A Moral and Legal
Imperative to Ban Killer Robots"
[https://www.hrw.org/report/2018/08/21/heed-call/moral-and-legal-imperative-ban-killer-robots#](https://www.hrw.org/report/2018/08/21/heed-call/moral-and-legal-imperative-ban-killer-robots)
Acc 12/27/20 TA\]

[Robots would also not possess the legal and ethical judgment necessary
to minimize harm]{.underline} on a case-by-case basis.\[65\] [Situations
involving use of force, particularly in armed conflict, are often
complex and unpredictable and can change quickly. Fully autonomous
weapons would therefore encounter significant obstacles to making
appropriate decisions regarding humane treatment]{.underline}. [After
examining numerous studies in which researchers attempted to program
ethics into robots, Sharkey found that robots exhibiting behavior that
could be described as "ethical" or "minimally ethical" could operate
only in constrained environments. Sharkey concluded that robots have
limited moral capabilities and therefore should not be used in
circumstances that "demand moral competence and an understanding of the
surrounding social situation]{.underline}."\[66\] Complying with
international law frequently requires subjective decision-making in
complex situations. Fully autonomous weapons would have limited ability
to interpret the nuances of human behavior, understand the political,
socioeconomic, and environmental dynamics of the situation, and
comprehend the humanitarian risks of the use of force in a particular
context.\[67\] These limitations would compromise the weapons' ability
to ensure the humane treatment of civilians and combatants and comply
with the first principle of humanity.

#### AI experts disagree -- they are calling for a preventive ban because they don't think that technology will improve enough.

**Docherty, 2014 - senior researcher in the Arms Division of Human
Rights Watch** \[Bonnie "Shaking the Foundations The Human Rights
Implications of Killer Robots" Human Rights Watch
http://www.hrw.org/sites/default/files/reports/ arms0514_ForUpload_0.pdf
Acc 12/27/20 TA\]

Fully autonomous weapons threaten to contravene foundational elements of
human rights law. They could violate the right to life, a prerequisite
for all other rights. Deficiencies in judgment, compassion, and capacity
to identify with human beings could lead to arbitrary killing of
civilians during law enforcement or armed conflict operations. Fully
autonomous weapons could also cause harm for which individuals could not
be held accountable, thus undermining the right to a remedy. Robots
could not be punished, and superior officers, programmers, and
manufacturers would all be likely to escape liability. Finally, as
machines, fully autonomous weapons could not comprehend or respect the
inherent dignity of human beings. The inability to uphold this
underlying principle of human rights raises serious moral questions
about the prospect of allowing a robot to take a human life. [Proponents
of fully autonomous weapons might argue that technology could eventually
help address the problems identified in this report, and it is
impossible to know where science will lead.85 In a 2013 public letter,
however, more than 270 roboticists, artificial intelligence experts, and
other scientists expressed their skepticism that adequate developments
would be possible.86 Given this uncertainty]{.underline}, [the potential
of fully autonomous weapons to violate human rights law]{.underline},
combined with other ethical, legal, policy, and scientific concerns,
[demands a precautionary approach]{.underline}. The precautionary
principle of international law states that "\[w\]here there are threats
of serious or irreversible damage, lack of full scientific certainty
shall not be used as a reason for postponing costeffective measures."87
[When applied to fully autonomous weapons, this principle calls for
preventive action to be taken now]{.underline}.

#### Autonomous AI systems still deny dignity -- the dignity of opposing soldiers

**Docherty, 2018 - senior researcher in the Arms Division of Human
Rights Watch** \[Bonnie August 21, "Heed the Call A Moral and Legal
Imperative to Ban Killer Robots"
[https://www.hrw.org/report/2018/08/21/heed-call/moral-and-legal-imperative-ban-killer-robots#](https://www.hrw.org/report/2018/08/21/heed-call/moral-and-legal-imperative-ban-killer-robots)
Acc 12/27/20 TA\]

[There is no way to regulate fully autonomous weapons short of a ban
that would ensure compliance with the principles of
humanity.]{.underline} Fully autonomous weapons would lack the
compassion and legal and ethical judgment that facilitate humane
treatment of humans. They would face significant challenges in
respecting human life. [Even if they could comply with legal rules of
protection, they would not have the capacity to respect human dignity.
Limiting the use of fully autonomous weapons to certain locations, such
as those where civilians are rare, would not sufficiently address these
problems]{.underline}. "Harm to others," which the principle of humane
treatment seeks to avoid, encompasses harm to civilian objects, which
might be present where civilians themselves are not. [The requirement to
respect human dignity applies to combatants as well as civilians, so the
weapons should not be permitted where enemy troops are
positioned.]{.underline} Furthermore, allowing fully autonomous weapons
to be developed and to enter national arsenals would raise the
possibility of their misuse. They would likely proliferate to actors
with no regard for human suffering and no respect for human life or
dignity. The 2017 letter from technology company CEOs warned that the
weapons could be "weapons of terror, weapons that despots and terrorists
use against innocent populations, and weapons hacked to behave in
undesirable ways."\[178\] Regulation that allowed for the existence of
fully autonomous weapons would open the door to violations of the
principles of humanity.

#### Even if the technology improves, it will never reach the point of being able to distinguish appropriate targets.

**Docherty, 2014 - senior researcher in the Arms Division of Human
Rights Watch** \[Bonnie "Shaking the Foundations The Human Rights
Implications of Killer Robots" Human Rights Watch
http://www.hrw.org/sites/default/files/reports/ arms0514_ForUpload_0.pdf
Acc 12/27/20 TA\]

[Although the ability of fully autonomous weapons to process complex
information might improve in the future, it seems implausible that they
could ever be identical to humans. As a result, these weapons would find
it difficult to]{.underline} meet the three criteria for use of force in
law enforcement or [comply with the rules of distinction and
proportionality in armed conflict. Fully autonomous weapons would have
the potential to kill arbitrarily and thus violate the right that
underlies all others, the right to life]{.underline}.

#### The question isn't "Can it be used morally?" -- it is "Is there a possibility it will be abused?"

**Heyns, 2016 - Professor of Human Rights Law, University of Pretoria**
\[Christof, Human Rights Quarterly 38 (2016) 350--378 " Human Rights and
the use of Autonomous Weapons Systems (AWS) During Domestic Law
Enforcement"
<https://www.academia.edu/37475669/Human_Rights_and_the_use_of_Autonomous_Weapons_Systems_AWS_During_Domestic_Law_Enforcement>
Acc 12/27/20 TA\]

[It should also be kept in mind that the answer to the question of
whether particular weapons should be banned does not merely depend on
whether they can within a limited range of conceivable
circumstances]{.underline} and will be used [in conformity with the
applicable legal regime]{.underline}, be it under international human
rights law or IHL. [Many of the weapons that are banned today can, in
fact, be used in a closely controlled environment in ways that comply
with the law. At some point, a practical decision needs to be made
whether the weapon in question poses an unacceptably high risk of not
being used in such a way.]{.underline}

### 

### AT OSCE CP

#### Solvency Deficit -- the OSCE lacks credibility -- it is irrelevant compared to NATO

**Sánchez-Cobaleda, 2020 - prof of Public International Law at the
University of Barcelona** \[Ana 2020-11-30 "Case study of the European
Security Architecture: NATO and OSCE"
<https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&cad=rja&uact=8&ved=2ahUKEwjz8v2g46j4AhWlIUQIHWIVDCgQFnoFCLICEAE&url=https%3A%2F%2Fwww.globe-project.eu%2Fcase-study-of-the-european-security-architecture-nato-and-osce_11317.pdf&usg=AOvVaw2AE86i3ZUkPrll4AK6-Y-y>
Acc 6/22/22 TA\]

[Despite its brief prominence in the early 1990s]{.underline} and its
significant daily work both on the diplomatic front and through its 16
field operations,72 [the OSCE has been, and indeed continues to be,
largely unknown to the general public]{.underline} (Dominguez, 2014, pp.
17--27; Mosser, 2015a, p. 590) [and lacking in credibility]{.underline}
(Trenin, 2003, p. 11; Webber et al., 2004, p. 19). The most inclusive
security organisation in Europe has been rapidly losing relevance after
the enlargements of NATO and the EU, and its geographical exclusivity
has been reduced to Russia, the Caucasus and Central Asia. [Its
secondary role in the European security architecture implies that
experts and observers question its relevance]{.underline} (Azintov,
2012, pp. 19--22; Fernandes, 2015, p. 92; Stewart, 2008, p. 268;
Zellner, 2005, p. 391) [especially when compared to NATO]{.underline} or
the EU (Aybet, 2000; Møller, 2008) and particularly on relevant dates
for the organisation such as 2020, which marks the 45th anniversary of
its creation.

#### No Solvency -- the OSCE does not address military concerns directly -- that is exclusively NATO's role

**Sánchez-Cobaleda, 2020 - prof of Public International Law at the
University of Barcelona** \[Ana 2020-11-30 "Case study of the European
Security Architecture: NATO and OSCE"
<https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&cad=rja&uact=8&ved=2ahUKEwjz8v2g46j4AhWlIUQIHWIVDCgQFnoFCLICEAE&url=https%3A%2F%2Fwww.globe-project.eu%2Fcase-study-of-the-european-security-architecture-nato-and-osce_11317.pdf&usg=AOvVaw2AE86i3ZUkPrll4AK6-Y-y>
Acc 6/22/22 TA\]

[NATO, the OSCE and the EU all strive to maintain peace and security in
their member states. The way they work to achieve this, however,
differs]{.underline}. While [NATO has traditionally focused
on]{.underline} direct territorial [defence]{.underline}, the EU has
pursued this goal more indirectly, carrying out missions beyond its
borders to ensure stability in its neighbourhood. And it is precisely in
the area of the EU's eastern neighbourhood where [the OSCE is working
most actively]{.underline} to also guarantee peace and security -- [in
its specific way]{.underline}, namely [through the advancement of
democracy and human rights]{.underline} throughout the territory of its
57 participating states. Nonetheless, "maintaining peace and security"
is too general an idea. Instead of condensing the objectives of these
actors to such a high degree, it is useful to assess in greater detail
which objectives coincide and where functional overlap is occurring.
[NATO defends the]{.underline} peace and [security of its
members]{.underline} in two ways: directly, [through its collective
defence objective]{.underline} (centred mainly on the eastern flank of
the Alliance and consisting of the maintenance of peace and security on
its eastern border), and indirectly, through the projection of stability
in other territories (especially, but not only, in the region bordering
the south of the Alliance). [In the collective defence field NATO
retains, for now, exclusivity]{.underline}. The relationship between the
EU and NATO in this area is clearly defined by role specialisation and
division of labour. The Alliance provides what the EU lacks insofar as
[NATO\'s raison d\'être is collective defence and its comparative
advantage remains its military power]{.underline}. Thus, they are
"interlocking" institutions that cooperate in an architecture based on
comparative advantage and effective multilateralism to address
challenges both in Europe and beyond. However, it is important to stress
the transitory nature of this situation. If the EU continues to develop
its cooperation in defence (as seems to be the case), it could well
result in a functional overlap in matters of defence not seen until now.
Stronger defence cooperation is indeed one of the EU's current
objectives, and one that is already regarded with suspicion by some of
the member states of both NATO and the EU itself. These member states do
not welcome the overlap for fear of possible rivalry.115 Beyond pure
defence objectives, both the EU and NATO are engaged in crisis
management. It is important to subdivide this in two different
dimensions, as the organizations' levels of involvement differ when it
comes to (a) the deployment of troops to monitor an agreement or a
ceasefire and (b) political and diplomatic efforts in crisis prevention
or post-conflict reconstruction. In the first dimension, field-level
overlap can exist, but this is not always the case, as it depends on the
configuration of missions and the organisations' mandates. While the
EU's legal framework establishes that territorial defence remains NATO's
responsibility, Page 64 from 109 the EU's capacity to act independently
from NATO's assets in external crisis management continues to grow.116
In fact, the EU's goal of obtaining full strategic autonomy has placed
the EU on a similar footing with NATO. It is in the second, more
civilian and political dimension, however, where the level of functional
overlap is the largest. NATO's and the EU's missions and operations
focus on preventive measures, training and, most of all, security sector
reform in partner countries. The convergence in this area that was
traditionally occupied by the EU has occurred since NATO progressively
expanded its agenda to become more than just a military Alliance. In
conclusion, the trend of greater regime complexity in the field of
European security has accelerated over the last decade, partly as a
result of the expansion of NATO\'s strategic concept and, potentially
soon, as a consequence of the still ongoing development of a stronger
security and defence component by the EU. [The OSCE, for its part, does
not have and neither does it foresee creating a defence component or a
military crisis management goal]{.underline}. Instead, the OSCE
continues its efforts to enhance cooperative security through arms
control agreements. Through this objective, it complements the EU's
efforts to maintain peace and stability in its neighbouring areas. In
addition, if the OSCE achieves its goals in this area, NATO would also
benefit from respect for arms control treaties and the reduction of
arsenals. Thus, while the OSCE does not focus on territorial defence nor
military crisis management, its cooperative security mandate can be
understood within that same division of labour framework.

#### The OSCE is collapsing - it played little role in the Ukraine crisis and Russia is a member.

**Zięba, 2018 -- prof at University of Warsaw** \[Ryszard April "The
Marginalization of the OSCE" In book: The Euro-Atlantic Security System
in the 21st Century (pp.213-224)
[https://www.researchgate.net/publication/324765097_The_Marginalization_of_the_OSCE
Acc
6/2/22](https://www.researchgate.net/publication/324765097_The_Marginalization_of_the_OSCE%20Acc%206/2/22)
TA\]

[The]{.underline} eighth [chapter]{.underline} briefly [describes the
decline of the OSCE]{.underline}---the largest security organization in
the Euro-Atlantic area, encompassing 57 states, [and disposing of
unique, so-called soft security ensuring instruments]{.underline}.
[This]{.underline} process [was due to the dominant position in the
entire system of NATO, an institution with hard security
guarantees]{.underline}. [Attempts to revive the OSCE were undertaken in
2010 at the Astana Summit, and then during the Ukraine crisis, in which
the OSCE]{.underline}, as the sole multilateral institution, [played a
modest role]{.underline} in the monitoring of the cease-fire agreements
concluded in Minsk in 2014 and 2015. [The OSCE]{.underline} still has a
chance of playing a greater role in the shaping of the Euro-Atlantic
security system, but this [would require an agreement between its
principal participants, especially the western countries and
Russia.]{.underline}

#### The OSCE fails -- nations do not trust each other and lack the political will to enforce OSCE policies. Russian tensions threaten the OSCE's survival. 

**Sánchez-Cobaleda, 2020 - prof of Public International Law at the
University of Barcelona** \[Ana 2020-11-30 "Case study of the European
Security Architecture: NATO and OSCE"
<https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&cad=rja&uact=8&ved=2ahUKEwjz8v2g46j4AhWlIUQIHWIVDCgQFnoFCLICEAE&url=https%3A%2F%2Fwww.globe-project.eu%2Fcase-study-of-the-european-security-architecture-nato-and-osce_11317.pdf&usg=AOvVaw2AE86i3ZUkPrll4AK6-Y-y>
Acc 6/22/22 TA\]

[The lack of trust between participating states, particularly the
situation of growing tension between Russia and the other]{.underline}
participating [states is not only an internal and political challenge
for the OSCE, but also directly related to its survival]{.underline}.
[This tension]{.underline}, which can be felt at various levels,73
[makes collaboration and decision-making within the OSCE very
difficult.]{.underline} Russia continues to exert great influence in the
region and it has proved to be willing to use its military force against
sovereign states to pursue political goals (Baqués Quesada, 2018, pp.
16--17). [The climate created by the conflicts in Georgia]{.underline}
in 2008 [and in Ukraine]{.underline} in 2014 -- both OSCE Member States
-- as well as the military developments in Crimea and the Donbass,
question the foundations laid down in the 1975 Helsinki Final Act, and
[call into question the effectiveness of the CBMs]{.underline} set out
in the Vienna Documents. Restoring the trust among participants would
also involve restoring the trust in the measures aimed at guaranteeing
transparency in the OSCE\'s acquis that has existed in Europe for 20
years (De Salazar Serantes, 2016, p. 367). Currently, building back
trust, re-establishing a "security community" and restoring the OSCE\'s
original function as one of the leading forums for mutually beneficial
dialogue and collective consensual decisions on European security issues
(Azintov, 2012, pp. 19--22) are both objectives and challenges.
Paradoxically, Ukraine\'s crisis, which is one of the main current
difficulties for the OSCE, has also been an opportunity for the
organisation to demonstrate its worth and relevance (Smolnik, 2019, p.
5; Zannier, 2018, pp. 35--36). Moreover, the conflict in Ukraine
underscores the need -- and difficulty -- to adapt the arms-control
regime in Europe due to the importance of military transparency like the
one achieved with the CFE, the Vienna Documents or the Open Skies Treaty
(Bieri & Nünlist, 2018, pp. 407--423). However, these and other [OSCE
instruments become useless in the absence of political will on the part
of one, several or all the parties involved]{.underline}. Being an
intergovernmental forum, the weight and influence of some participants,
the historical relations between states, and the lack of trust in
general (worsened in the last decade) make decision-making difficult,
even [leading the OSCE to be somewhat paralysed in taking
forceful]{.underline} and avant-garde [decisions]{.underline} (Bieri &
Nünlist, 2018). There seems to be an excess of tepidity. [This
difficulty stems from the very nature of the institution, whose
informality determines the limits of its ability to act, insofar it
cannot impose itself on the will of governments, whose trust in each
other is greatly weakened.]{.underline} Although improbable, it would be
desirable for the OSCE to increase its influence on individual
participating states. Political differences among OSCE\'s participating
states, apart from constantly delaying the adoption of budgets, have
also caused mistrust and the discontinuation of the organisation's work.

#### Permutation -- NATO can focus on the plan to change military doctrines and deter Russia, while the OSCE can work to increase transparency of the plan.

**Burns and Lute, 2019 - Prof of International Relations at the Harvard
Kennedy School and former Representative to the North Atlantic Council**
\[Nicholas and Douglas, February "NATO at Seventy: An Alliance in
Crisis"
[https://www.belfercenter.org/publication/nato-seventy-alliance-crisis
Acc
6/2/22](https://www.belfercenter.org/publication/nato-seventy-alliance-crisis%20Acc%206/2/22)
TA\]

[Allies must continue bolstering deterrence by ensuring consequences for
Russian actions.]{.underline} Recent examples include sustaining
U.S.-Canadian-EU economic sanctions five years after Russian aggression
in Ukraine, the Netherlands' public commitment to holding Russia
accountable for its role in the shooting down of Malaysia Airlines
Flight 17 in 2014 and the responses after the Novichok chemical agent
attacks in the U.K. 66 Going forward, the U.S. and the EU together must
never recognize the illegal annexation of Crimea and reaffirm economic
sanctions will stay on Moscow for as long as it occupies Ukrainian
territory. These measures, however, have not changed Russian behavior.
Even less impressive are the reactions to Russian interference in
elections and recent aggression in the Black Sea and denial of Ukrainian
access to the Sea of Azov.67 [While confronting Russian aggression and
bolstering deterrence, NATO must remain open to dialogue]{.underline}
with Russia when it is in the West's interest. Russia is a major
European power that must be taken into account.68 First, dialogue is
fundamental to deterrence, as Russia must clearly understand NATO's
intent and the consequences of aggression. Second, even in a period of
increased tensions, there are topics for dialogue that serve common
interests. [The NATO-Russia Council should continue to meet regularly
to]{.underline} address risk reduction measures, [provide transparency
on military exercises]{.underline} and exchange views on priority
political issues, including the conflict in Ukraine. [Allies should
press the]{.underline} Organization for Security and Cooperation in
Europe ([OSCE]{.underline}) [to update the Vienna Document to improve
predictability and transparency of conventional forces in the
region]{.underline}.69 Russia should return an ambassador to NATO and
NATO should re-open military-military contacts below the four-star
level. It is not in NATO's interest that the Russian military liaison
cell at Allied Command Operations' SHAPE remains closed. Third,
balancing deterrence and dialogue is essential to sustaining political
cohesion among allies some of whom have differing perspectives on the
nature of the Russian threat and the best responses to it.70 NATO should
not return to "business as usual" with Russia as before 2014, but
restricting dialogue is not an effective form of punishment. In periods
of increased tension, the risk of accident and unintended consequences
increases---dialogue can mitigate some of that risk. In short,
sustaining and even expanding dialogue with Russia is in NATO's
interest.

### AT CCW CP

#### Solvency deficit -- the UN focuses on diplomatic channels -- ethical AI requires action by defense departments and ministries, which are tied to NATO

**Stanley-Lockman, 2021 - Center for Security and Emerging Technology**
\[Zoe CSET Issue Brief August "Responsible and Ethical Military AI
Allies and Allied Perspectives"
https://cset.georgetown.edu/publication/responsible-and-ethical-military-ai/
Acc. 4/21/22 TA\]

Another reason the analysis focuses on AI and not autonomy in weapons
systems is that the topic is already well covered in other literature.14
[The United Nations Convention on Certain Conventional Weapons has
focused on lethal autonomous weapon systems]{.underline} (LAWS) since
2013, [and this has been the primary format for technical expertise,
civil society engagement, and diplomatic engagement to
converge]{.underline}.15 As a result, [questions about ethics
and]{.underline} legality of autonomy in weapons, and specifically
[LAWS, often involve diplomatic actors at the fore of domestic
government approaches. Concerns about the ethics of intelligent systems,
on the other hand, currently receive less attention in military
debates]{.underline}. To maintain a stricter focus on AI rather than
autonomy in weapons systems, [this study focuses more on technical and
policy approaches in defense ministries, which have more agency in
ethical and responsible AI policy]{.underline}.

#### UN Solutions will fail because they only focus on LAWs, not other applications of AI -- inflexibility limits potential solutions. 

**Kahn and Horowitz, 2021 -- Research and Senior Fellows at the Council
on Foreign Relations** \[Lauren and Michael, The Washington Quarterly
44:4 "Leading in Artificial Intelligence through Confidence Building
Measures" [https://doi.org/10.1080/0163660X.2021.2018794 Acc
6/6/22](https://doi.org/10.1080/0163660X.2021.2018794%20Acc%206/6/22)
TA\]

One might argue that the United States should let others lead on AI,
focusing instead on developing AI-enabled capabilities and not
concerning itself with how other countries behave. But there is no
substitute for American leadership and its ability to rally countries
around the world to support shared standards. If promoting norms of
responsible behavior with AI encourages other states to use military
applications of AI in more responsible ways, it will create a more
ethical and predictable security environment, likely benefiting the
United States. Additionally[, current international dialogue about
military uses of AI focuses almost exclusively on]{.underline} lethal
autonomous weapon systems ([LAWS]{.underline}), [the subject of a Group
of Governmental Experts in the Convention on Certain Conventional
Weapons]{.underline}.39 Currently, the international conversation has
been largely been driven by NGOs such as the Campaign to Stop Killer
Robots.40 [While such conversations help bring attention to some of
these issues, they oversimplify the risks and fixate on worst-case
scenarios that are more likely outcomes of artificial general
intelligence or human level machine intelligence rather than technology
today]{.underline}. [LAWS represent only a small fraction of the
universe of potential issues surrounding military applications of AI.
Broadening the international conversation about military uses of AI to
incorporate the full scope of potential applications would generate
better dialogue because it would include more of the real-world AI
scenarios likely to confront militaries. Expanding the discussion would
also allow states to pursue levels of control and regulation other than
an all-or nothing ban and create a more calibrated and flexible range of
approaches]{.underline} to different technologies with various levels
and types of associated risks.

#### The CCW fails because it gives Veto Power to participants.

**Bolton, 2021 - professor of political science at Pace University**
\[Matthew with Matilda Byrne, Ryan Gariepy, Emilia Javorsky, Volker
Lehmann, and Laura Nolan, January "Addressing The Threat Of Autonomous
Weapons Maintaining Meaningful Human Control"
http://library.fes.de/pdf-files/iez/17215.pdf Acc 5/27/22 TA\]

[Effective control of]{.underline} the humanitarian, human rights and
security risks posed by [LAWS will require]{.underline} legally-binding
obligations on states negotiated in [a multilateral forum. To date, the
most likely venue for negotiating such a mandate would be within the
CCW,]{.underline} in the form of negotiating a new protocol. [However,
the CCW's consensus rules of procedure have been interpreted as
requiring agreement of all states -- effectively granting a veto to the
most intransigent and often resulting in lowest common denominator
decisions.]{.underline} Given this context, more ambitious states and
other actors may in time consider other potential avenues, including a
UN General Assembly-mandated process or one analogous to the Ottawa and
Oslo processes on landmine and cluster munitions.

#### No Solvency -- CCW Protocols don't apply in peacetime, or to nuclear and cyber weapons.

**International Panel on the Regulation of Autonomous Weapons, 2021**
\[(iPRAW) coordinated by: German Institute for International and
Security Affairs, July "Building Blocks for a Regulation on LAWS and
Human Control Updated Recommendations to the GGE on LAWS"
https://www.readkong.com/page/building-blocks-for-a-regulation-on-laws-and-human-control-8617434
Acc 2/27/22 TA\]

Scope of application: Since human control is to be understood as a
feature of the design as well as use, the regulation would cover the
development and deployment of weapons -- effectively prohibiting the
development of weapons without sufficient options for human control,
i.e. LAWS. Nevertheless, [it is important to bear in mind that the CCW
and the Protocols related thereto merely apply in armed conflict and not
in peacetime.]{.underline} This would also hold true in case another
Protocol within the CCW framework was adopted, regulating LAWS. Thus,
the use of LAWS in policing scenarios would not be covered by a future
Protocol. However, human rights law and other (international) legal
regimes would still be applicable. Furthermore, [a CCW regulation would
only apply to conventional weapons and would not cover autonomous cyber
or nuclear capabilities]{.underline}.

### AT EU CP

#### Solvency deficits -- The EU is fragmented and cannot harmonize markets; EU action creates Brain Drain to the US and UK; and their regulatory system slows implementation

**Lawrence and Cordey, 2020 -- researchers for The Cyber Project at the
Belfer Center for Science and International Affairs** \[Christie and
Sean, August, The Cyber Project Paper "The Case for Increased
Transatlantic Cooperation on Artificial Intelligence Edited by Lauren
Zabierek and Julia Voo
https://www.belfercenter.org/sites/default/files/2020-08/TransatlanticAI.pdf
Acc. 4/21/22 TA\]

Challenges Due to the socio-technical transformative power of AI,
governing and harnessing its benefits is a complex affair. [There are
three fundamental issues the EU needs to overcome]{.underline}: [A
fragmented market and landscape: This patchwork landscape results from
the EU's political configuration of 27 different Member States, all of
which have different levels of maturity and competitiveness with regard
to its AI ecosystem.]{.underline} Some, like France or Germany, are
major players in their own right, but the majority of EU Member States
carry little weight on their own.248 This disparity in capability drives
the EU to pool its resources together. The UK's decision to leave the EU
constituted a serious blow to the EU's standing, as it deprived it of
one of its most mature, rich and innovative AI environments.249 In
addition, European AI assets (e.g., talent, education, expertise,
research, start-ups, and capital) are highly fragmented and
decentralized.250 As Professor of Theoretical Philosophy Thomas
Metzinger pointed out, this makes effective coordination of all the
different stakeholders complex but highly important. One example where
coordination could be improved is research funding both within the
existing EU research frameworks and between the EU government, academia,
industry, and the Member States.251 Europe is also fragmented along
market and cultural lines. Indeed, compared to the US or China, [the
EU's AI market is not yet consolidated nor harmonized.]{.underline} In
addition, some major sectors of the economy such as automotive,
agriculture, energy, as well as the public sector, present varying
degrees of AI maturity, penetration, and integration.252 According to a
McKinsey study,253 in 2017 only 25% of EU large enterprises and 10% of
small and medium enterprises used big data analytics.254 This slow
uptake of AI relates, in part, to the lack of trust of the general
public and companies around issues of algorithmic transparency and
biases. Meanwhile, [the EU's AI governance must also be able to navigate
and transcend potentially divisive differences that come from the
cultural, historical, strategic, and institutional differences that
characterize each and every Member State]{.underline}.255 Underlying
structural factors impede its development and competitiveness: Europe
lacks tech giants that characterize the US and Chinese tech landscape.
As a result, European start-ups and tech companies compete against peers
that have not only considerably larger investment capabilities, which
enable them to acquire the latest technologies and companies, but also
provide ample access to data and a greater ability to attract and retain
a skilled workforce.256 The lack of easily available data, which impedes
innovation, has emerged as a key issue for the EU and its private sector
companies.257 These concerns are additionally fueled by the relatively
more restrictive European privacy laws---and maybe soon mandatory
ethical guidelines.258 "[Brain drain," meanwhile, poses a unique
challenge to the EU as promising European researchers often choose to
move to the US, Canada, or the UK for academic
opportunities]{.underline}. Regarding the former for instance, 19% of
Europe's undergraduates move to the US to study, while 14% of European
graduates move to the US to work. Overall, 11% of the US's top tier AI
workforce---which represent 59% of the global workforce--- comes from
Europe.259 In some cases, AI talent also moves to the US to work at
larger international companies that acquire their start-ups.260 Examples
include the French Moodstock (acquired by Google) and the UK's Magic
Pony Technology (acquired by Twitter).261 [A strong but slow regulatory
process]{.underline}: While the policy process regarding AI has, under
the impetus of the EC, accelerated over the last few years, [the overall
regulatory process cannot be as easily accelerated and might take
years.]{.underline} According to the EC's calendar (see Figure 8), the
AI-related regulatory process will begin at the end of this year (2020),
but the actual drafting and passage of the associated legislation is
likely to take several years. As such, the EU runs the risk of not
keeping up with the pace of technological evolution in AI as well as
further politicization of regulation similar to the process of passing
GDPR. Finally, as with most democracies, the EU will also face the
challenge of operationalizing related regulations and principles,
carefully and skillfully, balancing private and public sector interests
throughout the process.

#### The EU does not trust the US to cooperate on AI -- they see the US as focused on competition

**Lawrence and Cordey, 2020 -- researchers for The Cyber Project at the
Belfer Center for Science and International Affairs** \[Christie and
Sean, August, The Cyber Project Paper "The Case for Increased
Transatlantic Cooperation on Artificial Intelligence Edited by Lauren
Zabierek and Julia Voo
https://www.belfercenter.org/sites/default/files/2020-08/TransatlanticAI.pdf
Acc. 4/21/22 TA\]

Both in the US and in the EU there seems to be political appetite for
international collaboration on AI; however [policymakers view specific
US-EU cooperation with some degree of skepticism. At the European level
for instance, the EU's AI strategy underlines that addressing risks
generated by AI should be a global effort]{.underline}. Accordingly, the
EC mentions "strengthening cooperation with like-minded partners such as
Japan, Canada or Singapore," particularly on ethical norms.266 The US,
however, is not specifically mentioned. [The US is instead depicted as a
competitor with greater capacity, resources, and ability to attract
skilled researchers and funding.]{.underline} At first blush, it seems
the US has also failed to treat US-EU cooperation with the
prioritization one would expect given the prominence placed in the
American AI Initiative on international cooperation to achieve a global
environment aligned with American values---values Europe largely shares.

#### Permutation -- the EU and NATO should cooperate on the plan. 

**Stanley-Lockman, 2021 - Center for Security and Emerging Technology**
\[Zoe CSET Issue Brief August "Responsible and Ethical Military AI
Allies and Allied Perspectives"
https://cset.georgetown.edu/publication/responsible-and-ethical-military-ai/
Acc. 4/21/22 TA\]

Other Opportunities for Multilateral Collaboration on Responsible and
Ethical AI in Defense: the European Union (EU) and Five Eyes [Working
with a number of multilateral institutions is critical to the United
States' stewardship of AI aligned with democratic values and
interests.164 In addition to NATO]{.underline} and the PfD, [the
EU]{.underline} and Five Eyes are highlighted as relevant formats for
cooperation on ethical and responsible military AI. European Union [Of
course, the EU is not an alliance---and the United States is not a
member. But the EU's potential contributions to responsible military AI
are worth discussing here because of]{.underline} the implications of
supranational EU policy on allies' own approaches to ethical and
responsible AI in defense, as well as on [EU-NATO
cooperation]{.underline}.165

#### The permutation helps German Credibility -- they are key to fostering NATO/EU cooperation

**Stanley-Lockman, 2021 - Center for Security and Emerging Technology**
\[Zoe CSET Issue Brief August "Responsible and Ethical Military AI
Allies and Allied Perspectives"
https://cset.georgetown.edu/publication/responsible-and-ethical-military-ai/
Acc. 4/21/22 TA\]

What this means is that the Federal Ministry of Defence is left with a
backseat role.124 [This also heightens the stakes of multilateral
efforts on responsible and ethical military AI]{.underline}, including
for assessments of ethical risk stemming from issues like explainability
or reliability. Indeed, [Germany may be more active in these formats,
especially in facilitating coordination between the EU and NATO given
its longstanding interest in encouraging and facilitating EU NATO
cooperation.125 Cooperation is already visible in other efforts related
to technology and ethics]{.underline}---most notably in that the German
Bundeswehr Defence Policy Office came up with views on future
implications of human augmentation in collaboration with the U.K.
Development, Concepts and Doctrine Centre.126 The two countries share
views on the future of operations, which may be productively channeled
through activities related to policy alignment or, potentially,
standardization.127 [Rather than going it alone, the German preference
to cooperate---in bilateral and especially multilateral formats---may be
seen as one way to focus on these issues with less domestic political
pressure]{.underline}, and to substantiate contributions to defense
partnerships.

#### Brexit undermines US / EU cooperation -- they are our closest ally in Europe.

**Lawrence and Cordey, 2020 -- researchers for The Cyber Project at the
Belfer Center for Science and International Affairs** \[Christie and
Sean, August, The Cyber Project Paper "The Case for Increased
Transatlantic Cooperation on Artificial Intelligence Edited by Lauren
Zabierek and Julia Voo
https://www.belfercenter.org/sites/default/files/2020-08/TransatlanticAI.pdf
Acc. 4/21/22 TA\]

340 [The European Union is not completely unified on its position
regarding European defense capabilities or the application of AI in this
realm]{.underline}. Ulrike Esther Franke, Policy Fellow at European
Council on Foreign Relations, has noted that [France tends to lead
conversations about the use of AI in the military]{.underline}, while
other countries like Germany and Austria are concerned about the use of
AI to create autonomous weapons systems ("killer robots").
[Additionally, the exit of the UK from the EU hinders US military
cooperation with the EU as the UK had a strong military and intelligence
relationship with the US, particularly given its place in Five
Eyes]{.underline}.

#### EU IP rules undermine cooperation -- US firms are excluded from EU projects.

**Lawrence and Cordey, 2020 -- researchers for The Cyber Project at the
Belfer Center for Science and International Affairs** \[Christie and
Sean, August, The Cyber Project Paper "The Case for Increased
Transatlantic Cooperation on Artificial Intelligence Edited by Lauren
Zabierek and Julia Voo
https://www.belfercenter.org/sites/default/files/2020-08/TransatlanticAI.pdf
Acc. 4/21/22 TA\]

355 [EDF's IP rules state that only EU-based entities can own IP from
projects and US firms are excluded from receiving defense funds. Some
worry these rules may lead to the exclusion of US firms from EDF and
PESCO, negatively impacting military interoperability and joint
R&D]{.underline} (See Christian Larsen, EU Should Remain Open to US
Defense Industry, National Defense (December 13, 2019).) While others
like Ulrike Franke believe EDF and PESCO are primarily European
collaboration vehicles and should not be the main avenue for
transatlantic cooperation.

### AT Ban LAWs CP

#### Turn -- The counterplan undermines interoperability and cohesion -- the US would not join, putting them at odds with their allies

**Stanley-Lockman, 2021 - Center for Security and Emerging Technology**
\[Zoe CSET Issue Brief August "Responsible and Ethical Military AI
Allies and Allied Perspectives"
https://cset.georgetown.edu/publication/responsible-and-ethical-military-ai/
Acc. 4/21/22 TA\]

7 This point has also been made in relation to the difficulties of a
prematurely prohibitive ban on lethal autonomous weapon systems.
Although autonomy is beyond the scope of the study, NSCAI Executive
Director Yll Bajraktari makes a similar point on the relationship
between ethics and interoperability: "[The effects of a prohibition
agreement likely would run counter to the U.S. strategic interests as
commitments from states such as Russia and China are likely to be empty
ones]{.underline}. So, the primary impact of an agreement would be to
increase pressure on those countries that abide by international law,
including the United States and its democratic allies and partners. [If
U.S. allies joined an agreement while the United States did not, the
diversion would likely hinder allied military interoperability. That
would be something really difficult for us and our allies. For these
reasons, we believe that practical and strategic problems with a
prohibition treaty outweigh the potential benefits for the United States
and its allies and partners]{.underline}." Craig Smith and Yll
Bajraktari, "Episode #071: AI and Center for Security and Emerging
Technology \| 72 National Security: US vs China," Eye on AI, May 5,
2021, transcript available at: <https://www.eye-on.ai/podcast-071>.

#### Solvency Deficit -- the UN focuses on LAWs, but AI is moving toward autonomous Non-lethal weapons -- the counterplan does not cover them.

**Freedberg, 2019 -- deputy editor for Breaking Defense** \[Sydney J
"The frontline of a new age in defense Artificial Intelligence"
https://cdn2.hubspot.net/hubfs/2097098/MCM120_BreakingDefense_AI_ebookR1%20(1).pdf
Acc 5/25/22 TA\]

What happens when Artificial Intelligence produces a war strategy too
complex for human brains to understand? Do you trust the computer to
guide your moves, like a traveler blindly following GPS? Or do you
reject the plan and, with it, the potential for a strategy so smart it's
literally superhuman? [The Pentagon wants AI to assist human combatants,
not replace them. The issue is what happens once humans start
taking]{.underline} military advice --- or even [orders --- from
machines. The reality is this happens already, to some extent. Every
time someone looks at a radar or sonar display, for example, they're
counting on complicated software to correctly interpret a host of
signals no human can see.]{.underline} The Aegis air and missile defense
system on dozens of Navy warships recommends which targets to shoot down
with which weapons, and if the human operators are overwhelmed, they can
put Aegis on automatic and let it fire the interceptors itself. This
mode is meant to stop massive salvos of incoming missiles but it could
also shoot down manned aircraft. Now, Aegis isn't artificial
intelligence. It rigidly executes pre-written algorithms, without
machine learning's ability to improve itself. But it is a long-standing
example of the kind of complex automation that is going to become more
common as technology improves. [While the US military won't let a
computer pull the trigger, it is developing target-recognition AI to go
on everything]{.underline} from recon drones to tank gun sights to
infantry goggles. [The armed services are exploring predictive
maintenance algorithms]{.underline} that warn mechanics to fix failing
components before mere human senses can detect that something's wrong,
[cognitive electronic warfare systems that figure out the best way to
jam enemy radar, airspace management systems that converge strike
fighters, helicopters, and artillery shells on the same target without
fratricidal collisions]{.underline}. Future "decision aids" might
automate staff work, turning a commander's general plan of attack into
detailed timetables of which combat units and supply convoys have to
move where, when. [And since these systems,]{.underline} unlike Aegis,
[do use machine learning, they can learn from experienc]{.underline}e
--- which means [they continually rewrite their own programming in ways
no human mind can follow]{.underline}.

#### Turn - Banning LAWs undermines the US Leadership that would be necessary to make Responsible AI norms work.

**Del Re, 2017 -- US Army Major** \[Amanda "Lethal Autonomous Weapons:
Take the Human Out of the Loop A paper submitted to the Faculty of the
US Naval War College in partial satisfaction of the requirements for the
Ethics of Emerging Military Technology Graduate Certificate. 16 June
2017 https://apps.dtic.mil/sti/citations/AD1041804 Acc 12/27/20 TA\]

In conclusion, [the US should lead the effort to employ Lethal
Autonomous Weapons]{.underline} in warfare. [As other nations are
already employing LAWS, the US needs to utilize them so that they are
fully understood. This understanding and experiencing is necessary to
establish international norms and treaties]{.underline}. As a
superpower, [it is the United States' burden to set the example in
employing new technology in accordance with international
norms]{.underline}. Finally, LAWS should be employed in warfare because
they will save money and most importantly, lives.

#### Banning LAWs fails because it relies on a static definition of weapons, and AI is dynamic -- weapons can be autonomous by the flip of a switch.

**Sauer, 2021 - Senior Research Fellow at Bundeswehr University**
\[Frank serves on the International Panel on the Regulation of
Autonomous Weapons IRRC No. 913 March "Stepping back from the brink: Why
multilateral regulation of autonomy in weapons systems is difficult, yet
imperative and feasible"
https://international-review.icrc.org/articles/stepping-back-from-brink-regulation-of-autonomous-weapons-systems-913
Acc 4/5/22 TA\]

[In the case of LAWS, however, the old pattern of defining and then
regulating a discrete category of military hardware is not applicable.11
After all, almost any current and future weapons system can conceivably
be endowed with autonomous functions, and no one will be able to tell
what any given system\'s level of dependence on human input is by merely
inspecting it from the outside]{.underline}. In the past, bilateral
nuclear arms control between the United States and the Soviet Union,
later Russia, implemented quantitative arms control by developing
precisely defined, shared understandings of counting rules and employing
them in verification regimes.12 Similarly, in the realm of [multilateral
conventional arms control]{.underline}, the now defunct Treaty on
Conventional Armed Forces in Europe [relied heavily on defining and
counting military hardware items.13 The challenge regarding LAWS,
however, is not met by trying to define a category of weapons
system]{.underline} -- "LAWS", as separated with a list of specific
criteria from "non-LAWS" -- [and then counting and capping its
numbers]{.underline}. In fact, in a modern military\'s system-of-systems
architecture, "[some AWS \[autonomous weapons system\] components are
intangible and can be geographically distributed, \[so\] it is far from
clear ... where and when an AWS begins and ends".]{.underline}14 [Hence,
the challenge, broadly speaking, lies in developing a new norm in order
to adjust the relationship between humans and machines]{.underline} in
twenty-first-century warfighting. [A qualitative rather than
quantitative approach is required,]{.underline} which, in turn, requires
new diplomatic language to grasp the underlying technological
developments, something that neither States Parties nor civil society
are well versed in yet.

#### NATO is more effective because it addresses the range and context of weapons and because it is more likely to foster cooperation.

**Trabucco and Stanley-Lockman, 2022 -- prof of Political Science,
University of Copenhagen and prof of Defense and Strategic Studies,
Nanyang Technological University** \[Lena and Zoe, The Oxford Handbook
of AI Governance, March, "NATO's Role in Responsible AI Governance in
Military Affairs"
https://www.oxfordhandbooks.com/view/10.1093/oxfordhb/9780197579329.001.0001/oxfordhb-9780197579329-e-69
Acc 4/16/22 TA\]

Lawyers, researchers, and civil society grapple with existing legal
regimes relevant to military operations and the uncertainty and
ambiguity surrounding automated decision-making, particularly in lethal
decision-making. Thus far, [the legal dialogue has been heavily
anchored]{.underline} in the applicability of international humanitarian
law (IHL), and other relevant legal regimes, [to lethal autonomous
weapons systems]{.underline}.72 IHL, also known as the laws of war or
the laws of armed conflict, regulate the means and methods of warfare
and, as such, is pivotal to the emergence of military technology and how
existing legal structures are disrupted. [The legal debate often
revolves around the prospect of a "treaty ban" of LAWS]{.underline}.73
[But the legal debate is much more nuanced than the likelihood of
international treaties banning any particular weapon system. Especially
because NATO is not a regulatory body, it cannot institute measures to
regulate emerging technology for the Allies. Instead, NATO's function in
the legal domain may be more effective]{.underline} outside the
traditional legal debates around emerging military technology [and more
embedded in fostering cooperation and coordination among military
partners.]{.underline} Other avenues of legal regulation may fall short
of an international convention or prohibition, but nevertheless factor
significantly in regulating and/or delineating state policies.
Additionally, [non-lethal applications of AI, as well as applications of
AI that do not figure into autonomous systems, also raise important
legal questions under international law]{.underline}. Arguably, norms
around non-lethal applications are more urgent as their development is
more advanced, harder to define, and less controversial in
integration.74 Ultimately, [NATO's facilitative power can help ensure
that integration of EDTs like AI into military capabilities and into
multinational coalition operations is consistent with member states'
legal obligations]{.underline}.

#### Turn -- a ban assumes LAWs in the future, which distracts from autonomous systems that already exist. It is better to establish norms for their ethical use. 

**Nadibaidze, 2021 - Ph.D. Student at the University of Southern
Denmark** \[Anna Entry submitted for the Second OSCE-IFSH Essay
Competition "Commitment to Control over Weaponised Artificial
Intelligence: A Step Forward for the OSCE and European Security"
https://www.osce.org/files/f/documents/8/3/507341.pdf Acc. 4/21/22 TA\]

In response to the risks outlined in this section, [several states,
scholars and civil society organisations have been arguing for a ban on
LAWS.]{.underline} Since 2013, this issue has been discussed within the
framework of the UN Convention on Certain Conventional Weapons (CCW). A
Group of Governmental Experts (GGE) on emerging technologies in the area
of LAWS was established in 2016 to pursue the debate in a more formal
setting. [However, the discussions are often framed in a futuristic way,
focusing on 'killer robots' and their potential to operate with full
autonomy and without human oversight. As this section has demonstrated,
this perspective misses the fact that existing weapons systems with
increasingly autonomous features already have the potential to affect
security and stability]{.underline}.

#### Banning LAWs is impossible -- the ban would be easy to circumvent and the weapons are already available.

**Thornton, 2019 - Senior Lecturer in the Centre for Defence Education
Research and Analysis, King's College** \[Rod, "One to ponder: the UK's
ethical stance on the use of Artificial Intelligence in weapons systems
https://defenceindepth.co/2019/06/17/one-to-ponder-the-uks-ethical-stance-on-the-use-of-artificial-intelligence-in-weapons-systems/
Acc 4/16/22 TA\]

Given its declared position, it might seem logical for the UK to push
for an international ban on the use of LAWS. Trying to level the playing
field so that no other state possessed them would seemingly work to the
UK's advantage[. A ban is also the favoured UN option]{.underline}. UN
Secretary General António Guterres has, for instance, described LAWS as
'morally repugnant'. Within the UN, [however, the UK is part of a group
of states]{.underline} (alongside Australia, Israel, Russia and US)
[that has collectively stated that]{.underline} [currently they do not
want to see any regulation that forbids the use of LAWS]{.underline}. To
explain the UK's position, an MOD spokesperson said that, 'We believe [a
pre-emptive ban is premature as there is still no international
agreement on the characteristics of lethal autonomous weapons
systems'.]{.underline} We are thus back to the thorny problem of
definitions. [If we do not know what something is then how can it be
banned?]{.underline} The question here, though, is why is the UK trying
to prevent a ban on a weapon it has 'no intention' of developing itself?
This does not look very ethical or, indeed, sensible. It seems to be
giving licence to potential adversaries to continue with their own
development of LAWS while the UK sits on its AI hands. Whatever the UK's
position, it seems that [LAWS will prove impossible to ban]{.underline}
anyway. [Firstly, because the world's major states will be seeing the
benefits of LAWS there will probably]{.underline} (and maybe
conveniently?) [never be an internationally agreed definition on them,
which would then allow any ban to accrue. Secondly, the technology that
underpins any 'killer robot' will come to be developed anyway in the
civilian sector]{.underline} -- with systems designed, for instance, to
deliver parcels or to tackle forest fires. [Any military organisation
could simply buy such systems off the shelf]{.underline} and convert
them readily into LAWS. [The genie will thus be out of the bottle on
LAWS fairly soon anyway and can never be put back in]{.underline}. It
will therefore, and unfortunately, be very hard for the UK to maintain a
credible stance as a 'pioneer in ethical AI'.

#### Focusing on banning LAWs diverts attention from cooperation on Responsible AI use.

**Trabucco and Stanley-Lockman, 2022 -- prof of Political Science,
University of Copenhagen and prof of Defense and Strategic Studies,
Nanyang Technological University** \[Lena and Zoe, The Oxford Handbook
of AI Governance, March, "NATO's Role in Responsible AI Governance in
Military Affairs"
https://www.oxfordhandbooks.com/view/10.1093/oxfordhb/9780197579329.001.0001/oxfordhb-9780197579329-e-69
Acc 4/16/22 TA\]

The second pillar examines legal norms as a domain wherein legal
uncertainty regarding AI has tangible implications for Allied legal
interoperability, a subset of larger coalition interoperability. [Thus
far, the legal debate regarding AI has been largely fixed on the issue
of a treaty banning the use of LAWS. In this section, we advocate for a
more nuanced legal picture in which NATO can facilitate legal
coordination and tackle some of the foundational legal issues which will
prevent successful legal interoperability in future
operations]{.underline}. The third pillar identifies safety and security
of AI systems as prerequisite to trustworthy and responsible AI in any
context, but especially so for the conduct of military activity. At the
[NATO]{.underline} level, Allied [forces must ensure their systems
interoperate safely and predictably both to ensure effective command and
control]{.underline} (C2) internally, [and to prevent disruptions from
attacks]{.underline}. [It is a foundational facet of coordination that
shows the overlap between NATO interests in military effectiveness and
incentivization for responsible innovation.]{.underline}

### 

### AT Cybernetics K

#### Prioritizing Humanist justifications for plan are essential to build political will for change

**Sauer, 2021 - Senior Research Fellow at Bundeswehr University**
\[Frank serves on the International Panel on the Regulation of
Autonomous Weapons IRRC No. 913 March "Stepping back from the brink: Why
multilateral regulation of autonomy in weapons systems is difficult, yet
imperative and feasible"
https://international-review.icrc.org/articles/stepping-back-from-brink-regulation-of-autonomous-weapons-systems-913
Acc 4/5/22 TA\]

How regulating weapon autonomy is feasible: Fostering a human control
norm The United States and China are demonstrating awareness of the
strategic risks of unmitigated weapon autonomy. The US directive on
weapon autonomy,95 albeit attempting to square the circle of using
autonomy while not inviting the accompanying risks, can be interpreted
this way. China has coined the term "battlefield singularity", a dreaded
situation in which war waged at machine speed is too fast for human
cognition to keep up.96 Nevertheless, [the current great power
rivalry]{.underline} between the United States, China and Russia, all
racing for dominance in the field of military AI, [is clearly not
conducive to regulation of weapon autonomy]{.underline}. With presidents
Trump, Xi and Putin in power, a breakthrough is not to be expected any
time soon. [But political will for regulation can also be generated from
the ground up.]{.underline} Growing political will from the grassroots
[Surveys consistently show publics from all over the world rejecting
LAWS]{.underline}. Opposition globally increased from 56% in 2016 to 61%
in 2018, according to KRC survey data.97 This conforms with earlier
online polling conducted by the Open Roboethics Initiative98 as well as
Heather Roff via IPSOS.99 Opposition in the United States, China and
Russia is at 52%, 59% and 60% respectively.100 In Europe, the numbers
range from 60% in Finland up to 81% in Ireland.101 [Survey data also
suggest that the public\'s opposition is primarily fueled not by legal
concerns or worries about unwanted escalation or crisis instability but
by the notion that delegating decisions over life and death on the
battlefield crosses a bright red moral line]{.underline}.102 So [while
there is certainly an interesting philosophical debate to be had
about]{.underline} the cultural pervasiveness of [human dignity as a
concept]{.underline} and its relevance to the LAWS issue from
utilitarian versus deontological viewpoints, [the concern]{.underline}
as presented in the preceding section [quite clearly resonates with the
general public. The notion that there is something fundamentally wrong
with having humans killed by mindless machines is thus well suited to
creating grassroots pressure on governments in order to muster more
political will on the issue]{.underline}. [This point is granted even by
sceptics of the human dignity argument as a whole: "There could be some
campaigning advantages. Saying that something is against human dignity
evokes a strong visceral response."]{.underline}103

#### Prioritizing Representations of Human Control ensures that we have an ethical compass to ensure that plan works as described.

**Kewley, 2021 -- Cohead of the Tech Group at Clifford Chance LLP**
\[Jonathan, Dec 7, "Artificial intelligence: Can we go from chaos to
cooperation?" AEI Panel Discussion - Moderator: Elisabeth Braw
https://www.aei.org/events/artificial-intelligence-can-we-go-fromchaos-to-cooperation/
Acc 5/11/22 TA\]

Jonathan Kewley: I suppose my closing point is about making this all
human centered. And what does that fix mean? It means putting the human
at the heart of technology. And [what we've seen in other areas so
far]{.underline} --- and we've got good precedence --- is that [if you
take the human out and you just go over a]{.underline} profit motive or
[in the defense sector, kind of a kill motive, then you've got a real
issue. So, putting morality, human beings at the heart of this will
ensure that we don't have chaos, because that moral compass will be
there]{.underline}. And we all have a part to play in this. And finally,
I would say that [multilateralism is so important]{.underline}. [We
can't have this debate on our own in Europe and not include China and
not include the US. It needs to be a]{.underline} big conversation, [a
global conversation and a nonexclusive one]{.underline}.

### AT Security K

#### The affirmative recognizes the Security Dilemma that AI creates, and attempts to act within that understanding. Our approach does not engage in zero sum competition -- ethical use respects the human security of all combatants. 

**Scharre, 2021 - Director of Studies at Center for New American
Security** \[Paul, Texas National Security Review Vol 4, Iss 3 Summer
"Debunking the AI Arms Race Theory"
https://tnsr.org/2021/06/debunking-the-ai-arms-race-theory/Artificial
Intelligence Acc 5/27/22 TA\]

AI Competition and the Security Dilemma Even if military AI spending
does not rise to the level of an "arms race," [many nations are
nevertheless engaged in a security competition in the adoption of
military AI, a competition that does pose risks]{.underline}. The
situation that states find themselves in with regard to [AI competition
is much more accurately described as a security dilemma,]{.underline}16
a more generalized competitive dynamic between states than the more
narrowly defined "arms race." In his 1978 article, "Cooperation Under
the Security Dilemma," [Robert Jervis defined the security dilemma as
follows: "\[M\]any of the means by which a state tries to increase its
security decrease the security of others."]{.underline}17 As Charles
Glaser has pointed out, it is not obvious from this definition why it
would be intrinsically bad for an increase in one state's security to
come at the expense of another's security.18 In fact, decreasing the
security of other states could have beneficial effects in enhancing
deterrence and reducing the risks of aggression or achieving a favorable
balance of power in a region, which could lead to greater political
influence. The problem comes in the second- and third-order effects that
could develop when another state reacts to having its security reduced.
Responses could include counterbalancing with a net effect of no change
in security (or worsening security). Glaser argues that there are some
situations in which security competition is a rational strategy for a
state to pursue even if competitors will arm in response. In other
situations, arming may be a suboptimal strategy for a state, which would
be better served by restraint or pursuing arms control.19 [Security
competition could even leave both states worse off than
before]{.underline}. This can occur during a traditional arms race if
nations expend vast sums of money in an unsuccessful attempt to gain an
advantage over one another, with the result that both nations divert
funds from non-defense expenditures. If the outcome of a security
competition is the same relative military balance as before, the balance
of power may not have meaningfully changed, but both nations could face
diminished economic and social well-being at home relative to if they
had avoided a security competition. Even absent this "guns vs. butter"
tradeoff, however, there are other ways in which security competition
can lead to a net negative outcome for both states. [One way this could
occur is if military innovation and the development of new capabilities
alter the character of warfare in a manner that is more harmful, more
destructive, less stable, or otherwise less desirable than
before]{.underline}. In his 1997 article, "The Security Dilemma
Revisited," Glaser gave the example of military capabilities that
shifted warfare to a more offense-dominant regime.20 There are other
ways in which warfare could evolve in a net negative direction as well.
For example, in World War I, Germany's interest in developing and
deploying chemical weapons was spurred in part due to fears about
France's developments in poison gas.21 The result was the introduction
of a weapon that increased combatant suffering on both sides, without
delivering a significant military advantage to either. [The same could
occur with AI: It could alter the character of warfare in a way that
would be a net negative for all participants]{.underline}.

#### Recognizing the constructions of security is not enough -- scenario planning is necessary to advance the conversation about military AI. 

**Horowitz and Scharre, 2021 - Senior Fellows at the Technology and
National Security Program at the Center for a New American Security**
\[Michael and Paul, Jan 12, "AI and International Stability: Risks and
Confidence-Building Measures"
[https://www.cnas.org/publications/reports/ai-and-international-stability-risks-and-confidence-building-measures
Acc
6/6/22](https://www.cnas.org/publications/reports/ai-and-international-stability-risks-and-confidence-building-measures%20Acc%206/6/22)
TA\]

The Role of Confidence-Building Measures AI potentially generates risks
for international security due to ways AI could change the character of
warfare, the limitations of AI technology today, and the use of AI for
specific military missions such as nuclear operations. Especially given
the uncertain technological trajectory of advances in AI, [what are
options to reduce the risks that military applications of AI can pose to
international stability? To advance the conversation about ensuring that
military AI adoption happens in the safest and most responsible way
possible, this paper outlines a series of potential confidence-building
measures aimed at mitigating risks from military uses of
AI]{.underline}.39 [We introduce these ideas]{.underline} as preliminary
concepts for future research, discussion, and examination, rather than
to specifically advocate for any of these options. [But progress in
mitigating the risks from military AI competition requires moving beyond
the recognition that risk mitigation is important to the hard work of
suggesting, evaluating, and examining the benefits and drawbacks of
specific mechanisms]{.underline}.40

#### 

#### We recognize the technological determinism embedded in AI discussions -- this is the most important starting point for the discussion. 

**International Panel on the Regulation of Autonomous Weapons, 2021**
\[(iPRAW) coordinated by: German Institute for International and
Security Affairs, July "Building Blocks for a Regulation on LAWS and
Human Control Updated Recommendations to the GGE on LAWS"
https://www.readkong.com/page/building-blocks-for-a-regulation-on-laws-and-human-control-8617434
Acc 2/27/22 TA\]

2 BUILDING BLOCK I: UNDERSTANDING THE CHALLENGES [In order to establish
a normative and operational framework aimed at tackling the challenges
posed by LAWS, it is imperative to understand the technological,
military, legal, ethical, and security aspects. In fact, a profound
grasp of the technological aspects, especially with regard to
computational methods, like artificial intelligence (AI)]{.underline}
and machine learning, [is the starting point of any discussion on a
regulatory framework on LAWS.]{.underline} By the same token, [it is of
pivotal importance to bear military considerations in mind and to
comprehend the nature and structure of military operations,]{.underline}
[in particular the targeting cycle]{.underline}. The same holds true for
legal considerations. International law, especially international
humanitarian law (IHL) constitutes the fulcrum of the debate on LAWS.
Furthermore, human dignity needs to be taken into account properly, as
emerging technologies in the military realm threaten to touch upon or
even violate human rights. Last but not least, [security aspects should
be considered thoroughly.]{.underline} The increased resort to
autonomous functions in combat operations will have repercussions on
conflict escalation and international stability and may even lead to an
AI arms race, potentially having devastating consequences.

## 
