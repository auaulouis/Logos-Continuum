# Lethal Autonomous Weapons System (LAWS) Affirmative

![C:\\Users\\em68\\AppData\\Local\\Microsoft\\Windows\\INetCache\\Content.Word\\MoState
Debate OpenEv2.png](media/image1.png){width="4.333333333333333in"
height="7.1409722222222225in"}

## 1AC

### 1AC -- Inherency 

#### We are at an incredibly dangerous crossroads with the use of artificial intelligence and military weaponry. Lethal Autonomous Weapons Systems, more commonly referred to as LAWS, are dangerously close to becoming the future of war. This would have devastating consequences that must be stopped. 

**Trager and Luca** (Robert F. Trager is an associate professor of
political science at the University of California, Los Angeles, and a
Centre for the Governance of AI representative to the U.N. Convention on
Certain Conventional Weapons; Laura M. Luca is a graduate student in
political science at the University of California, Los Angeles, and a
former delegate of Romania to the U.N. Convention on Certain
Conventional Weapons.) "Killer Robots Are Here---and We Need to Regulate
Them" May 11, **2022**
https://foreignpolicy.com/2022/05/11/killer-robots-lethal-autonomous-weapons-systems-ukraine-libya-regulation/

[[Swarms of robots with the ability to kill humans are no longer only
the stuff of science fiction]{.underline}]{.mark}. Lethal autonomous
weapons systems (LAWS) are here. In Ukraine, Moscow has allegedly
deployed an artificial intelligence (AI)-enabled Kalashnikov ZALA Aero
KUB-BLA loitering munition, while Kyiv has used Turkish-made Bayraktar
TB2 drones, which have some autonomous capabilities. Although it's
always hard to determine whether a weapon's autonomous mode is used,
these technologies have reportedly been employed in at least one
conflict: Last year, a United Nations report suggested Turkey used
autonomous firing by its Kargu-2 drones to hunt fleeing soldiers in
Libya's civil war (though the CEO of the Turkish company that produced
the drone denies it is capable of this). Unlike traditional drones,
these systems have the ability to navigate on their own, and some can
select targets. Although a human controller can still decide whether or
not to strike, such weapons are acquiring ever more autonomous
capabilities. Now that militaries and paramilitaries worldwide have
taken note, these technologies are poised to spread widely. **[[The
world today stands at the very moment before much more advanced versions
of these technologies become ubiquitous]{.underline}]{.mark}**. So far,
at least Israel, Russia, South Korea, and Turkey have reportedly
deployed weapons with autonomous capabilities---though whether this mode
was active is disputed---and Australia, Britain, China, and the United
States are investing heavily in developing LAWS with an ever-expanding
range of sizes and capabilities. Already, some LAWS can loiter in an
area to find targets that machine-learning algorithms have trained them
to recognize, including enemy radar systems, tanks, ships, and even
specific individuals. These weapons can look vastly different: For
instance, the Turkish Kargu-2 drone, which was introduced in 2020 and
used in Libya's war, is 2 feet long, weighs around 15 pounds, and can
swarm in groups. Autonomous systems can also be much larger, such as
unmanned AI-driven fighter jets like the modified L-39 Albatros, and
much smaller, such as rudimentary commercial drones repurposed with
autonomous software. Once these technologies have spread widely, they
will be difficult to control. [[The world thus urgently needs a new
approach to LAWS]{.underline}]{.mark}. So far, the international
community has done nothing more than agree that the issue needs to be
discussed. But [[what it really needs to do is take a page from the
nuclear playbook and establish a nonproliferation regime for
LAWS]{.underline}]{.mark}. Currently, countries at the forefront of LAWS
development resist any calls for their ban. The United States has
claimed that existing international humanitarian laws are sufficient to
govern LAWS; the U.S. Defense Department's policy is that they must be
designed to ensure "appropriate levels of human judgment over the use of
force." China has remained ambiguous, stating the importance of "full
consideration of the applicability of general legal norms" while
insisting on a narrow definition of LAWS. Russia, meanwhile, refuses to
even consider the issue, using diplomatic procedural tools to stall and
reduce the time the United Nations devotes to debating the subject. But
[[most countries have called for a ban on developing and using
LAWS]{.underline}]{.mark}---or, at a minimum, regulating them. In 2019,
U.N. Secretary-General António Guterres said [[LAWS are "politically
unacceptable, morally repugnant, and should be prohibited by
international law.]{.underline}]{.mark}" There are many reasons
countries, international nongovernmental organizations, scholars, and AI
experts worry about LAWS. Although they do not all agree in their
predictions of how such weapons could affect society, there's a growing
consensus that their spread could bring substantial and harmful
consequences. First, [[LAWS could facilitate violence on a large
scale]{.underline}]{.mark} since they're not restricted by the number of
people available to man them. Second, [[in combination with facial
recognition]{.underline}]{.mark} and other technologies[[, they can
target individuals or groups that fit certain descriptions, which could
appeal to violent groups and state militaries committing political
assassinations and ethnic cleansing]{.underline}]{.mark}. Third, LAWS
may make it easier for those who control them to hide their identities.
[[LAWS]{.mark}]{.underline} thus have the potential to upend political
orders and enable tighter authoritarian control. In addition, **[[they
can always malfunction, including by mistaking civilians for
combatants]{.underline}]{.mark}**. So far, the international community
has attempted---and failed---to regulate LAWS. In December 2021, after
eight years of technical discussions, government and civil society
representatives met at the U.N. in Geneva to set an agenda for
regulating LAWS for the first time in what was billed as a "historic
opportunity." Most attendees favored legally binding rules that apply
equally to all states to govern the development and use of these
technologies. Yet, by any standard, the meeting failed. Despite years of
preparatory discussions within the framework of the U.N. Convention on
Certain Conventional Weapons (CCW)---a forum for restricting the use of
weapons considered to cause unnecessary or unjustifiable suffering to
combatants or to affect civilians indiscriminately---the attendees
barely managed to agree on 10 more days of discussion this March and
July. This outcome was to be expected given the positions of the major
powers and the CCW rules requiring consensus before action is taken.
(Disclosure: Both authors have been affiliated with the CCW.) In
response, a wide array of actors---from Amnesty International and Human
Rights Watch to some of the states in favor of a LAWS ban, including
Argentina and the Philippines---are calling for a process to develop
legally binding prohibitions on these weapons outside of the CCW.
Alternative approaches to prohibition treaties have had some success in
the past, such as when countries agreed to give up land mines through
the 1997 Ottawa convention; cluster munitions through the 2008 Oslo
Accords; and even nuclear weapons through the 2017 Treaty on the
Prohibition of Nuclear Weapons, which was the first treaty to completely
ban nuclear weapons in line with international humanitarian law and
establish pathways for current nuclear weapon states to renounce them.
However, while many states signed these treaties, most of the powerful
states did not. Unfortunately, even this limited success is likely to be
elusive in the case of LAWS. The primary reason is that states are
increasingly aware that these non-substitutable technologies may become
crucial to their security and are thus unlikely to unilaterally abandon
them. [[If states' adversaries have them, they will likely believe they
need them---and absent the sort of nonproliferation regime that exists
for nuclear weapons, their adversaries will, in fact, continue to
rapidly develop LAWS without much oversight.]{.underline}]{.mark}

### 1AC -- Plan 

#### Resolved: The United States federal government should substantially increase its security cooperation with the North Atlantic Treaty Organization by banning Lethal Autonomous Weapons Systems. 

### 1AC -- Solvency

#### Despite current development by nation states, a weapons ban would be effective and guarantee global follow on. The result of the ban is a global ban on lethal autonomous weapons systems 

Mary **Wareham** (advocacy director in the arms division at Human Rights
Watch) "Summary: Stopping Killer Robots" August 10, **2020**
https://www.hrw.org/report/2020/08/10/stopping-killer-robots/country-positions-banning-fully-autonomous-weapons-and

Their active engagement in the CCW talks on killer robots demonstrates
growing awareness of and concerns about removing human control from the
use of force. There is widespread acknowledgment that technological
developments are enabling militaries to incorporate autonomy into
weapons systems. China, Israel, Russia, South Korea, the United Kingdom,
and the United States are investing heavily in the development of
various autonomous weapons systems, while Australia, Turkey, and other
countries are also making investments. [[Despite]{.underline}]{.mark}
this [[development, the vast majority of countries that have spoken to
date regard human decision-making, control, or judgment as critical to
the acceptability and legality of weapons systems. There is now
widespread agreement about the need to retain some form of human control
over the use of force, including over individual
attacks]{.underline}]{.mark}. In 2018, Austria, Brazil, and Chile
recommended launching negotiations on a legally binding instrument to
ensure meaningful human control over the critical functions of weapons
systems.\[11\] Banning fully autonomous weapons means prohibiting
weapons systems that lack meaningful human control. Since 2013, [[30
countries have called for a ban]{.underline}]{.mark} on such fully
autonomous weapons: Algeria, Argentina, Austria, Bolivia, Brazil, Chile,
China, Colombia, Costa Rica, Cuba, Djibouti, Ecuador, Egypt, El
Salvador, Ghana, Guatemala, Holy See, Iraq, Jordan, Mexico, Morocco,
Namibia, Nicaragua, Pakistan, Panama, Peru, State of Palestine, Uganda,
Venezuela, and Zimbabwe. [[China has called for a
treaty]{.underline}]{.mark} to ban the use of lethal autonomous weapons
systems, but not their development or production, which is unsurprising
given that it is also among the nations most advanced in pursuing such
weapons. [[Several groups of states]{.underline} [have endorsed
statements]{.underline}]{.mark} calling for a ban on killer robots.
[[The Non-Aligned Movement (NAM), which is comprised of approximately
125 member states, has called for a "legally binding international
instrument]{.underline}]{.mark} stipulating prohibitions and regulations
on lethal autonomous weapons systems" several times since 2018.\[13\]
Benin spoke in April and August 2018 on behalf of a group of African
states to recommend launching negotiations on a legally binding
instrument on fully autonomous weapons "at the earliest" as weapons
systems "that are not under human control should be banned."\[14\] All
CCW meetings on killer robots in 2014-2019 saw strong interest or
convergence on the importance of retaining human control over weapons
systems and the use of force. This is reflected in a principle on
human-machine interaction that CCW states agreed to in 2019.\[15\]
Human-machine interaction attracted the greatest interest by far during
the virtual Berlin Forum on lethal autonomous weapons systems attended
by more than 60 countries on April 1-2, 2020. There was widespread
recognition at the Rio Seminar on autonomous weapons on February 20,
2020 that human control is where states should focus their collective
work. A legally binding instrument is the optimal framework for dealing
with the many serious challenges raised by fully autonomous weapons. [[A
new international ban treaty could]{.underline}]{.mark} lay down
explicit rules to ensure appropriate constraints on autonomy in weapons
systems and resolve differing views on human control over the use of
force. Most importantly, a new treaty would [[show that states are
serious about responding appropriately and with urgency to this
existential threat to humanity.]{.underline}]{.mark}

#### LAWS are not accurate, undermining any positive benefit that could theoretically exist. We cannot allow this unregulated and dangerous technology. 

Zachary **Kallenborn** (research affiliate with the Unconventional
Weapons and Technology Division of the National Consortium for the Study
of Terrorism and Responses to Terrorism, a policy fellow at the Schar
School of Policy and Government, and a U.S. Army Training and Doctrine
Command "Mad Scientist.") "Applying arms control frameworks to
autonomous weapons" October 05, **2021**
https://www.brookings.edu/techstream/applying-arms-control-frameworks-to-autonomous-weapons/

[[In an autonomous weapon, the system decides when to engage by
processing environmental stimuli]{.underline}]{.mark}. Landmines, for
example, use simple pressure sensors---the sensor sensitivity determines
whether the heft of a tank or the hands of a child are enough to trigger
the explosion. Conversely, an anti-radar loitering munition homes in on
radar signals. [[The risk of error]{.underline}]{.mark}---and by
extension the arms control concern[[---depend]{.underline}]{.mark} on
the type of environmental stimuli, how the stimuli is processed, and the
type of decisions made. Emerging autonomous weapons using machine
learning process stimuli in more complex ways. [[Machine learning
systems rely on large amounts of data]{.underline}]{.mark} to draw
conclusions about what the system observes. [[But the data dependence
also makes them brittle. Color differences, tree branches, or foggy days
may confound the ability of the system to correctly identify a
target.]{.underline}]{.mark} Although some states may adopt robust
verification and testing programs to increase reliability, others may
not. [[As autonomous weapons are deployed]{.underline}]{.mark} in larger
numbers[[, arms control advocates fear a higher likelihood of something
going horrifyingly wrong.]{.underline}]{.mark}

### 1AC -- Terrorism Advantage 

**The use of lethal autonomous weapons systems make terrorism
inevitable. The affirmative has four ways they solve terror.**

**First, hacks.**

#### Lethal Autonomous Weapons systems are susceptible to hacks by terror groups and other nefarious actors. Only banning them solves. 

Coley **Felt** (International Policy Institute Cybersecurity Fellow)
"Autonomous Weaponry: Are Killer Robots in Our Future?" February 14,
**2020**
https://jsis.washington.edu/news/autonomous-weaponry-are-killer-robots-in-our-future/

In respect to the unclear line of accountability as an argument against
development, the concept of the war algorithm is a key component to
discussing autonomous weapons systems. According to the Harvard Law
School, a war algorithm is "any algorithm that is expressed in computer
code, that is effectuated through a constructed system, and that is
capable of operating in relation to armed conflict" (Lewis, Blum, &
Modirzadeh, 2016). When linking the war algorithm to accountability, the
line stretches from states and their armed forces, to developers,
operators, lawyers, industry bodies and more. [As]{.underline} these
[algorithms continue to advance, they challenge some of the fundamental
legal concepts that underpin the regulation of armed
conflict]{.underline}. Thus, the development of fully autonomous weapons
presents the possibility of "replacing human judgement with
algorithmically-derived decisions" on the battlefield (Lewis et al.,
2016). Furthermore, **[[there is always a risk of vulnerabilities which
raises concern about the potential of a hacker
takeover]{.mark}]{.underline}** (Lewis et al., 2016). [[Not only would
decisions made by these machines be the product of an algorithm, but
ensuring the security and reliability of that algorithm presents a new
set of issues. The capabilities that these weapons would attain could be
especially attractive to malicious actors, whether it be other states or
individuals]{.underline}]{.mark}. These unique characteristics rooted in
autonomous weapons systems suggest the need for a specific legal
category that can oversee both the design and the behavior of the
machines.

#### Second, the proliferation of autonomous weapons system development makes leaks inevitable. The more people that have the technology, the more likely it is for terrorists to get it. 

Jacob **Ware** (master's degree in security studies from Georgetown
University and an MA (Hons) in international relations and modern
history from the University of St Andrews) "TERRORIST GROUPS, ARTIFICIAL
INTELLIGENCE, AND KILLER DRONES" September 24, **2019**
https://warontherocks.com/2019/09/terrorist-groups-artificial-intelligence-and-killer-drones/

Firstly, [[modern terrorist organizations have advanced scientific and
engineering departments, and actively seek out skilled scientists for
recruitment]{.underline}]{.mark}. ISIL, for example, has appealed for
scientists to trek to the caliphate to work on drone and AI technology.
The individual technologies behind swarming killer robots --- including
unmanned aerial vehicles, facial recognition, and machine-to-machine
communication --- already exist, and have been adapted by terrorist
organizations for other means. According to a French defense industry
executive, "the technological challenge of scaling it up to swarms and
things like that doesn't need any inventive step. It's just a question
of time and scale and I think that's an absolute certainty that we
should worry about." Secondly, autonomous weapons technology will likely
proliferate through sales. Because AI research is led by private firms,
advanced AI technology will be publicly sold on the open market. As
Michael Horowitz argues, "militant groups and less-capable states may
already have what they need to produce some simple autonomous weapon
systems, and that capability is likely to spread even further for purely
commercial reasons." The current framework controlling high-tech weapons
proliferation --- the Wassenaar Arrangement and Missile Technology
Control Regime --- is voluntary, and is constantly tested by great-power
weapons development. Given interest in developing AI-guided weapons,
this seems unlikely to change. Ultimately, as AI expert Toby Walsh
notes, the world's weapons companies can, and will, "make a killing (pun
very much intended) selling autonomous weapons to all sides of every
conflict." Finally, [[autonomous weapons technology is likely to
leak]{.underline}]{.mark}. Innovation in the AI field is led by the
private sector, not the military, because of the myriad commercial
applications of the technology. [[This will make it more difficult to
contain the technology, and prevent it from proliferating to nonstate
actors]{.underline}]{.mark}. Perhaps the starkest warning has been
issued by Paul Scharre, a former U.S. defense official[[: "We are
entering a world where the technology to build lethal autonomous weapons
is available not only to nation-states but to individuals as well. That
world is not in the distant future. It's already
here."]{.underline}]{.mark}

#### Third, terror recruitment 

#### Lethal autonomous weapons cause massive radicalization---even a small increase in recruitment would trigger the impact

John **Feffer**, 5-25-**2016**, \"The Coming Drone Blowback\", Institute
for Policy Studies, <https://ips-dc.org/coming-drone-blowback/> \<John
Feffer is director of Foreign Policy In Focus at the Institute for
Policy Studies.\>, ke

The U.S. drone campaign isn't exactly a covert operation, though the CIA
has generally refused to acknowledge its role in the attacks (the
Pentagon is more open about its use of drones for strikes on more
conventional military targets). But critics of drone attacks --- myself
included --- have long argued that all the civilian casualties caused by
drone attacks will produce blowback[[. Drone strikes and the anger they
generate]{.mark} effectively [serve to recruit people into the Taliban
and other extremist organizations.]{.mark} Even those involved in the
program have come to the same conclusion]{.underline}. Consider, for
instance, this impassioned plea to President Obama from four Air Force
veterans who piloted drones. "The innocent civilians we were killing
only fueled the feelings of hatred that ignited terrorism and groups
like ISIS, while also serving as a fundamental recruitment tool," they
argued in a letter last November. "[The administration and its
predecessors have built a drone program that is one of the most
devastating driving forces for terrorism and destabilization around the
world]{.underline}." But now along comes Aqil Shah, a professor at the
University of Oklahoma, who has just published a report attempting to
debunk this claim. According to a set of 147 interviews he conducted in
North Waziristan, an area in Pakistan's FATA that has sustained the
largest number of drone strikes, 79 percent of respondents support the
campaign. A majority believes that the strikes rarely kill
non-combatants. Further, according to experts cited by Shah, "most
locals prefer drones to the Pakistan military's ground and aerial
offensives that cause more extensive damage to civilian life and
property." I don't doubt these findings. Most people in Pakistan have no
sympathy for the Taliban. According to a recent Pew poll, 72 percent of
respondents in Pakistan had an unfavorable view of the Taliban (with
earlier polls suggesting that this lack of support extends to FATA).
Drones are no doubt better than Pakistan's military operations, just as
they represent an improvement over the scorched-earth policies used by
the United States in the Vietnam War to destroy large sections of
Southeast Asia. Shah's research was not exactly scientific. He admits
that his interviews were "not statistically representative" --- and then
goes on to draw conclusions about the entire population of FATA. It's
also true that several other polls suggest that Pakistanis throughout
the country oppose the drone program and believe that it encourages
militancy, but these polls have generally not included FATA. But Shah's
most controversial conclusion is that the high level of support for the
drone program means that no blowback has taken place. Even if his
interviews were statistically representative, I don't understand this
analytical leap. [[Blowback doesn't require universal opposition. Only a
small percentage of the mujahedeen went on to fight with Osama bin
Laden]{.mark}. Only a certain number of Contras were involved in
operations that pumped drugs into the United States. It's not as if the
entire population of FATA is going to join the Taliban. [If only a
couple thousand young men join the Taliban out of anger over drone
strikes, that counts as blowback]{.mark}]{.underline}. There are over 4
million people living in the FATA. A fighting force of 4,000 people is 1
percent of the population --- and that easily falls within the 21
percent of respondents who disapproved of drones in Shah's findings. And
what of the suicide bomber who embarks on his path of extremism because
a drone strike took out his brother? The Times Square bomber, Faisal
Shahzad, was motivated at least in part by drone strikes in Pakistan,
even though they hadn't killed anyone in his family[. [Ultimately,
blowback can be just one angry and determined person who makes his mark
on history without first showing up in a survey.]{.mark}]{.underline}

**Fourth, ending the strategy of needless intervention and the war on
terror**

Matthew **Anzarouth** (Harvard Political Review) "Robots that Kill: The
Case for Banning Lethal Autonomous Weapon Systems" December 02, **2021**
https://harvardpolitics.com/robots-that-kill-the-case-for-banning-lethal-autonomous-weapon-systems/

The Danger in Killer Robots. [[The use of LAWS would lower the threshold
for states going to war,]{.underline}]{.mark} increasing the likelihood
of conflict. Many philosophers, political scientists and governments
have expressed the concern that militaries will resort to conflict more
often if they do not need to rely on soldiers and can use LAWS instead.
Domestic populations will be less wary of conflict if it no longer means
seeing fellow citizens risk their lives on the battlefield. The
threshold-lowering effect of LAWS is particularly relevant in the
context of a current bipartisan trend in the U.S. against intervention.
It is plausible that [[without LAWS, the era of U.S. unilateral
interventions and the war on terror would come to an
end.]{.underline}]{.mark} Recognizing the failures of wars in Vietnam,
Iraq and Afghanistan, politicians on both sides of the political
spectrum are pushing not to send troops abroad to risk their lives[[.
But the option of using LAWS and sidestepping the costs to a country's
soldiers threatens to reverse this anti-war trend and provide militaries
with a politically palatable way of fighting wars]{.underline}]{.mark}.
There could be catastrophic consequences if we liberate militaries from
political constraints preventing them from going to war. The first wave
of the proliferation of LAWS may simply look like the natural
progression of our current drone capabilities. For instance, Russia may
have already used autonomous drones to attack targets in Syria, but
these weapons are only different from current semi-autonomous drones in
the greater degree of risk assumed by eliminating human intervention. In
other instances, however, the use of LAWS will present substantial
advantages that make them different in kind from drones as we know them.
Consider, for example, Azerbaijan's use of Israeli-supplied IAI Harop
drones in the war with Armenia in 2020. The loitering munition system
used by the military allowed tiny and hardly-detectable autonomous
drones to circle over the enemy's defense line, pick out targets and
attack them, an ability that proved decisive in Azerbaijan's victory in
the war. To understand what a world with LAWS will look like in the long
term requires a bit of imagination. Perhaps a post-withdrawal
Afghanistan will involve weapons like the Harop drones constantly
roaming the skies and diving into the ground to take out targets. Or
maybe we will see the chilling predictions of science fiction come true.
In their book AI 2041, writers Chen Qiufan and Kai-Fu Lee express their
fear that [[LAWS will fall into the hands of armed groups and
terrorist]{.underline}]{.mark}s. They describe a "Unabomber-like
scenario in which a terrorist carries out the targeted killing of
business elites and high-profile individuals," using autonomous drones
that rely on facial recognition to identify their targets. Leading
expert in artificial intelligence Toby Walsh warns of [[these weapons
falling into the hands of dictators and being used as tools of ethnic
cleansing.]{.underline}]{.mark} Even if we assume that LAWS are operated
primarily by legitimate militaries, additional complications arise when
we consider what happens in the case of unjust killings. Philosopher
Robert Sparrow argues that the autonomy of LAWS makes it impossible to
hold anyone accountable for illegitimate killings they commit. If the
robot acted autonomously, tracing accountability back to another agent
seems morally objectionable and legally infeasible. But it would also be
unjust to not punish illegitimate killings. This dilemma presents a
so-called 'responsibility gap', where no one can be held responsible for
illegitimate killings, and wrongful acts of war go undeterred.

#### And a ban is key to set international norms. Even if terror groups can access the weapons, they won't use them. Its try or die for the affirmative

Jacob **Ware** (master's degree in security studies from Georgetown
University and an MA (Hons) in international relations and modern
history from the University of St Andrews) "TERRORIST GROUPS, ARTIFICIAL
INTELLIGENCE, AND KILLER DRONES" September 24, **2019**
https://warontherocks.com/2019/09/terrorist-groups-artificial-intelligence-and-killer-drones/

Secondly, the international community could look to ban AI use in the
military through an international treaty sanctioned by the United
Nations. This has been the strategy pursued by activist groups such as
the Campaign to Stop Killer Robots, while leading artificial
intelligence researchers and scientific commentators have published open
letters warning of the risk of weaponized AI. That said, great powers
are not likely to refrain from AI weapons development, and a ban might
outlaw positive uses of militarized AI. [[The international
community]{.mark} [could]{.mark}]{.underline} also look to [[stigmatize,
or delegitimize, weaponized AI and lethal autonomous weapons
sufficiently to deter terrorist use.]{.underline}]{.mark} Although
modern terrorist groups have proven extremely willing to improvise and
innovate, and effective at doing so, [[there is an extensive list of
weapons --- chemical weapons, biological weapons, cluster munitions,
barrel bombs, and more --- accessible to terrorist organizations, but
rarely used.]{.underline}]{.mark} [[This is partly down to the
international stigma associated with those munitions --- **if a norm is
strong enough, terrorists might avoid using a
weapon**]{.mark}**.**]{.underline} However, norms take a long time to
develop, and are fragile and untrustworthy solutions. Evidently, good
counter-terrorism options are limited. The U.S. government and its
intelligence agencies should continue to treat AI and lethal autonomous
weapons as priorities, and identify new possible counter-terrorism
measures. Fortunately, some progress has been made: Nicholas Rasmussen,
former director of the National Counterterrorism Center, admitted at a
Senate Homeland Security and Governmental Affairs Committee hearing in
September 2017 that "there is a community of experts that has emerged
inside the federal government that is focused on this pretty much full
time. Two years ago this was not a concern ... We are trying to up our
game." Nonstate actors are already deploying drones to attack their
enemies. Lethal autonomous weapon systems are likely to proliferate to
terrorist groups, with potentially devastating consequences. [[The
United States and its allies should urgently address the rising threat
by preparing stronger defenses against possible drone and swarm attacks,
engaging with the defense industry and AI experts warning of the threat,
and supporting realistic international efforts to ban or stigmatize
military applications of artificial intelligence]{.underline}]{.mark}.
Although the likelihood of such an event is low, [[a killer robot attack
could cause massive casualties, strike a devastating blow to the U.S.
homeland, and cause widespread panic. **The threat is imminent, and the
time has come to act.**]{.underline}]{.mark}

**Terrorism causes nuclear war and turns all impacts**

**Arguello and Buis 2018.** Irma Arguello and Emiliano J. Buis, 2018,
Arguello is founder and chair of the NPSGlobal Foundation, and head of
the secretariat of the Latin American and Caribbean Leadership Network.
She holds a degree in physics, a Master's in business administration,
and completed graduate studies in defense and security. Arguello
previously worked on nuclear projects for the Argentine National Atomic
Energy Commission. She is a member of the Steering Committee of the
Fissile Materials Working Group, and a Chatham House Associate Fellow.
Since 2010, she has participated in all the official non-governmental
events at the Nuclear Security Summits. Buis is a lawyer specializing in
international law. He holds a PhD from the University of Buenos Aires
(UBA), a Master's in Human and Social Sciences from the University of
Paris/Panthéon-Sorbonne, and a postgraduate diploma in national defense
from the National Defense School. Currently he is a professor in
international law at UBA, and co-director of the UNICEN Center for Human
Rights in Azul. He is also a researcher and professor at the NPSGlobal
Foundation. "The global impacts of a terrorist nuclear attack: What
would happen? What should we do?" Bulletin of Atomic Scientists.
\[https://www.tandfonline.com/doi/full/10.1080/00963402.2018.1436812?scroll=top&needAccess=true\]/mnw

[Though hard to accept, the detonation of a nuclear device -- by states
or non-state actors -- is today a plausible scenario.]{.underline} And
while much of the world's focus has been on the current nuclear weapons
arsenals possessed by states -- about 14,550 warheads, all of which
carry the risk of intentional or unintentional use -- **[[the threat of
nuclear terrorism is here and increasing.]{.underline}]{.mark}** [For
more than a decade, Al Qaeda, Aum Shinrikyo, and other terrorist groups
have expressed their desire to acquire fissile material to build and
detonate an improvised nuclear bomb. None of them could fulfill that
goal -- so far. But that does not mean that they will not succeed in the
future.]{.underline} Making matters worse, [[there is evidence of an
illicit market for nuclear]{.mark} weapons-usable
[materials]{.mark}.]{.underline} There are sellers in search of
potential buyers, as shown by the dismantlement of a nuclear smuggling
network in Moldova in 2015. [There certainly are plenty of sites from
which to obtain nuclear material.]{.underline} According to the 2016
Nuclear Security Index by the Nuclear Threat Initiative, [24 countries
still host inventories of nuclear weapons-usable materials, stored in
facilities with different degrees of security]{.underline}. And in terms
of risk, it is not necessary for a given country to possess nuclear
weapons, weapons-usable materials, or nuclear facilities for it to be
useful to nuclear terrorists: Structural and institutional weaknesses in
a country may make it favorable for the illicit trade of materials.
Permeable boundaries, high levels of corruption, weaknesses in judicial
systems, and consequent impunity may give rise to a series of
transactions and other events, which could end in a nuclear attack. [The
truth is that, at this stage, [no country in possession of nuclear
weapons or weapons-usable materials can guarantee their full protection
against nuclear terrorism or nuclear smuggling.]{.mark}]{.underline}
Because we live in a world of growing insecurity, where explicit and
tacit agreements between the relevant powers -- which upheld global
stability during the postCold War -- are giving way to increasing
mistrust and hostility, a question arises: How would our lives be
affected if a current terrorist group such as the Islamic State (ISIS),
or new terrorist groups in the future, succeed in evolving from today's
Manchester style "low-tech" attacks to a "high-tech" one, involving a
nuclear bomb, detonated in a capital city, anywhere in the world? We
attempted to answer this question in a report developed by a high-level
multidisciplinary expert group convened by the NPSGlobal Foundation for
the Latin American and Caribbean Leadership Network. We found that there
would be multiple harmful effects that would spread promptly around the
globe (Arguello and Buis 2016); a more detailed analysis is below, which
highlights the need for the creation of a comprehensive nuclear security
system. [[The consequences of a terrorist nuclear attack]{.mark} A small
and primitive 1-kiloton fission bomb]{.underline} (with a yield of about
one-fifteenth of the one dropped on Hiroshima, and certainly much less
sophisticated; cf. Figure 1), [detonated in any large capital city of
the developed world, [would cause an unprecedented catastrophic
scenario.]{.mark} An estimate of direct effects in the attack's location
includes a death toll of 7,300-to-23,000 people and 12,600-to-57,000
people injured, depending on the target's geography and population
density.]{.underline} Total physical destruction of the city's
infrastructure, due to the blast (shock wave) and thermal radiation,
would cover a radius of about 500 meters from the point of detonation
(also known as ground zero), while ionizing radiation greater than 5
Sieverts -- compatible with the deadly acute radiation syndrome -- would
expand within an 850-meter radius. From the environmental point of view,
such an area would be unusable for years. In addition, radioactive
fallout would expand in an area of about 300 square kilometers,
depending on meteorological conditions (cf. Figure 2). [But the
consequences would go far beyond the effects in the target country,
however, and promptly propagate worldwide. Global and national security,
economy and finance, international governance and its framework,
national political systems, and the behavior of governments and
individuals would all be put under severe trial.]{.underline} The
severity of the effects at a national level, however, would depend on
the countries' level of development, geopolitical location, and
resilience. Global security and regional/national defense schemes would
be strongly affected. [[An increase in global distrust would spark
rising tensions among countries]{.mark} and blocs, [that could even lead
to]{.mark} the brink of **[nuclear weapons use by
states]{.mark}**]{.underline} (if, for instance, a sponsor country is
identified). [The consequences of such a shocking scenario would include
a decrease in states' self-control, an escalation of present conflicts
and the emergence of new ones, [accompanied by an increase in military
unilateralism and military expenditures.]{.mark} Regarding the economic
and financial impacts, [a severe global economic depression would rise
from the attack,]{.mark} likely lasting for years.]{.underline} Its
duration would be strongly dependent on the course of the crisis. The
main results of such a crisis would include a 2 percent fall of growth
in global Gross Domestic Product, and a 4 percent decline of
international trade in the two years following the attack (cf. Figure
3). In the case of developing and less-developed countries, the economic
impacts would also include a shortage of high-technology products such
as medicines, as well as a fall in foreign direct investment and a
severe decline of international humanitarian aid toward low-income
countries. [We expect an increase of unemployment and poverty in all
countries. Global poverty would raise about 4 percent after the attack,
which implies that at least 30 million more people would be living in
extreme poverty,]{.underline} in addition to the current estimated 767
million. In the area of international relations, we would expect a
breakdown of key doctrines involving politics, security, and relations
among states. [These international tensions could lead to a collapse of
the nuclear order as we know it today, with a consequent setback of
nuclear disarmament and nonproliferation commitments.]{.underline} In
other words, the whole system based on the Nuclear Non- Proliferation
Treaty would be put under severe trial. After the attack, there would be
a reassessment of existing security doctrines, and a deep review of
concepts such as nuclear deterrence, no-firstuse, proportionality, and
negative security assurances. [Finally, the behavior of governments and
individuals would also change radically. Internal chaos fueled by the
media and social networks would threaten governance at all levels, with
greater impact on those countries with weak institutional
frameworks.]{.underline} Social turbulence would emerge in most
countries, with consequent attempts by governments to impose
restrictions on personal freedoms to preserve order -- possibly by
declaring a state of siege or state of emergency -- and legislation
would surely become tougher on human rights. There would also be a
significant increase in social fragmentation -- with a deepening of
antagonistic views, mistrust, and intolerance, both within countries and
towards others -- and a resurgence of large-scale social movements
fostered by ideological interests and easily mobilized through social
media. Prevention, preparedness, response Given the severity of the
impacts, no country in possession of nuclear weapons or weapons-usable
materials can guarantee its full protection against nuclear terrorism or
nuclear smuggling for proliferation purposes. Nor is it realistic to
conceive of full compensation to others in the international community,
if a catastrophic event happens because of any country's acts or
omissions. [Therefore, we consider that prevention is the only
acceptable way forward to preserve global stability.]{.underline}
Consequently, it is essential for countries to make every effort to
prevent nuclear terrorists from fulfilling their goals. It is true that
the "primitivism" of currently active terrorist organizations gives a
certain space to do what is necessary to enhance the current nuclear
security effort concerning prevention and response. [However, [the
perception of the "low likeliness" of a nuclear terrorist attack
neutralizes the required sense of urgency]{.mark} in decision-making.
[Being in fact a "high-risk" scenario]{.mark}, it is imperative that
governments consider this reality when setting priorities and making
decisions about nuclear security.]{.underline}

### 1AC -- Democracy Advantage

**LAWS are a unique threat to human rights and democracy.**

Kyle **Matthews** (Executive Director of the Montreal Institute for
Genocide and Human Rights at Concordia University. Alexandrine Royer is
a Youth Fellow at the Montreal Institute for Genocide and Human Rights
Studies.) "Artificial intelligence has been weaponized in China. That
should be a wake-up call for the world" May 21, **2019**
https://www.cbc.ca/news/opinion/ai-china-1.5140612

[[Beyond its use by repressive regimes, AI can directly interfere with
human rights]{.mark} in democratic and open societies.]{.underline}
[[The infinite collection of]{.mark} personal [data]{.mark}]{.underline}
by AI systems for micro-ad targeting [[limits the rights of
privacy]{.mark}.]{.underline} AI-enabled online content monitoring
[[impedes freedom]{.mark} of expression and opinion]{.underline}, as
access to and the sharing of information by users is controlled in
opaque and inscrutable ways. Vast AI-powered disinformation campaigns
--- from troll bots to deepfakes (altered video clips) --- [[threaten
societies\' access to accurate information, can disrupt elections and
erode social cohesion]{.underline}]{.mark}. An equally frightening
scenario is the use of AI in conflict situations. Human Rights Watch has
warned that [[AI could be used in the future to target]{.mark} certain
[populations in war zones through deploying lethal autonomous weapon
systems]{.mark}, commonly known as killer robots]{.underline}.

**Continued democratic decline causes great power conflict**

Larry **Diamond**, 20**19**, writing democracy good cards since '88,
also has a PhD in Sociology from Stanford, and Senior Fellow at the
Hoover Institution. "Ill Winds: Saving Democracy from Russian Rage,
Chinese Ambition, and American Complacency" Accessed 2-19-2020.
\[https://www.amazon.com/Ill-Winds-Democracy-Ambition-Complacency-ebook/dp/B07HLR7R7F\]/mnw

In such a near future, my fellow experts would no longer talk of
"democratic erosion." We would be spiraling downward into a time of
democratic despair, recalling Daniel Patrick Moynihan's grim observation
from the 1970s that liberal democracy "is where the world was, not where
it is going." 5 The world pulled out of that downward spiral---but it
took new, more purposeful American leadership. [The planet was not so
lucky [in the 1930s]{.mark}, when the global [implosion of democracy led
to a **catastrophic world war**]{.mark}, between a rising axis of
emboldened dictatorships and a shaken and economically depressed
collection of self-doubting democracies.]{.underline} These are the
stakes. [[Expanding democracy]{.mark}---with its liberal norms and
constitutional commitments---[is a crucial foundation for **world peace
and security**]{.mark}. Knock that away, and our most basic hopes and
assumptions will be imperiled.]{.underline} The problem is not just that
the ground is slipping. [It is that [we are perched on a **global
precipice.**]{.mark}]{.underline} That ledge has been gradually giving
way for a decade. [[If the erosion continues, we may]{.mark} well [reach
a tipping point where democracy goes bankrupt]{.mark}
suddenly---[plunging the world into depths of **oppression and
aggression**]{.mark} that [we have not seen since]{.mark} the end of
**[World War II.]{.mark}**]{.underline} As a political scientist, I know
that our theories and tools are not nearly good enough to tell us just
how close we are getting to that point---until it happens.

**The best analysis proves democratic peace theory**

Dan **Reiter**, 20**17**, Professor at the Department of Political
Science at Emory University. \"Is Democracy a Cause of Peace?\" Oxford
Research Encyclopedia. Accessed 2-19-2020
\[https://oxfordre.com/politics/view/10.1093/acrefore/9780190228637.001.0001/acrefore-9780190228637-e-287\]/mnw

This is not to take a maximalist position that quasi-experiments add
nothing, or that adding variables is never advised. It does suggest,
however, considering other means of assessing causation, in addition to
the conventional approach of seeing if adding plausible exogenous
variables renders the democracy-peace correlation to be statistically
insignificant. [Scholars have explored other means of assessing
causation in the democratic peace, and have amassed three other types of
evidence that support the conclusion that [democracy causes
peace]{.mark}: evidence demonstrating support for other empirical
patterns suggested by democratic peace theory; evidence produced using
experimental methods; and evidence produced using case studies. The
first type of [evidence]{.mark} explores for the existence [of]{.mark}
other [empirical patterns]{.mark} predicted by democratic peace
theory.]{.underline} If a theory predicts the existence of a variety of
empirical patterns and these patterns are demonstrated through tests, we
can be more confident in the validity of the theory, and in turn that
observed correlations are causal and not just spurious. And, indeed,
there is a wide array of quantitative empirical studies that provide
support for various assumptions or implications of democratic peace
theory, especially for institutionalist accounts of the democratic
peace. [Perhaps the central institutionalist explanation of the
democratic peace proposes that [elected leaders are motivated to
avoid]{.mark} fighting [wars]{.mark}, because the [costs]{.mark} of wars
will [incite]{.mark} popular [discontent]{.mark} in turn threatening
their hold on power. Studies have demonstrated a number of empirical
patterns consistent with this view. **Democracies fight shorter wars**
(Reiter & Stam, 2002, ch. 7). **Democracies suffer fewer casualties when
they fight wars** (Valentino et al., 2010), and when they fight,
**popular [support]{.mark} for the leadership [declines as casualties
escalate]{.mark}** (Mueller, 1973).]{.underline} The benefits of
victorious wars may sometimes push democratic publics to accept the
costs of war when they are confident of victory, and accordingly
democracies almost never start wars they go on to lose (Reiter & Stam,
2002). During war, public support erodes as the perceived likelihood of
victory declines (Gelpi et al., 2009). As the institutional explanation
of the democratic peace would predict, variations of institutional and
leadership form within democracies also affects conflict behavior, as in
general more constrained states are less conflict prone (Reiter &
Tillman, 2002). Consistent with the audience costs explanation,
democracies can more effectively signal their resolve than at least some
kinds of autocratic states (Schultz, 2001; Weeks, 2014). There are also
some studies supporting elements of the normative explanation. [For
example, some studies have found that democracies are especially likely
to use mediation or binding arbitration to resolve interstate
disputes]{.underline} (Dixon, 1993; Raymond, 1994, 1996). In total,
though there are certainly scholarly debates about some of these
observed patterns,6 this collection of studies improves our confidence
that democracy is causing peace in the manners described by democratic
peace theory. [The second type of evidence uses [experimental
methods]{.mark}. Some [have]{.mark} proposed that **experimental methods
enjoy critical [advantages over]{.mark} the analysis of [observational
data]{.mark} in assessing causation**]{.underline}. **[Experimental
methods are able largely to skirt some of the biggest causal inference
problems associated with quasi-experimental methods, [such as biased
samples and nonrandom assignments]{.mark} of treatment]{.underline}**.
That said, the limitation of experimental methods is that, especially in
international relations, they can only be used to test some arguments,
or some components of arguments. For example, regarding the democratic
peace an experimenter cannot take a set of states and then randomly
assign some to be democratic and others to be non-democratic. That said,
[scholars]{.underline} have thus far been able to conduct [survey and
laboratory experiments that have tested some elements of the democratic
peace]{.underline}. [A number of surveys have found support for one of
the core assertions of dyadic democratic peace theory: that **[citizens
of democracies are]{.mark} significantly [less likely to support]{.mark}
the [use of force]{.mark} against democracies as compared to using force
against non-democracies**]{.underline} (Geva et al., 1993; Johns &
Davies 2012; Lacina & Lee, 2013; Mintz & Nehemia, 1993; Rousseau, 2005,
pp. 219--232; Tomz & Weeks, 2013) Other experiments have tested elements
of the audience costs variant of the democratic peace, showing that the
public does inflict audience costs on leaders who back down in a crisis
(Horowitz & Levendusky, 2012; Tomz, 2007; Trager & Vavreck, 2011). [A
[third]{.mark} empirical means of demonstrating causation [is]{.mark} to
engage in process tracing through [case studies]{.mark}. Scholars have
presented several individual case studies of the democratic peace in
[events such as]{.mark} 19th-century American [diplomatic
crises]{.mark}, the 1898 Fashoda Crisis, the onset of [World War
II]{.mark}, the Spanish-American War, [and many
others]{.mark}]{.underline} (see Elman, 1997; Owen, 1997; Ray, 1995;
Risse-Kappen, 1995; Rousseau, 2005; Schultz, 2001; for case studies
presenting evidence against the democratic peace, see Layne, 1994).
[Some of these case studies [demonstrate]{.mark} specific parts of the
causal logic of the [democratic peace, such as]{.mark} the
[ability]{.mark} of democracies [to signal more effectively]{.mark}
[through]{.mark} invoking greater [audience]{.mark} [costs]{.mark}
(Schultz, 2001), [or]{.mark} the [inability]{.mark} of elected leaders
[to manipulate]{.mark} public [opinion or secretly drag]{.mark} their
[nations into wars]{.mark} the public would otherwise avoid (Reiter,
2012a). Perhaps the most striking case study of democratic peace
dynamics is the pacification of Western Europe after World War II,
democracy helping to dissolve immediately and completely one of the most
violent interstate conflicts in modern history, the France-Germany
rivalry (Russett & Oneal, 2001).]{.underline}

### 1AC -- War Escalation Advantage

#### Failure to ban lethal autonomous weapons locks us into a cycle of ever-increasing conflict. Every new interaction could lead to a dramatic escalation

Matthew **Anzarouth** (Harvard Political Review) "Robots that Kill: The
Case for Banning Lethal Autonomous Weapon Systems" December 02, **2021**
https://harvardpolitics.com/robots-that-kill-the-case-for-banning-lethal-autonomous-weapon-systems/

Preventing the Next Arms Race. Despite these grave concerns, countries
are pushing ahead in the research and development of LAWS. [[With large
military powers leading the race, there are two potential
outcomes]{.underline}]{.mark} if this trend goes uninterrupted. [[One is
that LAWS become tools with which powerful militaries destabilize other
regions, starting a new chapter of the 'forever
wars']{.underline}]{.mark} without boots on the ground. The second
potential outcome is that LAWS become front and centre in conflict
between the large military powers leading the race. [[They may drag us
into a new war between superpowers **without the mutually assured
destruction that prevents nuclear warfare** since LAWS can engage in a
series of smaller, yet still extremely impactful, attacks that will not
be deterred by the threat of retaliation]{.underline}]{.mark}. The
movement against LAWS is small, but it is growing. More and [[more
countries have expressed concern about the destabilizing
effects]{.underline}]{.mark} of these weapons [[and stressed the need
for a collective agreement to rule them out]{.underline}]{.mark}, much
like existing treaties that limit chemical, biological and
intermediate-range nuclear weapons. However, military powers like the
U.S. and Russia have blocked regulations on LAWS at the Convention on
Conventional Weapons and are quietly leading what some are calling the
third revolution in warfare. The challenge in regulating or banning
LAWS, as with many forms of international cooperation, is overcoming
collective action problems. The development of LAWS seems like a
textbook example of a "security dilemma," wherein one country perceives
heightened security measures by another as a threat and decides to adopt
similar measures in response. Together, these factors increase the risk
of escalation to an outcome neither party desires. [[Our best hope in
confronting this dilemma is to foster discussions **in international
negotiations**]{.underline}]{.mark} that expose to military superpowers
the great risks that LAWS present. While many countries may fear falling
behind if they make the first move to disarm and de-escalate, it is
possible that [[when the stakes are]{.underline}]{.mark} sufficiently
[[high]{.underline}]{.mark} and it is clear that nobody, including
dominant powers, is immune to the dangers of LAWS,
[[we]{.underline}]{.mark} may [[see]{.underline}]{.mark} sufficient
[[international will]{.underline}]{.mark} to address them. While LAWS
still appear to be in their infancy, [[we are running out of time to
prevent their uncontrolled proliferation]{.underline}]{.mark}. Once one
country uses these weapons to significantly tilt the playing field in
its favor, others may have no choice but to follow suit. [[It is
therefore imperative that we switch off the robots before they take over
the battlefield and the horrors of science fiction become
reality]{.underline}]{.mark}.

#### Autonomous weapons will inevitably become weapons of mass destruction. This makes global war inevitable. 

Stuart **Russell** (professor of computer science at the University of
California, Berkeley, and coauthor of the standard textbook \"Artificial
Intelligence: A Modern Approach.\") "Lethal Autonomous Weapons Exist;
They Must Be Banned" June 16, **2021**
https://spectrum.ieee.org/lethal-autonomous-weapons-exist-they-must-be-banned

We produced "Slaughterbots\" to educate the public and policymakers
alike about the potential imminent dangers of small, cheap, and
ubiquitous lethal autonomous weapons systems. Beyond the moral issue of
handing over decisions over life and death to algorithms, the video
pointed out that [[autonomous weapons will, inevitably, turn into
weapons of mass destruction, precisely because they require no human
supervision and can therefore be deployed in vast
numbers.]{.underline}]{.mark} (A related point, concerning the tactical
agility of such weapons platforms, was made in Spectrum last month in an
article by Natasha Bajema.) Furthermore, like small arms, [[autonomous
weaponized drones will proliferate easily on the international arms
market]{.underline}]{.mark}. As the "Slaughterbots\" video\'s epilogue
explained, all the component technologies were already available, and we
expected militaries to start deploying such weapons very soon. That
prediction was essentially correct. The past few years have seen a
series of media reports about military testing of ever-larger drone
swarms and battlefield use of weapons with increasingly autonomous
functions. In 2019, then-Secretary of Defense Mark Esper, at a meeting
of the National Security Commission on Artificial Intelligence,
remarked, "As we speak, the Chinese government is already exporting some
of its most advanced military aerial drones to the Middle East.

#### A ban now is key -- continued development of lethal autonomous weapons could be catastrophic

Michael T. **Klare** (professor emeritus of peace and world security
studies at Hampshire College and senior visiting fellow at the Arms
Control Association) "Autonomous Weapons Systems and the Laws of War"
March **2019**
https://www.armscontrol.org/act/2019-03/features/autonomous-weapons-systems-laws-war

Assessing the Risks Given the likelihood that China, Russia, the United
States, and [[other nations will deploy increasingly autonomous robotic
weapons]{.mark}]{.underline} in the years ahead, policymakers must
identify and weigh the potential [[risks]{.underline}]{.mark} of such
deployments. These [[include not only the potential for accident and
unintended escalation, as would be the case with any new weapons that
are unleashed on the battlefield, but also a wide array of moral,
ethical, and legal concerns arising from the diminishing role of humans
in life-and-death decision-making]{.underline}]{.mark}. The potential
dangers associated with the deployment of AI-empowered robotic weapons
begin with the fact that [[much of the technology involved is new and
untested under the conditions of actual combat]{.underline}]{.mark},
where unpredictable outcomes are the norm. For example, it is one thing
to test self-driving cars under controlled conditions with human
oversight; it is another to let such vehicles loose on busy highways. If
that self-driving vehicle is covered with armor, equipped with a gun,
and released on a modern battlefield, algorithms can never anticipate
all the hazards and mutations of combat, no matter how well "trained"
the algorithms governing the vehicle's actions may be. In war, accidents
and mishaps, some potentially catastrophic, are almost inevitable.
[[Extensive testing]{.underline}]{.mark} of AI image-classification
algorithms has shown that [[such systems can easily be fooled by slight
deviations]{.underline}]{.mark} from standardized representations---in
one experiment, a turtle was repeatedly identified as a rifle9---and are
vulnerable to trickery, or "spoofing," as well as hacking by
adversaries. Former Navy Secretary Richard Danzig, who has studied the
dangers of employing untested technologies on the battlefield, has been
particularly outspoken in cautioning against the premature deployment of
AI-empowered weaponry. "Unfortunately, the uncertainties surrounding the
use and interaction of new military technologies are not subject to
confident calculation or control," he wrote in 2018.10 This danger is
all the more acute because, on the current path, autonomous weapons
systems will be accorded ever-greater authority to make decisions on the
use of lethal force in battle. Although U.S. authorities insist that
human operators will always be involved when life-and-death decisions
are made by armed robots, [[the trajectory of technology is leading to
an ever-diminishing human role in that capacity, heading eventually to a
time when humans are uninvolved entirely]{.underline}]{.mark}. This
could occur as a deliberate decision, such as when a drone is set free
to attack targets fitting a specified appearance ("adult male armed with
gun"), or as a conditional matter, as when drones are commanded to fire
at their discretion if they lose contact with human controllers. A human
operator is somehow involved, by launching the drones on those missions,
but no human is ordering the specific lethal attack. These principles
pose a particular challenge to fully autonomous weapons systems because
they require a capacity to make fine distinctions in the heat of battle.
It may be relatively easy in a large tank-on-tank battle, for example,
to distinguish military from civilian vehicles; [[but in many recent
conflicts, enemy combatants have armed ordinary pickup
trucks]{.underline}]{.mark} and covered them with a tarpaulins, making
them almost indistinguishable from civilian vehicles. [[Perhaps a
hardened veteran could spot the difference, but an intelligent robot?
Unlikely]{.underline}]{.mark}. Similarly, how does one gauge
proportionality when attempting to attack enemy snipers firing from
civilian-occupied tenement buildings? For robots, this could prove an
insurmountable challenge. Advocates and critics of autonomous weaponry
disagree over whether such systems can be equipped with algorithms
sufficiently adept to distinguish between targets to satisfy the laws of
war. "Humans possess the unique capacity to identify with other human
beings and are thus equipped to understand the nuances of unforeseen
behavior in ways that machines, which must be programmed in advance,
simply cannot," analysts from Human Rights Watch (HRW) and the
International Human Rights Clinic of Harvard Law School wrote in 2016.12
Another danger arises from the speed with which automated systems
operate, along with plans for deploying autonomous weapons systems in
coordinated groups, or swarms. The Pentagon envisions a time when large
numbers of drone ships and aircraft are released to search for enemy
missile-launching submarines and other critical assets, including mobile
ballistic missile launchers. At present, U[[.S. adversaries rely on
those missile systems to serve as an invulnerable second-strike
deterrent to a U.S. disarming first strike. Should Russia or China ever
perceive that swarming U.S. drones threaten the survival of their
second-strike systems, those countries could feel pressured to launch
their missiles when such swarms are detected, lest they lose their
missiles to a feared U.S. first strike.]{.underline}]{.mark}

#### That escalates and causes extinction 

**Starr 17** \[Steven Starr is the director of the University of
Missouri\'s Clinical Laboratory Science Program, as well as a senior
scientist at the Physicians for Social Responsibility, 1-9-2017,
\"Turning a Blind Eye Towards Armageddon --- U.S. Leaders Reject Nuclear
Winter Studies,\" FAS,
[https://fas.org/2017/01/turning-a-blind-eye-towards-armageddon-u-s-leaders-reject-nuclear-winter-studies\]](https://fas.org/2017/01/turning-a-blind-eye-towards-armageddon-u-s-leaders-reject-nuclear-winter-studies%5d)

The [detonation]{.underline} of an atomic bomb with this explosive power
[will instantly ignite fires]{.underline} over a surface area of three
to five square miles. In the recent studies, the scientists calculated
that [the blast, fire, and radiation]{.underline} from a war fought with
100 atomic bombs [could produce direct fatalities comparable to all of
those worldwide in World War II,]{.underline} or to those once estimated
for a "counterforce" nuclear war between the superpowers. However, [the
long-term environmental effects of the [war could]{.mark} significantly
[disrupt]{.mark} the **[global weather]{.mark}**]{.underline} for at
least a decade, [which would]{.underline} likely [[result in]{.mark} a
**vast [global famine]{.mark}**]{.underline}. The scientists predicted
that [nuclear]{.underline} **[[firestorms]{.underline}]{.mark}** in the
burning cities [would cause]{.underline} at least five million tons of
**[[black carbon smoke]{.underline}]{.mark}** to quickly rise above
cloud level into the stratosphere, where it could not be rained out. The
smoke would circle the Earth in less than two weeks and would [form a
**global**]{.underline} stratospheric smoke **[layer]{.underline}**
[that would remain for]{.underline} more than [a]{.underline}
**[decade]{.underline}**. The smoke would [[absorb]{.underline}]{.mark}
warming **[[sunlight]{.underline}]{.mark}**, which would heat the smoke
to temperatures near the boiling point of water, [[producing **ozone
losses**]{.underline}]{.mark} of 20 to 50 percent over populated areas.
This would almost double the amount of UV-B [reaching]{.underline} the
most populated regions of the mid-latitudes, and it would create [UV-B
indices **[unprecedented in]{.mark}** ~~human~~ \[humyn}
[history]{.mark}]{.underline}. In North America and Central Europe, the
time required to get a painful sunburn at mid-day in June could decrease
to as little as six minutes for fair-skinned individuals. As the smoke
layer blocked warming sunlight from reaching the Earth's surface, it
[would produce]{.underline} the **[cold]{.underline}**est average
surface **[temperatures]{.underline}** in the last 1,000 years. The
scientists calculated that [global food production would decrease by 20
to 40 percent during a five-year period following such a
war.]{.underline} Medical experts have predicted that [the shortening of
growing seasons and corresponding decreases in agricultural production
could cause up to **two billion** people to **perish** from
famine.]{.underline} The climatologists also investigated the effects of
a nuclear war fought with the vastly more powerful modern thermonuclear
weapons possessed by the United States, Russia, China, France, and
England. Some of the thermonuclear weapons constructed during the 1950s
and 1960s were 1,000 times more powerful than an atomic bomb. During the
last 30 years, the average size of thermonuclear or "strategic" nuclear
weapons has decreased. Yet today, each of the approximately 3,540
strategic weapons deployed by the United States and Russia is seven to
80 times more powerful than the atomic bombs modeled in the
India-Pakistan study. The smallest strategic nuclear weapon has an
explosive power of 100,000 tons of TNT, compared to an atomic bomb with
an average explosive power of 15,000 tons of TNT. Strategic nuclear
weapons produce much larger nuclear firestorms than do atomic bombs. For
example, a standard Russian 800-kiloton warhead, on an average day, will
ignite fires covering a surface area of 90 to 152 square miles. A war
fought with hundreds or thousands of **[U.S. and Russian]{.underline}**
strategic nuclear [weapons]{.underline} would ignite immense nuclear
firestorms covering land surface areas of many thousands or tens of
thousands of square miles. The scientists calculated that these fires
would produce up to 180 million tons of black carbon soot and smoke,
which would form a dense, global stratospheric smoke layer. The smoke
would remain in the stratosphere for 10 to 20 years, and it would block
as much as 70 percent of sunlight from reaching the surface of the
Northern Hemisphere and 35 percent from the Southern Hemisphere. So much
sunlight would be blocked by the smoke that the noonday sun would
resemble a full moon at midnight. Under such conditions, it
[would]{.underline} only [require a matter of days or weeks for daily
minimum [temperatures]{.mark} to [fall **below freezing in**]{.mark}
**the largest [agricultural areas]{.mark}**]{.underline} of the Northern
Hemisphere, where freezing temperatures would occur every day for a
period of between one to more than two years. Average surface
temperatures would become colder than those experienced 18,000 years ago
at the height of the last Ice Age, and the prolonged cold would cause
average rainfall to decrease by up to 90%. [Growing seasons would be
completely eliminated for more than a decade; it would be too cold and
dark to grow food crops, [which would **doom**]{.mark} **the majority of
[the]{.mark} ~~human~~ [\[humyn\] population]{.mark}**.]{.underline}
NUCLEAR WINTER IN BRIEF The profound cold and darkness following nuclear
war became known as nuclear winter and was first predicted in 1983 by a
group of NASA scientists led by Carl Sagan. During the mid-1980s, a
large body of research was done by such groups as the Scientific
Committee on Problems of the Environment (SCOPE), the World
Meteorological Organization, and the U.S. National Research Council of
the U.S. National Academy of Sciences; their work essentially supported
the initial findings of the 1983 studies. The idea of nuclear winter,
published and supported by prominent scientists, generated extensive
public alarm and put political pressure on the United States and Soviet
Union to reverse a runaway nuclear arms race, which, by 1986, had
created a global nuclear arsenal of more than 65,000 nuclear weapons.
Unfortunately, this created a backlash among many powerful military and
industrial interests, who undertook an extensive media campaign to brand
nuclear winter as "bad science" and the scientists who discovered it as
"irresponsible." Critics used various uncertainties in the studies and
the first climate models (which are primitive by today's standards) as a
basis to criticize and reject the concept of nuclear winter. In 1986,
the Council on Foreign Relations published an article by scientists from
the National Center for Atmospheric Research, who predicted drops in
global cooling about half as large as those first predicted by the 1983
studies and described this as a "nuclear autumn." The nuclear autumn
studies were later shown to be deeply flawed, but the proof came too
late to stop a massive smear campaign that effectively discredited the
initial studies. Nuclear winter was subject to criticism and damning
articles in the Wall Street Journal and Time magazine. In 1987, the
National Review called nuclear winter a "fraud." In 2000, Discover
Magazine published an article that described nuclear winter as one of
"The Twenty Greatest Scientific Blunders in History." The endless smear
campaign was successful; the general public, and even most anti-nuclear
activists, were left with the idea that nuclear winter had been
scientifically disproved. REJECTION BY LEADERS Yet the scientists did
not give up. In 2006, they returned to their labs to perform the
research I have previously described. Their new research not only upheld
the previous findings but also found that the earlier studies actually
underestimated the environmental effects of nuclear war. Dr. Robock of
Rutgers and Dr. Toon of the University of Colorado have spent years
attempting to bring official attention to their work and get follow-up
research studies done by appropriate agencies in the federal government.
In a recent (2016) interview, Dr. Toon stated: The Department of Energy
and the Department of Defense, which should be investigating this
problem, have done absolutely nothing. They have not published a single
paper, in the open literature, analyzing this problem ... We have made a
list of where we think the important issues are, and we have gone to
every \[federal\] agency we can think of with these lists, and said
"Don't you think someone should study this?" Basically, everyone we have
tried so far has said, "Well that's not my job." In the same interview,
Dr. Robock also noted: The Department of Homeland Security really should
fund this. They will fund you to study one terrorist bomb in New York
City. When you explain to them that a war between India and Pakistan is
a much greater threat to the U.S. homeland than one terrorist bomb, as
horrible as that is, they respond with "Oh, well that's not my job, go
talk to some other program manager" --- who, of course, doesn't exist.
After the more recent series of studies were published in 2007 and 2008,
Drs. Robock and Toon also made a number of requests to meet with members
of the Obama administration. The scientists offered to brief Cabinet
members and the White House staff about their findings, which they
assumed would have a great impact upon nuclear weapons policy. Their
offers were met with indifference. Finally, after several years of
trying, Drs. Robock and Toon were allowed an audience with John Holdren,
Senior Advisor to President Barack Obama on Science and Technology. Dr.
Robock also eventually met with Rose Gottemoeller, then Under Secretary
of State for Arms Control and International Security. Dr. Robock has
written to me that, after these meetings, he and Dr. Toon were left with
the impression that neither Holdren nor Gottemoeller think the nuclear
winter research "is correct." But it is not only Holdren and
Gottemoeller who reject the nuclear winter research. Greg Mello, of the
Los Alamos Study Group, cites a source who confirms that the group that
determines the "full range of activities related to the development,
production, maintenance (upkeep) and elimination (retirement,
disassembly and disposal) of all United States nuclear weapons --- the
members of the U.S. Nuclear Weapons Council --- have stated that "the
predictions of nuclear winter were disproved years ago." The members of
the U.S. Nuclear Weapons Council include: Under Secretary of Defense for
Acquisition, Technology, and Logistics Vice ~~Chairman~~ \[Chairperson}
of the Joint Chiefs of Staff Under Secretary for Nuclear Security of the
Department of Energy Under Secretary of Defense for Policy Commander of
the United States Strategic Command It is important to understand that
some members of this group --- especially the Commander of the U.S.
Strategic Command (USSTRATCOM) --- also develop the policies that guide
the use of nuclear weapons. Perhaps General John Hyten, Head of
USSTRATCOM, who is in charge of the U.S. nuclear triad, and General Paul
Selva, Vice ~~Chairman~~ \[Chairperson\] of the Joint Chiefs of Staff,
the second highest ranking officer in the United States, have never seen
or heard of the 21st century nuclear winter studies. Perhaps when they
hear a question about "nuclear winter," they only remember the smear
campaigns done against the early studies. Or, maybe, they just choose
not to accept the new scientific research on nuclear winter, despite the
fact that it has withstood the criticism of the global scientific
community. Regardless, the rejection of nuclear winter research by the
top leaders of the United States raises some profoundly important
questions: Do U.S. military and political leaders fully understand the
consequences of nuclear war? Do they realize that even a "successful"
nuclear first-strike against Russia could cause most Americans to die
from nuclear famine? In 2010, Drs. Toon and Robock wrote in Physics
Today: We estimate that the direct effects of using the 2012 arsenals
would lead to hundreds of millions of fatalities. The indirect effects
would likely eliminate the majority of the ~~human~~ \[humyn\]
population. In 2013, Drs. Toon and Robock wrote in the Bulletin of
Atomic Scientists that: A nuclear war between Russia and the United
States, even after the arsenal reductions planned under New START, could
produce a nuclear winter. Hence, an attack by either side could be
suicidal, resulting in Self-Assured Destruction. RENEWED COLD WAR
Although president-elect Trump appears to favor a return to the policy
of détente with Russia, many if not most U.S. political leaders appear
to support the Obama administration's policies of [direct confrontation
with Putin's Russia]{.underline}. Mainstream corporate media, including
the editorial boards of The New York Times and The Washington Post,
routinely engage in anti-Russian and anti-Putin rhetoric that surpasses
the hate speech of the McCarthy era. Under President Obama, the United
States has [renewed the **Cold War**]{.underline} with Russia, with
little or no debate or protest, [and]{.underline} has subsequently
[engaged in **proxy wars**]{.underline} [with Russia in **Ukraine and
Syria,**]{.underline} [as well as threatening **military action against
China**]{.underline} [in the **South China Sea**]{.underline}. In
response to what NATO leaders describe as Russia's "dangerous and
aggressive actions," [NATO has built up a **"rapid-response
force"**]{.underline} of 40,000 troops [on the Russian border in the
Baltic States and Poland.]{.underline} This force includes hundreds of
tanks, armored vehicles, and heavy artillery. NATO troops stationed in
Estonia are [within **artillery range of St. Petersburg**]{.underline},
the second largest city of Russia. The United States has [deployed its
**Aegis**]{.underline} Ashore Ballistic Missile Defense
(**[BMD]{.underline}**) system [in]{.underline}
**[Romania]{.underline}** [and is constructing another]{.underline} such
BMD system [in **Poland**]{.underline}. [[The Mark 41 launch
system]{.underline}]{.mark} used [[in the Aegis Ashore systems
can]{.underline}]{.mark} be used to [[launch]{.underline}]{.mark} a
variety of missiles, including **[[long-range nuclear]{.mark}-armed
cruise [missiles]{.mark}]{.underline}**. [In other words, [the
U]{.mark}nited [S]{.mark}tates has built and [is building **launch
sites**]{.mark} **for nuclear missiles [on the Russian
border]{.mark}**]{.underline}. This fact has been widely reported on
Russian TV and has infuriated the Russian public. In June, Russian
President Putin specifically warned that [Russia would be **forced to
retaliate**]{.underline} against this threat. While Russian officials
maintain that its actions are normal and routine, [Russia]{.underline}
now [appears to be **preparing for war**]{.underline}. On October 5,
2016, Russia [conducted a nation-wide civil defense drill that included
40 million]{.underline} of its people being [directed to fallout
shelters]{.underline}. Reuters reported two days later that
[Russia]{.underline} had [moved its Iskander]{.underline}
nuclear-capable [missiles to **Kaliningrad**]{.underline}, which borders
Poland. While the United States ignores the danger of nuclear war,
Russian scholar Stephen Cohen reports that the danger of war with the
United States is the leading news story in Russia. Cohen states: Just as
there is no discussion of the most existential question of our time, in
the American political class --- the possibility of war with Russia ---
it is the only thing being discussed in the Russian political class . .
. These are two different political universes. In Russia, all the
discussion in the newspapers, and there is plenty of free discussion on
talk show TV, which echoes what the Kremlin is thinking, online, in the
elite newspapers, and in the popular broadcasts, the number 1, 2, 3, and
4 topics of the day are the possibility of war with the United States.
Cohen goes on to say: I conclude from this that [the leadership of
Russia **actually believes**]{.underline} now, [in reaction to what the
United States and NATO have said and done over the last two years, and
particularly in reaction to the breakdown of the proposed cooperation in
Syria, and the rhetoric coming out of Washington, that **war is a real
possibility**]{.underline}. I can't remember when, since the Cuban
Missile Crisis, that the Moscow leadership came to this conclusion in
its collective head. Perhaps this narrative will change under
president-elect [Trump]{.underline}. However, he [is inheriting a
situation **fraught with danger**]{.underline}, [which retains the
possibility of **direct military conflict**]{.underline}
[with]{.underline} [**Russia in Ukraine and Syria**, as well
as]{.underline} increasingly militarized confrontation
[with]{.underline} **[China in the South China Sea]{.underline}**.

### 1AC -- NATO Cohesion Advantage 

#### Disagreements exist but the overwhelming consensus is that LAWS need to be banned. 

**Congressional Research Service** "Defense Primer: U.S. Policy on
Lethal Autonomous Weapon Systems" November 17, **2021**
https://crsreports.congress.gov/product/pdf/IF/IF11150

In addition, [[approximately 30 countries and 165 nongovernmental
organizations have called for a preemptive ban on LAWS due to ethical
concerns, including concerns about operational risk, accountability for
use, and compliance with the proportionality and distinction
requirements of the law of war.]{.underline}]{.mark} The U.S. government
does not currently support a ban on LAWS and has addressed ethical
concerns about the systems in a March 2018 white paper, "Humanitarian
Benefits of Emerging Technologies in the Area of Lethal Autonomous
Weapons." The paper notes that "automated target identification,
tracking, selection, and engagement functions can allow weapons to
strike military objectives more accurately and with less risk of
collateral damage" or civilian casualties.

#### The multilateral framework can create positive norms on LAWS and solve internal struggle

Jay **Ettinger, 20**, (Jay Ettinger, Jay is JD & Legal Intern, UN High
Commissioner for Human Rights, Fall 2020, "Overcoming International
Inertia: The Creation of War Manual for Lethal Autonomous Weapons
Systems," Minnesota Journal Of International Law,
https://minnjil.org/wp-content/uploads/2021/09/Ettinger-MACRO.pdf,
6-27-2022) SCade

\*IHL = International Humanitarian Law

B. [Challenges Facing the Existing Approach to Building a Legal
Framework for LAWS Development and Use]{.underline}. [[With many nations
aggressively pursuing LAWS]{.mark}]{.underline} technology, **[[there is
an urgent need to develop standards to]{.underline}]{.mark}** influence
and regul**[[ate the testing and deployment of this new
tech]{.underline}]{.mark}**nology.162 [[The]{.underline}]{.mark} current
[[UN-focused approach is not progressing quickly
enough]{.underline}]{.mark} [to provide meaningful guidance to
States]{.underline}.163 As stated by one observer, "[[the pace of
diplomacy \[is]{.underline}]{.mark}\] [[falling behind the speed of
technological advancement]{.underline}]{.mark}."164 Historically, [[the
development of IHL]{.mark} [has been heavily dependent on state
practice]{.mark} [and consequently takes a significant amount of time
for custom to ripen]{.mark}]{.underline}.165 Additionally, given [the
high stakes of creating a body of law that grants the use of deadly
force in the name of national security, [the codification of state
practice into multilateral treaties is a highly sensitive]{.mark} and
contested process]{.underline}.166 [The process is also highly
pluralistic, which while valuable for accounting for diverse
interests]{.underline}, can make progress challenging.167 As described
by Michael Schmitt, "\[c\]onfronted with a cacophony of inputs---private
and public, military and civilian, domestic and international---the IHL
lawyer frequently finds clarity and consensus elusive."168

**NATO cooperation and cohesion solves pandemics, bioD loss and climate
change**

Sherri **Goodman and** Katarina **Kertysova, 22**, (Sherri Goodman,
Katarina Kertysova, 2-1-2022, NATO Review, NATO Review,
https://www.nato.int/docu/review/articles/2022/02/01/nato-an-unexpected-driver-of-climate-action/index.html,
6-27-2022) SCade

[NATO's climate security agenda]{.underline}

[[Climate change]{.underline}]{.mark} has long been [[known as a threat
multiplier]{.underline}]{.mark} and is increasingly recognised as a
"shaping threat" that [dramatically alters the environments in which
Allied militaries will have to operate in the coming
decades]{.underline}. From higher frequency and intensity of storms,
through extreme heat and cold, to reduced supplies of drinking water and
faster wear and tear of military equipment, [[climate change has
significant implications for NATO on]{.underline}]{.mark} the
[[tactical, operational and strategic levels]{.underline}]{.mark}. In
addition to **[[climate-related risks to military infrastructure and
force readiness]{.underline}]{.mark}**, more **[[extreme weather
events]{.underline}]{.mark}** can also **[[increase conflict and
migration potential]{.underline}]{.mark}** in and beyond NATO's
immediate neighbourhood. Born of the Cold War and designed to defend its
members against any external aggression, [[NATO is evolving to reflect
the new security reality]{.underline}]{.mark} of actorless threats,
**[[such as pandemics, biodiversity loss and climate
change]{.underline}]{.mark}**. As a security organisation, NATO cannot
be indifferent to these challenges. For NATO to be able to fulfil its
core mission of keeping the Euro-Atlantic space safe, building
resilience to the impacts of a changing climate and integrating
sustainable practices into military planning and capability development
is a necessity, not a choice. Evolving consensus The good news is that
**[[the Alliance is not starting from scratch]{.underline}]{.mark}**.
For over 50 years now, [NATO has been paying attention to environmental
challenges]{.underline}, mostly through a wide range of scientific
research activities. [[NATO]{.underline}]{.mark}
[[has]{.underline}]{.mark} also [[developed six environmental protection
standards]{.underline}]{.mark} (STANAGs) that concern military camps,
management of waste, and sustainability of military training areas.
Climate change was written into the 2010 Strategic Concept and has been
factored into summit declarations since then. In 2014, NATO adopted a
Green Defence Framework and integrated energy efficiency and other
environmental considerations into the design of the current NATO
headquarters, which was completed in 2018. **[[The building blocks for a
more ambitious and visible role with respect to climate security are
already there]{.underline}]{.mark}**. However, [[NATO]{.mark} as an
alliance of 30 countries [works by consensus]{.mark}, which is always
evolving]{.underline}. As a former UN Special Envoy on Climate Change,
Jens Stoltenberg began [[advocating for NATO to take greater
climate-related]{.underline}]{.mark} [[action]{.underline}]{.mark} many
years ago, but [[his efforts]{.underline}]{.mark} may have been
[[stymied during the previous U.S. administration]{.underline}]{.mark}.
The growing number of climate and weather related disasters, which
continue to impact lives and livelihoods both within and outside of
NATO's borders, has marked an evident shift in awareness and acceptance
of climate change as an issue of national security across the Alliance.
[[In view of increasing societal pressure]{.underline}]{.mark} and the
current political momentum, [[which includes the renewed U.S. leadership
on climate change]{.underline}]{.mark}, [[NATO]{.mark}
[is]{.mark}]{.underline} now **[[poised to push a more ambitious climate
agenda]{.underline}]{.mark}**.

**Warming outweighs\-\--its [*irreversible* and exacerbates biodiversity
loss, conflict, disease]{.underline}**

**Torres 16** (Phil Torres; author, Affiliate Scholar @ Institute for
Ethics and Emerging Technologies, founder of the X-Risks Institute,
published articles for Bulletin of the Atomic Scientists, Salon, Journal
of Future Studies, and the Journal of Evolution and Technology;
7-22-2016, \"Op-ed: Climate Change Is the Most Urgent Existential
Risk,\" FLI - Future of Life Institute,
http://futureoflife.org/2016/07/22/climate-change-is-the-most-urgent-existential-risk/,
accessed 8-9-2016)

For example, **[according to the]{.underline}**
**[I]{.underline}**ntergovernmental **[P]{.underline}**anel on
**[C]{.underline}**limate **[C]{.underline}**hange, [the effects of
[climate change will be]{.mark} "severe]{.underline}," "pervasive," [and
["**irreversible**]{.mark}]{.underline}." Or, as [[a 2016
study](http://www.climate.unibe.ch/~stocker/papers/clark16natcc.pdf)
**published in Nature** and authored by over twenty
scientists]{.underline} puts it, [the consequences of climate change
"will extend longer than the entire history of human civilization thus
far."]{.underline} Furthermore, [a recent
article](http://advances.sciencemag.org/content/1/5/e1400253.full?con=&dom=pscau&src=syndication)
in Science Advances [confirms]{.underline} that humanity has already
escorted the biosphere into the sixth mass extinction event in life's
3.8 billion year history on Earth. Yet [another
study](http://www.nature.com/nature/journal/v486/n7401/full/nature11018.html)
suggests that [**we could be approaching [a sudden]{.mark},**
irreversible, **[catastrophic collapse of the global
ecosystem]{.mark}**]{.underline}. If this were to occur, **[it [could
result in]{.mark} "[widespread ]{.mark}]{.underline}**social [**unrest,
[economic instability]{.mark} [and]{.mark}** loss of human
life**."**]{.underline} Given the potential for environmental
degradation to **[[elevate the likelihood of nuclear
war]{.underline}]{.mark}**s**[, nuclear
[terrorism]{.mark},]{.underline}** engineered
**[[pandemics]{.underline}]{.mark}**, a superintelligence takeover, and
perhaps even an [impact
winter](https://en.wikipedia.org/wiki/Impact_winter), [[it ought to take
precedence over all other risk concerns]{.underline}]{.mark} --- at
least [in the near-term]{.underline}. Let's make sure we get our
priorities straight.

## Case Extensions

### Inherency 

#### Action now is key to prevent the most catastrophic impacts 

Coley **Felt** (International Policy Institute Cybersecurity Fellow)
"Autonomous Weaponry: Are Killer Robots in Our Future?" February 14,
**2020**
https://jsis.washington.edu/news/autonomous-weaponry-are-killer-robots-in-our-future/

Furthermore, an open letter calling for "a ban on offensive autonomous
weapons beyond meaningful human control" was signed by Elon Musk,
Stephen Hawking and more than 3,000 AI and robotics experts in 2017
(Etzioni, 2017). The letter highlights that [[the development of this
technology requires no costly or hard-to-obtain raw materials, raising
concern about how easy it is to produce these types of
weapons]{.underline}]{.mark}. Furthermore, researchers emphasize that
[[LAWS are ideal for certain malicious tasks such as assassinations,
reducing populations, destabilizing nations and selectively killing
specific groups]{.underline}]{.mark} (Busby, 2018). In the United
States, tension is high between Silicon Valley and the federal
government around this controversial topic. For example, the United
States' Project Maven aims to invest billions of dollars into artificial
intelligence research and development pertaining to the military. After
Google was originally contracted to work on the project, an employee
protest led to the company pulling out once the contract expired. Many
Google employees are not supportive of fully autonomous weaponry and
refused to be part of its possible development (Fryer-Biggs, 2018).

## Solvency

### Solvency -- General

#### The use of a moratorium creates a period to expand security cooperation- solves for LAWS destruction

**Arkin**[, Ronald C,]{.underline} **et al**[. October
20]{.underline}**19** "A Path towards Reasonable Autonomous Weapons
Regulation." IEEE Spectrum, IEEE Spectrum, 21 Oct. 2019,
spectrum.ieee.org/a-path-towards-reasonable-autonomous-weapons-regulation.
Accessed 28 June 2022. ([Ronald C.
Arkin](http://www.cc.gatech.edu/aimosaic/faculty/arkin/), an IEEE
Fellow, is Regents\' Professor, Director of the Mobile Robot Laboratory,
and Associate Dean for Research in the College of Computing at the
Georgia Institute of Technology. His research focuses on multiagent
robotic systems, and he\'s published numerous articles and book chapters
on human-robot interaction and robot ethics. Cutby:neigh)

Component 1: [[States should consider adopting a]{.mark} five-year,
renewable [moratorium on]{.mark} the development, deployment, transfer,
and use of anti-personnel [lethal autonomous weapon systems.]{.mark}
Anti-personnel lethal autonomous weapon systems are defined as weapons
systems that, once activated, can select and engage dismounted human
targets without further intervention by a human operator]{.underline},
possibly excluding systems such as: Fixed-point defensive systems with
human supervisory control to defend human-occupied bases or
installations Limited, proportional, automated counter-fire systems that
return fire in order to provide immediate, local defense of humans
Time-limited pursuit deterrent munitions or systems Autonomous weapon
systems with size above a specified explosive weight limit that select
as targets hand-held weapons, such as rifles, machine guns, anti-tank
weapons, or man-portable air defense systems, provided there is adequate
protection for non-combatants and ensuring IHL
compliance[5](https://spectrum.ieee.org/a-path-towards-reasonable-autonomous-weapons-regulation#footnote-5)
The moratorium would not apply to: Anti-vehicle or anti-materiel weapons
Non-lethal anti-personnel weapons Research on ways of improving
autonomous weapon technology to reduce non-combatant harm in future
anti-personnel lethal autonomous weapon systems Weapons that find,
track, and engage specific individuals whom a human has decided should
be engaged within a limited predetermined period of time and geographic
region Motivation: [[This moratorium would pause development and
deployment of anti-personnel lethal autonomous weapons systems to allow
states to better understand the systemic risks of their use and to
perform research that improves their safety, understandability, and
effectiveness.]{.mark}]{.underline}

#### No autonomous weapons now -- they are not inevitable and it is not too late to stop them

Michael T. **Klare** (professor emeritus of peace and world security
studies at Hampshire College and senior visiting fellow at the Arms
Control Association) "Autonomous Weapons Systems and the Laws of War"
March **2019**
https://www.armscontrol.org/act/2019-03/features/autonomous-weapons-systems-laws-war

Autonomous weapons systems are lethal devices that have been empowered
by their human creators to survey their surroundings, identify potential
enemy targets, and independently choose to attack those targets on the
basis of sophisticated algorithms. Such systems require the integration
of several core elements: a mobile combat platform, such as a drone
aircraft, ship, or ground vehicle; sensors of various types to
scrutinize the platform's surroundings; processing systems to classify
objects discovered by the sensors; and algorithms directing the platform
to initiate attack when an allowable target is detected. The U.S.
Department of Defense describes an autonomous weapons system as a
"weapons system that, once activated, can select and engage targets
without further intervention by a human operator."2 [[Few weapons in
active service presently exhibit all of these
characteristics]{.underline}]{.mark}. Many militaries employ close-in
naval defense weapons such as the U.S. Phalanx gun system that can fire
autonomously when a ship is under attack by enemy planes or missiles.
Yet, such systems cannot independently search for and strike enemy
assets on their own, and human operators are always present to assume
control if needed.3 Many air-to-air and air-to-ground missiles are able
to attack human-selected targets, such as planes or tanks, but cannot
hover or loiter to identify potential threats. One of the few systems to
possess this capability is Israel's Harpy airborne anti-radiation drone,
which can loiter for several hours over a certain area to search for and
destroy enemy radars.4 Autonomy, then, is a matter of degree, with
machines receiving ever-increasing capacity to assess their surroundings
and decide what to strike and when. As described by the U.S.
Congressional Research Service, autonomy is "the level of independence
that humans grant a system to execute a given task." Autonomy "refers to
a spectrum of automation in which independent decision-making can be
tailored for a specific mission." Put differently, autonomy refers to
the degree to which humans are taken "out of the loop" of
decision-making, with AI-empowered machines assuming ever-greater
responsibility for critical combat decisions. [[This emphasis on the
"spectrum of automation" is important because, for the most part,
nations have **yet to deploy fully autonomous weapon systems on the
battlefield**]{.underline}]{.mark}. Under prevailing U.S. policy, as
enshrined in a November 2012 Defense Department directive, "autonomous
and semi-autonomous weapons systems shall be designed to allow
commanders and operators to exercise appropriate levels of human
judgment over the use of force." Yet, this country, like others,
evidently is developing and testing weapons that would allow for
ever-diminishing degrees of human control over their future use.

**\**

### Solvency -- NATO Key/Says Yes

#### Banning lethal autonomous weapons systems should be your moral and ethical imperative -- multilateral action key to effectiveness

Mary **Wareham** (advocacy director in the arms division at Human Rights
Watch) "Summary: Stopping Killer Robots" August 10, **2020**
https://www.hrw.org/report/2020/08/10/stopping-killer-robots/country-positions-banning-fully-autonomous-weapons-and

Weapons systems that select and engage targets without meaningful human
control are unacceptable and need to be prevented. [[All countries have
a duty to protect humanity]{.underline}]{.mark} from this dangerous
development [[by banning fully autonomous weapons**. Retaining
meaningful human control over the use of force is an ethical imperative,
a legal necessity, and a moral obligation**]{.underline}]{.mark}. In the
period since Human Rights Watch and other nongovernmental organizations
launched the Campaign to Stop Killer Robots in 2013, the question of how
to respond to concerns over fully autonomous weapons has steadily
climbed the international agenda.\[1\] The challenge of [[killer
robots]{.underline}]{.mark}, like climate change**[[, is widely regarded
as a grave threat to humanity that deserves urgent multilateral
action.]{.underline}]{.mark}**\[2\] [[A growing number of
legislators]{.underline}]{.mark}, policymakers, private companies,
international and domestic organizations, and ordinary individuals have
[[endorsed the]{.underline}]{.mark} call to [[ban]{.underline}]{.mark}
fully autonomous weapons.\[3\] Since 2018, the United Nations
Secretary-General António Guterres has repeatedly urged states to
prohibit weapons systems that could, by themselves, target and attack
human beings, calling them "morally repugnant and politically
unacceptable."

### Solvency -- Ban Works

#### Now is key -- LAWS are coming in mass now unless a ban is put into place immediately 

Michael T. **Klare** (professor emeritus of peace and world security
studies at Hampshire College and senior visiting fellow at the Arms
Control Association) "Autonomous Weapons Systems and the Laws of War"
March **2019**
https://www.armscontrol.org/act/2019-03/features/autonomous-weapons-systems-laws-war

Yet another approach gaining attention is a concentrated focus on the
ethical dimensions of fielding fully autonomous weapons systems. This
outlook holds that [[**international law and common standards of ethical
practice ordain that only humans possess the moral capacity to justify
taking another human's life** and that machines can never be vested with
that power]{.underline}]{.mark}. Proponents of this approach point to
the Martens clause of the Hague Convention of 1899, also inscribed in
Additional Protocol I of the Geneva Conventions, stating that even when
not covered by other laws and treaties, civilians and combatants "remain
under the protection and authority of the principles of international
law derived from established custom, from the principles of humanity and
from the dictates of human conscience." Opponents of fully autonomous
weapons systems claim that such weapons, [[by removing humans from
life-and-death decision-making, are inherently contradicting principles
of humanity and dictates of human conscience and so should be
banned]{.underline}]{.mark}. Reflecting awareness of this issue, the
Defense Department has reportedly begun to develop a set of guiding
principles for the "safe, ethical, and responsible use" of AI and
autonomous weapons systems by the military services. Today, [[very few
truly autonomous robotic weapons are in active combat use, but many
countries are developing and testing a wide range of machines possessing
high degrees of autonomy]{.underline}]{.mark}. Nations are determined to
field these weapons quickly, lest their competitors outpace them in an
arms race in autonomy. Diplomats and [[policymakers must seize this
moment before fully autonomous weapons systems become widely deployed to
weigh the advantages of a total ban]{.underline}]{.mark} [[and consider
other measures to ensure they will never be used to commit unlawful acts
or trigger catastrophic escalation.]{.underline}]{.mark}

 

### Solvency -- Accuracy

**A2: Humans are more accurate but they believe the tech so it has to be
removed**

Shira **Ovide** (New York Times) "A Case for Banning Facial Recognition"
June 09, **2020**
https://www.nytimes.com/2020/06/09/technology/facial-recognition-software.html?auth=login-google

But a police officer or eyewitness could also look at surveillance
footage and mug shots and misidentify someone as Jim Smith. [Is software
more accurate or less biased than humans?]{.underline} That depends.
[[Our analysis showed that for many, facial recognition was way less
accurate than humans.]{.underline} [The other problem is something
called automation bias.]{.underline} [If your intuition tells you that
an image doesn't look like Smith, but the computer model tells you that
it is him with 99 percent accuracy, you're more likely to believe that
model]{.underline}]{.mark}[.]{.underline} There's also an imbalance of
power. Facial recognition can be completely accurate, but it can still
be used in a way that is detrimental to certain groups of people. [The
combination of overreliance on technology, misuse and lack of
transparency]{.underline} --- we don't know how widespread the use of
this software is --- [is dangerous.]{.underline}

**\**

### Solvency -- Modeling

#### China agrees

Mary **Wareham** (advocacy director in the arms division at Human Rights
Watch) "Summary: Stopping Killer Robots" August 10, **2020**
https://www.hrw.org/report/2020/08/10/stopping-killer-robots/country-positions-banning-fully-autonomous-weapons-and

At the Human Rights Council in May 2013, [[China supported beginning
multilateral talks on lethal autonomous weapons
systems,]{.underline}]{.mark} which it described as "highly
complex."\[65\] [[China has highlighted the potential for fully
autonomous weapons to upset the international strategic balance and
affect arms control]{.underline}]{.mark}.\[66\] In December 2016, China
said it that such weapons "present considerable uncertainties" for
compliance with international humanitarian law and expressed its desire
for precautionary measures, highlighting the precedent provided by the
ban on blinding lasers.\[67\] In April 2018, China called for a ban on
fully autonomous weapons, but later clarified its call was limited to
use only and not development and production.\[68\] Since then, China has
not explicitly repeated its call for a new international treaty to ban
fully autonomous weapons. [[China participated in every CCW meeting on
killer robots in 2014-2019.]{.underline}]{.mark}

**LAWS meet only one part of the IHL threshold -- do not meet on
targeting law -- their evidence**

**Ettinger, 20** (Jay Ettinger, 2020, accessed on 6-29-2022, 30 Minn. J.
Int\'L. 153, \"1. NOTE: Overcoming International Inertia: The Creation
of War Manual for Lethal Autonomous WeaponsSystems, 30 Minn. J. Int\'l
L. 153\",
<file:///C:/Users/ncb12/Downloads/Nathan%20Boyle%20Overcoming%20International%20Inertia_%20The%20Creation%20of%20War%20Manual%20for%20Lethal%20Autonomous%20Weapons%20Sy.PDF>)
(NB)

[[In order for LAWS to be compliant with IHL \"targeting law,\" \"they
must be able to reliably and predictably distinguish between combatants
and non-combatants, as well as make rapid judgments on the
proportionality of an attack against its potential collateral
harms.]{.underline}]{.mark} 67First, [[there is a question as to whether
computer algorithms will be able to gauge the complex,
context-dependent, and humanistic clues that soldiers must use to
distinguish combatants and non-combatants]{.underline}]{.mark} in the
modern battlefield where combatants often attempt to conceal their
identities. 68Second, even if such a distinction is technically
\[\*163\] feasible, [[there is a question as to whether these systems
can reliably make sound decisions given the vast array and often rapidly
changing nature of battlefield contexts]{.underline}]{.mark}. 69For
example, one potential risk to the system\'s reliability is the
introduction of bias to decision-making originating in the data sets
used to train the AI system. 70Even if both of these technical
feasibility questions can be adequately addressed, there is a further
question as to whether lethal decision-making inherently requires human
involvement under IHL. 71And if there is such a requirement, what degree
of human involvement is sufficient to meet IHL obligations must also be
determined.

### Solvency -- DOD Allows Use

#### DOD policy does not prevent use 

Gregory **Allen** (AI Governance Project and Senior Fellow - Center for
Strategic and International Studies) "DOD Is Updating Its Decade-Old
Autonomous Weapons Policy, but Confusion Remains Widespread" June 06,
**2022**
https://www.csis.org/analysis/dod-updating-its-decade-old-autonomous-weapons-policy-confusion-remains-widespread

In November 2012, the Department of Defense (DOD) released its policy on
autonomy in weapons systems: DOD Directive 3000.09 (DODD 3000.09[[).
Despite being nearly 10 years old, the policy remains frequently
misunderstood,]{.mark}]{.underline} including by leaders in the U.S.
military. For example, in February 2021, Colonel Marc E. Pelini, who at
the time was the division chief for capabilities and requirements within
the DOD's Joint Counter-Unmanned Aircraft Systems Office, said, "Right
now we don\'t have the authority to have a human out of the loop. Based
on the existing Department of Defense policy, [[you have to have a human
within the decision cycle at some point to authorize the engagement.\"
He is simply wrong. No such requirement appears in DODD 3000.09, nor any
other DOD policy.]{.underline}]{.mark} Misconceptions about DODD 3000.09
appear to extend even to high-ranking flag officers. In April 2021,
General Mike Murray, the then-four-star commander of Army Futures
Command, said, "Where I draw the line---and this is, I think well within
our current policies---\[is\], if you're talking about a lethal effect
against another human, you have to have a human in that decision-making
process." Breaking Defense, a news outlet that reported on Murray's
remarks at the time, stated that the requirement to have a human in the
decisionmaking process is "official Defense Department policy." It is
not. [[DODD 3000.09 does not ban autonomous weapons or establish a
requirement than U.S. weapons have a "human in the loop." In fact, that
latter phrase never appears in DOD policy]{.underline}]{.mark}. Instead,
DODD 3000.09 formally defines what an autonomous weapon system is and
requires any DOD organization proposing to develop one to either go
through an incredibly rigorous senior review process or meet a
qualifying exemption. Regarding the latter, cyber weapons systems, for
example, are exempted.

## Terrorism Advantage

### Terrorism Advantage -- Recruitment 

#### Drone strikes increase terrorism and anti-us sentiment in Pakistan 

Rafat **Mahmood** (University of Western Australia and Pakistan
Institute of Development Economics), Michael **Jetter** (University of
Western Australia, IZA and CESifo) **APRIL 2019** IZA Institute of Labor
economics, "Military Intervention via Drone Strikes" Accessed 7/2/2019
\*mw\*

This paper introduces an empirical strategy to isolate the causal
effects of drone strikes in Pakistan on subsequent terrorism, anti-US
sentiment, and radicalization, employing wind as the key IV. We
hypothesize that wind decreases the likelihood of the US military
employing a drone strike, conditional on observable characteristics,
whereas wind is otherwise orthogonal to terrorist activities. Both
assumptions receive support in our sample of 4,018 days from 2006 to
2016. Results from 2SLS estimations suggest **[[drone strikes increase
the number of terror attacks in Pakistan]{.underline}]{.mark}** in the
upcoming days and weeks. [This finding prevails in a host of alternative
estimations and robustness checks]{.underline}. Extending the timeframe
of subsequent terrorism, we find evidence indicating **[[drone
strikes]{.underline}]{.mark}** do not just affect the timing of attacks
(e.g., by moving forward planned attacks) but rather **[[increase the
total number of attacks. In terms of magnitude, one drone strike today
causes over four additional terror attacks per day in the next seven
days which implies drone strikes are responsible for 16 percent of all
terror attacks in Pakistan]{.underline}]{.mark}**. A
back-of-the-envelope calculation suggests **[[2,964 people died from
terror attacks because of drone strikes]{.underline}]{.mark}**. We then
explore mechanisms, distinguishing between insiders, i.e., those who
already belong to terrorist organizations, and outsiders, i.e., regular
Pakistanis. Specifically, we study anti-US sentiment in the major
English-language newspaper in Pakistan, anti-US protests, and online
searches for terms that may be indicative of radicalization (jihad,
Taliban video, and Zarb-e-Momin). In line with the blowback hypothesis,
results from 2SLS estimations suggest **[[the general populace
increasingly turns to anti-US and radical expressions after drone
strikes]{.underline}]{.mark}** as all these measures rise substantially
because of drone strikes. It is important to put the results pertaining
to Pakistani news and Google search behavior in context. We are not
suggesting Google Trends as the perfect yardstick to measure radical
attitudes -- an online search for a radical term does not make a
terrorist. Further, identifying more negative emotions and anger 33 in
US-related articles does not necessarily prove anti-US attitudes. For
instance, articles mentioning the US may systematically apply negative
language to their enemies. However, the persistency with which we
identify signs of radicalization and anti-US sentiment because of drone
strikes in the general Pakistani populace is consistent with the
hypothesis that drone strikes systematically turn Pakistanis toward
radical groups and against the US. In fact, given a literacy rate of 58
percent (Government of Pakistan, 2017) and the hypothesis that the
tendency to radicalize usually decreases with education in Pakistan
(Fair et al., 2014), studying an English-language newspaper and online
search behavior (requiring literacy and internet access) may actually
present a lower bound estimate of anti-US sentiment. To our knowledge,
this is the first empirical analysis that is able to isolate causal
effects of drone strikes. Contrary to the current opinion in the US
military which suggests drone strikes curb terrorism, we find evidence
to the contrary: **[[Drone strikes (i) lead to more terrorism, (ii) make
the US more unpopular in Pakistan, and (iii)steer Pakistanis toward
radical ideas]{.underline}]{.mark}**. In other words, not only are
insidersretaliating against the US but outsiders appear to change their
attitudes. As a consequence, the pool of militants may grow, if
anything. As the US military continues to build and expand its drone
program (e.g., in Yemen), we hope our research provides useful insights
into the underlying consequences.

####  Each drone attack amounts to even more terrorist attacks in the future

Anouk S. **Rigterink, 18**, Rigterink S., Anouk. (Postdoctoral Research
Fellow at the University of Oxford and has Ph.D. at the London School of
Economics and Political Science) "The Wane of Command∗: Evidence on
drone strikes and control within terrorist organizations." Department of
Economics: University of Oxford. 10-30-18. Accessed 7-1-19.
(Pembrokehill-MLT)

Targeting terrorist leaders has become a commonly used US
counterterrorism policy since 9/11. [According to the US National
Strategy for Counterterrorism 2018, [targeting key terrorists remains
the number one priority action]{.mark}.]{.underline} An underlying goal
of this policy is to undermine control within terrorist organizations:
the ability of individuals higher up the hierarchy -- or more central to
the organization -- to determine what others in the organization do.
[[This policy is primarily implemented using armed drones]{.mark},
Unmanned Aerial Vehicles, [which can surveil and kill targets]{.mark}.
The US now owns hundreds of these armed drones]{.underline}, and 28
other countries have acquired weaponized drones in the last ten years.
Targeted killing of terrorist leaders is also referred to as "cutting
off the head of the snake", implying that if one does so, the body will
die. The snake analogy portrays the terrorist group as a single
organism: a unitary actor. Many theoretical models similarly considering
terrorist groups as unitary actors straightforwardly predict that
targeting terrorist leaders, and undermining control within terrorist
organizations, decreases terrorism (Sandler and Arce, 2003; Sandler and
Siqueira, 2006; Powell, 2007; Bandyopadhyay and Sandler, 2011). However,
if we consider terrorist organizations as non-unitary actors,
predictions are less straightforward. Models that consider terrorist
groups as organizations subject to collective action and principal-agent
problems, which I will call problems of control for short, suggest that
undermining control within terrorist groups may not be effective, or may
even backfire. This leaves an empirical question, which this paper
addresses: how does counterterrorism which undermines control within
terrorist organizations affect terrorism? Specifically, this paper
investigates how drone strikes killing terrorist leaders affect
terrorist attacks. To investigate this empirically, this paper exploits
a natural experiment provided by drone strikes 'hitting' and 'missing'
terrorist leaders in the Federally Administered Tribal Areas (FATA) of
Pakistan, which is inspired by Jones and Olken (2009). I construct a new
dataset of drone attempts on terrorist leaders' lives, executing several
cross-checks to safeguard data quality. This dataset captures variation
across time and terrorist organizations. I argue that conditional on a
drone attempt to kill a terrorist leader, drone 'hits' and 'misses' are
quasirandom. Drone hits and misses are not statistically significantly
different from one another on an extensive range of characteristics,
including pre-trends in terrorist violence. Narratives of why drones
miss also suggest that misses are largely driven by chance. This enables
a difference-in-difference design, investigating changes in attacks by a
terrorist organization before and after a drone hit on its leader,
compared to before and after a miss. [[Results indicate that a drone hit
on a terrorist organization's leader is associated with an increase in
the number of terrorist attacks by this
organization]{.underline}]{.mark}, compared to a miss. [[Estimates
indicate that this increase amounts to 29 additional terrorist attacks
in total across the world in the six months after a drone hit, an
increase of 43% compared to the six months after a
miss.]{.underline}]{.mark} This result cannot be explained by terrorist
organizations 'speeding up' the timing of attacks, or by a decrease in
the lethality of attacks. Results are robust to estimating the main test
statistic using randomization inference, including leader-fixed effects,
alternative aggregations of terrorist groups, controlling for Pakistani
military action and peace agreements between the terrorist group and the
Pakistani government, and using alternative econometric specifications.
Several game-theoretical models provide explanations for the results
obtained. A first family of models considers problems of control. A
principal-agent model by Shapiro (2013) suggests that [leader killing,
by increasing the costs for the leader to control his operatives, could
lead to increased terrorist violence if operatives have a greater
preference for (indiscriminate) violence than the leader.]{.underline}
Modelling terrorist violence as a public good to terrorists, Enders and
Jindapon (2010) suggest that terrorist networks strategically decrease
the number of network connections in response to targeted leader
killing, leading individual nodes to increase their efforts to commit
attacks as they can no longer free-ride on the efforts of others in the
network. Other families of models provide alternative explanations of
why targeted leader killing may increase terrorist violence: terrorist
groups may respond to a loss in capacity by substituting a few
high-capacity attacks with many low-capacity attacks, [pro-active
[counterterrorism policies that result in civilian casualties may create
backlash if they spur terrorist recruitment, and terrorist organizations
may commit more terrorist attacks after their leader has been killed to
signal strength]{.mark}]{.underline}. Further analysis suggests that
problems of control explain the main result better than alternative
theoretical mechanisms. The observed increase in terrorist attacks is
primarily driven by attacks that leaders of terrorist organizations do
not favour: attacks on civilian and private targets -- which terrorist
leaders repeatedly instruct their followers to avoid -- and attacks by
its members that the organization does not publicly claim responsibility
for. The observed effect is also stronger for terrorist organizations
and leaders that rely more strongly on central control. Furthermore, a
drone hit is associated with proxies for network breakdown: an increase
in group splintering and infighting. Simultaneously however, [[results
suggest that a drone strike on one group's leader is associated by an
increase in terrorist attacks by other groups, affiliates in its
network.]{.underline}]{.mark} A considerable number of existing papers
investigate empirically the impact of targeted killing of leaders of
terrorist organizations, yet results are mixed. Some authors conclude
that targeted leader killing is effective, either because it speeds up
the decline of terrorist organizations (Price, 2012) or diminishes the
number or intensity of terrorist attacks (Jaeger and Paserman, 2009;
Johnston, 2012). Others conclude that it has no effect (Jordan, 2009;
Mannes, 2008; Hafez and Hatfield, 2006), or even an adverse effect --
either on terrorist organizations in general or on particular categories
of terrorist organizations (Kaplan et al., 2006; Jordan, 2009; Mannes,
2008; Abrahms and Potter, 2015; Abrahms and Mierau, 2017).

#### The US failing to address the drone problem strengthens terrorist groups

Abdulrasheed **Al-Faqih, 18**, (Abdulrasheed Al-Faqih is the Executive
Director of Mwatana Organization for Human Rights, an independent Yemeni
organization aiming to defend and protect human rights in Yemen.)
Abdulrasheed Al-Faqih. \"Civilian Casualties and Effectiveness of U.S.
Drone Strikes in Yemen.\" Just Security. 4-3-2018. 7-1-2019.
\<https://www.justsecurity.org/54464/
civilian-casualties-effectiveness-u-s-drone-strikes-yemen-part/\>
(PembrokeHill-MLT)

[In 2017, [the United States military said that it [carried
out](https://www.defense.gov/News/Article/Article/1401445/centcom-officials-provide-update-on-recent-counterterrorism-strikes-in-yemen/)
more than 120 strikes in Yemen]{.mark}]{.underline}, more than three
times as many as strikes as
[2016](https://www.thebureauinvestigates.com/projects/drone-war/yemen).
For many years, we at the [Mwatana Organization for Human
Rights](http://mwatana.org/en) documented the impact of U.S. drone
strikes in Yemen through detailed field research. In 2017 we
[investigated](http://mwatana.org/ar/2512018679) [[eight drone strikes
and ground operations]{.mark}]{.underline} and found that U.S.
operations [[were responsible for the deaths of at least 32 civilians --
including 16 children and six women -- and injured ten others, including
five children]{.mark}.]{.underline} (The results of these investigations
will be released in a forthcoming report.) [[The 32 civilian deaths and
ten injuries are the latest in a long list of victims harmed by U.S.
military operations in Yemen who have been waiting for justice for many
years. Incidents of civilian harm]{.underline}]{.mark} in Yemen continue
to [[negatively affect the reputation of the United States in the
country and push local communities to consider violence and revenge as
the only solution to the harm they suffer]{.underline}.]{.mark} With
U.S. operations in Yemen
[continuing](http://www.centcom.mil/MEDIA/PRESS-RELEASES/Press-Release-View/Article/1433499/centcom-updates-)
in 2018, it is time for much greater attention to be given to the
civilians harmed and the effects of this unwise and destructive policy.
Documenting U.S. strikes The United States
[began](https://www.thebureauinvestigates.com/drone-war/data/yemen-reported-us-covert-actions-2001-2011)
targeted killing operations in Yemen in 2002 under the Bush
administration, which
[increased](https://www.thebureauinvestigates.com/projects/drone-war/yemen)
dramatically within the first few years of the Obama administration.
Under President Trump, strikes have again soared. Within Yemen, U.S.
operations have raised a great deal of questions, about the
[secrecy](https://www.outoftheshadowsreport.com/), ethics, objectives,
results, effectiveness, effects, and legality of these strikes. Human
rights advocates have also
[questioned](http://www.mwatana.org/en/1442015397) the U.S. government
about the fate of the hundreds of civilian victims, about the
government's lack of acknowledgment and accountability, and the ways to
ensure no new victims are added to the growing list. So far, the U.S.
government's response to many of these questions remains inadequate or
nonexistent. During 2013 and 2014, I worked on a team researching
civilian victims of U.S. drone strikes. We visited areas and villages in
different parts of Yemen that had been affected by U.S. actions. We
interviewed witnesses, survivors, doctors, and local social leaders. We
inspected the scenes of the attacks, the remains of the munitions, and
the effects of such operations on the lives of civilians. We released
our findings in a report, Death by Drone: Civilian Harm Caused by U.S.
Targeted Killings in Yemen, which we co-authored with the Open Society
Justice Initiative. It included the results of our field research as
well as recommendations to ensure U.S. operations comply with the law
and provide accountability for civilians harmed by U.S. strikes. Sadly,
many of those whom we spoke to would speak of the harsh reality of their
lives. Our village is poor. We do not have schools, hospitals, roads, or
any type of public services. The only thing that we have in the way of
progress and development in a modern world are these deadly missiles.
This is what Muhammad Nasser Al Jarrah, a villager from Sailat Al Jarrah
[told](http://www.mwatana.org/en/1912017603) us during our visit to his
remote village on May 31, 2013, after an alleged U.S. strike hit a home
full of women and children. Our visit to Sailat Al Jarrah was nearly
five years ago, but Muhammad's somber sentiment was shared with us by a
number of survivors and witnesses over the years and in different parts
of Yemen. It is now sixteen years after the first U.S. drone strike in
Yemen, and we are still working on documenting new attacks and
researching their effect on civilians. [[With Trump's renewed effort to
increase lethal operations in Yemen, it seems that the U.S. has still
not learned lessons to prevent civilian harm in the
country]{.mark}.]{.underline} Release the kill list: A tool to reduce
civilian harm [Strikes]{.underline} that we documented [across
Yemen]{.underline}, from Sana'a, to Dhamar, to
[Rada'a](http://www.newsweek.com/wedding-became-funeral-us-still-silent-one-year-deadly-yemen-drone-strike-291403)
[highlight the problematic use of U.S. drone strikes and other
counterterrorism operations in what---contrary to the U.S. claim that it
is in a global conflict against Al Qaeda and ISIS---is better understood
as a local, social, and political conflict.]{.underline} From time to
time, some of the people we met would also suggest ways that the U.S.
could better protect civilians in Yemen. [One of the demands that
survivors of drone strikes would ask for is a list of wanted
individuals. A list that is clear and available to the public so that
they can avoid targeted individuals, protect their children, and not
allow U.S. targets to have a presence in their areas]{.underline}. Some
of the local residents we would interview would claim that a number of
the attacks targeted children or ordinary civilians that were not doing
anything that might cause suspicion or indicate that they were dangerous
or involved in terrorist acts. To them, having a list of wanted
individuals would perhaps prevent further civilian harm. Nasser Mabkhout
is one of two survivors of an alleged U.S. drone strike on December 2,
2012. He was driving a group of civilians from a market back to the
village of Al Sabool, when the car was hit by a U.S. drone. At the time,
the attack was one of the deadliest strikes ever, killing 12 civilians,
including three children and a pregnant woman. Two civilians were also
injured. During an interview at his home in Al Sabool, Nasser told us:
"I was not worried at all when I saw the plane flying above us. I was
sure that they had specific targets, and that these targets were members
of terror groups, while we are just vendors and workers. I had heard a
lot that these planes were very smart, and that they knew their targets
and were very accurate in their strikes. While we were watching the
plane, we were laughing and making jokes until we were stopped by one of
its missiles, which hit my car and devastated the people in it." Local
security officials, social leaders, and witnesses of U.S. drone strikes
in Yemen told us that the strikes targeted individuals in areas and
conditions where it was possible for them to be arrested, investigated,
and tried in a court of law. As recently as late last year, the Governor
of Mareb [told](https://www.justsecurity.org/47103/yemen-strike-raises)
researchers that the United States carried out a drone strike in
November 2017 against a target that his security forces could access.
The Governor lamented the failure of the U.S. government to provide
information to local forces that might have led to the capture of
terrorism suspects. This leads to a number of questions. How dangerous
are the suspects that are targeted in these operations? How feasible is
it to arrest, investigate, and try them in court? What do American
agencies use to determine who is a suspect when they make the kill list?
These and many other questions remain unanswered. Cycle of violence Our
research team also traveled to a village in the area of Qaifah, Yemen on
May 26, 2013. This area is one that has been targeted the most by drone
strikes. While we were there, we heard a number of men who were
relatives of the civilian victims of a drone strike loudly discussing
whether they should kidnap us to pressure the U.S. government to look
into the cases of Yemeni drone strike victims. The group that had this
idea was trying to convince the rest of the men that we were working for
an American organization, the [Open Society
Foundations](https://www.opensocietyfoundations.org/), and that this
organization could pressure the U.S. government to look into cases of
Yemeni drone strike victims. We were lucky that one of the village men,
who was also our guide in the area, told the others that he could not
allow them to kidnap us because we were his guests and under his
protection. Ultimately, we were able to leave the area without any
problems. The incident, however, shows how the families of the victims,
many of whom are poor farmers who have waited so long for an
acknowledgment for the harm they suffered, and for justice through legal
means, begin to consider violent "solutions" to their problems. The U.S.
government should realize that Yemenis on the ground feel that U.S.
practices that ignore civilian harm are not only dehumanizing but are
also counter-productive to the United States' long-term counterterrorism
objectives. [Acknowledgment of accidental civilian deaths can be a vital
step toward preventing further acts of violence.]{.underline} I've seen
some cases where the relatives of civilian drone strike victims are
first convinced that the attack was a mistake, that the United States
and its Yemeni government ally will officially apologize for the attack,
and provide the victims with justice and a remedy. When this apology and
remedy does not come, the relatives work to draw attention to their case
to highlight that their relatives are civilian victims. And when there
is no reaction or response from the U.S. government after those
attempts, the families are left only with thoughts of revenge. In these
moments, Al Qaeda and similar jihadist groups stand ready to capitalize
and exploit these feelings of discontent and injustice. The potential
for violent groups to take advantage of civilian discontent has only
grown in recent years. Since the start of the current conflict involving
the sectarian Houthi group on one side, and the Saudi-led coalition
supported by its international partners on the other, Yemen has been
torn apart and its institutions almost completely destroyed. This
environment has made it easier for jihadist groups to prosper. The war
has allowed Al Qaeda to come out of isolation and expand, so it is now
more present in areas that, as the crisis in Yemen drags on, will become
more friendly to such groups---not least if the United States continues
raining down drone strikes and launching lethal raids on the people of
Yemen. We are fortunate that, in the different areas in which we have
worked, it was clear to us that many people, for now, reject Al Qaeda
and jihadist groups, even as they are outraged that drone strikes killed
their civilian relatives. This has limited Al Qaeda's influence in
Yemen. But [[every time the U.S. fails to acknowledge a drone strike
that harms civilians, the risk that people will turn to Al Qaeda only
grows.]{.underline}]{.mark}

### Terrorism Advantage -- Impact

**Terrorist attacks are poised to go nuclear**

**Ayson 2010** (Robert Ayson, Professor of Strategic Studies and
Director of the Centre for Strategic Studies: New Zealand at the
Victoria University of Wellington, "After a Terrorist Nuclear Attack:
Envisaging Catalytic Effects," Studies in Conflict & Terrorism, Volume
33, Issue 7, July, Available Online to Subscribing Institutions via
InformaWorld)

A terrorist nuclear attack, and even the use of nuclear weapons in
response by the country attacked in the first place, would not
necessarily represent the worst of the nuclear worlds imaginable.
Indeed, [there are reasons to wonder whether nuclear terrorism should
ever be regarded as belonging in the category of truly existential
threats. A contrast can be drawn here with the global catastrophe that
would come from a massive nuclear exchange between two or more of the
sovereign states that possess these weapons in significant numbers. Even
the worst terrorism that the twenty-first century might bring would fade
into insignificance alongside considerations of what a general nuclear
war would have wrought in the Cold War period.]{.underline} And it must
be admitted that as long as the major nuclear weapons states have
hundreds and even thousands of nuclear weapons at their disposal, there
is always the possibility of a truly awful nuclear exchange taking place
precipitated entirely by state possessors themselves. [But]{.underline}
these two nuclear worlds---a non-state actor nuclear attack and a
catastrophic interstate nuclear exchange---are not necessarily
separable. It is just possible that some sort of terrorist attack, and
especially [an act of [nuclear terrorism, could precipitate]{.mark} a
chain of events leading to [a massive exchange of nuclear weapons
between]{.mark} two or more]{.underline} of the
[[states]{.underline}]{.mark} that possess them. In this context,
today's and tomorrow's terrorist groups might assume the place allotted
during the early Cold War years to new state possessors of small nuclear
arsenals who were seen as raising the risks of a catalytic nuclear war
between the superpowers started by third parties. These risks were
considered in the late 1950s and early 1960s as concerns grew about
nuclear proliferation, the so-called n+1 problem. It may require a
considerable amount of imagination to depict an especially plausible
situation where an act of nuclear terrorism could lead to such a massive
inter-state nuclear war. For example, in the event of a terrorist
nuclear attack on the United States, it might well be wondered just how
Russia and/or China could plausibly be brought into the picture, not
least because they seem unlikely to be fingered as the most obvious
state sponsors or encouragers of terrorist groups. They would seem far
too responsible to be involved in supporting that sort of terrorist
behavior that could just as easily threaten them as well. Some
possibilities, however remote, do suggest themselves. For example, how
might the United States react if it was thought or discovered that the
fissile material used in the act of nuclear terrorism had come from
Russian stocks,40 and if for some reason Moscow denied any
responsibility for nuclear laxity? The correct attribution of that
nuclear material to a particular country might not be a case of science
fiction given the observation by Michael May et al. that while the
debris resulting from a nuclear explosion would be "spread over a wide
area in tiny fragments, its radioactivity makes it detectable,
identifiable and collectable, and a wealth of information can be
obtained from its analysis: the efficiency of the explosion, the
materials used and, most important ... some indication of where the
nuclear material came from."41 Alternatively, [[if the
act]{.underline}]{.mark} of nuclear terrorism [[came as a]{.mark}
complete [surprise]{.mark}]{.underline}, and American officials refused
to believe that a terrorist group was fully responsible (or responsible
at all) [[suspicion would shift]{.mark} immediately [to state
possessors.]{.mark} Ruling out Western ally countries]{.underline} like
the United Kingdom and France, and probably Israel and India as well,
authorities in [Washington would be left with a very short list
[consisting of North Korea]{.mark}]{.underline}, perhaps
[[Iran]{.underline}]{.mark} if its program continues, and possibly
[[Pakistan]{.underline}]{.mark}. But at what stage would [[Russia and
China]{.underline}]{.mark} be definitely ruled out in this high stakes
game of nuclear Cluedo? In particular, if the act of nuclear terrorism
occurred [against a backdrop of existing tension in Washington's
relations with [Russia and]{.mark}]{.underline}/or
[[China]{.underline}]{.mark}, and at a time when threats had already
been traded between these major powers, [[would]{.mark} officials and
political leaders not be tempted to [assume the
worst]{.mark}?]{.underline} Of course, the chances of this occurring
would only seem to increase if the United States was already involved in
some sort of limited armed conflict with Russia and/or China, or if they
were confronting each other from a distance in a proxy war, as unlikely
as these developments may seem at the present time. [The
reverse]{.underline} might well [apply too: should a nuclear terrorist
attack occur in Russia or China]{.underline} during a period of
heightened tension or even limited conflict with the United States,
[[could Moscow and Beijing resist the pressures]{.underline}]{.mark}
that might rise [domestically to consider the U]{.underline}nited
[S]{.underline}tates [as a possible perpetrator]{.underline} or
encourager of the attack[?]{.underline}

**Nuclear terrorism leads to extinction -- [retaliation]{.underline} --
terrorists have [capabilities]{.underline}**

**Dobbins 15** (William Dobbins, Graduate Fellow, Countering Weapons of
Mass Destruction National Defense University, Major USMC, 2015. "Right
of Boom: The Aftermath of Nuclear Terrorism",
http://thesimonscenter.org/wp-content/uploads/2015/05/IAJ-6-2-Spring-2015-77-79.pdf)

Right of Boom: The Aftermath of Nuclear Terrorism provides a glimpse of
the issues associated with responding to a nuclear terrorist attack, in
this case, the detonation of a small nuclear weapon in Washington, D.C.
Against this background, Benjamin [Schwartz describes the inherent
danger of a world with nuclear-armed states]{.underline} (some [which
may not have the will or capability to appropriately secure such
weapons]{.underline}) [and new types of terror threats]{.underline}, the
lessons learned in nuclear deBterrence and counter terrorism, [[the
**global impact** of]{.mark} a [nuclear terror]{.mark} attack, and the
"red lines" that would forever change as a result]{.underline}. [What
emerges [is]{.mark} a [**bleak**]{.mark} picture of potential political
and policy consequences of both the terror attack and the American
response]{.underline}. In the aftermath of a nuclear terrorist attack on
the most important political capital in the world, the confusion, the
desire for attribution and retaliation coupled together with the
overarching question, "How did this happen?" combine to produce the
environment that political leaders would face. At first glance, the
author posits, [the response to such an event seems
straightforward]{.underline}: **[[The U.S.]{.mark} undergoes a nuclear
attack, and the U.S. [responds in kind]{.mark}]{.underline}**[.]{.mark}
However, the follow-on questions reveal that things are not nearly so
simple: From where did the weapon come? In a terror attack in which
attribution is not certain, against whom do national leaders direct a
response? [The history of international nuclear agreements]{.underline},
[from the]{.underline} Non-Proliferation Treaty ([NPT]{.underline}) [to
the]{.underline} Nunn-Lugar Cooperative Threat Reduction
([CTR]{.underline}) [program are all predicated on international
relationships that operated at the nation-state level]{.underline}.
**[However]{.underline}**, [the attack in question reveals that these
state-to-state agreements **may no longer be sufficient in the face of
nuclear terrorism**]{.underline}. [[The]{.mark}
**[prolif]{.mark}**eration [of]{.mark} **[nuclear knowledge]{.mark}**
[and]{.mark} [tech]{.mark}nology now [makes the nuclear terror]{.mark}
threat **[plausible]{.mark}**]{.underline}. Hence, Schwartz argues,
"**[[we are more vulnerable]{.mark} to nuclear terrorism [than at any
time]{.mark} since the dawn of the nuclear age]{.underline}**." Could an
attack like this actually happen? On the one hand, Schwartz notes
legitimate reasons for skepticism. After all, since 9-11, there have
been no societally significant terror attacks and no nuclear terror
attacks. Moreover, the continual crying of "wolves at the door" by
national leaders only makes the lack of terrorist success more
pronounced. [[The **failure of intel**]{.mark}**ligence** regarding WMD
threats]{.underline}---leading to the Global War on Terror---has also
cast a pall over the intelligence community's predictive powers. On the
other hand, Schwartz also notes that neither past intelligence failures
nor the absence of a nuclear terrorist attack changes the fact that the
proliferation of knowledge and technology---especially dual-use
technology---**[[increases the threat]{.mark} of nuclear
terrorism]{.underline}**. [Access to the knowledge necessary for nuclear
proliferation has itself proliferated since World War II]{.underline}.
Even if the hardest part of developing a nuclear weapon is acquiring
fissile material, [the plans for simple weapon systems can now be found
with a quick internet search]{.underline}. Hence, Schwartz argues that,
[while the threat of thermonuclear war recedes into
history]{.underline}, [a new threat]{.underline}---like that of the
detonation of a small nuclear weapon in a city such as Washington,
D.C.---[is actually growing]{.underline}. [The breakup of the former
USSR states created one of the most dangerous situations for the "loose
nuke" phenomenon]{.underline}. [While CTR provided one of the most
successful bi-lateral programs to protect against that **possible
threat**]{.underline}, [[nations]{.underline}]{.mark}, such as Pakistan
or North Korea, [outside the scope of CTR may [provide]{.mark}
**[material support]{.mark} to terror organizations**]{.underline}, [or
[they]{.mark} may simply [have **insufficient control over**]{.mark}
**their [nuclear materials]{.mark}**]{.underline}. In light of these
complexities, the question naturally, but uncomfortably, arises as to
the ongoing role of nuclear deterrence. [The working assumption has been
that, when faced with total annihilation between two warring states,
each opponent may be deterred from **nuc**lear weapon**s**
employment]{.underline}. [Although this very model arguably worked well
for 60 years between the USSR and the United States]{.underline},
Schwartz highlights that **[the present nuclear world is faced with a
different problem]{.underline}**: [How does a nation-state deter a
stateless organization]{.underline}? For that matter, [how does a
nuclear nation-state deter even a non-nuclear nation-state committed to
the support of nuclear terrorists]{.underline}? As a testament to such,
since World War II, four out of the five NPT nuclear powers has "lost" a
war to a non-nuclear foe without ever using nuclear weapons. [In the
immediate aftermath of a nuclear terror attack in the United States,
**the political need to demonstrate control, resolve and to hold someone
accountable will be intense**]{.underline}. Yet, how does the U.S.
determine what objectives to pursue? It is simple to say, "Go kill the
terrorists!" It is another to comply with that statement. Individual
targets in multiple nations with multiple governments involved do not
necessarily constitute an effective counter-terrorism program. How does
the U.S. build an international coalition when political interests
diverge and intelligence agencies have different opinions about
governmental complicity? How does it create a coalition of allied
governments whose very citizens may be involved? How does it counter
terrorism when "terrorism" itself is an amorphous concept? On this
account, Schwartz provides historical examples that run from the
Comanche to the United States' current fight with Al Qaeda and its
affiliates. [Countering terrorism will be the battle of the future; it
will not be an easy one]{.underline}---[particularly if it acquires a
nuclear dimension]{.underline}. [A nuclear terror attack on the United
States will affect more than the U.S. It will re-write the international
legal system]{.underline}. [There may still be [treaties]{.mark} and
agreements, but **[after]{.mark} a nuclear weapon detonates, they [may
simply be pieces of paper]{.mark}**]{.underline}. Establishing new
arrangements and treaties will have to follow. The idea that what
happens within the borders of another nation-state is only that
nation-state's business will be robustly challenged. [The risk is simply
too great to trust that another government would even be capable of
keeping its nuclear issues within its borders]{.underline}. **[[The
Peace of Westphalia may]{.mark} simply [fade
away]{.mark}]{.underline}**. Further, the people of the United States
could be affected by unprecedentedly intrusive surveillance of goods,
materials, and information being imported and exported. In this
environment, it is possible that the Baruch plan---or a reasonable
facsimile---may be pursued with broader support than it originally had.
In the end, Schwartz suggests, an event like this would do more to
change the global security calculus than did 9-11. "Catastrophic
events", particularly as the term gets applied to weapons of mass
destruction events, are, curious as it may seem, easy to dismiss as
someone else's problem in the big, lumbering federal bureaucracy. This
is so because the events thus characterized are so overwhelming that
they befuddle the imagination (and certainly would exhaust the
resources) of any one agency that sought to deal with them. However, it
is this fact which, more than any other, makes the response to
catastrophic events the quintessential interagency challenge. The
present anthology assembles the work of some of America's most
insightful public servants and clearly demonstrates that every organ of
government at every level---tribal, local, state, and federal---is
remiss if it fails to ask the question, "What is my role when the
unthinkable happens?" and "With whom should I be talking as I imagine
the unthinkable?"

## Democracy Advantage

### Democracy Advantage - Link

**Facial recognition software is being used to undermine democracy and
the right to protest.**

Shira **Ovide** (New York Times) "A Case for Banning Facial Recognition"
June 09, **2020**
https://www.nytimes.com/2020/06/09/technology/facial-recognition-software.html?auth=login-google

Timnit Gebru, a leader of Google's ethical artificial intelligence team,
explained why she believes that [[facial recognition is too dangerous to
be used]{.underline}]{.mark} right now [[for law enforcement
purposes]{.underline}.]{.mark} These are edited excerpts from our
virtual discussion at the Women's Forum for the Economy & Society on
Monday. Ovide: What are your concerns about facial recognition? Gebru: I
collaborated with Joy Buolamwini at the M.I.T. Media Lab, who led [[an
analysis]{.underline}]{.mark} that [[found very high disparities in
error rates]{.mark} \[in facial identification systems\], especially
between lighter-skinned men and darker-skinned women]{.underline}. In
melanoma screenings, imagine that there's a detection technology that
doesn't work for people with darker skin. I also realized that [even
perfect facial recognition can be misused]{.underline}. I'm a black
woman living in the U.S. who has dealt with serious consequences of
racism. [[Facial recognition is being used against the black
community]{.underline}.]{.mark} [Baltimore [police]{.mark} during the
Freddie Gray protests [used facial recognition to identify
protesters]{.mark} by linking images to social media
profiles]{.underline}.

### Democracy Advantage -- Impact 

**Democratic collapse is an existential threat \-\-- controls every
impact**

**Kasparov 17** (Garry Kasparov, Chairman of the Human Rights
Foundation, former World Chess Champion, "Democracy and Human Rights:
The Case for U.S. Leadership," Testimony Before The Subcommittee on
Western Hemisphere, Transnational Crime, Civilian Security, Democracy,
Human Rights, and Global Women\'s Issues of the U.S. Senate Committee on
Foreign Relations, February 16^th^,
https://www.foreign.senate.gov/imo/media/doc/021617_Kasparov\_%20Testimony.pdf)

[As]{.underline} one of the [countless millions]{.underline} of people
who [were]{.underline} freed or [protected from totalitarianism by the
United States]{.underline} of America, [it is easy for me to talk about
the past]{.underline}. To talk about the belief of the American people
and their leaders that this country was exceptional, and had special
responsibilities to match its tremendous power. [That a nation founded
on freedom was bound to defend freedom everywhere]{.underline}. I could
talk about the bipartisan legacy of this most American principle, from
the Founding Fathers, to Democrats like Harry Truman, to Republicans
like Ronald Reagan. I could talk about how the American people used to
care deeply about human rights and dissidents in far-off places, and how
this is what made America a beacon of hope, a shining city on a hill.
[[America led by example and]{.mark} set a high standard, a standard
that [exposed]{.mark} the hypocrisy and cruelty of
[dictatorships]{.mark} around the world. But there is no time for
nostalgia]{.underline}. Since the fall of the Berlin Wall, the collapse
of the Soviet Union, and the end of the Cold War,
[[Americans]{.underline}]{.mark}, and America, [[have retreated]{.mark}
from those principles, and **[the world has become much worse
off]{.mark} as a result**]{.underline}. American skepticism about
America's role in the world deepened in the long, painful wars in
Afghanistan and Iraq, and their aftermaths. Instead of applying the
lessons learned about how to do better, lessons about faulty
intelligence and working with native populations, the main outcome was
to stop trying. This result has been a tragedy for the billions of
people still living under authoritarian regimes around the world, and it
is based on faulty analysis. You can never guarantee a positive
outcome--- not in chess, not in war, and certainly not in politics. The
best you can do is to do what you know is right and to try your best. I
speak from experience when I say that the citizens of unfree states do
not expect guarantees. They want a reason to hope and a fighting chance.
[People living under dictatorships want the opportunity for
freedom]{.underline}, the opportunity to live in peace and to follow
their dreams. From the Iraq War to the Arab Spring to the current
battles for liberty from Venezuela to Eastern Ukraine, people are
fighting for that opportunity, giving up their lives for freedom. [The
United States must not abandon them. The United States and the rest of
the free world has an unprecedented advantage in economic and military
strength today. What is lacking is the will.]{.underline} The will to
make the case to the American people, [the will to take risks and invest
in the long-term security of the country, and the world]{.underline}.
This will require investments in aid, in education, in security that
allow countries to attain the stability their people so badly need. Such
investment is far more moral and far cheaper than the cycle of
[**[terror, war]{.mark}**, refugees, [and]{.mark} **military
i[ntervention]{.mark}**]{.underline} that [[results when America leaves
a vacuum]{.mark} of power.]{.underline} The best way to help refugees is
to prevent them from becoming refugees in the first place. The Soviet
Union was an existential threat, and this focused the attention of the
world, and the American people. There [**[existential threat]{.mark}**
today is not found on a map, but it **[is very
real]{.mark}**.]{.underline} The [forces of the past are making steady
progress against the modern world order. **[Terrorist]{.mark}**
movements in the Middle East, [extremist parties]{.mark} across Europe,
a paranoid tyrant in **[North Korea]{.mark} threatening [nuclear
blackmail,]{.mark}** [and]{.mark}]{.underline}, at the center of the
web, [an **[aggressive]{.mark} KGB dictator in
[Russia]{.mark}**[.]{.mark} They all want to turn the world back to a
dark past because their survival is threatened by]{.underline} the
[values of the free world, epitomized by the United States. And **they
[are thriving as the U.S. has retreated]{.mark}**.]{.underline} The
global freedom index has declined for ten consecutive years. No one like
to talk about the United States as a global policeman, but [**this is
what happens when there is no cop on the beat. [American leadership
begins at home]{.mark}**, right here. America cannot lead the world on
democracy and human rights if there is no unity on]{.underline} the
[meaning and importance of these things. []{.mark}**Leadership is
required to make that case clearly and powerfully**. Right now,
Americans are engaged in politics at a level not seen in decades. It is
an opportunity for them to rediscover that making America great begins
with believing America can be great.]{.underline} The Cold War was won
on American values that were shared by both parties and nearly every
American. Institutions that were created by a Democrat, Truman, were
triumphant forty years later thanks to the courage of a Republican,
Reagan. This bipartisan consistency created the decades of strategic
[stability]{.underline} that [is the great strength of democracies.
Strong institutions that outlast politicians allow for long-range
planning]{.underline}. In contrast, dictators can operate only
tactically, not strategically, because they are not constrained by the
balance of powers, but cannot afford to think beyond their own survival.
This is why a dictator like Putin has an advantage in chaos, the ability
to move quickly. This can only be met by strategy, by long-term goals
that are based on shared values, not on polls and cable news. The fear
of making things worse has paralyzed the United States from trying to
make things better. There will always be setbacks, but the United States
cannot quit. [The spread of **[democracy is the only]{.mark}** proven
**[remedy for]{.mark}** nearly [**every crisis that plagues the world
today. War, famine, poverty, terrorism**--all are generated]{.mark} and
exacerbated [by authoritarian regimes]{.mark}]{.underline}. A policy of
[America First inevitably puts American security last. **[American
leadership is required because there is no one
else]{.mark}**]{.underline}, and because it is good for America. [There
is no weapon or wall that is more powerful for security than America
being envied, imitated, and admired around the world. Admired not for
being perfect, but for having the exceptional courage to always try to
be better]{.underline}. Thank you.

## War Escalation Advantage

### War Escalation -- Conflict Multiplier

#### LAWS are a threat multiplier; everything is worse in the world of LAWS availability 

**Future of Life Institute** "AUTONOMOUS WEAPONS: WHAT ARE THEY, AND WHY
DO THEY MATTER?" Summer **2021**
https://futureoflife.org/2021/11/30/an-introduction-to-the-issue-of-lethal-autonomous-weapons/

What's the problem[[? Weapons that use algorithms to kill, rather than
human judgement are immoral and a grave threat to national and global
security]{.underline}]{.mark}. Immoral: [[Algorithms are incapable of
comprehending the value of human life, and so should never be empowered
to decide who lives and who dies]{.underline}]{.mark}. Indeed, the
United Nations Secretary General António Guterres agrees that "machines
with the power and discretion to take lives without human involvement
are politically unacceptable, morally repugnant and should be prohibited
by international law." Threat to Security: [[Algorithmic decision-making
allows weapons to follow the trajectory of software: faster, cheaper,
and at greater scale. This will be highly destabilising on both national
and international levels because it introduces the threats of
proliferation, rapid escalation, unpredictability, and even the
potential for weapons of mass destruction]{.underline}]{.mark}.

#### A2: We need a certain weapons system that already exists

#### The plan only ends fully autonomous systems -- affirmative can still engage in necessary warfighting capabilities

**USNI News** "Report to Congress on Lethal Autonomous Weapon Systems"
November 19, **2021**
https://news.usni.org/2021/11/19/report-to-congress-on-lethal-autonomous-weapon-systems-3

DODD 3000.09 defines [[LAWS as "weapon system\[s\] that, once activated,
can select and engage targets without further intervention by a human
operator."]{.underline}]{.mark} This concept of autonomy is also known
as "human out of the loop" or "full autonomy." [[The directive contrasts
LAWS with human-supervised, or "human on the loop," autonomous weapon
systems, in which operators have the ability to monitor and halt a
weapon's target engagement.]{.underline}]{.mark} Another category is
semi-autonomous, or "human in the loop," weapon systems that "only
engage individual targets or specific target groups that have been
selected by a human operator." [[Semi-autonomous weapons include
so-called "fire and forget" weapons, such as certain types of guided
missiles, that deliver effects to human-identified targets using
autonomous functions.]{.underline}]{.mark}

#### Plan key to maintaining ethical norms and accountability to war crimes

Michael T. **Klare** (professor emeritus of peace and world security
studies at Hampshire College and senior visiting fellow at the Arms
Control Association) "Autonomous Weapons Systems and the Laws of War"
March **2019**
https://www.armscontrol.org/act/2019-03/features/autonomous-weapons-systems-laws-war

This poses obvious challenges because virtually all human ethical and
religious systems view the taking of a human life, whether in warfare or
not, as a supremely moral act requiring some valid justification.
Humans, however imperfect, are expected to abide by this principle, and
most societies punish those who fail to do so. [[Faced with the horrors
of war, humans have sought to limit the conduct of belligerents in
wartime, aiming to prevent cruel and excessive
violence]{.underline}]{.mark}. Beginning with the Hague Convention of
1898 and in subsequent agreements forged in Geneva after World War I,
international jurists have devised a range of rules, collectively, the
laws of war, proscribing certain behaviors in armed conflict, such as
the use of poisonous gas. Following World War II and revelations of the
Holocaust, diplomats adopted additional protocols to the Hague and
Geneva conventions intended to better define the obligations of
belligerents in sparing civilians from the ravages of war, measures
generally known as international humanitarian law. [[So long as humans
remain in control of weapons, in theory they can be held accountable
under the laws of war]{.underline}]{.mark} and international
humanitarian law for any violations committed when using those devices.
[[What happens when a machine makes the decision to take a life and
questions arise over the legitimacy of that action? Who is accountable
for any crimes found to occur, and how can a chain of responsibility be
determined]{.underline}]{.mark}? These questions arise with particular
significance regarding two key aspects of international humanitarian
law, the requirement for distinction and proportionality in the use of
force against hostile groups interspersed with civilian communities.
Distinction requires warring parties to discriminate between military
and civilian objects and personnel during the course of combat and spare
the latter from harm to the greatest extent possible. Proportionality
requires militaries to apply no more force than needed to achieve the
intended objective, while sparing civilian personnel and property from
unnecessary collateral damage.11

### War Escalation -- Great Power War Impact

**Yes great power war** -- realism, fear of worse alts, failed political
processes, violent human nature -- **their evidence twists definitions
to exclude our scenarios**

-On point answer to Mueller, Pinker and Mandelbaum

**Lyon 14** {Rod, director of the strategy and international program at
the Australian Strategic Policy Institute, executive editor of The
Strategist, "No, Great Power War Isn't Obsolete," The Diplomat, 8/22,
[http://thediplomat.com/2014/08/no-great-power-war-isnt-obsolete/](http://thediplomat.com/2014/08/no-great-power-war-isnt-obsolete/#THUR)}

August has seen a wave of reflection on major war. It's a question we
seem to revisit every time the key anniversaries of WWI and WWII roll
around, but especially this year because its the 100th anniversary of
the outbreak of WWI. Some [pundits are keen to draw parallels between
1914 and 2014---though on its face **it's not apparent** to me why 2014
should be more like 1914 than 2013.¶]{.underline} Academic strategists
familiar with their disciplinary history will know that [the issue of
whether]{.underline} [major war's obsolete received]{.underline} a
detailed [coverage]{.underline} back in Survival magazine [in the late
1990s]{.underline}. To save readers the trouble of digging through their
archives, one contributor, John [[Mueller]{.underline}]{.mark},
[[argued]{.underline}]{.mark} that [[it was
obsolete]{.underline}]{.mark}---gone the way of slavery and
dueling---while others wrestled partly over how to define obsolescence
and even more over how to define major war. Was the Vietnam War "major?"
Was the Cold War a "war?" Michael [[Mandelbaum]{.mark}
[argued]{.mark}]{.underline} that perhaps [major war was just a [poor
policy]{.mark}]{.underline} option nowadays[---because of the steep rise
in the costs and]{.underline} the [thin rewards for
success]{.underline}.¶ [It's intriguing]{.underline} that [the
question]{.underline} about the obsolescence of war [is **typically
qualified by the adjective "major**.]{.underline}" [[No one]{.mark}
seems particularly keen to [claim]{.mark} that nasty [little
wars]{.mark}]{.underline}---in particular, nasty little wars in faraway
places---[[are obsolete]{.mark}, perhaps [because **they**]{.mark}
**patently [aren't]{.mark}**.]{.underline} From memory, Mueller didn't
want to call those conflicts "wars," though; he saw those more as
"opportunistic predation" (That's the reason the cover of his book, The
Remnants of War, features an image---from the Balkan conflict in
1991---of a thug swigging from a bottle.)¶ [Then [9/11 came
along]{.mark} and **sideswiped that whole debate**. The nasty little
wars of the 1990s didn't stay in faraway places. [A **superpower**
]{.mark}**got up and [marched]{.mark} off [to
war]{.mark}**]{.underline}---albeit a war against al Qaeda, its
supporters, and all its works. [Somewhere]{.underline} along the line
[the mission became conflated with a host of]{.underline} other
[problems, and Washington ended up obsessing about the Global War on
Terror]{.underline} for longer than it probably should have done. But
[[Washington's behavior]{.mark} at least [answered]{.mark} one
question]{.underline} related to the Big One: [**did [great
powers]{.mark} still [go to war]{.mark}? Yes**. Now, the question still
unanswered]{.underline}---unanswered since 1945 if you think major war
has to be hot; unanswered since 1991, if you think major war can be
cold---[is whether or not major powers still go to war with each
other.¶]{.underline} Psychologist Steven [Pinker]{.underline} has
[recently argued]{.underline} that [the better angels of our nature are
making us turn away from violence. I'm not wholly convinced]{.underline}
by his argument---the [better angels]{.underline} of our nature **[seem
pretty militant to me, and always have been.]{.underline}** (See
Ephesians, 6:12.) But academic research from a few decades back suggests
that great-power wars against each other aren't common. Jack Levy in his
research on war in the international system between 1495 and 1975 found
only nine of what he would call "world wars"---wars where almost all
great powers were involved. Much more commonly, he found "interstate
wars"---113 of which engaged a great power.¶ I cite those figures to
underline two points. First, if world wars are rare, maybe we don't need
special explanations to say why there hasn't been one since 1945 (hot)
or 1991 (cold). Second, that definition of major war is still a
problem.¶ [Let's put aside the academic arguments and look straight at
the case that most worries us. [Is]{.mark} a great-power [war
between]{.mark} the [U.S.]{.mark} and [China]{.mark}
possible]{.underline}? I think we could answer that question directly:
possible, **[[yes]{.underline}]{.mark}**; likely, no. [Great
powers]{.underline}, especially nuclear-armed ones, [don't go to war
with each other lightly. But **[sometimes wars happen]{.mark}**. And
they aren't accidents. **[They're about international order]{.mark}**.
They're about]{.underline}, as Raymond Aron said, [the **life and death
of states.** And the **principal** reason for fighting them is that not
doing so looks like a worse alternative.¶]{.underline} [**Moreover**,
the paths to war---including]{.underline} rare [major-power war---are
not reserved solely for conventionally-armed states.]{.underline} [Where
both powers are nuclear-armed we should]{.underline} expect a conflict,
even one at the lower rungs of the escalation ladder, to be fought with
a high degree of [political control]{.underline}, and an understanding
that the objectives of the conflict are limited. Naturally, [it would
help if both sides shared a common understanding of where the firebreaks
were]{.underline} between conventional and nuclear conflict, and already
had in place a set of crisis-management procedures, [but **it's possible
that neither of those conditions might exist**]{.underline}. (Neither
would prevent a war, but both would provide a better sense of the likely
escalation dynamics of a particular conflict.) [Indeed, it's [because
major war **is possible**]{.mark} that [we retain]{.mark} such a keen
[interest in war termination]{.mark}. Unconstrained escalation **doesn't
lead to a happy place.**]{.underline}

**Yes great power war\-\--rising geopolitical rivalries.**

**Brands & Feaver 2017**---Hal Brands is the Henry A. Kissinger
Distinguished Professor of Global Affairs at Johns Hopkins // Peter
Feaver is Professor of Political Science and Public Policy at Duke and
the director of the American Grand Strategy Program and the Triangle
Institute for Security Studies \["Stress-Testing American Grand
Strategy," *Survival*, Vol. 58, No. 6, December/January, p. 104-105\]

[[This **assumption** is]{.mark} now [being
**tested**]{.mark}]{.underline}, however, [as [the spectre of greatpower
war **revives**]{.mark}]{.underline}. [[Russia and
China]{.underline}]{.mark} -- two key powers [that were never fully
reconciled to the post-Cold War order]{.underline} -- [[are]{.mark} now
**[pushing back]{.mark}** against that order [more **assertively** than
ever]{.mark} before]{.underline}. Russian President Vladimir
[[Putin]{.mark} has [used force to halt the]{.mark} feared [spread of
Western influence]{.mark} and institutions into the former Soviet
space]{.underline}. [He]{.underline} has also [used Russia's revived
military power [to intimidate US allies]{.mark} in the Baltics and
Eastern Europe, [and to harass]{.mark} US and [NATO]{.mark} forces in
international waters and airspace]{.underline}.
[[China]{.underline}]{.mark}, as noted previously,
[[is]{.underline}]{.mark} likewise [[using]{.mark} military and
paramilitary [forces to coerce US allies]{.mark}, to [adjust maritime
boundaries]{.mark} by force [and]{.mark} to [exert pressure on
neighbours]{.mark} from Japan to Vietnam]{.underline}.

[[Both]{.mark} Moscow and Beijing]{.underline}, moreover, [[are
developing **warfighting capabilities** and strategies]{.mark} designed
[to deny Washington access]{.mark} to their 'nearabroads', [and]{.mark}
to [prevail in a]{.mark} limited military [conflict with
the]{.mark}]{.underline} [[U]{.underline}]{.mark}nited
[[S]{.underline}]{.mark}tates [and its allies]{.underline}. As one US
Navy official has noted, [Chinese forces have been training for a 'short
sharp war' with Japan]{.underline} -- [and]{.underline} presumably, by
extension, [with America]{.underline} as well.34 For its part, [Moscow
has regularly staged major military exercises along NATO's eastern
flank, and has re-emphasised nuclear weapons in its rhetoric and
planning]{.underline}.35 In other words, [[neither Russia nor China is
acting]{.mark} -- or talking -- [like it believes that great-power war
is obsolete]{.mark}]{.underline}. [And [neither]{.mark}]{.underline},
for that matter, [[is the U]{.underline}]{.mark}nited
[[S]{.underline}]{.mark}tates, [as the Pentagon invests in a Third
Offset Strategy meant to re-establish American military dominance
vis-à-vis great-power rivals]{.underline}.36

At present, few analysts believe that either Russia or China wants a war
with Washington, and there are still powerful brakes on the possibility
of great-power conflict. But it is clear that [[America]{.mark} once
again [has **great-power rivals**]{.mark}, that those rivals are
[increasingly willing to **assert** themselves]{.mark} even [at risk
of]{.mark} **heightened [geopolitical tensions]{.mark}**, and that
[**the risk** of great-power conflict has]{.mark} therefore [risen to a
level **higher** than at any time since]{.mark}]{.underline}
19[[89]{.underline}]{.mark}. As the [US National Military Strategy
warned in 2015]{.underline}, 'Today, [the probability of US involvement
in interstate war with a major power is]{.underline} assessed to be low
but [growing]{.underline}.'37 If great-power geopolitical competition
continues to intensify in the coming 10--20 years -- as most
commentators expect it will -- American [**[assumptions]{.mark}** [about
the obsolescence of]{.mark} major-power [war]{.mark}]{.underline}, and
the striking great-power peace that has characterised the post-Cold War
era, [[will only be **further challenged**]{.underline}]{.mark}. We may
find that [[the **seeming respite** from history]{.mark} that
accompanied the end of the Cold War [is]{.mark} finally **[coming to an
end]{.mark}**; the world]{.underline}, and the United States, [may find
itself heading 'back to the future' of international
affairs]{.underline}, as John Mearsheimer predicted a quarter-century
ago.38

**War still likely \-\-- the world is more dangerous now than during the
Cold War.**

Paul **Miller**, 12/20/**2011**. Assistant professor of international
security studies at the National Defense University, former director for
Afghanistan on the National Security Council and political analyst in
the U.S. intelligence community, specializing in South Asia. "[How
Dangerous is the World? Part
IV](http://shadow.foreignpolicy.com/posts/2011/12/19/how_dangerous_is_the_world_part_iv),"
Foreign Policy,
http://shadow.foreignpolicy.com/posts/2011/12/19/how_dangerous_is_the_world_part_iv.

In my
[previous](http://shadow.foreignpolicy.com/posts/2011/12/16/how_dangerous_is_the_world_part_i_by_paul_miller)
[three](http://shadow.foreignpolicy.com/posts/2011/12/16/how_dangerous_is_the_world_part_ii)
posts, I argued that **[[the world today is more dangerous than]{.mark}
it was during [the Cold War]{.mark} because [the threat from Russia and
China is still present]{.mark}, on top of which [we face new threats
from new nuclear autocracies hostile to the U]{.mark}nited
[S]{.mark}tates, [including North Korea]{.mark}, soon [Iran,]{.mark}
[and]{.mark} possibly [Pakistan]{.mark}]{.underline}**[.¶]{.mark} **[In
addition to the old-fashioned state-centric threats of hostile nuclear
powers, [the U]{.mark}nited [S]{.mark}tates now [faces a whole new
category of threats]{.mark}]{.underline}** that simply did not exist
during the Cold War: **[the threats that come [when state failure meets
globalization, when non-state actors can operate with impunity outside
the]{.mark} write of any [law but act with global reach]{.mark} because
of new technology]{.underline}**. These are the threats that are the
current fads of IR and security studies: pirates, organized crime, drug
cartels, human traffickers, WikiLeaks, hackers, the global Islamist
\"pansurgency,\" and, yes, terrorists. (Throw in pandemic disease and
ecological disaster and you get all the research funding you want.)¶
There is nothing new about the existence of many of these actors, of
course. Pirates and terrorists have existed for centuries. However,
their ability to present an immediate and large-scale threat to the
United States is new, or at least greater than during the Cold War.
Travel and communication is easier and weapons technology is more
lethal, state failure is more widespread (giving them more space to
operate with impunity), while U.S. and allied border, port, and
infrastructure security has not kept up.¶ I earlier argued that the
faddish, new-fangled theories about non-state actors were overstated.
They are, but that doesn\'t mean they\'re completely wrong. Osama bin
Laden and Julian Assange clearly did massive and irrevocable harm to the
United States in ways literally inconceivable for a non-state actor
during the Cold War; the same may be true of the drug gangs in Mexico
today. Coupled with the United States\' almost complete lack of homeland
security, and there is a very real possibility of large-scale, massive,
direct harm to the U.S. homeland from a globalized non-state actor.¶ The
preeminent threat of this type is, of course, the global campaign by
violent Islamist militants and terrorists to eject the \"west\" from
\"Muslim lands,\" overthrow secular governments and replace them with
Islamic regimes, and establish the supremacy of their brand of Islam
across the world. (I agree here with David Kilcullen\'s
[characterization](http://smallwarsjournal.com/documents/kilcullen.pdf)
of the conflict as a global insurgency). Violent Islamist movements have
done most of their direct damage to people and states across the Middle
East, North Africa, and South Asia. But those attacks certainly don\'t
make the world safer for the United States, nor would their victory in,
for example, Pakistan or Saudi Arabia. And the movement has, of course,
directly attacked the United States and our European allies. Note that
violent Islamist groups-whether al Qaida or Hamas or Hezbollah or al
Shabaab or Lashkar-e Taiba-typically flourish in and around weak and
failing states.¶ The only thing comparable to the global proliferation
of Islamist insurgencies and terrorist movements over the last two
decades was the Soviet Union\'s sponsorship of communist insurgencies
around the world during the Cold War. But the Islamist insurgencies are
likely to be more resilient, harder to defeat, and more dangerous
because they are decentralized, because their ideology is not linked to
the fate of one particular regime, because globalization has made it
easier for them to operate on a global scale, and because of the higher
risk that Islamists will acquire and use weapons of mass destruction
since they are not accountable to a deterable sponsoring power.¶ Even
setting the threat from violent Islamism aside, a host of other
non-state actors threaten the world order and make American leadership
more costly. In fact, **[[the aggregate effect of state failure]{.mark}
multiplied across scores of states across the world [is so great that
\"failed states may eventually present a systemic risk to the liberal
world order, of which the Un]{.mark}ited [S]{.mark}tates [is the
principal architect]{.mark} ////]{.underline}** **[and
beneficiary]{.underline}**,\" as I argue in the [current issue of
PRISM](http://www.ndu.edu/press/how-to-fix-failed-states.html). State
failure and the rise of non-state actors-a problem non-existent during
the cold war-is a threat to American national security.¶ Conclusion ¶
**[Essentially, [the U]{.mark}nited [S]{.mark}tates thus [faces two
great families of threats today]{.mark}: first, [the nuclear-armed
authoritarian powers]{.mark}, of which there are at least twice as many
as there were during the Cold War; second, [the aggregate consequences
of state failure and the rise of non-state actors in much of the
world]{.mark}]{.underline}**, which is a wholly new development since
the Cold War. **[On both counts, [the world is more dangerous
than]{.mark} it was before 1989]{.underline}**. Essentially take the
Cold War, add in several more players with nukes, and then throw in
radicalized Islam, rampant state failure, and the global economic
recession, and you have today.¶ **[I recognize that [the world doesn\'t
feel as dangerous as it did during the Cold War]{.mark}]{.underline}**.
During the Cold War we all knew about the threat and lived with a
constant awareness-usually shoved to the back of ours minds to preserve
our sanity-that we might die an instantaneous firey death at any moment.
We no longer feel that way. ¶ **[[Our feelings are wrong. The Cold War
engaged our emotions more because it was simple,]{.mark} easily
understood, and, as an ideological contest, demanded we take sides and
laid claim to our loyalties. [Today\'s environment is more complex and
many-sided]{.mark} and [so it is harder to feel the threat the same way
we used to. Nonetheless, the danger is
real]{.mark}]{.underline}**[.]{.mark}

**Yes it's nuclear and escalates---multiple warrants and empirics
provide a strong basis for our predictions**

**Lieber and Press 2013** \[Keir A. Lieber¶ ¶ Associate Professor,
Edmund A. Walsh School¶ ¶ of Foreign Service, Georgetown University¶ ¶
Daryl G. Press¶ ¶ Associate Professor of Government, Dartmouth College¶
¶ Coordinator of War and Peace Studies at the John Sloan ¶ ¶ Dickey
Center Spring 2013 Strategic Studies Quarterly "The New Era of Nuclear
Weapons, Deterrence, and Conflict"
http://www.au.af.mil/au/ssq/digital/pdf/spring_13/lieber.pdf\]

A second set of arguments stems from the problem of nuclear escalation
and the future of the US nuclear arsenal. Our main claim is that
[deterring nuclear conflict will be much more difficult in the coming
decades]{.underline} ¶ than many analysts realize. [[As nuclear weapons
proliferate]{.mark}, [it becomes]{.mark} ¶ increasingly [likely]{.mark}
that [the United States will find itself in conventional conflicts with
nuclear-armed adversaries]{.mark}]{.underline}. [Those [adversaries
understand ¶ the consequences of losing a war to the United
States]{.mark}---prison or death]{.underline} ¶ typically awaits enemy
leaders.¶ 7¶ [[Coercive nuclear escalation]{.mark} [as a means ¶ of
creating stalemate]{.mark} and remaining in power [is one of the only
trump ¶ cards]{.mark} available]{.underline} to countries fighting the
United States.¶ Some [analysts might scoff at the notion that a rational
leader would ¶ use nuclear weapons against a superpower]{.underline}
like the United States. But ¶ [that retort conflates the logic of
peacetime deterrence with the logic ¶ of war, and it ignores
history]{.underline}. During peacetime, almost any course of ¶ action is
better than starting a nuclear war against a superpower. But ¶ [during
war]{.underline}---when that superpower's planes are bombing command and
¶ leadership sites, and when its tanks are seizing territory---[the
greatest ¶ danger may be to refrain from escalation and let the war run
its course]{.underline}. ¶ [[Leaders of weaker
states]{.underline}]{.mark}---those [unlikely to prevail on the
conventional ¶ battlefield---[face life-and-death pressures to compel a
stalemate]{.mark}]{.underline}. And ¶ [[nuclear weapons provide]{.mark}
a better means of [coercive escalation]{.mark} than ¶ virtually any
other]{.underline}.¶ The notion of countries escalating conflict to
avoid conventional defeat may sound far-fetched, but it is well grounded
in history. [When ¶ nuclear-armed states face overwhelming conventional
threats]{.underline}---or worry ¶ about the possibility of catastrophic
conventional defeat---[they often ¶ adopt coercive escalatory doctrines
to deter war]{.underline} or stalemate a conflict ¶ that erupts.
[Pakistan openly intends to use nuclear weapons to counter ¶ an
overwhelming conventional Indian invasion. Russia claims it needs ¶
theater nuclear weapons to counter NATO's conventional
advantages]{.underline}. ¶ Israel expects to win its conventional wars
but retains the capability for ¶ nuclear escalation to prevent conquest
in case its conventional forces ¶ suffer a catastrophic defeat. ¶ The
discussion of coercive nuclear escalation should sound familiar ¶ to
Western analysts, as [[it was NATO's strategy for three
decades]{.underline}.]{.mark} From ¶ the mid 1960s until the end of the
Cold War, NATO planned to deter ¶ war, and stalemate it if necessary,
through coercive nuclear escalation. ¶ NATO understood that---by the mid
1960s---it could no longer win a ¶ nuclear war against the Soviet Union,
but it still based its national security ¶ strategy on coercive
escalation because it believed Warsaw Pact conventional forces were
overwhelming.¶ In short, [the escalatory dynamics that existed during
the Cold War exist ¶ today]{.underline}---and they are just as powerful.
[States still face the same critical ¶ national security problem they
faced]{.underline} during the Cold War and [throughout history: namely,
how to prevent stronger countries from conquering them. The high-stakes
poker game of international politics has not ¶ ended;]{.underline} the
players and the cards dealt have merely changed. Those who ¶ were weak
during the Cold War are now strong, and another set of ¶ militarily
"weak" countries---such as North Korea, Iran, Pakistan, and ¶ even China
and Russia---now clutch or seek nuclear weapons to defend ¶ themselves
from overwhelming military might, just as NATO once did.¶ [What can the
United States do to mitigate the problem of escalation? ¶ Ideally, it
should avoid wars against nuclear-armed enemies. But that ¶ option may
not be possible given current US foreign policy and
alliances]{.underline}. [[War may erupt on the Korean
Peninsula]{.underline}]{.mark}, ensnaring the United ¶ States in a
battle against a desperate nuclear-armed foe. In the future, ¶
[Washington may fight [a nuclear-armed Iran]{.mark} over sea lanes in
the Persian ¶ Gulf. And [the U]{.mark}nited [S]{.mark}tates
[could]{.mark} someday [be dragged into war by a ¶ clash between Chinese
and Japanese naval forces]{.mark}]{.underline} near disputed islands. ¶
Alternatively, the United States could seek to develop conventional ¶
war plans designed to wage limited war without triggering enemy
escalation. Development of alternative plans is sensible, but [history
shows ¶ that [wars are difficult to contain]{.mark}, and [modern
conventional warfare is ¶ inherently
escalatory]{.mark}]{.underline}[.]{.mark} ¶ A third option to mitigate
these dangers is to retain, and improve, ¶ US nuclear and nonnuclear
counterforce capabilities. [[Fielding powerful]{.mark} ¶ counterforce
[weapons may help deter adversary escalation during war---¶ by
convincing enemy leaders to choose a "golden parachute]{.mark}" rather
than ¶ escalation---[and would give US leaders better]{.mark} response
[options if deterrence failed]{.mark}]{.underline}. In particular, [the
United States should]{.underline} retain and [develop ¶ nuclear weapons
that bring together]{.underline} three key characteristics of
counterforce: [high accuracy, flexible yield, and prompt
delivery]{.underline}.¶ To be clear, sharpening US counterforce
capabilities is not a "solution" ¶ to the problem of adversary nuclear
weapons. Although, ceteris paribus, ¶ it would be better to have
excellent counterforce capabilities than to lack ¶ them, given enough
time and motivation, many countries could greatly ¶ increase the
survivability of their forces. But [given the plausible prospect ¶ that
the United States will find itself waging war against nuclear-armed ¶
states, and [given the powerful incentives of US adversaries to]{.mark}
brandish ¶ or [use nuclear weapons]{.mark}, [it would be reckless to
proceed without]{.mark} a full ¶ suite of modern
[nuclear]{.mark}]{.underline} and nonnuclear [counterforce
[capabilities]{.mark}]{.underline}.

## NATO Cohesion Advantage

### NATO Cohesion -- Warming Impact

**Every bit of mitigation matters.**

**Nuccitelli 12** (Dana Nuccitelli is an environmental scientist at a
private environmental consulting firm in the Sacramento, California
area. This piece was originally published at Skeptical Science and was
reprinted with permission. "Realistically What Might The Future Climate
Look Like?" ThinkProgress
http://thinkprogress.org/climate/2012/09/01/784931/realistically-what-might-the-future-climate-look-like/)

This is Why Reducing Emissions is Critical

[[We're not yet committed to surpassing 2°]{.mark}C]{.underline} global
[warming]{.underline}, [[but]{.underline}]{.mark} as Watson noted,
[**[we are]{.mark} quickly [running out of time]{.mark}** to
realistically give ourselves a chance to stay below that 'danger
limit']{.underline}. However, 2°C is not a do-or-die threshold.
**[[Every bit of]{.mark} CO2 [emissions]{.mark} we can reduce [means
that much avoided future warming]{.mark}]{.underline}**, [[which
means]{.mark} that much [avoided]{.mark} [climate change]{.mark}
impacts]{.underline}. As Lonnie Thompson noted, [[the more]{.mark}
global warming [we]{.mark} manage to [mitigate]{.mark}, [the
less]{.mark} adaption and [suffering]{.mark} we will be forced to cope
with [in the future.]{.mark}]{.underline}

Realistically, based on the current political climate (which we will
explore in another post next week), limiting global warming to 2°C is
probably the best we can do. However, [there is a big difference between
2°C and 3°C]{.underline}, between 3°C [and 4°C]{.underline}, and
**[[anything greater than 4°]{.mark}C [can]{.mark} probably accurately
[be]{.mark} described as [catastrophic]{.mark}]{.underline}**,
[[since]{.mark} various [tipping points are]{.mark} expected to be
[triggered]{.mark} at this level.]{.underline} Right now, **[we are on
track for the catastrophic consequences]{.underline}** ([widespread
[coral mortality]{.mark}, [mass extinctions]{.mark}, [hundreds of
millions of people]{.mark} adversely [impacted]{.mark} by droughts,
floods, heat waves]{.underline}, etc.). **[[But we're not
stuck]{.underline}]{.mark}** on that track just yet, and [[we need
to]{.mark} move ourselves **[as far off]{.mark}** of it [as
possible]{.mark} by reducing our greenhouse gas emissions **as soon and
as much as possible**.]{.underline}

There are of course many people who believe that the planet will not
warm as much, or that the impacts of the associated climate change will
be as bad as the body of scientific evidence suggests. That is certainly
a possiblity, and we very much hope that their optimistic view is
correct. However, what we have presented here is the best summary of
scientific evidence available, and it paints a very bleak picture if we
fail to rapidly reduce our greenhouse gas emissions.

[If we continue forward on our current path, catastrophe is not just a
possible outcome, it is the most probable outcome]{.underline}. And an
[intelligent [risk management]{.mark}]{.underline} approach [[would
involve]{.mark} taking steps to prevent a catastrophic
scenario]{.underline} if it were a mere possibility, let alone the most
probable outcome. [This is especially true since]{.underline} the most
important component of the solution -- [[carbon
pricing]{.underline}]{.mark} -- **[can be implemented at a relatively
low cost]{.underline}**, [and a far lower cost than trying to adapt to
the climate change consequences we have discussed here]{.underline}
(Figure 4).

**Not inevitable -- cuts now have an IMMEDIATE effect**

**Desjardins 13** (Cléa, member of Concordia university Media Relations
Department, academic writer, citing Damon Matthews; associate professor
of the Department of Geography, Planning and Environment at Concordia
University, PhD, Member of the Global Environmental and Climate Change
Center, "Global Warming: Irreversible but Not Inevitable,"
http://www.concordia.ca/now/what-we-do/research/20130402/global-warming-irreversible-but-not-inevitable.php)

[[Carbon dioxide emission cuts will **immediately affect** the rate of
future global warming]{.underline}]{.mark} Concordia and MIT researchers
show Montreal, April 2, 2013 -- [There is a persistent misconception
among both scientists and the public that there is a delay between
emissions of carbon dioxide]{.underline} (CO2) [and the climate's
response to those emissions]{.underline}. This misconception has led
policy makers to argue that CO2 emission cuts implemented now will not
affect the climate system for many decades. [This **erroneous line of
argument** makes the climate problem **seem more intractable** than it
actually is]{.underline}, say Concordia University's Damon Matthews and
MIT's Susan Solomon in a recent Science article. The researchers show
that [[**immediate decreases** in CO2 emissions would in fact result in
an **immediate decrease** in the rate of climate
warming]{.underline}]{.mark}. Explains Matthews, professor in the
Department of Geography, Planning and Environment, "[If we can
successfully decrease CO2 emissions in the near future, this change will
be felt by the climate system when the emissions reductions are
implemented]{.underline} **[-- not in several decades]{.underline}**.\"
"[The potential for a **quick climate response** to prompt cuts in CO2
emissions opens up the possibility that the climate benefits of
emissions reductions would occur on the same timescale as the political
decisions themselves.]{.underline}" In their paper, Matthews and
Solomon, Ellen Swallow Richards professor of Atmospheric Chemistry and
Climate Science, show that [the onus for slowing the rate of global
warming falls squarely on current efforts at reducing CO2 emissions, and
the resulting future emissions that we produce]{.underline}. This means
that there are critical implications for the equity of carbon emission
choices currently being discussed internationally. Total emissions from
developing countries may soon exceed those from developed nations. But
developed countries are expected to maintain a far higher per-capita
contribution to present and possible future warming. "[This]{.underline}
disparity [clarifies the urgency for low-carbon technology investment
and diffusion]{.underline} to enable developing countries to continue to
develop," says Matthews. "[[Emission **cuts made now** will have an
**immediate effect** on the rate of global
warming]{.underline},"]{.mark} he asserts. "I see more hope for averting
difficult-to-avoid negative impacts by accelerating advances in
technology development and diffusion, than for averting climate system
changes that are already inevitable. Given the enormous scope and
complexity of the climate mitigation challenge, [clarifying these points
of hope is critical to motivate change]{.underline}."

**Adaptation fails**

**JRC 16 (**Joint Research Centre is the European Commission\'s science
and knowledge service which employs scientists to carry out research in
order to provide independent scientific advice and support to EU policy.
August 11, 2016
https://www.sciencedaily.com/releases/2016/08/160811101332.htm)

[Global [change]{.mark} will [strike]{.mark}]{.underline}
[[the]{.underline}]{.mark} oldest and [[most complex]{.mark}
[ecosystems]{.mark} of the world [hardest]{.mark}, [regardless
of]{.mark} their [past stability]{.mark}]{.underline}. This alarming
finding is reported in a JRC-led article published in *Nature
Communications*. The authors hypothesized that [invasive
species]{.underline}, the [[warming]{.mark} climate and environmental
degradation]{.underline} have [[altered]{.mark} natural
[habitats]{.mark} **[so deeply]{.mark}** that species
[adaptation]{.mark} to historical conditions [may not be helpfu]{.mark}l
under these new circumstances]{.underline}. Interestingly, the authors
[found]{.underline} independent [[support]{.underline}]{.mark} for this
hypothesis [[from]{.mark} [both]{.mark} **[computer
simulations]{.mark}** [and]{.mark} **[real-world
data]{.mark}**.]{.underline} Starting from a single ancestor digital
organism, the authors let artificial life communities evolve for
hundreds of thousands of generations under different, stable
environmental settings. These simulated communities included both
free-living and \'parasite\' digital organisms that helped researchers
investigate how biodiversity and ecological networks develop over time,
under different environmental conditions. Over several generations, both
hosts and parasites diversified, and their interactions became more
complex. The [authors]{.underline} then investigated how these
communities would respond to different scenarios of biodiversity loss.
They [found]{.underline} that [[when species]{.mark} [become]{.mark}
[extinct]{.mark} in a sequence [consistent with]{.mark} their degree of
adaptation to]{.underline} the [[\'natural\']{.mark} environmental
[conditions]{.mark} within which they had evolved, [their
extinction]{.mark} [has]{.mark} only a [limited
effect]{.mark}]{.underline} on the overall diversity of the community.
[Any [deviation from this]{.mark} pattern [however]{.mark}, may
[trigger]{.mark} **[extinction cascades]{.mark}**]{.underline},
eventually [leading to the **[collapse]{.mark}** of **[the entire
network]{.mark}**. The tendency of consuming species to rely and
specialise]{.underline} (develop in a way most suited to the
environment) [on dependable resources]{.underline} has
[enabled]{.underline} the [evolution of complex systems. This basic
mechanism may have doomed many species to extinction]{.underline} \--
the authors demonstrate it by comparing the results of their artificial
life simulations with several empirical host-parasite networks of
different animal groups. Resources that had been largely available in
the past are now becoming increasingly scarce, putting at risk the
species that rely on them.

## Other Solvency Mechanisms

### Establish an International Framework for use

#### NATO regulations key to global regulation of LAWS -- lack of regulation causes misuse

Brian **Michelson** (Center for European Policy Analysis) "Why NATO
Needs Lethal Autonomous Weapon Standards" February 23, **2021**
https://cepa.org/why-nato-needs-lethal-autonomous-weapon-standards/

[[Lethal autonomous weapon systems will come to dominate warfare in the
coming years. NATO needs to harmonize its approach]{.underline}]{.mark}
to their development and use, or risk being left behind. [[The rapid
weaponization of artificial intelligence]{.underline}]{.mark}, "big
data," social media, robotics, and a host of other technologies
[[presents a clear competitive challenge]{.underline}]{.mark} to NATO,
an alliance with members that exist on a wide spectrum of
military-technological capabilities. [[**The future effectiveness of
NATO will be driven in large part by how it handles these challenges**
from hobbling its ability both to act in unison and to prevail in a
contest of wills]{.underline}]{.mark}. While there are numerous
potential technology gaps, one that will likely only increase is partner
nations' ability and willingness to employ lethal autonomous weapon
systems. These systems will inevitably grow more capable, and more
necessary, in the coming decade. Technological gaps are inevitable
considering the disparities in GDP and military budgets. The United
States accounts for over 70 percent of NATO's overall military spending,
while the next three largest contributors (the United Kingdom, France,
and Germany) provide approximately half of the remaining 30 percent. And
with most NATO nations continuing to fund their militaries at under the
2 percent GDP goal, technological gaps will continue to grow. For
perspective, the 2021 United States Department of Defense research and
development budget is approximately equal to the entire defense outlay
of France and Germany combined. With such a large differential, what can
be done to help enable effective investments in autonomous weapons by
smaller nations? Even more specifically, how can smaller nations provide
capabilities that can integrate into, and contribute to the alliance? To
better invest limited funds, [[now is the time to look at a NATO
standard for lethal autonomous weapons and their ethical
use.]{.mark}]{.underline} While there is no agreed-upon international
definition of lethal autonomous weapons systems, the U.S. Department of
Defense defines them as "weapon system\[s\] that, once activated, can
select and engage targets without further intervention by a human
operator." While these are not Schwarzenegger-style Terminators and
still have a degree of human control over them, [[the technology
enabling these systems is maturing rapidly]{.underline}]{.mark}, and
military necessity will increasingly demand that these systems gain
broader parameters of autonomous action. Yet despite the complexity of
these systems and the inevitability of their proliferation, [[NATO does
not currently have a common standard for their use or
development]{.underline}]{.mark}. In fact, some NATO countries even have
opposing views of how to handle them. NATO standards are designed to
ensure compatibility among weapon systems, communication architecture,
and a host of other warfighting systems. The 7.62mm small arms round is
a good example of this. But what is the 7.62mm equivalent standard for
the development and employment of autonomous weapon systems? This opens
a host of related questions regarding the employment of these systems:
What Identification -- Friend -- Foe (IFF) capability should ground and
air units require to prevent fratricide? What degree of certainty does a
lethal autonomous weapon system require before final engagement? What
level of collateral damage is acceptable? What degree of compatibility
between systems is required? Should all these parameters (and others) be
adjustable, and if so, at what command level? The attendant ethics also
need to be addressed. NATO's experience in Afghanistan was a case study
in the challenges of coalition warfare. Differing risk tolerances, legal
requirements, ethical views, domestic political concerns, and at times
simply combat capability, all combined into to complex policy cocktail
that impeded the effectiveness of combat operations. While modern
militaries have accountability, legal, and ethical systems incorporated
into their command structures, they are not uniform and leaders in
differing militaries have varying degrees of authority. [[The key
questions hinge on two issues: Who gets to decide to employ an
autonomous weapon, and who is responsible should things go
wrong?]{.underline}]{.mark} The Kunduz hospital strike in October of
2015 was driven primarily by human error. Responsibility was fixed on
the chain of command and 16 leaders were disciplined. [[Who will be
responsible if a member nation conducts a NATO-authorized strike and it
goes terribly wrong?]{.underline}]{.mark} [[If this framework is not
thoroughly established ahead of time, not only is it likely that
commanders may hesitate to use this capability, the risk-aversion
inherent in bureaucracies may limit the development of autonomous
weapons that will be needed in future conflicts. In the emerging field
of lethal autonomous weapons, **establishing a common NATO standard for
the development and use of autonomous weapons will help address the
gap** in capabilities among NATO member nations]{.underline}]{.mark}.
[By establishing these standards, nations can ensure that their defense
expenditures on autonomous weapons will create systems that are
interoperable, able to contribute to NATO's capability, and can be
employed within defensible ethical guidelines]{.underline}.

### Regulation

#### Regulation Mech

Frank **Sauer** (Senior Research Fellow, Bundeswehr University Munich)
"Autonomy in weapons systems: playing catch up with technology"
September 29, **2021**
https://blogs.icrc.org/law-and-policy/2021/09/29/autonomous-weapons-systems-technology/

Hence the challenge of regulation is not met by trying to categorically
separate 'LAWS' from 'non-LAWS'. Instead, the challenge is met by
developing a new norm in order to adjust human-machine-interaction on
the battlefield: Who or what -- human or machine -- is supposed to
perform which function on the targeting cycle where and when? Finding
context-dependent and differentiated answers to this question will yield
the desired regulation on how autonomous weapons technologies are
applied in a manner that is ethically acceptable, compliant with IHL and
prudent in terms of the preservation of international security and
stability. Luckily, considerable headway has been made this year, also
due to the recent papers circulated by the Belgian chair of the GGE,
which underlined that the issue under discussion is best characterized
by asking what the circumstances of autonomous target selection and
engagement are within a framework of human command and control.
Convergence is slowly but surely not only taking place in terms of
conceptualization, resulting in much less 'talking past each other' at
the CCW. It is also observable regarding the structure of a possible
regulation which, potentially, could take shape in the fully fleshed-out
'normative and operational framework'. A two-pronged approach combining
prohibitions and regulations is taking shape: First, there are specific
applications of autonomy in the critical functions of weapons systems
that are not acceptable to many members of the international community
and should thus be prohibited. Here, the ICRC as well as the Campaign to
Stop Killer Robots and a recently formed group of ten States at the CCW
especially highlight the targeting of human beings. In addition, the
ICRC as well as many States Parties suggest that uncontrollable
autonomous weapon systems should be ruled out as well due to their
potentially unforeseeable or indiscriminate effects on the battlefield.
Second, when applying force against target profiles other than those
intended to represent humans, such as various military objects, autonomy
in the critical functions is acceptable, but it requires certain limits
and constraints, that is, positive obligations to curb ethical risks,
safeguard IHL compliance and address security and safety concerns. Those
limits and constraints can be temporal, spatial and, generally speaking,
be subsumed under the notion that human control -- no matter if
eventually characterized as 'meaningful', 'substantial' or 'effective'
-- must be preserved by design and in use of a weapons system, even and
especially when it, at times, performs its critical functions
autonomously. The 2021 CCW RevCon and beyond Arguably, a soft 'proto
norm' on weapon autonomy has emerged and socially taken hold already.
After all, in 2021, virtually no one is able to contemplate and discuss
autonomy in a weapon's critical functions without being pointed to the
serious concerns involved, the open letters published by the scientific
community, the ongoing UN debates, emerging domestic legislations, large
bodies of scholarly works in moral philosophy, international law,
political science, and so on and so forth. In other words, the debate
overall, including the UN deliberations on weapon autonomy in Geneva,
has come a long way. The conversation at the CCW in particular has
gotten more productive and constructive recently, with convergence
increasing. That said, especially the regulatory structure sketched
above is far from being universally accepted; nor is the notion that the
next step should be codifying it as a legally binding instrument. The
most recent GGE meeting in August demonstrated that at least a handful
of States Parties are clearly having none of it, this way signaling
their intent to prevent the consensus body from making headway. At the
same time, pressure on the CCW keeps increasing. In light of the
upcoming RevCon, States Parties need to produce tangible results. If
they fail to do so, the volume of calls for moving the process into
another forum will most certainly increase. Then, the CCW would -- once
again -- have served as only an incubator. New, binding international
law is needed. While weapon autonomy presents welcome opportunities in
the optimization of defenses against munitions -- protecting soldiers'
lives -- leaving it unchecked and unregulated will make the world a more
unsafe, uncertain, unstable, and inhumane place. Technology will not
wait. It is time for UN diplomacy to catch up.

### Code of Conduct 

#### Solvency mech

Kyle **Hiebert** (researcher and analyst formerly based in Cape Town and
Johannesburg, South Africa, as deputy editor of the Africa Conflict
Monitor) "Are Lethal Autonomous Weapons Inevitable? It Appears So"
January 27, **2022**
https://www.cigionline.org/articles/are-lethal-autonomous-weapons-inevitable-it-appears-so/

Finding the Middle Ground: Responsible Use. Even in the event that a ban
on killer robots could be reached and somehow enforced, the algorithms
used by autonomous weapons systems to identify, select and surveil
targets are already streamlining and enhancing the use of lethal force
by human actors. Banning hardware without including the underlying
software would arguably be a half measure at best. But governments are
badly struggling with how to regulate AI --- and expanding the scope of
the proposed ban would add enormous complexity to an already stalled
process. Instead, establishing acceptable norms around their use ---
what one US official has called a non-binding code of conduct --- in
advance of broad adoption may represent an alternative means to harness
the potential positives of LAWS while avoiding the most-feared outcomes.
These norms could be based primarily on a shared commitment to avoid
so-called unintended consequences. According to Robert Work, the former
US defence official, LAWS should be totally excluded from systems that
can independently launch pre-emptive or retaliatory attacks, especially
those involving nuclear weapons. A code of conduct could include an
expectation as well to keep autonomous weapons technology out of the
hands of non-state actors. Numerous countries party to the CCW also
believe that there are grounds to extend established international human
rights law, such as the Geneva Convention, to cover autonomous weapons
systems, by applying the law to the human authority that ordered their
use. Some proponents of LAWS agree. These are imperfect solutions ---
but they may prevent dystopian sci-fi fantasies from becoming reality.
One way or another, killer robots are coming.

### CCW ban

#### Solvency mech

Michael T. **Klare** (professor emeritus of peace and world security
studies at Hampshire College and senior visiting fellow at the Arms
Control Association) "Autonomous Weapons Systems and the Laws of War"
March **2019**
https://www.armscontrol.org/act/2019-03/features/autonomous-weapons-systems-laws-war

Out of this process, some clear strategies for limiting these systems
have emerged. The first and most unequivocal would be the adoption under
the CCW of a legally binding international ban on the development,
deployment, or use of fully autonomous weapons systems. Such a ban could
come in the form a new CCW protocol, a tool used to address weapon types
not envisioned in the original treaty, as has happened with a 1995 ban
on blinding laser weapons and a 1996 measure restricting the use of
mines, booby traps, and other such devices.13 Two dozen states, backed
by civil society groups such as the Campaign to Stop Killer Robots, have
called for negotiating an additional CCW protocol banning fully
autonomous weapons systems altogether. Proponents of such a measure say
it is the only way to avoid inevitable violations of international
humanitarian law and that a total ban would help prevent the unintended
escalation of conflict. Opponents argue that autonomous weapons systems
can be made intelligent enough to overcome concerns regarding
international humanitarian law, so no barriers should be placed on their
continued development. As deliberations by CCW member states are
governed by consensus, a few states with advanced robotic projects,
notably Russia, the United Kingdom, and the United States, have so far
blocked consideration of such a protocol. Another proposal, advanced by
representatives of France and Germany at the experts' meetings, is the
adoption of a political declaration affirming the principle of human
control over weapons of war accompanied by a nonbinding code of conduct.
Such a measure, possibly in the form of a UN General Assembly
resolution, would require human responsibility over fully autonomous
weapons systems at all times to ensure compliance with the laws of war
and international humanitarian law and would entail certain assurances
to this end. The code could establish accountability for states
committing any misdeeds with fully autonomous weapons systems in battle
and require that these weapons retain human oversight to disable the
device if it malfunctions. States could be required to subject proposed
robotic systems to predeployment testing, in a thoroughly transparent
fashion, to ensure they were compliant with these constraints.14 Those
who favor a legally binding ban under the CCW claim this alternative
would fail to halt the arms race in fully autonomous weapons systems and
would allow some states to field weapons with dangerous and
unpredictable capabilities. Others say a total ban may not be achievable
and argue that a nonbinding measure of this sort is the best option
available.

## Off- Case Position Answers

### Politics 

#### Plan is bipartisan

Kyle **Hiebert** (researcher and analyst formerly based in Cape Town and
Johannesburg, South Africa, as deputy editor of the Africa Conflict
Monitor) "Are Lethal Autonomous Weapons Inevitable? It Appears So"
January 27, **2022**
https://www.cigionline.org/articles/are-lethal-autonomous-weapons-inevitable-it-appears-so/

National Interests Undermine Collective Action While Turkey may have
been the first to allegedly deploy live killer robots, their
wide-ranging use is likely to be driven by Beijing, Moscow and
Washington. Chinese President Xi Jinping and Russian President Vladimir
Putin both openly loathe the Western-oriented human rights doctrines
that underpin calls to ban killer robots. And **[[despite America's
domestic division and dysfunction, its political class still has a
bipartisan desire for the United States to remain the world's global
military hegemon.]{.mark}]{.underline}** With a GDP just slightly larger
than that of the state of Florida, Russia's inability to compete in a
great power competition economically renders it reliant on exploiting
asymmetric power imbalances wherever possible, including through
furthering its AI capability for military and espionage purposes.
[[Autonomous weapons could be well-suited to secure the resource-rich
but inhospitable terrain of the Arctic]{.underline}]{.mark}, a region
where the Kremlin is actively trying to assert Russia's primacy. The
country is also the world's second-largest arms exporter behind the
United States, accounting for one-fifth of global arms sales since 2016
--- a key source of government revenue and foreign influence. Its recent
anti-satellite weapons test underscores the Kremlin's willingness to
explore controversial weapons technologies too, even in the face of
international condemnation.

### A2 Business Confidence/Economy Disadvantage

#### No link and turn -- AI developments like the plan don't pay off. The plan actually saves money and helps the economy by ending development before the industry crashes 

John **Horgan** (Center for Science Writings at the Stevens Institute of
Technology) Will Artificial Intelligence Ever Live Up to Its Hype?
December 04, **2020**
https://www.scientificamerican.com/article/will-artificial-intelligence-ever-live-up-to-its-hype/

[[There are also signs that investments in AI are not paying
off]{.underline}]{.mark}. Technology analyst Jeffrey Funk recently
examined 40 start-up companies developing AI for health care,
manufacturing, energy, finance, cybersecurity, transportation and other
industries. [[Many of them were not "nearly as valuable to society as
all the hype would suggest]{.underline}]{.mark}," Funk reports in IEEE
Spectrum. [[Advances in AI "are unlikely]{.underline}]{.mark} to be
nearly as disruptive---for companies, for workers, or [[for the economy
as a whole]{.underline}]{.mark}---as many observers have been arguing."
Science reports that "[[core progress in AI has
stalled]{.underline}]{.mark} in some fields," such as information
retrieval and product recommendation. A study of algorithms used to
improve the performance of neural networks found "no clear evidence of
performance improvements over a 10-year period." The longstanding goal
of "general" artificial intelligence, possessing the broad knowledge and
learning capacity to solve a variety of real-world problems, as humans
do, remains elusive. "We have machines that learn in a very narrow way,"
Yoshua Bengio, a pioneer in the AI approach called deep learning,
recently complained in WIRED. "They need much more data to learn a task
than human examples of intelligence, and [[they still make stupid
mistakes]{.underline}]{.mark}." Writing in The Gradient, an online
magazine devoted to tech, AI entrepreneur and writer Gary Marcus accuses
AI leaders as well as the media of exaggerating the field's progress.
AI-based autonomous cars, fake news detectors, diagnostic programs and
chatbots have all been oversold, Marcus contends. He warns that "[[if
and when the public, governments, and investment community recognize
that they have been sold an unrealistic picture of AI's strengths and
weaknesses that doesn\'t match reality, a new AI winter may
commence]{.underline}]{.mark}."

### T - LAWS

#### Definition of LAWS/How LAWS Work

**Future of Life Institute** "AUTONOMOUS WEAPONS: WHAT ARE THEY, AND WHY
DO THEY MATTER?" Summer **2021**
https://futureoflife.org/2021/11/30/an-introduction-to-the-issue-of-lethal-autonomous-weapons/

What are lethal autonomous weapons? Slaughterbots, also called [["lethal
autonomous weapons systems" or "killer robots", are weapons systems that
use artificial intelligence (AI) to identify, select, and kill human
targets without human intervention. Whereas in the case of unmanned
military drones the decision to take life is made remotely by a human
operator, in the case of lethal autonomous weapons the decision is made
by algorithms alone. Slaughterbots are pre-programmed to kill a specific
"target profile." The weapon is then deployed into an environment where
its AI searches for that "target profile" using sensor data, such as
facial recognition. When the weapon encounters someone the algorithm
perceives to match its target profile, it fires and
kills]{.underline}]{.mark}.

#### The complexity of the system is irrelevant -- lethal autonomous weapons systems are defined based on whether humans are in the loop in the decisions making process 

**Congressional Research Service** "Defense Primer: U.S. Policy on
Lethal Autonomous Weapon Systems" November 17, **2021**
https://crsreports.congress.gov/product/pdf/IF/IF11150

U.S. Policy Definitions. There is no agreed definition of lethal
autonomous weapon systems that is used in international fora. However,
Department of Defense Directive (DODD) 3000.09 (the directive), which
establishes U.S. policy on autonomy in weapons systems, provides
definitions for different categories of autonomous weapon systems for
the purposes of the U.S. military. [[These definitions are principally
grounded in the role of the human operator with regard to target
selection and engagement decisions, rather than in the technological
sophistication of the weapon system]{.underline}]{.mark}. [[DODD 3000.09
defines LAWS as "weapon system\[s\] that, once activated, can select and
engage targets without further intervention by a human operator." This
concept of autonomy is also known as "human out of the loop" or "full
autonomy." The directive contrasts LAWS with humansupervised, or "human
on the loop,"]{.underline}]{.mark} autonomous weapon systems, in which
operators have the ability to monitor and halt a weapon's target
engagement. Another category is semi-autonomous, or "human in the loop,"
weapon systems that "only engage individual targets or specific target
groups that have been selected by a human operator." Semiautonomous
weapons include so-called "fire and forget" weapons, such as certain
types of guided missiles, that deliver effects to human-identified
targets using autonomous functions.

### Military Readiness

#### The United States does not currently have LAWS in their inventory. This proves they aren't key to U.S. military protections 

**Congressional Research Service** "Defense Primer: U.S. Policy on
Lethal Autonomous Weapon Systems" November 17, **2021**
https://crsreports.congress.gov/product/pdf/IF/IF11150

Lethal autonomous weapon systems (LAWS) are a special class of weapon
systems that use sensor suites and computer algorithms to independently
identify a target and employ an onboard weapon system to engage and
destroy the target without manual human control of the system. Although
these systems are not yet in widespread development, it is believed they
would enable military operations in communications-degraded or -denied
environments in which traditional systems may not be able to operate.
Contrary to a number of news reports, U.S. policy does not prohibit the
development or employment of LAWS. Although [[the United States does not
currently have LAWS in its inventory]{.underline}]{.mark}, some senior
military and defense leaders have stated that the United States may be
compelled to develop LAWS in the future if U.S. competitors choose to do
so. At the same time, [[a growing number of states and nongovernmental
organizations are appealing to the international community for
regulation of or a ban on LAWS due to ethical
concerns.]{.underline}]{.mark}

#### 
