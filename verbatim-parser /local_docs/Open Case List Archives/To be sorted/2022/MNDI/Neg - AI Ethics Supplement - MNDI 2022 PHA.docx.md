# Neg

### Inherency Resps

#### NATO has already adopted ethical principles for cooperation on AI -- Recent meetings and strategies prove

**NATO Press Release, 2021** \[Oct 22, "NATO releases first-ever
strategy for Artificial Intelligence"
<https://www.nato.int/cps/en/natohq/news_187934.htm> LMSi\]

[NATO Defence Ministers agreed to NATOs first-ever strategy for
Artificial Intelligence (AI).]{.underline} [The strategy outlines how AI
can be applied to defence and security in a protected and ethical way.
As such, it sets standards of responsible use of AI technologies, in
accordance with international law and NATOs values. It also addresses
the threats posed by the use of AI by adversaries and how to establish
trusted cooperation with the innovation community on AI]{.underline}.
[Artificial Intelligence is one of the seven technological areas which
NATO Allies have prioritized for their relevance to defence and
security.]{.underline} These include quantum-enabled technologies, data
and computing, autonomy, biotechnology and human enhancements,
hypersonic technologies, and space. Of all these dual-use technologies,
Artificial Intelligence is known to be the most pervasive, especially
when combined with others like big data, autonomy, or biotechnology. To
address this complex challenge, [NATO Defence Ministers also approved
NATOs first policy on data exploitation.]{.underline} Individual
strategies will be developed for all priority areas, following the same
ethical approach as that adopted for Artificial Intelligence.

### AI Leadership Adv Resps

**No solvency -- DOD bureaucracy, outdated acquisitions and culture all
block AI innovation.**

**Nurkin and Konaev, 2022 - senior fellows at the Center for Strategy
and Security at the Atlantic Council** \[Tate and Margarita, 5-25-2022,
"Eye to eye in AI: Developing artificial intelligence for national
security and defense" Atlantic Council
<https://www.atlanticcouncil.org/in-depth-research-reports/report/eye-to-eye-in-ai/>
ARD\]

[To accelerate AI adoption, the Pentagon must confront its demons: a
siloed bureaucracy that frustrates efficient data-management efforts and
thwarts the technical infrastructure]{.underline} needed to leverage DoD
data at scale; [antiquated acquisition and contracting
processes]{.underline} that inhibit the DoD's ability to bring in
external innovation and transition successful AI technology prototypes
to production and deployment; [and a risk-averse culture at odds with
the type of openness, experimentation, and tolerance for failure known
to fuel innovation.]{.underline}8 [Several efforts are under way to
tackle some of these problems]{.underline}. Reporting directly to the
under secretary of defense, the chief data and artificial intelligence
officer (CDAO) role was recently announced to consolidate the office of
the chief data officer, the Joint Artificial Intelligence Center (JAIC),
and the Defense Digital Service (DDS). This reorganization brings the
DoD's data and AI efforts under one roof to deconflict overlapping
authorities that have made it difficult to plan and execute AI
projects.9 Expanding use of alternative acquisition methods,
organizations like the Defense Innovation Unit (DIU) and the Air Force's
AFWERX are bridging the gap with the commercial technology sector,
particularly startups and nontraditional vendors. [Still, some tech
leaders believe these efforts are falling short, warning that "time is
running out."]{.underline}10

**No solvency -- it is too late to win the AI arms race -- China has
Already won -- experts admit.**

**Faulconbridge, 2021 -- Reuters staff writer** \[Guy Oct 11, Reuters,
"China has won AI battle with U.S., Pentagon\'s ex-software chief says"
https://www.reuters.com/technology/united-states-has-lost-ai-battle-china-pentagons-ex-software-chief-says-2021-10-11/,
BK\]

LONDON, Oct 11 (Reuters) - [China has won the artificial intelligence
battle]{.underline} with the United States [and is heading towards
global dominance because of its technological advances, the Pentagon\'s
former software chief told the]{.underline} Financial
[Times]{.underline}. [China, the world's second largest economy, is
likely to dominate many of the key emerging technologies, particularly
artificial intelligence]{.underline}, synthetic biology and genetics
[within a decade or so, according to Western intelligence
assessments.]{.underline} Nicolas Chaillan, the Pentagon\'s first chief
software officer who resigned in protest against the slow pace of
technological transformation in the U.S. military, said [the failure to
respond was putting the United States at risk.]{.underline} \"[We have
no competing fighting chance against China in 15 to 20
years.]{.underline} [Right now, it's already a done deal; it is already
over in my opinion,\"]{.underline} he told the newspaper. \"[Whether it
takes a war or not is kind of anecdotal.\"]{.underline} [China was set
to dominate the future of the world, controlling everything from media
narratives to geopolitics,]{.underline} he said. [Chaillan blamed
sluggish innovation, the reluctance of U.S. companies]{.underline} such
as Google (GOOGL.O) to work with the state on AI and extensive ethical
debates over the technology. Google was not immediately available for
comment outside business hours. [Chinese companies]{.underline},
Chaillan said, [were]{.underline} obliged to work with their government
and were [making \"massive investment\" in AI without regard to ethics.
He said U.S. cyber defences in some government departments were at
\"kindergarten level\".]{.underline}

**No solvency -- the DOD will not enhance JAIC authority to solve the
case -- empirically, they have fragmented acquisitions.**

**Tarraf, Shelton, Parker 2019 - Senior Information Scientist, Senior
Engineer, and physical Scientist at the RAND Corporation** \[ Danielle
C., William, Edward, et al RAND Cooperation, "The Department of Defense
Posture for Artificial Intelligence",
<file:///Users/MiraAgarwal/Downloads/RAND_RR4229%20(2).pdf>, Acc
6/18/22, M.A.\]

What [DoD needs to do now is continue on that path]{.underline} by
providing the requisite high-level support, visibility, and authorities
(including directive and budget authorities) [to enable the JAIC to
enact change. Doing so would ensure that the JAIC has a chance of
succeeding at its mandate of scaling AI and its impact across
DoD.]{.underline} It would also ensure that DoD's intent, messaging, and
actions are all consistent. Having said that, we recognize that [this
option runs counter to DoD history and precedents, particularly because
of the recent reform leading to the dissolution of the USD for
Acquisition]{.underline}, [Technology, and Logistics]{.underline}, and
subsequent creation of the USD(R&E) and USD(A&S) 3 The DMAG is the
primary civilian-military management forum that supports the Secretary
of Defense and addresses top DoD issues that have resource, management,
and broad strategic and/or policy implications. The DMAG's primary
mission is to produce advice for the DSD in a collaborative environment
and to ensure that the DMAG execution aligns with the Secretary of
Defense's priorities and the planning and programming schedule. The DMAG
is cochaired by the DSD and Vice Chairman of the Joint Chiefs of Staff,
with secretaries of the military departments, chiefs of the military
services, and DoD principal staff assistants holding standing
invitations. See U.S. Department of Defense, Chief Management Officer,
"Deputy's Management Action Group (DMAG)," webpage, undated. 68 [The
Department of Defense Posture for Artificial Intelligence]{.underline}
that [took effect in]{.underline} February [2018.]{.underline} [By
enacting this reform, Congress intentionally weakened the directive
authorities that OSD principals had over the services, and devolved
significant procurement and acquisition authorities back to the
services]{.underline}. We also recognize that it might not be entirely
appropriate to compare DoD with a large company but rather to a large
conglomerate because of the historical role and independence of the
services.

**No solvency -- Alternate causalities -- AI acquisition bureaucracy
will keep the US behind China and Russia.**

**Knapp, 2018 -- Journalist specializing in emerging technologies**
\[Brandon, 10 April, 2018, C4isrNet, "DoD official: US not part of AI
arms race,"
<https://www.c4isrnet.com/it-networks/2018/04/10/dod-official-us-not-part-of-ai-arms-race/>,
6/22/22 MD\]

One of the Pentagon's top acquisition officials believes that [the
United States military is currently on the sidelines of an ongoing
[artificial intelligence arms
race](https://www.c4isrnet.com/it-networks/2018/04/04/how-artificial-intelligence-went-from-an-advantage-to-a-worldwide-threat/)
between global powers]{.underline} such as [Russia and
China]{.underline}. "[There might be an artificial intelligence arms
race, but we're not yet in it]{.underline}," Dr. Michael Griffin, the
under secretary of defense for research and engineering, said April 9 at
the Future Wars conference held in Washington, D.C. "Our adversaries
understand very well the possible future utility of machine learning,"
Griffin continued. "I think it's time we did as well." Griffin's
comments echo some of the language written into [the new [National
Defense
Strategy](https://www.defense.gov/Portals/1/Documents/pubs/2018-National-Defense-Strategy-Summary.pdf)]{.underline}
revealed earlier this year. The strategy prioritizes maintaining a
military advantage over Russia and China and calls for the Pentagon to
increase investment in emerging technologies such as artificial
intelligence. [The National Defense Strategy]{.underline} also [calls
for reforming the business and acquisition practices within the Defense
Department]{.underline}, which Griffin described at the conference as
outdated. When the current acquisition system was designed, the United
States' technological advantage was largely uncontested, Griffin said.
This allowed the Pentagon to engage in a lengthy acquisition process
with no risk of a serious challenge from adversaries. However, those
[defense acquisition practices now put the country at a serious
disadvantage.]{.underline} "[If the decision is \[between\] how can I be
fair to everyone rather than \[do\] what's best, we'll always be
behind,]{.underline}" Griffin said. "[We can either maintain this
process or maintain preeminence, but we probably can't do
both.]{.underline}" Fueling these concerns is the fact that [China's
government,]{.underline} [[unconstrained by]{.underline} such
[bureaucratic]{.underline}
[processes]{.underline}](https://www.c4isrnet.com/home/2018/03/15/how-is-china-developing-ai-technology-so-much-faster-than-the-us/),
has been particularly focused on developing artificial intelligence.
[The Chinese state council recently issued a well-funded [national
strategy](https://www.newamerica.org/cybersecurity-initiative/blog/chinas-plan-lead-ai-purpose-prospects-and-problems/)
designed to harness the power of artificial intelligence with the goal
of making the nation "the world's primary AI innovation center" by
2030.]{.underline}

**No Solvency -- The DOD lacks an AI workforce due to slow hiring and
low pay**

**Tarraf et. al 2019 - Senior Information Scientist at the RAND
Corporation** \[Danielle with William Shelton, Edward Parker, Brien
Alkire, Diana Gehlhaus, Justin Grana, Alexis Levedahl, Jasmin Léveillé,
Jared Mondschein, James Ryseff, "The Department of Defense Posture for
Artificial Intelligence: Assessment and Recommendations", LMSi\]

[DoD struggles to grow and cultivate AI talent. Our interviews suggest a
mixed appreciation for what technical AI talent consists of and which AI
talent is needed. Several entities we interviewed, such as the service
labs, had a clear sense of AI talent needs, but the majority were still
in the beginning stages of such considerations and were more likely to
emphasize contracting out for technical talent.]{.underline} Moreover,
for those that were clear on AI talent needs, it was a challenge to
define the exact knowledge, skills, and abilities they perceived that
were required[. Ultimately, the AI talent needs of DoD (type,38
quantity, and mix) will depend on the broader strategy pursued for
scaling AI, and the extent to which scaling AI will rely on the
development of products in-house as opposed to through contracting and
outsourcing]{.underline}. The skill sets needed for development of
products in-house are significantly different from those needed for
contracting and outsourcing, though all AI talent (technical or
managerial) is difficult to access in the present market. Nonetheless,
the consensus is that [DoD faces stiff competition for AI skills and
expertise, as evidenced by our interviews across academia, industry, and
DoD.39 Many of our DoD interviewees discussed the challenges related to
attracting and recruiting technical talent more generally, and expressed
the belief that AI talent would be no different.]{.underline} In that
spirit, we point to a recent RAND study on career paths for data
scientists within the Defense Intelligence Agency[.40 Interviews across
DoD cited intense competition with the private sector, the limited
ability to compete on salary, and long hiring processes.]{.underline} At
the same ML developers (see section "Industry: Talent" in Appendix C),
that approach will not lead to the development of ML experts. 38 Our
industry interviews highlighted four types of AI talent: experts, ML
developers, application developers, and project or program managers (see
section "Industry: Talent" in Appendix C). 39 Reasons for such stiff
competition include salaries and inability to hire at competitive speed.

**Solvency is too late - China will inevitably win the AI arms race --
we have already lost**

**Nurkin and Konaev, 2022 - senior fellows at the Center for Strategy
and Security at the Atlantic Council** \[Tate and Margarita, 5-25-2022,
"Eye to eye in AI: Developing artificial intelligence for national
security and defense" Atlantic Council
<https://www.atlanticcouncil.org/in-depth-research-reports/report/eye-to-eye-in-ai/>
ARD\]

[In September 2021, the Air Force's first chief software officer,
Nicolas Chaillan, resigned in protest of the bureaucratic and cultural
challenges that have slowed technology adoption and hindered the DoD
from moving fast enough to effectively compete with China. In Chaillan's
view, in twenty years, the United States and its allies "will have no
chance competing in a world where China has the drastic advantage in
population]{.underline}."41 Later, [he added that China has essentially
already won]{.underline}, [saying, "Right now, it's already a done
deal."]{.underline}42 Chaillan's assessment of the United States engaged
in a futile competition with China is certainly not shared across the
DoD, but it reflects what many see as a lack of urgency within the
risk-averse and ponderous culture of the department.

**US cannot be an AI leader -- our most important AI project failed due
to data security and interoperability problems**

**Nurkin and Konaev, 2022 - senior fellows at the Center for Strategy
and Security at the Atlantic Council** \[Tate and Margarita, 5-25-2022,
"Eye to eye in AI: Developing artificial intelligence for national
security and defense" Atlantic Council
<https://www.atlanticcouncil.org/in-depth-research-reports/report/eye-to-eye-in-ai/>
ARD\]

[Among the Pentagon's most important modernization priorities is the
Joint All-Domain Command and Control (JADC2) program, described as a
"concept to connect sensors from all the military services...into a
single network."]{.underline}46 According to the Congressional Research
Service, "JADC2 intends to enable commanders to make better decisions by
collecting data from numerous sensors, processing the data using AI
algorithms to identify targets, then recommending the optimal
weapon---both kinetic and non-kinetic---to engage the target."47 If
successful, JADC2 holds the potential to eliminate silos between service
C2 networks that previously slowed the transfer of relevant information
across the force and, as a result, generate more comprehensive
situational awareness upon which commanders can make better and faster
decisions. Figure 5. The JADC2 Placemat reflects the complexity and
ambition associated with the Department of Defense's JADC2
Implementation Plan. Source: US Department of Defense. AI is essential
to this effort, and the DoD is exploring how best to safely integrate it
into the JADC2 program.48 In December 2021, reports emerged that the
JADC2 cross-functional team (CTF) would start up an "AI for C2" working
group, which will examine how to leverage responsible AI to enhance and
accelerate command and control, reinforcing the centrality of
responsible AI to the project.49 In March 2022, the DoD released an
unclassified version of its JADC2 Implementation Plan, a move that
represented, in the words of General Mark Milley, chairman of the Joint
Chiefs of Staff, "irreversible momentum toward implementing" JADC2.50
However, observers have highlighted several persistent challenges to
implementing JADC2 along the urgent timelines required to maintain (or
regain [advantage]{.underline} in perception, processing, and cognition,
[especially vis-à-vis China. Data security]{.underline} and
cybersecurity, [data-governance]{.underline} and sharing issues,
[interoperability with allies]{.underline}, and issues
[associated]{.underline} [with integrating]{.underline} the service's
[networks]{.underline} have all been cited as [challenges
with]{.underline} [recognizing]{.underline} the ambitious promise of
[JADC2's]{.underline} [approach]{.underline}. Some have also highlighted
that allencompassing ambition as a challenge as well. The Hudson
Institute's Bryan Clark and Dan Patt argue that "the [urgency
of]{.underline} today's [threats]{.underline} and the opportunities
emerging from new technologies [demand that Pentagon]{.underline}
leaders [flip]{.underline} [JADC2's focus]{.underline} from what the US
military services want to what warfighters need."51

**The DOD cannot implement an AI policy because there is no precise
definition of AI to guide programs.**

**Tarraf, Shelton, Parker 2019 - Senior Information Scientist, Senior
Engineer, and physical Scientist at the RAND Corporation** \[ Danielle
C., William, Edward, et al RAND Cooperation, "The Department of Defense
Posture for Artificial Intelligence",
<file:///Users/MiraAgarwal/Downloads/RAND_RR4229%20(2).pdf>, Acc
6/18/22, M.A.\]

Currently, it can catalogue these activities, but it is unclear how
doing so would help [scale AI across DoD]{.underline}. Of course, that
[assumes that what constitutes an AI activity is known. However, it is
not currently clear how the determination of what constitutes an AI
initiative or activity is made, by whom, and whether that determination
is consistent across DoD.]{.underline}8 The JAIC lacks a five-year
strategic road map, and a precise objective allowing it to formulate
one. Our industry interviews (see section "Industry: Organization in
Appendix C) and relevant literature highlight the need for five-year
strategic road maps to execute organizational transformation,9
particularly a transformation of the magnitude envisioned in the DoD AI
strategy and that the JAIC has been tasked with executing. In that
context, [our industry interviews also emphasized the need for an
objective articulated in precise-enough terms to enable the formulation
of such a strategic road map]{.underline} (see section "Industry:
Organization" in Appendix C). DoD experience with technology also
highlights the importance of clearly defined, measurable goals in 8 [We
touched upon this point earlier in Chapter Three while discussing the
definition of AI.]{.underline} Although [it is not clear that enforcing
a DoD-wide definition of AI is]{.underline} either
[feasible]{.underline} or helpful, [the question of how DoD identifies
and tracks AI activities or programs remains an]{.underline} important
[open question]{.underline}. 9 John M. Bryson, Lauren Hamilton Edwards,
and David M. Van Slyke, "Getting Strategic About Strategic Planning
Research," Public Management Review, Vol. 20, No. 3, 2018. 48 The
Department of Defense Posture for Artificial Intelligence enhancing
success (see section "Adoption and Scaling of Unmanned Aircraft Systems"
in Appendix D). [The JAIC's mission]{.underline}, which we have
distilled to scale AI and its impact across DoD, [is too vague to
serve]{.underline} as a five-year objective for [the purpose of this
road map. The JAIC needs a refined objective that is
precise]{.underline}, ambitious, [and potentially feasible]{.underline}
in the time frame, and [that can serve to guide the development of an
agile, strategic road map]{.underline} to include shorter-term
(one-year) goals and metrics to assess progress along these goals. The
existence of a five-year strategic road map would also help focus the
selection of NMIs and justify their relevance to the overall objective
(see "Organization: At OSD Level" in Appendix B).

**Alternate causality -- America's computer industry refuses to
cooperate with the military on AI**

**Scharre, 2019 - Vice President and Director of Studies at CNAS**
\[Paul, May-June, "Killer Apps: The Real Dangers of an AI Arms Race,"
https://omnilogos.com/killer-apps-real-dangers-of-ai-arms-race/6/18/22
MD\]

[Equally alarming for U.S. policymakers is the sharp divide between
Washington and Silicon Valley over the military use of AI. Employees at
Google and Microsoft have objected to their companies\' contracts with
the Pentagon, leading Google to discontinue work on a project using AI
to analyze video footage]{.underline}. China\'s authoritarian regime
doesn\'t permit this kind of open dissent. [Its model of
\"military-civil fusion\" means that Chinese technology innovations will
translate more easily into military gains. Even if the United States
keeps the lead in AI, it could lose its military advantage.]{.underline}
The logical response to the threat of another country winning the AI
race is to double down on one\'s own investments in AI. The problem is
that AI technology poses risks not just to those who lose the race but
also to those who win it.

**Lack of formalized communication between AI builders and users limits
openness and trust necessary for successful AI adoption**

**Tarraf, Shelton, Parker 2019 - Senior Information Scientist, Senior
Engineer, and physical Scientist at the RAND Corporation** \[ Danielle
C., William, Edward, et al RAND Cooperation, "The Department of Defense
Posture for Artificial Intelligence",
<file:///Users/MiraAgarwal/Downloads/RAND_RR4229%20(2).pdf>, Acc
6/18/22, M.A.\]

[Communication channels among the builders---and users--- of AI within
DoD are sparse.]{.underline} For example, one of the takeaways from our
interviews is that [communication among the research organizations
appears to be limited,]{.underline} and when it does occur, it is driven
primarily by personal connections among program managers or researchers
(see section "Advancement and Adoption" in Appendix B). [This sparsity
of communication is inconsistent with the culture of openness and
sharing that was emphasized by our academic and industry interviewees as
a driver of success]{.underline} (see section "Industry: Innovation" in
Appendix C, and section "Academia: Advancement and Adoption" in Appendix
C).24 Likewise, we noted AI RDT&E activities throughout the services,
but our takeaway from the interviews was that visibility into these
activities is limited, both within and across the services and from OSD.
Finally, mechanisms of interactions between the developers 23 Isaac R.
Porche, III, Shawn McKay, Megan McKernan, Robert Warren Button, Bob
Murphy, Katheryn Giglio, and Elliot Axelband, Rapid Acquisition and
Fielding for Information Assurance and Cyber Security in the Navy, Santa
Monica, Calif.: RAND Corporation, TR-1294-NAVY, 2012. 24 We should
emphasize here [that the sparsity of communication appeared to be driven
by the lack of formalized communication channels]{.underline} rather
than an unwillingness to [communicate. 54 The Department of Defense
Posture for Artificial Intelligence]{.underline} (e.g., research
entities) [and users]{.underline} (e.g., warfighters, analytics
officers) [of AI are limited or nonexistent]{.underline}.25 [There are
many potential impediments to users adopting AI technologies. Those
include]{.underline} an inherent resistance to change--- including in
roles and TTPs; concerns about the potential loss of an individual's
value to the organization as a result of the adoption of AI
capabilities; and [lacking trust in the technologies]{.underline}.26
These perceived impediments are not unique to DoD; our interviews in
industry and academia highlighted similar concerns (Appendix C).
Nonetheless, [these are serious concerns, and ones that DoD needs to
address to effectively scale AI.]{.underline} There is a lack of
consensus on the delineation of AI investments within DoD. This finding
points to a set of practical questions that DoD needs to answer: For the
purpose of accounting for AI investments, what counts as an AI activity
and what does not? As is also the case with software, DoD budgets do not
account for AI when it is a small part of a larger platform, making it
hard to track overall spending on AI. We note here that adopting a
DoD-wide definition of AI does not necessarily provide an answer to
these practical problems.27

**JAIC fails -- lack of long term funding certainty undermines industry
support**

**Tarraf, Shelton, Parker 2019 - Senior Information Scientist, Senior
Engineer, and physical Scientist at the RAND Corporation** \[ Danielle
C., William, Edward, et al RAND Cooperation, "The Department of Defense
Posture for Artificial Intelligence",
<file:///Users/MiraAgarwal/Downloads/RAND_RR4229%20(2).pdf>, Acc
6/18/22, M.A.\]

[The lack of longer-term budget commitments might hinder the JAIC's
success]{.underline}. [This observation is not just about the amount of
funding for the JAIC]{.underline}---for which we have no basis to judge
at present--- [but also the horizon, certainty (or lack thereof), and
general trends of funding commitments.]{.underline} [Our insights
gleaned from industry indicate that a sizable, long-term funding
commitment,]{.underline} generally ramping up to accompany the five-year
strategic road map, [is critical to ensuring success in organizational
transformations to enable scaling of AI]{.underline} (see section
"Industry: Organization" in Appendix C). Based on our interactions with
the JAIC, we were unable to determine whether the JAIC is able to submit
budget requests through the programming, planning, budgeting and
execution (PPBE) system as an independent entity, allowing it to request
funds for the Future Years Defense Plan (FYDP) and also allowing
high-level leadership to demonstrate support for the JAIC's mission by
prioritizing these budget requests.

**The US will lose the AI competition -- we lack a coordinating council
and a workforce. We depend on foreign semiconductors and we don't have a
national AI infrastructure.**

**Schmidt 2021 -- Chairman of the National Security Commission on
Artificial Intelligence** \[Eric, March 12, "House Armed Services
Subcommittee on Cyber, Innovative Technologies and Information Systems
and House Oversight and Reform Subcommittee on National Security Hold
Joint Hearing on AI and the National Security Commission",
https://congressional-proquest-com.proxy.lib.umich.edu/congressional/result/congressional/congdocumentview?accountid=14667&groupid=95663&parmId=180DE199B78&rsId=180DE197DA6\]

[We reached a number of overarching judgments, the first is that the
government is not organized, nor resourced to win the technology
competition against a committed competitor and it\'s not prepared to
defend against AI enabled threats and we strongly believe that our
nation needs to be AI ready by 2025, to defend and compete in the coming
era of AI accelerated competition and conflict]{.underline}. So we put
the report into two parts, the first part is Part I: Defending America
In the AI Era, and it\'s fundamentally how the U.S. government can use
AI technologies to protect the American people and our interests. It
focuses on the implications of applications of AI for defense and
security. The second part is Winning the Technology Competition and its
obvious, by the way, that we\--we should win that, recommends government
actions to promote AI innovation, promote national competitiveness and
protect critical U.S. advantages in the larger strategic competition in
China. In the idea of simplifying what we need to get done, we came up
with four priorities area\--priority with a great many details that
you\'ll hear about. The first one is leadership, the\--[the government
isn\'t quite ready for this fight, it\'s not organized in the right way.
We need organizational structures that accelerate the government\'s
integration of AI and the promotion of AI across the
country.]{.underline} [There needs to be]{.underline} something at the
White House, we\'re proposing [a Technology Competitiveness Council
reporting into the vice-president]{.underline} that would precisely
monitor and drive this transformation that we need and by the way, it\'s
not just the government it\'s also in private sector. [Talent as you\'ve
identified, a number of you have\--in your opening comments, there\'s a
huge talent deficit in the government. We need to build new digital
talent pipelines and expand existing programs]{.underline}; we need to
cultivate AI talent nation-wide and ensure the best technologists come
to the U.S. and stay in the U.S. and don\'t go to our competitors. It
seems sort of obvious, but incredibly important to emphasize. In
hardware, the [AI systems are critically dependent upon powerful
hardware and we as a country, are too dependent on semiconductor
manufacturing in East Asia and Taiwan]{.underline}, in particular. Most
cutting-edge plants are produced in a specific plant that\'s 110 miles
from China. That\'s gotta be an issue. [We must revitalize U.S. cutting
edge semiconductor fabs, and implement a national microelectronics
strategy.]{.underline} We state very clearly in our report that the
objective is to stay two generations ahead of the Chinese effort, it
could not be clearer in our view. And the fourth of course, is
innovation. AI research is very expensive. We need the government to
help set the conditions for broad-based innovation across the country,
[we need]{.underline} for example [a national AI research
infrastructure]{.underline}, [so more than the top five companies have
the resources to innovate]{.underline} and in particular, startups and
universities need this facility. And we also need to add, we think, over
five, six, seven years up to \$40 billion dollars in annual funding in
the next five years to cover AI R&D for defense and non-defense
purposes. And as you highlighted, Mr. Chairman in your comments, there
are other things that are crossing edge. the first is partnerships.

### Arms Race Adv Resps

**The military will not become prematurely automated -- the importance
of judgement with available data will ensure human machine teaming**

**Goldfarb and Lindsay, 2022 -- Chair in AI and health care at the Univ
of Toronto, Prof of Cybersecurity at the Georgia Tech** \[Avi, Jon,
2/25/22, <https://doi.org/10.1162/isec_a_00425>, "Prediction and
Judgement: Why Artificial Intelligence Increases the Importance of
Humans in War" 6/18/22, LND\]

We have argued that the strategic environment shapes the quality of
data, and organizational institutions shape the difficulty of judgment,
which gives rise to four different categories of AI performance in
military tasks. Quality data and clear judgment enable "automated
decision-making," which is most feasible for bureaucratically
constrained administration and logistics tasks. Low-quality data and
difficult judgments, which are common in strategy and command tasks,
necessitate "human decision-making." [Clear judgments applied to
low-quality data create risks of "premature automation," especially when
AI systems are authorized to execute fire and maneuver tasks. Quality
data and difficult judgments can be combined in "human-machine teaming,"
which can be used to improve intelligence and planning tasks. We expect
that many, if not most, practical military applications of AI are likely
to fall into this last category.]{.underline} Even highly bureaucratized
tasks that seem to fit in the "automated decision-making" category can
require human judgment, especially when budget and personnel decisions
are at stake or when resource scarcity creates difficult operational
trade-offs. Likewise, highly nuanced command tasks that seem to fit in
the "human decision-making" category can usually be broken down into a
subset of tasks that might benefit from AI decision aids. Most
practitioners who implement military AI systems are aware of the risks
of "premature automation" in fire and maneuver, in part due to
widespread apprehension about "killer robots."[126](javascript:;) [To
determine the appropriate division of labor between humans and machines,
therefore, humans must decide what to predict, and they must create data
policies and AI learning plans that detail who should do what with such
predictions.[127](javascript:;) The dynamic circumstances of military
operations will require ongoing finessing of the human-machine teaming
relationship]{.underline}.

### Solvency Extensions

### \--Extend -- Human Control Fails

#### Humans in the loop cannot prevent accidents -- they can only hope to limit the damage after the fact.

**Scharre, 2016 \-- Vice President and Director of Studies at CNAS**
\[Paul, Feb 2016, Center for New American Security, "Autonomous weapons
and operational risk",
https://s3.us-east-1.amazonaws.com/files.cnas.org/documents/CNAS_Autonomous-weapons-operational-risk.pdf?mtime=20160906080515&focal=none,
Acc 6/21/22, M. A.\]

[Autonomous Weapons]{.underline} and Operational Risk 11 [have a higher
inherent hazard than]{.underline} one on a closed track, [the type of
human control over the car could significantly change the damage
potential]{.underline}. A self-driving car that is equipped with a
steering wheel and brake to allow the human operator to take control and
stop the vehicle (humansupervised autonomy) has, in principle, lower
damage potential than a fully autonomous car where the human is merely a
passenger along for the ride. The speed of interactions matters
significantly, however. Giving [the human operator the
ability]{.underline} to grab the wheel of an autonomous vehicle
traveling at highway speeds in dense traffic[, particularly if the
operator is not paying attention, is merely the illusion of
control.]{.underline} Conversely, a brake and steering wheel on an
autonomous car moving slowly under the supervision of [an attentive
human]{.underline} operator might add real value by allowing the human
to function as an additional fail-safe. The driver [may not be able to
prevent all accidents]{.underline} (after all, humans are not great
drivers even when directly in control of the vehicle), but he or she
could prevent an autonomous car from running rampant, senselessly mowing
down pedestrians. [An unfortunate reality]{.underline} of both
supervised autonomous and even semi-autonomous operation [is that the
human operator may not become aware that the system is failing until
after a failure occurs. A human in the loop]{.underline} or on the loop
[will not]{.underline} necessarily [prevent failures from
occurring]{.underline}. However, the ability of a human to undertake
corrective action can help limit the damage potential of a system if it
fails. Thus, in these circumstances, the human functions as a fail-safe.
The human operator cannot necessarily prevent failure, but he or she can
help ensure that if or when the system fails, the damage is limited.

**No solvency -- there is no agreed upon standard for Human Control
yet.**

**Amoroso and Tamburrini, 2021 - Prof of International Law at the
University of Cagliari and Prof of Philosophy of Science and Technology
at the University of Naples Federico** \[Daniele and Guglielmo, Feb
Italian Journal of International Affairs "In Search of the 'Human
Element': International Debates on Regulating Autonomous Weapons
Systems"
<https://www-tandfonline-com.proxy.lib.umich.edu/doi/full/10.1080/03932729.2020.1864995>
TM\]

The 'weaponisation' of AI and robotics, especially their convergence in
autonomous weapons systems (AWS), is undoubtedly a matter of
international concern. AWS academic and diplomatic debates have revolved
around AWS' distinguishing features and why their destructive force is
especially troublesome from ethical and legal standpoints. Discussions
about steps the international community can take to allay these
normative concerns effectively have progressively been zooming in on the
distinctive role the 'human element' plays in the use of force. Indeed,
[there is broad agreement that the identification of normatively
acceptable human-weapon interactions must constitute the keystone of any
future regulation of AWS.]{.underline} Both the Campaign and [an
increasing number of states maintain that]{.underline} a ruling should
be established under international law that [any weapons system must be
subject to meaningful human control (MHC). This, however, is exactly
where international consensus stops; it is far from clear, even among
those favouring an MHC requirement, exactly what its actual content
should be or, to put it more accurately, what is normatively demanded to
make human control over weapon systems truly 'meaningful'.]{.underline}

#### Human control is ambiguous because the human in control is constantly changing their role. 

**Goldfarb and Lindsay, 2022 -- Chair in AI and health care at the Univ
of Toronto, Prof of Cybersecurity at the Georgia Tech** \[Avi, Jon,
2/25/22, <https://doi.org/10.1162/isec_a_00425>, "Prediction and
Judgement: Why Artificial Intelligence Increases the Importance of
Humans in War" 6/18/22, LND\]

[Human-machine teaming often entails not only task
performance]{.underline} (i.e., balancing the cognitive load across
people and AI) [but also task design]{.underline} (i.e., adjusting the
load as circumstances change). Viewed at a more granular level, [a task
that falls into the human-machine teaming category in our framework
might be disaggregated into subtasks that fall into two of the
framework\'s other categories. That is, human practitioners will have to
partition a complex decision task into either fully automated or fully
human decision-making subtasks. This subdivision requires making mindful
decisions about monitoring and controlling the risks of premature
automation. For example, human-machine teaming in drone operations
involves having both the drone and the drone operators perform certain
tasks autonomously. The drone might automatically perform flying tasks
(i.e., maintaining course and bearing or reacquiring a lost datalink),
while human drone operators might deliberate over legal targeting
criteria.]{.underline} T[he overall partition (i.e., the location of the
human in the loop) should be adjusted over time as conditions change,
which will require humans to be mindful of how the division of labor
between humans and machines relates to the task environment and the
organizational mission]{.underline}. [This balance will be further
complicated by interdependencies across tasks and organizations, data
access, interpretability, and interoperability issues, as well as
competing priorities]{.underline} such as speed, safety, secrecy,
efficiency, effectiveness, legality, cybersecurity, stability,
adaptability, and so on. Importantly, as [figure
1](javascript:;) shows[, the organizational and political institutions
that are exogenous to decision-making tasks establish the priorities for
these different objectives. Humans are the ultimate source of judgment
in all AI systems.]{.underline}

#### Human control fails -- our soldiers are not adequately trained for human/machine teams

**Goldfarb and Lindsay, 2022 -- Chair in AI and health care at the Univ
of Toronto, Prof of Cybersecurity at the Georgia Tech** \[Avi, Jon,
2/25/22, <https://doi.org/10.1162/isec_a_00425>, "Prediction and
Judgement: Why Artificial Intelligence Increases the Importance of
Humans in War" 6/18/22, LND\]

[A fundamental organizational challenge is to recruit, train, and retain
the human talent required for human-machine teaming]{.underline}. We
anticipate that AI systems will increase the influence of junior
personnel, giving more leverage to their judgment and decisions. Yet[,
we also expect that the junior officers, noncommissioned officers,
civilian employees, and government contractors who maintain and operate
AI systems will struggle to understand the consequences of their actions
in complex political situations.]{.underline} Gen. Charles Krulak
highlights the role of "the strategic corporal" on twenty-first-century
battlefields.[112](javascript:;) Krulak argues that operational
complexity makes tactical actions more strategically consequential, for
better or worse, which places a premium on the character and leadership
ability of junior personnel. AI will further increase the burden of
judgment on them. Forward personnel will have to see the predictions
from AI systems, assess whether the data that created the predictions
are reliable, and make value judgments about how and why automated
systems can advance the mission. Furthermore, [AI systems will require
constant reconfiguration and repair as the context of human-machine
teaming changes during actual operations. Military personnel have long
engaged in field-expedient, bottom-up
innovation]{.underline}.[113](javascript:;) We expect personnel will
likewise hack AI systems to improve mission performance, as they
understand it, even as unauthorized modifications put them into conflict
with system configuration managers elsewhere in the
bureaucracy.[114](javascript:;) It is important to emphasize the human
capital requirements of combining a sophisticated understanding of the
politico-military situation with the technical savvy to engineer AI in
the field. The strategic corporal in the AI era must be not only a
Clausewitzian genius but also a talented hacker. This may not be a
realistic requirement. [The importance of human-machine teaming is
increasingly appreciated in organizations that implement AI systems.
Amid all the hype about AI and war, plenty of thoughtful work seeks to
discern the relative advantages of humans and machines and to devise
methods of pairing them together in order to improve
decision-making]{.underline}.[115](javascript:;) As the U.S. Department
of Defense AI strategy states, ["The women and men in the U.S. armed
forces remain our enduring source of strength; we will use AI-enabled
information, tools, and systems to empower, not replace, those who
serve]{.underline}."[116](javascript:;) Yet, the strategy\'s stated goal
of "creating a common foundation of shared data, reusable tools,
frameworks and standards, and cloud and edge services" is more of a
description of the magnitude of the problem than a blueprint for a
solution.[117](javascript:;) As AI creates potential for large-scale
efficiency improvements, it also creates potential for large-scale
collective action problems. New military staff specialties are sure to
emerge to manage data and judgment resources, creating new institutional
equities and integration challenges. Perhaps even more challenging is
the problem of nurturing trust among all the engineers, administrators,
analysts, operators, and lawyers involved in designing, using, and
repairing AI systems.[118](javascript:;) As cheap prediction makes human
judgment more vital in a wide variety of tasks, and as more judgment is
needed to coordinate human-machine teaming, we anticipate that military
bureaucracies will face complicated command decisions about why, and
how, to conjoin humans and machines. Commercial firms that embrace AI
often adjust their boundaries and business models by contracting out
tasks involving data, prediction, and action (e.g., manufacturing,
transportation, advertising, and service provision) while developing
in-house judgment capacities that are too difficult to
outsource.[119](javascript:;) Military organizations, likewise, may find
it advantageous to share specialized resources (sensors, shooters,
intelligence products, and logistics) across a decentralized network of
units, even as they struggle to make sense of it all. AI is thus part of
a broader historical trend that has been described with terms like
"networkcentric warfare," "joint force operations," "integrated
multi-domain operations," and "interagency cross-functional teams." The
whole is more than the sum of its parts, but each part must exercise
excellent judgment in how it leverages shared assets. Historical
experience suggests that military interoperability and shared
sensemaking are difficult, but not necessarily impossible, to
achieve.[120](javascript:;) We thus expect military and political
judgment will become even more difficult, diffused, and geographically
distributed. Indeed, the ongoing involvement of the "strategic corporal"
in conversations about politico-military ends could end up politicizing
the military. In the United States, as Risa Brooks argues, the normative
separation of political ends from military means has some paradoxically
adverse consequences: it enables service parochialism, undermines
civilian oversight, and degrades strategic
deliberation.[121](javascript:;) Greater reliance on AI could exacerbate
all these problems, precisely because AI is a force multiplier that
requires military personnel to exercise greater judgment. Brooks\'s
argument implies that an AI-intensive defense bureaucracy could become
both more powerful and more politically savvy. If machines perform the
bulk of data gathering, prediction, and tactical warfighting, then the
judgments of human engineers, managers, and operators will be highly
consequential, even as ethical questions of accountability become harder
to answer. Some military personnel may be unable to perform at such a
high level of excellence, as attested by the many scandals during the
wars in Iraq and Afghanistan (from targeting errors to prisoner abuse).
Increasing reliance on AI will magnify the importance of leadership
throughout the chain of command, from civilian elites to enlisted
service members.

#### Human control cannot solve for instability -- adversaries can never know if the system's behavior followed human intentions or AI

**Horowitz and Scharre 2021 - Director of the Emerging Capabilities
Policy Office in the Office of the Under Secretary of Defense for Policy
and  Vice President and Director of Studies at CNAS** \[Michael, Paul,
January 12 2021, "AI and International Stability: Risks and Confidence-
Building Measures",
<https://www.cnas.org/publications/reports/ai-and-international-stability-risks-and-confidence-building-measures>,
Acc 6/18/22. M.A.\]

Finally, [autonomous systems raise novel challenges of signaling in
contested areas because of ambiguity about whether their behavior was
intended by human commanders]{.underline}. [Even if the system performs
as intended, adversaries may not know whether an autonomous system's
behavior was consistent with human intent because of the aforementioned
command-and-control issues. This can create ambiguity in a crisis
situation about how to interpret an autonomous system's
behavior]{.underline}. For example, if an autonomous system fired on a
country's forces, should that be interpreted as an intentional signal by
the commanding nation's political leaders, or an accident? This, again,
is not a novel problem; a similar challenge exists with human-commanded
military forces. Nations may not know whether the actions of an
adversary's deployed forces are fully in line with their political
leadership's guidance. [Autonomous systems could complicate this dynamic
due to uncertainty about whether the actions of an autonomous system are
consistent with any human's intended action.]{.underline}

#### Human control cannot solve all risks -- delay in response times increases the chance of mistakes

**Scharre, 2016 \-- Vice President and Director of Studies at CNAS**
\[Paul, Feb 2016, Center for New American Security, "Autonomous weapons
and operational risk",
https://s3.us-east-1.amazonaws.com/files.cnas.org/documents/CNAS_Autonomous-weapons-operational-risk.pdf?mtime=20160906080515&focal=none,
Acc 6/21/22, M. A.\]

[In a semi-autonomous system, after each task the machine stops and
waits for human approval before continuing. Under this control type, the
human operator has the ability to observe the machine's actions in
the]{.underline} [environment and confirm that the behavior is
appropriate before continuin]{.underline}g.7 In supervised autonomous
operation, in principle the human operator has the ability to intervene
if necessary. [In practice, there is likely to be some time delay
between when a failure occurs and when the human is able to actually
correct the behavior of the system. This could occur because of a time
delay in communications or because it may take the human some period of
time to understand that the system is performing inappropriately and
determine the appropriate corrective action.]{.underline} For fully
autonomous systems, the human operator lacks the ability to observe the
autonomous system's behavior and undertake corrective action in
sufficient time if the system fails to perform appropriately. Presumably
at some point in time the human operator will become aware of how the
system performed. For example, a household thermostat is operating
"fully autonomously" while one is away from the home. Once one returns
home, one discovers whether the thermostat was performing as one
intended or not.8 [Thus a key element of risk in autonomous systems is
the time between when a system begins failing]{.underline} (performing
in a manner other than what the human operator intended) [and when the
human operator can undertake corrective action.]{.underline} Even in
fully autonomous systems, presumably the system ceases operation at a
certain point in time once the task is complete and the results of its
actions can be observed. The damage potential of a system depends on its
inherent hazard and the time from failure to corrective action [When it
comes to risk, we are concerned with an autonomous system's damage
potential. Damage potential is the amount of damage an autonomous system
could do, if it failed to perform appropriately, before a human operator
could take corrective actio]{.underline}n. Damage potential depends upon
the inherent hazard of the system---the type of task being performed and
the environment in which it is operating---as well as the control type.
For supervised autonomous systems, [the speed of the system's operation
and any potential time delays also are significant factors]{.underline}.
[A system that in principle has a human on the loop to intervene in the
event of system failure might in practice still have a high damage
potential if the system performs tasks much more rapidly than human
operators can react.]{.underline}

### \--Extend -- No Definition of Autonomous Weapons

#### There is no agreed upon definition of Autonomous Weapons

**Scharre, 2016 \-- Vice President and Director of Studies at CNAS**
\[Paul, Feb 2016, Center for New American Security, "Autonomous weapons
and operational risk",
https://s3.us-east-1.amazonaws.com/files.cnas.org/documents/CNAS_Autonomous-weapons-operational-risk.pdf?mtime=20160906080515&focal=none,
Acc 6/21/22, M. A.\]

[The term autonomous is sometimes reserved for systems that exhibit some
degree of learning, adaptation, or evolutionary behavior]{.underline}.
Others, however, might use the term "autonomous" to refer to [complex
rule-based systems that exhibit goal-oriented behavior,
system]{.underline}s that some might call "automated." Examples of
learning systems include robots that teach themselves how to move around
their environment or the Nest "learning thermostat."9 • Finally, we
sometimes refer [to autonomous systems that are capable of human-level
cognitive tasks, at least for narrow problems, as
"intelligent."]{.underline}

#### No solvency\-- we cannot develop proper AI without first defining AI, which we have not yet done

**Roff, 2019 - research fellow at the Centre for the Future of
Intelligence at Cambridge** \[Heather M., 29 April "The frame problem:
The AI "arms race" isn't one,"
<https://thebulletin.org/2019/04/the-frame-problem-the-ai-arms-race-isnt-one/>,
6/22/22, MD\]

There needs to be a change in thinking about AI. [Those dealing with AI
must insist on greater clarity about its definition. If policy makers
and other leaders are not clear about what the term means and entails,
they cannot possibly formulate best practices and governance
mechanisms]{.underline}. It would help matters if artificial
intelligence discussions were framed in an "AI +" framework, because in
many cases, [AI is merely a tool included in a system involving other
functions or capabilities]{.underline}. The news [media should stop
framing the global artificial intelligence competition as an "arms
race."]{.underline} [This misrepresents the competition going on among
countries.]{.underline} The policy community needs a clear-eyed
appraisal of AI's capabilities and limitations. Without that
orientation, [those who hope to steer research and development in
positive directions will create more problems than they
solve.]{.underline}

### \--Extend -- Civilian Deaths Links

**Military AI reduces civilian casualties -- it can be used to avoid
accidentally attacking hospitals**

**Lewis 2019 - project lead for the DOD's Joint Lessons Learned
studies** \[Larry, "AI Safety: Charting out the High Road", War on the
Rocks Dec 19,
https://warontherocks.com/2019/12/ai-safety-charting-out-the-high-road/
LMSi\]

That last commitment, made in both the strategy and in U.S. government
position papers, is probably the one that draws the most skepticism.
When Hollywood portrays AI and autonomous systems and the use of force,
it is often to show machines running amok and killing innocents, such as
seen in the Terminator series of movies. But [using AI for good in war
is not a fanciful notion:]{.underline} At [CNA]{.underline}, our
analysis of real-world incidents shows specific areas where AI can be
used for this purpose. We [have worked with the U.S. military and others
to better understand the reasons that civilian casualties occur and what
measures can be taken to avoid them]{.underline}. Based on analysis of
underlying causes of over 1,000 incidents, [AI technologies could be
used to better avoid civilian harm in ways including: Monitoring
targeted areas for potential changes in the estimated collateral damage
in order to avoid civilian casualties]{.underline}; [Mining military and
open source data to better identify and reduce the risk to civilian
infrastructure]{.underline} (e.g., power, water) in conflict areas,
helping to avoid longer-term humanitarian impact from the use of force;
[Using image processing techniques to better identify hospitals and
avoid inadvertent attacks]{.underline}; and [Using AI-driven adaptive
learning to improve military training for civilian harm
mitigation.]{.underline} These are just some examples of concrete
applications of AI to promote civilian protection in conflict. [The
Department of Defense could be a leader in this area, and it is easy to
imagine other countries following a U.S. lead]{.underline}. For example,
[many countries lament the frequency of military attacks on hospitals in
recent operations]{.underline}, with a UN Security Council Resolution
passed unanimously to promote protection of medical care in conflict in
2016. [If the United States were to announce it was leading an effort to
use AI to better protect hospitals, it is likely there would be interest
from other countries in cooperating with such an effort.]{.underline}

### Cybernetics K Links

#### Focusing on Human Control ignores the distancing caused by merged human/machine intelligence -- it hides the violence in Human Control, and the inability of regulation to resolve our fears.

**Kalpozos, 2020 - Prof of Law at Harvard** \[Ioannis 3-16-2020, ,
Leiden Journal of International Law, "Double elevation: Autonomous
weapons and the search for an irreducible law of war\",
https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3545332 accessed on
6-18-2022\]

[The major stance in opposition to technological escalation towards full
autonomy is centring on the taboo of delegating life/death decisions to
machines and the innate inability of machines to properly apply
law.]{.underline} A public expression of this position is that of the
Campaign to Ban Killer Robots.144 The campaign is notable in resisting
both superficial techno-optimism and dangerously instrumental
pragmatism. The potential power of mobilization of public opposition
notwithstanding, [the position presents some analytical, strategic and
conceptual shortcomings.]{.underline} [Firstly, the Campaign's approach
starts from an assumption of a fundamental 'change of paradigm'.145 This
does not always appreciate the continuing role of technology in
war]{.underline} and its function in the double elevation described
here. Indeed, I have argued that [the escalation towards full autonomy
and merged heteronomy represents a continuation of an existing
trajectory of distancing through the elevation above one's enemy and
above oneself]{.underline}, albeit with a perhaps justified presentiment
of an 'avalanche', a violent acceleration of pace, out of control.
[Secondly, present practice does not suggest that a prospect of
successful imposition of a ban or moratorium is reali]{.underline}stic.
This is due both to the anticipated military advantage and the
inscription of this process in socioeconomic structures and
expectations, as reflected in the evidence of enthusiastic investment in
the acceleration of this trajectory. Instead, as the analysis above has
suggested, while the absolute of full autonomy (combining kinetic and
cognitive elements) is kept at bay, the ground is constantly prepared.
[Thirdly, the primary focus on preventing full autonomy or, inevitably
anthropomorphized, 'killer robots' is in danger of missing the target.
Autonomy is complemented by increasingly merged heteronomy]{.underline}.
As discussed in Section 2.1 above, merged heteronomy addresses the
logistical limitations of spatially spread human/machine networks.
Crucially, while presenting itself as respecting the moral taboo of
life-and-death delegation, merged heteronomy advances the mechanization
of judgement in pursuit of double elevation. [To the extent that the
maintenance of 'meaningful human control' is primarily focused on
'keeping humans in the loop', it is in danger of ignoring the gradual
change in the nature and function of that very loop. As reliance on
artificial intelligence increases, it is humans who are becoming the
'killer robots']{.underline}. Finally, to the extent that the position
relies on the incompatibility of autonomous weapons with international
humanitarian law, it may be vulnerable to the complicity, discussed in
Section 3.3, of certain strands of legal thinking with an understanding
of knowledge as 'a large store of neutral data'146 and the promise of
the piece-meal resolution of technical legal problems. [It]{.underline}
also [allows one's intuitive angst to be assuaged by]{.underline}
promises of, or indeed [steps towards, the panacea of global
regulation]{.underline}.147 As important as [such
regulation]{.underline} may be in structuring the ambitions of both
state and private actors, it [would not]{.underline}, per se, [address
the most fundamental dangers of the mechanization of
judgement]{.underline}. [Law will neither ban nor regulate away what
causes our angst. To the contrary, it may be adapted to serve mechanized
judgement.]{.underline} If we are to oppose double elevation and the
mechanization of judgement, and hope to use law to this effect, we need
legal thinking to serve this purpose. [Otherwise, all we can do is
surrender to the stance of agnostic abeyance, until the code is
engineered.]{.underline}

#### Autonomous weapons reinforce the idea of perfection through technology that will elevate us above our enemies. This elevation is always accompanied by Imperial violence.

**Kalpozos, 2020 - Prof of Law at Harvard** \[Ioannis 3-16-2020, ,
Leiden Journal of International Law, "Double elevation: Autonomous
weapons and the search for an irreducible law of war\",
https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3545332 accessed on
6-18-2022\]

3\. Double elevation and the distancing of judgement [Technology is not
neutral]{.underline}.68 Assuming the neutrality of technology -- and
attaching to it the assumed neutrality of law -- precludes any critical
understanding of either. [Technology]{.underline}, its development and
use, [reflects both theoretical and practical commitments: it 'is covert
philosophy']{.underline}.69 It exists in a relationship of co-production
with culture, politics, and law.70 As Paul Edwards put it in a seminal
study of Cold War weapons technology, 'we can make sense of the history
of computers as tools only when we simultaneously grasp their roles as
metaphors in ... the period's ... science, politics and culture'.71 In
this section, I argue that [the promise of technological progress and
automation in war]{.underline}, as in general, [is a promise of
civilization, a promise of improvement. It entails a double elevation:
above one's enemy and above one's self. At the centre of it there is a
paradoxical assumption, namely that the non-human can be more humane
than the human. The elevation above one's enemy combines military
distance with a perception of civilizational and moral superiority. The
elevation above oneself aims at creating a distance from human features
perceived as weak or unreliable.]{.underline} Both full autonomy and
merged heteronomy require the increasing mechanization of human
judgement. What we have learned to understand as the civilization of
war-fighting rests on and pursues its mechanization. 3.1. Rising above
one's enemy [Technological distancing aims at developing asymmetry and
invulnerability and elevating oneself above one's enemy in both strictly
speaking military and broader civilizational terms. The latter type of
elevation allows not simply a geographical distance but also a moral
distance with significant consequences for the role of law and judgement
in killing]{.underline}. [Military technology is central to early
imperialist expansion72 and its concomitant civilizational pretension,
culminating in the steep military and moral asymmetry achieved in
nineteenth century colonial warfare]{.underline}. Churchill's
description in the context of the Boer war of the British infantry
'steadily and solidly' firing against the Sudanese Dervishes in 'the
most signal triumph ever gained by the arms of science over barbarians',
while 'the mere physical act became tedious',73 is illustrative.
Technology allows military superiority, guaranteeing the physical safety
and invulnerability of one's forces; the asymmetry achieved reflects an
already assumed civilizational distance which allows a moral
dissociation from the act of killing, expressed in the ennui of physical
exertion; the civilization of the technologically advanced party is
enforced.74 The role of military technology in the elevation above one's
enemy is most closely associated with the growth of air power and the
aspirations of invulnerability associated with it. Air power, especially
in situations of colonial asymmetry, constituted a relationship of
vertical distance, allowing the surveillance and policing of one's
inferior enemy, both at initial conquest and through the protracted
practice of colonial administration and pacification.75 That colonial
relationship achieved new technological heights in the context of the
Cold War. Towards the end of the 1960s, the Vietnam impasse pushed for
the assertion of asymmetry through the development of an automated
battlefield to improve targeting capacity and protect American soldiers.
Operation Igloo White attempted the surveillance of the Ho Chi Minh
Trail in Laos through the use of camouflaged sensors designed to detect
different types of human activity, including body heat, vehicle noise or
the smell of human urine,76 or sweat.77 When picked up, such activities
appeared on the screens in the headquarters' terminals in Thailand and
fed into the targeting system of military aircraft. A 'kill box'78 was
constructed and targeted. The operation's centralized, computerized,
automated method of 'interdiction' relied on an active global defence
and aspirations for the full automation of the battlefield. These are
set out by General William Westmoreland, the Chief of Staff of the US
Army at the time, in a tenor strongly evocative of our present debate:
On the battlefield of the future, enemy forces will be located, tracked,
and targeted almost instantaneously through the use of data links,
computer assisted intelligence evaluation, and automated fire control.
... I see battlefields on which we can destroy anything we locate
through instant communications and the almost instantaneous application
of highly lethal firepower. ... \[A\]n improved communicative system ...
would permit commanders to be continually aware of the entire
battlefield panorama down to squad and platoon level ... I am confident
the American people expect this country to take full advantage of its
technology - to welcome and applaud the developments that will replace
wherever possible the man with the machine ... With cooperative effort,
no more than 10 years should separate us from the automated
battlefield.79 As it turns out, Operation Igloo White was a complete
failure.80 And yet the technological ambition remained. In 1973 the New
Scientist echoed General Westmoreland's technological/military optimism.
There was 'at present, great interest in the development of remotely
piloted vehicles (RPV's) for missions such as reconnaissance, electronic
warfare, ground attack and air-to-air combat'.81 Increasingly, to these
purposes was added another: targeted assassination. The ambition of the
precision of a self-sustaining intelligence/targeting loop in drone
warfare illustrates the confluence of offensive and defensive
imperatives in elevating oneself above one's enemy. Markus Gunneflo has
shown how the practice of and legal justification for targeted killings
was developed in Israeli and US policy as a means of constitutional
protection of the citizens to be distinguished from unlawful
assassination.82 In such 'active defence', especially when exercised
globally, we see the merging of the offensive distance of air power,
seeking to impose a vertical relationship of war, and the defensive
distance of integrated human/machine surveillance systems. This vision
is reflected in the prioritization in the 1990s of drone research.83
Drones, both the surveillance and the targeting kind, have been seen as
symbolizing a 'change of paradigm' in the conduct of war.84 They,
however, follow the trajectory discussed -- that of achieving an
elevation above one's enemy, associated with geographical distancing and
the moral/civilizational distance associated with governing through war
from above.85 The present ambition, of both escalated weapon
emancipation and human/machine merging, follows that same path. However
autonomous, further distancing remains the goal.86 This is not a
paradigm change.87 However, to the extent that there is a rapid
acceleration of technological development we could, perhaps, refer to an
'avalanche': 'when conditions are ripe, individual events, even small
ones, can trigger a massive, downward rush'.88 This metaphor may serve
to describe a well-established trajectory combined with the feeling that
things may be spiralling out of control. [From colonial asymmetry to the
post-Cold War fighting of 'terror', the elevation above one's enemy
through weapons technology guarantees physical and moral distance; it
also]{.underline} denotes, and [imposes, the pretention of a higher
civilization]{.underline}. As we will see, the promise of precision,
professionalization, optimization of decision making -- with humans
involved, but assisted by technology -- underlines another kind of
elevation: one that supposedly saves humans from themselves.

#### The Affirmative relies on dehumanizing assumptions -- the argument that laws can be established to prevent AI is based on the same assumption that LAWs can perfect human warfare. Both rely on enlightened rationalist ideas of progress, ignoring how this perfection numbs us to dehumanizing violence. 

**Kalpozos, 2020 - Prof of Law at Harvard** \[Ioannis 3-16-2020, ,
Leiden Journal of International Law, "Double elevation: Autonomous
weapons and the search for an irreducible law of war\",
https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3545332 accessed on
6-18-2022\]

This article intervenes in a rather crowded debate. [Why is there such a
flurry of current interest in autonomous weapons and their legal
regulation?]{.underline} [A visceral reaction to the idea of mechanized
killing, combined with the perception of the acceleration of artificial
intelligence research,12 may be a sufficient answer. The anticipation of
incoming practical problems to-be-solved]{.underline}, questions posed
by powerful interested parties, [may also be an incentive]{.underline}
for lawyers to place their analyses in the marketplace of ideas. But I
believe there is more to it. The present is the time to imagine the
future, especially when one is both propelled by and unmoored from the
past. [The mix of artificial intelligence, war, and law draws some of
its headiness from a specific historical moment -- a culmination of and
a departure from enlightenment rationalism]{.underline}; [the apex of
progress and the moment when we fear it will get out of
hand.]{.underline} 'The end of the end of history'13 marks a daunting
beginning. The law of war carries this tension, with respect to both the
articulation of rules and their enforcement. The legal institution of
war rests on the survival of a soldier's individual sense of humanity at
the time when his life is laid out for the collective. So does
individual (criminal) liability: without that assumption -- without that
fiction -- it makes no sense. I argue that the [increasing mechanization
of warfare pursues the creation of distance from our enemies and from
ourselves]{.underline} and reduces the knowledge and intelligence of the
law and its application through individual judgement. I further argue
that legal [research on new weapons technology should focus less on
questions of compatibility between given legal rules and the algorithmic
and kinetic features of new weapons, as most current scholarship does,
and more on an understanding of law as non-reducible to algorithmic
engineering.]{.underline} I start, in Section 2, by reviewing the state
of the art-in-the-making. Rather than providing an exhaustive taxonomy I
aim to highlight the teleological nature of artificial intelligence
research and the industry's investment in the dialectic of increasing
machine autonomy and human/machine merging, or 'merged heteronomy'. I
then see, in Section 3, this relationship between technology and war in
some historical perspective, with a view to discerning functions
relevant to law. I argue that [the role of technology in war entails a
double elevation: above one's enemy and above oneself. The elevation
above one's enemy,]{.underline} discussed in Section 3.1, [serves a both
offensive and defensive impetus and aspires to both spatial and
moral/civilizational distance. The elevation above oneself,]{.underline}
discussed in Section 3.2, [is for self-perfection. It is often
associated with]{.underline} a certain understanding of Cartesian
dualism and [a belief in rational improvement that may see humanity as
the cause of inhumanity and de-humanization as our best chance for
humanization. It seeks to mechanize judgement and therefore establish a
distance from human failings. It is served by war as governance from a
distance and by the increasing physical and cognitive merging of humans
and machines.]{.underline} Both physically and, to some extent, in moral
and civilizational terms, [technology and automation promise such
improvement through establishing a distance from the human -- our human
enemy and our human self.]{.underline} The establishment of this
distance entails a decreasing role for human judgement and the weakening
of responsibility for such judgement. I further argue, in Section 3.3,
that law, or certain strands of mainstream jurisprudence, are complicit
in such mechanization, to the extent that law is treated as logic even
conceivably reducible to algorithm. And that [the reaction against this
process of distancing and de-humanization, to the extent that it
idealizes the proscriptive or regulatory role of law, is bound to
disappoint]{.underline}.

#### Requiring Human Control is a trap -- it stops our resistance to the Distancing and Elevation of modern warfare.

**Kalpozos, 2020 - Prof of Law at Harvard** \[Ioannis 3-16-2020, ,
Leiden Journal of International Law, "Double elevation: Autonomous
weapons and the search for an irreducible law of war\",
https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3545332 accessed on
6-18-2022\]

'[Fully autonomous systems' then, in relation to war-fighting, are
systems that, once deployed, are able to adapt, receive, and process
feedback, and display a level of functional autonomy that effectively
does not distinguish them from human decision makers]{.underline}.24 If
anything, in fact -- and that is really the point -- such systems may be
capable of a higher level of tactical or even strategic decision-making
(cognitive) and war-fighting (kinetic) capacity. This seems to be a
shared understanding among states and NGOs, otherwise holding different
positions in the autonomous weapons debate. Accordingly, the US
Department of Defense refers to '\[a\] weapons system that, once
activated, can select and engage targets without further intervention by
a human operator'.25 Human Rights Watch, while setting out its position
against autonomous weapons systems, defines them as '\[r\]obots that are
capable of selecting targets and delivering force without any human
input or interaction'.26 Other states27 and organizations28 provide
similar definitions. [These functions of autonomy]{.underline}, and
their escalation, [entail both physical and cognitive distancing from
human agents in the overall process of targeting. Although the
concept]{.underline} and image [of autonomy dominates the
discourse]{.underline} in a way that influences, as we will see in the
next section, much of the scientific research and development, [it only
partially describes technological escalatio]{.underline}n. In fact, the
artificial intelligence of war-fighting complements increasing autonomy
with what has been called 'merged heteronomy'. [Increasing cognitive
autonomy, and distance, coexists with increasingly close physical
proximity]{.underline}. This serves pragmatic aims: [Technological
limitations in the fragmented development of different aspects of
autonomy,]{.underline} the continuing necessity of human input, and the
fragility of human/machine networks [mean that the individual's
continuing presence in the loop remains an operational necessity. This
also means that a full-on confrontation with]{.underline} social and
political resistance [to the reality of distinct killer robots is placed
in abeyance. Continuous, if vague, assurances of the human remaining in
the loop and retaining meaningful control are thereby
facilitated]{.underline}. [And yet, the abeyance is a trap and our
presence 'in the loop' is no guarantee. To the extent that 'full
autonomy' is understood to require the separate physical existence of
an, often anthropomorphized, robot other, it obscures the crucial role
that increasing cognitive autonomy plays in a nominally heteronomous
decision-making process]{.underline}. Cognitively, as well as physically
and kinetically, humans and machines become decreasingly separate, less
and less other. Their understanding of the rules, the nomos, is merging,
as is their physical existence. Increasing autonomy and merged
heteronomy serve the same purpose, the same teleology of mechanization.
['The loop', itself, is changing, increasingly relying on artificial
intelligence.]{.underline}

### Logistics AI Solvency Resps

#### AI assisted Logistics break down in practice -- the political and bureaucratic conflicts and fog of war make "low hanging fruit" an illusion.

**Goldfarb and Lindsay, 2022 -- Chair in AI and health care at the Univ
of Toronto, Prof of Cybersecurity at the Georgia Tech** \[Avi, Jon,
2/25/22, <https://doi.org/10.1162/isec_a_00425>, "Prediction and
Judgement: Why Artificial Intelligence Increases the Importance of
Humans in War" 6/18/22, LND\]

We expect automation to improve the efficiency and scale of routinized
activities that entail filling in missing information, measuring
technical performance, tracking personnel, and anticipating future
needs. Indeed, AI may enhance many routine tasks associated with
developing budgets, recruiting and training personnel, identifying
leadership potential, scheduling unit rosters, designing and procuring
weapon systems, planning and evaluating exercises, caring for the morale
and welfare of service members and their families, and providing health
care to service members and veterans.[66](javascript:;) At the same
time, it is important to recognize that [seemingly trivial procedures
can become politicized when budgets and authorities are
implicated]{.underline}.[67](javascript:;) Even in the absence of
parochialism, [the complexity of administrative systems introduces
interpretive challenges for personnel. These internal frictions
undermine the conditions for successful administrative automation.
Logistics supply chains may also be good candidates for
automation.]{.underline} Indeed, firms like DHL and FedEx have leveraged
AI to streamline their delivery networks. Standardized parts,
consumption rates, repetitive transactions, and preventive maintenance
schedules generate abundant data about defined tasks. Using historical
performance data, predictive maintenance systems can monitor consumption
rates and automatically order replacement parts before a weapon or
platform breaks[.]{.underline} For example, one U.S. Air Force system
uses a predictive algorithm to decide when mechanics should perform an
inspection, which allows them to tailor the maintenance and repairs for
individual aircraft rather than adhere to generic
schedules.[68](javascript:;) [But we contend that]{.underline} [the
prediction of supply and demand for just-in-time delivery will be more
difficult in war. While bureaucrats may be insulated from the turmoil of
the battlefield, supply lines are more exposed]{.underline}. [The enemy
can interdict or sabotage logistics]{.underline}. As wartime attrition
consumes spare parts, units may squabble about which ones should be
resupplied. Friendly units may resort to using platforms and parts in
unconventional ways. [All this turbulence will cause predictions to
fail, which essentially shifts AI into the category of premature
automation,]{.underline} discussed below. The classical military
solution to such problems is to stockpile an excess of supplies,
precisely because wartime consumption is so hard to
predict.[69](javascript:;) If organizations eliminate slack resources
with AI systems in pursuit of efficiency, however, then they may
sacrifice effectiveness when systems encounter unforeseen
circumstances.[70](javascript:;) In sum, we expect AI to be most useful
for automating routine tasks that are bureaucratically insulated from
battlefield turbulence[.]{.underline} Administration and [logistics
tasks]{.underline} that [are repetitious and standardized]{.underline}
are more likely to have both quality data and clear goals[.]{.underline}
Humans still provide judgment to define those clear goals, but this
happens in advance. [Although these conditions are ideal for automation,
they can be elusive in practice, especially if there are contested
resources and personnel decisions]{.underline}. [As a result, even the
low-hanging fruit applications will often fall into the other three
categories in [table 2](javascript:;), particularly human-machine
teaming.]{.underline}

#### Incremental US AI development speeds up the Arms Race -- allies will lose trust, and competitors will see an opportunity

**Horowitz, 2018 -- Professor of Political Science at UPenn** \[Michael,
September "The Algorithms of August: The AI arms race won\'t be like
previous competitions, and both the United States and China could be
left in the dust,"
https://foreignpolicy.com/2018/09/12/will-the-united-states-lose-the-artificial-intelligence-arms-race/
6/18/22 MD\]

It\'s also possible, though unlikely, that AI will propel emerging
powers and smaller countries to the forefront of defense innovation
while leaving old superpowers behind. [Washington\'s current focus on
U.S.-Chinese competition in AI misses an even more important trend.
There is a risk that the United States, like many leading powers in the
past, could take an excessively cautious approach to the adoption of AI
capabilities because it currently feels secure in its conventional
military superiority. That could prove to be a dangerous form of
complacency, especially if relations between the United States and many
of its current
[allies](https://go-gale-com.proxy.lib.umich.edu/ps/i.do?p=AONE&u=umuser&id=GALE%7CA556838648&v=2.1&it=r)
and partners continue to fray over time]{.underline}. [Faced with a less
reliable United States, shunned NATO partners would, for example, have
even more incentives to invest in alternatives]{.underline}, [such as
experimenting more with how AI can bolster their
capabilities]{.underline} in a world without clear superpower
leadership. [If these countries decide to strike out on their own, while
China and Russia continue to invest in capabilities with the explicit
goal of disrupting U.S. military superiority]{.underline}, and parts of
the U.S. tech industry remain reluctant to work with the Defense
Department, [the U.S. military could even find itself]{.underline} in a
position it has not faced for more than 75 years: [playing catch-up when
it comes to deploying cutting-edge technology on the
battlefield]{.underline}.
