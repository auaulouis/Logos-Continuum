# DA -- AI Arms Racing

## Notes

### Explanation

#### This is a supplement for the AI Bad DA in the AI case negative.

## Negative

### 1NC -- DA \[China\]

DA -- AI Arms Race

#### China has moved to take [global control]{.underline} global AI standards ― SDO investment has [closed a waning gap]{.underline} with the west

**Copan & Gupta 22** (Walter G. Copan, holds a **PhD** in physical
chemistry from Case Western Reserve University and a certificate in
advanced business administration studies from Harvard Business School,
recipient of the 2021 Bayh-Dole award for innovation and technology
transfer from AUTM, senior advisor and cofounder of the Renewing
American Innovation Project at CSIS, former director (16^th^) of the
National Institute of Standards and Technology; Kirti Gupta, holds a
**PhD** in economics from the University of California, San Diego,
senior advisor at CSIS, chief economist and vice president of Qualcomm;
\"Renewing U.S. Leadership in Standards\", 6-13-2022, CSIS,
https://www.csis.org/analysis/renewing-us-leadership-standards, DOA:
6-27-2022)//ATJ \*SDO = Standards Development Organization

China's Standards Challenge

As the main U.S. competitor on the world stage, [[China
recognizes]{.mark} that **global [leadership]{.mark}** [in standards
provides]{.mark} distinct commercial and **[national]{.mark} security
[advantages]{.mark}**. The [China]{.mark} Standards 2035
plan]{.underline}, notably, [[is designed to **take**]{.mark}
**leadership or [control]{.mark}** [of]{.mark} the **process** that sets
[global standards for **emerging tech**]{.mark}nologies [like]{.mark}
5G, internet of things (**[IoT]{.mark}**), [and]{.mark}
**[AI]{.mark}**]{.underline}---as reflected in the title of a recent
Wall Street Journal article, From Lightbulbs to 5G, China Battles West
for Control of Vital Technology Standards.

[There are **several mechanisms** through which Chinese firms and
state-backed enterprises are increasing participation]{.underline},
leadership, [and control of certain standards:]{.underline}

Creating strategic linkage/investment incentives in China:
[[China's]{.mark} government is prioritizing investments on
standards-related skillsets, along with industrial policy goals for
taking global leadership in standards.]{.underline}

Growing SDO participation: [China is **[increasing]{.mark} its
participation in [SDOs]{.mark}**, [**closing the gap** with]{.mark} the
number of delegates from [the U]{.mark}nited [S]{.mark}tates and the
European Union.]{.underline} For example, the number of delegates
affiliated with Chinese entities participating in 3GPP--- the 3rd
Generation Partnership Project standards body responsible for design on
5G standards---has grown tenfold since 2000.

Gaining SDO leadership roles: [[China]{.mark} has [sought to
promote]{.mark} the election of [**its representatives** for]{.mark}
**important leadership [positions]{.mark}** [at
SDOs]{.mark}]{.underline}---for example, to the 3GPP.

Manipulating the value of standards-related technologies: [[China has
sought]{.mark} in some cases to **[change the rules]{.mark}** of
standards bodies, [even **after**]{.mark} the [standards have been
established]{.mark}. China is also using anti-suit and anti-anti-suit
injunctions]{.underline} (for example at the U.S.-based Institute of
Electrical and Electronics Engineers) [to set global rates and provide
favorable rates for domestic firms.]{.underline}

Establishing China-led processes: [China is [trying to establish a
**China-led process**]{.mark} for critical standards [by **excluding
non-Chinese firms**]{.mark} in the first round of **standards
development**.]{.underline} [For]{.underline} some [**critical
standards**, such as the future version of how the internet,
IoT]{.underline}, and connected cars, [various **China-only standards
working groups now exist** to exert a growing influence on how these
standards are shaped.]{.underline}

[**Taking the Initiative** to Maintain a Rules-Based Standards
Ecosystem]{.underline}

[[U.S. policymakers must take]{.mark} **specific [actions]{.mark}** with
**clear policy objectives** [to **preserve** a rules-based]{.mark}
**global [standards ecosystem]{.mark}**]{.underline}, adhering to and
maintain a market- and consumer-driven innovation agenda (rather than
one that is state-driven), and technology selection based on consensus
and merit.

Various parts of the U.S. administration have recognized the importance
of [[**maintaining** U.S. leadership in standards]{.mark} that are
[critical for]{.mark} economic growth and **[national
security]{.mark}**]{.underline}:

Executive action: Last year, for example, the National Security
Commission on Artificial Intelligence (NSCAI) released a final report
making critical recommendations for maintaining U.S. leadership in AI
standards. Various actions by the administration have underlined the
importance of maintaining leadership in 5G standards, including the
president's Executive Order on "Promoting Competition in the American
Economy" in July 2021.

Cooperate with allies and strategic partners: [[The U]{.mark}nited
[S]{.mark}tates [should work with]{.mark} its **[allies]{.mark}** and
strategic partners [to]{.mark} **[establish]{.mark} and maintain [a
rules-based ecosystem globally]{.mark}**.]{.underline} Currently, this
should include greater attention to cooperation on standards issues
through the U.S.- EU Trade and Technology Council and the Quad
initiative with Australia, India, and Japan.

#### **BUT Western AI regulation overshoots and constricts developmental pace ― ensures we lose the AI arms race**

**Straub 21** (Jeremy Straub, **PhD**, Assistant Professor in the North
Dakota State University Department of Computer Science and a NDSU
Challey Institute Faculty Fellow; \"Would Regulation Prevent AI From
Becoming an Evil Overlord?\", 10-1-2021, University of North Dakota,
https://dda.ndus.edu/ddreview/would-regulation-prevent-ai-from-becoming-an-evil-overlord/,
DOA: 4-19-2022)//ATJ

AI IS GOING TO HAPPEN---HERE?

[Many discussions of U.S. **[regulations]{.mark}** seem to
[presume]{.mark} that [**American laws** can **restrict**]{.mark} or
**prevent** AI **[development]{.mark}**. However, **[this is
demonstrably not the case]{.mark}**. While the U.S. has led the world in
the development of key computing technologies]{.underline} and several
of the world's largest software companies\[30\]---Microsoft, Google,
Oracle, IBM, Apple and Adobe---are American firms, [**the U.S. is [not
the only place]{.mark} where [AI is]{.mark} being
[developed]{.mark}**.]{.underline} Russian president Vladimir
[[Putin]{.mark} has [**heralded** AI]{.mark} as "**the future**, not
only for Russia, but **for all humankind**."]{.underline}\[31\] In
September 2017, he went as far as to tell Russian students that the
nation that "becomes the leader in this sphere will become the ruler of
the world."\[32\]

[[With **Russia**]{.mark} and other nations **[embracing
AI]{.mark}**]{.underline},\[33\] [**[nations that don't innovate]{.mark}
in AI technologies**---or worse, those that actually restrict its
development---run the risk of **[fall]{.mark}**ing **[behind]{.mark}**
and [not]{.mark} being [able to compete]{.mark} with the countries that
promote AI development.]{.underline}\[34\] [Advanced [AIs]{.mark} can
[create advantages for]{.mark} a nation's businesses and its
**[defense]{.mark}**. Nations **without AI** or with **less mature AI**
systems might be **placed at [a severe disadvantage]{.mark}** and forced
to buy systems]{.underline} with whatever capabilities the more advanced
nations are willing to let their firms sell to other countries. While
the state of nations after the introduction of AI is inherently unclear,
one thing is apparent: [**[restricting AI]{.mark}** development in the
U.S. won't stop it from being developed. In fact, this may [make
it]{.mark} **far more [likely]{.mark}** that the **[eventual]{.mark}**
winning [AI]{.mark} systems **[won't respect]{.mark} our [societal
values]{.mark}**, because they have been [developed by **another
country**]{.mark} or group that doesn't share them.]{.underline}

#### Which causes rivals to hijack control of AI norms at the public and private levels ― greenlighting unrestricted AI development

Michael **Auslin 18**, 10-19-2018, \[MICHAEL AUSLIN is the
Williams-Griffis Fellow in Contemporary Asia at the Hoover Institution
at Stanford University, Can the Pentagon Win the AI Arms Race?,
https://www.foreignaffairs.com/articles/united-states/2018-10-19/can-pentagon-win-ai-arms-race,
accessed 6-27-2022//BMNT\]

When the stingray-shaped object took off and landed lightly on the deck
of the USS George H. W. Bush in July 2013, some hailed it as a moment in
aviation history to rank with the first heavier-than-air powered flight,
at Kitty Hawk, in 1902. The X-47B drone flew itself, decided its own
flight path, and completed on its own a mission given to it by humans.
The dawn of autonomous weapons systems seemed undeniable. Yet the drone
was hardly independent, as humans still programmed all its possible
decisions, leaving it to choose from a menu of options. Half a decade
later, [experts are making]{.underline} new [claims that the **future of
warfare** is about to change. Today]{.underline}, artificial
intelligence ([[AI]{.underline}) [is the **new frontier
of**]{.underline}]{.mark} **[military
[competition]{.mark}]{.underline}**, and [[with **China and Russia**
making headway]{.underline}]{.mark} in the field, [the Pentagon is
starting to rush]{.underline}, some say belatedly, [into the new
era.]{.underline}

THE NEXT WAVE OF AI

In a move that reflects the reliability of current machine learning
capabilities, [the]{.underline} U.S. [Department of Defense]{.underline}
recently [awarded]{.underline} Booz Allen Hamilton [a contract worth
\$885 million]{.underline} over five years [to introduce the first
**large-scale use of AI**]{.underline} systems [to analyze]{.underline}
the flood of [data provided by drones]{.underline}, as well as to
diagnose diseases from medical data. [The **Defense Intelligence
Agency**]{.underline} (DIA), meanwhile, [is building the
Machine-Assisted Analysis Rapid-Repository System (MARS),
a]{.underline}n information [database to make]{.underline} the
[interaction between]{.underline} human [analysts, the cloud, and
automated data processing]{.underline} systems [more
efficient]{.underline}.

In September, the Defense Advanced Research Projects Agency
([[DARPA]{.underline}]{.mark}), which helped kick-start the AI
revolution back in the 1960s, [[announced]{.mark} an even more ambitious
initiative]{.underline}: a [\$2 billion]{.underline} program [to
foster]{.underline} [[the]{.underline}]{.mark} next era of AI
technologies, or ["[**third wave**"]{.mark} of AI. Unlike the first two
waves]{.underline} of AI, which made possible first narrowly defined
machine-conducted tasks and later statistical pattern recognition based
on large data sets, [the new initiative]{.underline}, which DARPA is
dubbing AI Next, [will focus on making]{.underline} it "possible for
[machines]{.underline} to [**adapt to changing circumstances**." The
goal]{.underline}, according to the agency, [is [to enable]{.mark}
better [decision-making in]{.mark} "**complex, [time-critical
battlefield environments.]{.mark}**["]{.mark} That could mean quicker
identification of threats]{.underline}, faster and more [**precise
targeting**, or]{.underline} creating [flexible options for commanders
based on changing conditions]{.underline} on the battlefield.

Although half a century old as a concept, [artificial intelligence is
still]{.underline} at a **[relatively immature]{.underline}** state of
development. The Booz Allen contract focuses on the most proven level,
what is known as "[narrow AI,]{.underline}" where a computer program
focuses on a particular task, often automating what humans previously
did (think of a spam filter for e-mail). This type of AI [has been
transforming]{.underline} the [health]{.underline} field, [allowing
for]{.underline} quicker [diagnosis of disease, and]{.underline} the
world of [surveillance]{.underline}, through facial and voice
recognition. The Defense Department is looking for cost- and time-saving
measures that will free up humans for more complex tasks. [Yet this
level of AI is still **dependent on**]{.underline} much **[human
input]{.underline}**, what is known as "supervised learning," where the
machines use preprogrammed algorithms designed for carrying out a
particular task. An example would be distinguishing from video footage a
machine gun-carrying motorcycle rider from an unarmed civilian.

The [[DARPA]{.underline}]{.mark} project [is far **more
ambitious**]{.underline}. According to DARPA Director Steven Walker, [AI
Next [seeks]{.mark} "to explore how machines can acquire
**[human-like]{.mark}**]{.underline} communication and
**[[reasoning]{.underline}]{.mark}** capabilities, [with the ability to
recognize new]{.underline} situations and [environments and
**adapt**]{.underline} to them." [The goal]{.underline}, as explained on
the AI Next website, [is to achieve]{.underline} the "**[far greater
levels of intelligence]{.underline}**" that machines will need in order
[to allow for]{.underline} more [autonomous weapons systems, going far
beyond]{.underline} the types of [human-controlled drones]{.underline}
that have been part of the military arsenal for years.

[The next generation of AI]{.underline} that DARPA
[proposes]{.underline} to develop---["**contextual
reasoning**"]{.underline}---relates to what AI scientists call
"unsupervised learning," where algorithms themselves try to identify
patterns in data. [Neural networks]{.underline} (also known as "deep
learning") [can carry out classification and prediction]{.underline}
tasks [by linking]{.underline} thousands of [processing nodes that
comprise machine learning]{.underline} programs and [algorithms. The
processes]{.underline} they follow [are not pre-programmed, but allow
them to **learn from observation**]{.underline}; for example, by
comparing thousands of pictures of buildings with a known example,
neural networks can learn what is a castle as opposed to a hut.
Neuroevolution goes beyond unsupervised learning, to enable AI to
develop more effective AI, as though Frankenstein's monster were put in
charge of creating his own bride. [The **holy grail**]{.underline} for
AI programmers [is to move from correlative outcomes, which
is]{.underline} essentially [what all AI]{.underline} today [does, to
**causative outcomes** that]{.underline} in essence [include
**intuition** and **cognitive insight**]{.underline}. Indeed, DARPA's
objective is for the machines that spring from the project to become
reliable colleagues to humans.

[Washington is just]{.underline} now [beginning to explore the policy
implications]{.underline} of the next wave of AI, [ranging
from]{.underline} technological [feasibility to human impact to ethical
questions]{.underline}. Few in U.S. [policymaking circles]{.underline}
have any but the most rudimentary understanding of what AI is or how the
field might develop. They [are]{.underline}, however, [keenly attuned to
the **threat of adversarial nations leapfrogging** American AI
capabilities. At the top of the list]{.underline} of concerns [is China,
followed]{.underline} closely [by Russia. **[China]{.mark}**
is]{.underline} already [considered a **world leader in AI**, [has
committed]{.mark}]{.underline} over [[\$2 billion]{.mark} to building an
AI industrial park, and hopes to foster a \$150 billion]{.underline} AI
[industry in less than a generation; **[Russia]{.mark}**,
while]{.underline} seen as [farther behind]{.underline} in the AI race,
[[is beginning a **comprehensive plan**]{.mark} to increase automation
throughout]{.underline} society and [the armed forces]{.underline},
establish a national center for AI development, [and begin]{.underline}
a series of [AI **war games**]{.underline} to understand the
technology's potential on the battlefield. Because of developments such
as these, many worry that the United States is already playing a
catch-up game on AI, especially with China.

As the U.S. government struggles to come up with a comprehensive
national AI plan, [[the **Pentagon**]{.mark} **is moving
forward**]{.underline} on its own accord. This summer, [the Defense
Department announced the]{.underline} establishment of the Joint
[Artificial Intelligence Center (JAIC), under the]{.underline} direction
of the [Department of Defense's chief information officer]{.underline}.
With support from the advisory Defense Innovation Board, chaired by
former Google head Eric Schmidt, [the center will study the role of
AI]{.underline} and machine learning [in **military
systems**]{.underline}. More specifically, [it [will]{.mark}
coordinate]{.underline} work on [high-priority AI initiatives,
**[increase collaboration with the private sector]{.mark}**]{.underline}
and academia, [and]{.underline} try to [develop the **next generation**
of AI talent]{.underline}. The council will also likely support the work
of the Defense Innovation Unit---Experimental
(**[[DIUx]{.underline}]{.mark}**), which [was established near Silicon
Valley]{.underline} in 2015 [to **[partner with]{.mark} civilian
[companies]{.mark}** to introduce high-tech]{.underline}, nontraditional
[approaches for DOD programs]{.underline}, many of which employ AI
processes. [Examples of DIUx partnerships include one]{.underline} with
a company [to]{.underline} identify, track, and [autonomously remove
rogue drones]{.underline} from the sky, [and another to]{.underline} use
algorithms to [predict]{.underline} mechanical [breakdowns in
Army]{.underline} armored [fighting vehicles]{.underline} for preventive
maintenance.

WILL WASHINGTON MAINTAIN ITS EDGE?

[Given the **rapid pace** of development]{.underline} in the field, [the
key to U.S. success in AI may]{.underline} well [lie with
**public-private partnerships**]{.underline} of the kind fostered by
DIUx. Yet opposition at Google and other tech firms to working with
Washington could leave the U.S. government searching for willing
partners. Google CEO Sundar Pichai, for example, promised this summer
that Google would never work on militarized applications of AI. This
pledge arose in response to the backlash to the company's cooperation in
the U.S. Air Force's Project Maven, an initiative to automate pattern
recognition from the massive amount of moving and still imagery captured
by drones and satellites. There is little question that [the Pentagon's
ultimate interest in AI is to]{.underline} be able to
[operate]{.underline} more [**effectively and efficiently**, and that
means]{.underline} more [destructively]{.underline}. Although the
counterculture that gave rise to Silicon Valley's tech leaders may have
mellowed into self-interested middle age, working with the U.S. military
may remain a bridge too far.

Such reticence could become a devastating weakness for the United
States. Silicon Valley above all knows that technology never sleeps, and
[the]{.underline} current [lack of cutting-edge AI
investment]{.underline} in the defense industry [could leave Washington
at a **decided disadvantage** in the **next generation's arms
race**]{.underline}. To avoid falling behind, the first priority for the
Pentagon is to find or fund AI startups that are willing to work with
the military and are doing cutting-edge research.

[[China]{.mark} may have access to]{.underline} the [massive amounts of
data]{.underline} needed [to refine algorithms]{.underline} for faster
targeting, pattern recognition, and decision-making, [but it [continues
to **lag**]{.mark} **on**]{.underline} the [**basic technologies** that
power AI, including hardware]{.underline} development. This means that
[[**for now** the U]{.underline}]{.mark}nited
[[S]{.underline}]{.mark}tates [[retains a]{.mark} **slim [edge]{.mark}**
over China in the AI arms race]{.underline}, a period in which [it can
**decisively integrate AI** into emerging weapons]{.underline} systems.
Thus, [the second priority]{.underline} for the Pentagon [should be to
push ahead]{.underline} as quickly as possible [on
integrating]{.underline} well-[proven AI technologies]{.underline}, such
as in pattern recognition, into operational capabilities. A third
priority, as DARPA proposes, is sponsoring basic research into the third
generation of AI, so as to position the military for a potentially
AI-centric future in certain areas in 20 or 30 years' time.

[Absent these steps, **Washington's edge will diminish**]{.underline}
over time, and perhaps more [quickly]{.underline} than most anticipate.
[[As Chinese]{.mark} armed [forces operate]{.mark}]{.underline} more
widely **[[within the Indo-Pacific]{.underline}]{.mark}** region [and
beyond, its future [AI prowess]{.mark} may [make it]{.mark}]{.underline}
an even [more **[formidable]{.mark}**]{.underline} force than it already
is [thanks to decades of conventional modernization. [An AI-dominant
Chinese military would]{.mark}]{.underline} in turn [[**change the
calculations** of nations]{.underline}]{.mark} large and small,
potentially leading them to embrace accommodation, so as to avoid
confrontations with Beijing.

[[Arms races are ugly]{.underline}]{.mark} things,
[[but]{.underline}]{.mark} throughout history, [[no one has]{.mark}
**successfully [contained]{.mark}** technological [advances]{.mark} that
make for]{.underline} more [lethal militaries. Only]{.underline} those
[nations that are unable to **protect their interests** fail to
adapt]{.underline}. The age of artificial intelligence is upon us, and
[in a world of]{.underline} ever more [**assertive authoritarian
powers**, [the U.S.]{.mark}]{.underline} military [[will have to
embrace]{.underline}]{.mark} [and incorporate]{.underline} the [[new
technologies]{.mark} into its arsenal]{.underline} as quickly and
thoroughly as it can.

#### Extinction ― autocratic norms create a cascade of risks

**Jain 19**---(senior fellow with the Scowcroft Center for Strategy and
Security, where he oversees the Atlantic Council's Democratic Order
Initiative and D10 Strategy Forum). Ash Jain and Matthew Kroenig.
10/30/2019. "Present at the Re-Creation: A Global Strategy for
Revitalizing, Adapting, and Defending a Rules-Based International
System." Stowcroft Center for Strategy and Security. Atlantic Council
Strategy Papers.
<https://www.atlanticcouncil.org/wp-content/uploads/2019/10/Present-at-the-Recreation.pdf>.
Accessed 7/27/21.

[The system must]{.underline} also [be **adapted**]{.underline} [to deal
with new issues]{.underline} that were not envisioned when the existing
order was designed. [Foremost among these issues is **emerging and
disruptive tech**nology]{.underline}, [including **AI**,]{.underline}
additive manufacturing (or **[3D printing]{.underline}**)[, **quantum
computing**, **genetic engineering**, **robotics**, **directed energy**,
the **Internet of things**]{.underline} (IOT)[, **5G**, **space**,
**cyber**, and many **others**]{.underline}. [Like other disruptive
technologies before them, these innovations promise great benefits, but
also carry **serious downside risks**.]{.underline} For example, AI is
already resulting in massive efficiencies and cost savings in the
private sector. Routine tasks and other more complicated jobs, such as
radiology, are already being automated. In the future, autonomous
weapons systems may go to war against each other as human soldiers
remain out of harm's way.

Yet, AI is also transforming economies and societies, and generating new
security challenges. [**Automation** will lead to widespread
unemployment.]{.underline} The final realization of driverless cars, for
example, will put out of work millions of taxi, Uber, and long-haul
truck drivers. [**[Populist movements]{.mark}** in the West have been
driven by **those disaffected**]{.underline} [by globalization and
technology, and]{.underline} mass unemployment caused by [automation
[will further]{.mark} grow those ranks]{.underline} and provide new fuel
to grievance politics. Moreover, some fear that [**[autonomous
weapons]{.mark}** systems will become "**killer robots**" that select
and engage targets without human input, and [could]{.mark}]{.underline}
eventually [[turn on]{.mark} their [creators, resulting in]{.mark}
**human [extinction]{.mark}**.]{.underline} The other technologies on
this lisgt similarly balance great potential upside with great downside
risk. [**3D printing**, for example, can be used to "make anything
anywhere,"]{.underline} reducing costs for a wide range of manufactured
goods and encouraging a return of local manufacturing industries.61 [At
the same time, advanced **[3D printers]{.mark}** [can]{.mark} also [be
used]{.mark} by **revisionist and rogue states**]{.underline} [to print
component parts [for]{.mark}]{.underline} advanced weapons systems or
even [[WMD programs, spurring **arms races** and]{.mark} weapons
**[prolif]{.mark}**eration[.]{.mark}]{.underline}62 [[**Genetic
engineering** can]{.mark} wipe out entire classes of disease through
improved medicine, or [wipe out]{.mark} entire **classes of
[people]{.mark}** through genetically engineered superbugs.
**[Directed-energy]{.mark} missile defenses** [may]{.mark} defend
against incoming missile attacks, while also
[undermin]{.mark}]{.underline}ing [global [**strategic
stability**.]{.mark}]{.underline}

Perhaps [[the **greatest risk**]{.mark} to global strategic stability
from new technology]{.underline}, however, [[comes from]{.mark} the risk
that **revisionist [autocracies]{.mark}** may win the new tech arms
race[.]{.mark}]{.underline} Throughout history, [[states
that]{.underline}]{.mark} have [[dominated]{.mark} the commanding
heights of **[tech]{.mark}**nological progress have also
[dominated]{.mark} **[i]{.mark}**nternational **[r]{.mark}**elations[.
The **U**]{.mark}nited [**S**]{.mark}tates [has been the **world's
innovation leader**]{.mark} from Edison's light bulb to nuclear weapons
and the Internet[. Accordingly, **stability has been
maintained**]{.mark}]{.underline} in Europe and Asia [for decades
because the United States and its democratic allies possessed a
favorable economic and military balance of power in those key
regions.]{.underline} Many believe, however, that China may now have the
lead in the new technologies of the twenty-first century, including AI,
quantum, 5G, hypersonic missiles, and others. [[If
China]{.mark}]{.underline} succeeds in [**[master]{.mark}**ing the
[tech]{.mark}]{.underline}nologies [of the future [before the
**democratic core**]{.mark}]{.underline}, then [[this could]{.mark} lead
to a drastic and rapid shift in the balance of power,
**[upset]{.mark}**ting [global strategic stability]{.mark}]{.underline},
and the call for a democratic- led, rules-based system outlined in these
pages.63

### 1NC -- DA \[Russia\]

#### Russia is pushing for leadership on AI usage― the US is barely staying ahead

**Tadjdeh 21** (Yasmin Tadjdeh; \"Algorithmic Warfare: Russia Expanding
Fleet of AI-Enabled Weapons\", 7-20-2021, National Defense,
https://www.nationaldefensemagazine.org/articles/2021/7/20/russia-expanding-fleet-of-ai-enabled-weapons,
DOA: 4-19-2022)//ATJ \[language edited\]

The nation \[[[Russia]{.underline}]{.mark}\] [[wants to **use AI**
for]{.mark} electronic **[warfare]{.mark}**, **intelligence**,
**[surveillance]{.mark}**, **reconnaissance** [and]{.mark} strategic
**[decision-making]{.mark}** processes [as leaders pursue
**info**]{.mark}rmation **[dominance]{.mark}** on the
**battlefield**]{.underline}, Groen said.

While Russia is not a leader in commercial and academic AI research ---
as the United States and China are --- it would be a grave mistake for
the Pentagon to take its eyes off the threat, he said

"[Russia was **not** a **major leader** in the **development** of the
internet or computer networking, but **[Russia]{.mark} has become [a
leader in weaponizing]{.mark} those [tech]{.mark}nologies [for advanced
cyberattacks]{.mark} and cybercrime capabilities**]{.underline}," he
noted.

[[The Russian military has]{.mark} taken **significant steps** to reform
and **[improve]{.mark}** the organization of [its]{.mark} **research and
[development]{.mark}** enterprise]{.underline}, he noted. This was done
in part because [**Moscow** believed its **previous structures** were
**stifling innovation** in technology areas such as **AI**.]{.underline}

[The [**scale** of these **reforms**]{.mark}]{.underline} --- such as
creating a new advanced R&D organization modeled on the Pentagon's
Defense Advanced Research Projects Agency --- [[demonstrates]{.mark} the
nation's [**seriousness** about fielding]{.mark} an **[AI-enabled
fighting]{.mark} force**]{.underline}, he said.

Vice Chairman of the Joint Chiefs of Staff Gen. John Hyten noted that
[Russia has [invested **enormous resources** into]{.mark} the
**development of [a]{.mark}**rtificial **[i]{.mark}**ntelligence, **big
data** and software technologies.]{.underline}

[The country is [**moving quickly** across]{.mark} many areas, including
[**nuclear weapons, space and cybe**r]{.mark}]{.underline}, he said
during remarks at the Defense Department's AI Symposium in June.
[[Embedded in each]{.mark} of those elements [is **new**]{.mark}
software, processing and **[a]{.mark}**rtificial
**[i]{.mark}**ntelligence **[systems]{.mark}**.]{.underline}

"[[Russia is a **significant threat**, especially]{.mark} in the **[near
term]{.mark}**]{.underline}," he said. "[[It is a **challenge** to not
just **keep up**]{.mark} with them [but **stay ahead**]{.mark} of
them]{.underline}."

#### **BUT Western AI regulation overshoots and constricts developmental pace ― ensures we lose**

**Straub 21** (Jeremy Straub, **PhD**, Assistant Professor in the North
Dakota State University Department of Computer Science and a NDSU
Challey Institute Faculty Fellow; \"Would Regulation Prevent AI From
Becoming an Evil Overlord?\", 10-1-2021, University of North Dakota,
https://dda.ndus.edu/ddreview/would-regulation-prevent-ai-from-becoming-an-evil-overlord/,
DOA: 4-19-2022)//ATJ

AI IS GOING TO HAPPEN---HERE?

[Many discussions of U.S. **[regulations]{.mark}** seem to
[presume]{.mark} that [**American laws** can **restrict**]{.mark} or
**prevent** AI **[development]{.mark}**. However, **[this is
demonstrably not the case]{.mark}**. While the U.S. has led the world in
the development of key computing technologies]{.underline} and several
of the world's largest software companies\[30\]---Microsoft, Google,
Oracle, IBM, Apple and Adobe---are American firms, [**the U.S. is [not
the only place]{.mark} where [AI is]{.mark} being
[developed]{.mark}**.]{.underline} Russian president Vladimir
[[Putin]{.mark} has [**heralded** AI]{.mark} as "**the future**, not
only for Russia, but **for all humankind**."]{.underline}\[31\] In
September 2017, he went as far as to tell Russian students that the
nation that "becomes the leader in this sphere will become the ruler of
the world."\[32\]

[[With **Russia**]{.mark} and other nations **[embracing
AI]{.mark}**]{.underline},\[33\] [**[nations that don't innovate]{.mark}
in AI technologies**---or worse, those that actually restrict its
development---run the risk of **[fall]{.mark}**ing **[behind]{.mark}**
and [not]{.mark} being [able to compete]{.mark} with the countries that
promote AI development.]{.underline}\[34\] [Advanced [AIs]{.mark} can
[create advantages for]{.mark} a nation's businesses and its
**[defense]{.mark}**. Nations **without AI** or with **less mature AI**
systems might be **placed at [a severe disadvantage]{.mark}** and forced
to buy systems]{.underline} with whatever capabilities the more advanced
nations are willing to let their firms sell to other countries. While
the state of nations after the introduction of AI is inherently unclear,
one thing is apparent: [**[restricting AI]{.mark}** development in the
U.S. won't stop it from being developed. In fact, this may [make
it]{.mark} **far more [likely]{.mark}** that the **[eventual]{.mark}**
winning [AI]{.mark} systems **[won't respect]{.mark} our [societal
values]{.mark}**, because they have been [developed by **another
country**]{.mark} or group that doesn't share them.]{.underline}

#### Russia will [capitalize]{.underline} on AI development, zeroing norms and greenlighting unrestricted AI development 

**Konaev et al. 19** (Margarita Konaev; Samuel Bendett; \"Russian
AI-Enabled Combat: Coming to a City Near You?\", 7-31-2019, War on the
Rocks,
https://warontherocks.com/2019/07/russian-ai-enabled-combat-coming-to-a-city-near-you/,
DOA: 4-19-2022)//ATJ

It should be clear by now that [[Russia aims to **master**]{.mark} the
psychological dimension of **[info]{.mark}**rmation
**[op]{.mark}**erations]{.underline}, focused on undermining the state
institutions and belief systems of its adversaries. And [technological
advances in [**AI** have]{.mark} the [potential to "**hyperpower
Russia**]{.mark}]{.underline}'s use of disinformation." It's well known
that Russian influence operations and election interference campaigns
leveraged machine learning to tailor propaganda to specific audiences
based on race, ethnicity, ideology, demographics, and geographic
location. Yet, while the thousands of fake and stolen social media
accounts deployed during these operations were manufactured manually,
[AI can be used to [**automate**, **accelerate**, and **scale**]{.mark}
synthetic accounts and content.]{.underline} Another worrisome example
of an emerging technology the Russian defense establishment could
potentially use in information operations entails recent developments in
text-generation technology that mimics how humans write. Experts are
concerned that this technology will allow governments and non-state
actors to spread disinformation on a tremendous scale, as well as that
disinformation campaigns can evade detection by generating subtly
different content. In this sense, developments in AI could make Russian
information operations more efficient, far-reaching, and widespread.

[Russia has a **long history** of waging **information warfare**. But
[modern tech]{.mark}nology not only [**furthers** its **reach**]{.mark}
and [**magnifies** its **impact**]{.mark} but also arguably
[renders]{.mark} its **[democratic adversaries]{.mark}** --- with their
open societies and free flows of information --- **[more
vulnerable]{.mark} than ever before**.]{.underline} Looking ahead,
[Russia is **likely to continue leveraging machine learning** and
advances in natural language processing to **refine** its
**micro-targeting**]{.underline} of malicious content and to construct
emotionally sophisticated and relevant propaganda for more effective
information operations in future conflicts, including, and
[[especially]{.underline}]{.mark}, those [[conflicts that **entangle
Western forces**]{.mark} **in** urban **fighting**.]{.underline}

Key Takeaways

Predicting how AI will impact the future of strategic competition and
warfare is difficult because it requires us to assess technologies that
are still mostly immature. That said, contextualizing militarily
relevant AI applications and other emerging technologies within the
appropriate operations environment is the best way to understand their
potential impact on the battlefield.

[More extensive **[AI]{.mark}** and autonomous **capabilities**
**[infused into Russian]{.mark} armed [drones]{.mark}** and unmanned
ground vehicles, as well as the **incorporation of AI** as an enabler of
rapid command, [could]{.mark} potentially [**undermine** the
U.S.]{.mark} military's ability to **maintain [overmatch]{.mark}** in
**multi-domain battle**.]{.underline} Overall, [Russian advances in
military applications of AI [threaten to **erode American**]{.mark}
**technological and [op]{.mark}erational [advantages]{.mark}** on future
battlefields, including [in]{.mark} urban
**[warfare]{.mark}**.]{.underline}

[[Russia is]{.mark} also [likely to **capitalize** on **breakthroughs in
AI**]{.mark}, big-data analytics, and machine learning to conduct more
targeted, scalable, and impactful information operations, which should
alarm both civilian and military U.S. decision-makers.]{.underline} Thus
far, the bulk of commercial and defense investments in the application
of AI for detecting, analyzing, and countering disinformation have
largely focused on identifying and filtering out malicious content and
blocking bots. These are, at best, damage control measures. Moreover,
such an approach is inherently limited given the speed within which
disinformation and propaganda spread, and the significant and often
irreversible damage such disinformation can cause for public opinion and
perceptions of legitimacy in urban military operations.

#### Extinction ― autocratic norms create a cascade of risks

**Jain 19**---(senior fellow with the Scowcroft Center for Strategy and
Security, where he oversees the Atlantic Council's Democratic Order
Initiative and D10 Strategy Forum). Ash Jain and Matthew Kroenig.
10/30/2019. "Present at the Re-Creation: A Global Strategy for
Revitalizing, Adapting, and Defending a Rules-Based International
System." Stowcroft Center for Strategy and Security. Atlantic Council
Strategy Papers.
<https://www.atlanticcouncil.org/wp-content/uploads/2019/10/Present-at-the-Recreation.pdf>.
Accessed 7/27/21.

[The system must]{.underline} also [be **adapted**]{.underline} [to deal
with new issues]{.underline} that were not envisioned when the existing
order was designed. [Foremost among these issues is **emerging and
disruptive tech**nology]{.underline}, [including **AI**,]{.underline}
additive manufacturing (or **[3D printing]{.underline}**)[, **quantum
computing**, **genetic engineering**, **robotics**, **directed energy**,
the **Internet of things**]{.underline} (IOT)[, **5G**, **space**,
**cyber**, and many **others**]{.underline}. [Like other disruptive
technologies before them, these innovations promise great benefits, but
also carry **serious downside risks**.]{.underline} For example, AI is
already resulting in massive efficiencies and cost savings in the
private sector. Routine tasks and other more complicated jobs, such as
radiology, are already being automated. In the future, autonomous
weapons systems may go to war against each other as human soldiers
remain out of harm's way.

Yet, AI is also transforming economies and societies, and generating new
security challenges. [**Automation** will lead to widespread
unemployment.]{.underline} The final realization of driverless cars, for
example, will put out of work millions of taxi, Uber, and long-haul
truck drivers. [**[Populist movements]{.mark}** in the West have been
driven by **those disaffected**]{.underline} [by globalization and
technology, and]{.underline} mass unemployment caused by [automation
[will further]{.mark} grow those ranks]{.underline} and provide new fuel
to grievance politics. Moreover, some fear that [**[autonomous
weapons]{.mark}** systems will become "**killer robots**" that select
and engage targets without human input, and [could]{.mark}]{.underline}
eventually [[turn on]{.mark} their [creators, resulting in]{.mark}
**human [extinction]{.mark}**.]{.underline} The other technologies on
this lisgt similarly balance great potential upside with great downside
risk. [**3D printing**, for example, can be used to "make anything
anywhere,"]{.underline} reducing costs for a wide range of manufactured
goods and encouraging a return of local manufacturing industries.61 [At
the same time, advanced **[3D printers]{.mark}** [can]{.mark} also [be
used]{.mark} by **revisionist and rogue states**]{.underline} [to print
component parts [for]{.mark}]{.underline} advanced weapons systems or
even [[WMD programs, spurring **arms races** and]{.mark} weapons
**[prolif]{.mark}**eration[.]{.mark}]{.underline}62 [[**Genetic
engineering** can]{.mark} wipe out entire classes of disease through
improved medicine, or [wipe out]{.mark} entire **classes of
[people]{.mark}** through genetically engineered superbugs.
**[Directed-energy]{.mark} missile defenses** [may]{.mark} defend
against incoming missile attacks, while also
[undermin]{.mark}]{.underline}ing [global [**strategic
stability**.]{.mark}]{.underline}

Perhaps [[the **greatest risk**]{.mark} to global strategic stability
from new technology]{.underline}, however, [[comes from]{.mark} the risk
that **revisionist [autocracies]{.mark}** may win the new tech arms
race[.]{.mark}]{.underline} Throughout history, [[states
that]{.underline}]{.mark} have [[dominated]{.mark} the commanding
heights of **[tech]{.mark}**nological progress have also
[dominated]{.mark} **[i]{.mark}**nternational **[r]{.mark}**elations[.
The **U**]{.mark}nited [**S**]{.mark}tates [has been the **world's
innovation leader**]{.mark} from Edison's light bulb to nuclear weapons
and the Internet[. Accordingly, **stability has been
maintained**]{.mark}]{.underline} in Europe and Asia [for decades
because the United States and its democratic allies possessed a
favorable economic and military balance of power in those key
regions.]{.underline} Many believe, however, that China may now have the
lead in the new technologies of the twenty-first century, including AI,
quantum, 5G, hypersonic missiles, and others. [[If
China]{.mark}]{.underline} succeeds in [**[master]{.mark}**ing the
[tech]{.mark}]{.underline}nologies [of the future [before the
**democratic core**]{.mark}]{.underline}, then [[this could]{.mark} lead
to a drastic and rapid shift in the balance of power,
**[upset]{.mark}**ting [global strategic stability]{.mark}]{.underline},
and the call for a democratic- led, rules-based system outlined in these
pages.63

### 2NC -- T/C -- AI Controls

#### Regulations fail and block responsible AI creation 

Robert A. **Freitas 22** Jr., JD from the University of Santa Clara
(Santa Clara, CA), School of Law, Research Fellow at the Institute for
Molecular Manufacturing, Won the 2009 Feynman Prize in Nanotechnology
for Theory, BS in Physics and Psychology from Harvey Mudd College,
"Molecular Manufacturing: Too Dangerous to Allow?", Nanotechnology
Perceptions, Volume 2, Number 1, Republished at The Lifeboat Foundation,
<https://lifeboat.com/ex/molecular.manufacturing>

\*edited for language\*

[[Attempts to]{.mark} block or ["relinquish"]{.mark}]{.underline} \[3,
12\] molecular manufacturing [research will [make the world]{.mark} a
**[more]{.mark}, not less, [dangerous]{.mark}** place]{.underline}
\[13\]. [This paradoxical conclusion is founded on two premises. [First,
**attempts**]{.mark} to block the research will [**fail**.
Second]{.mark}, such attempts [will **preferentially**]{.mark} block or
[slow]{.mark} the development of [**defensive** measures by
**responsible** groups]{.mark}]{.underline}. One of the clear
conclusions reached by Freitas \[4\] was that **[effective
[countermeasures]{.mark}]{.underline}** against self-replicating systems
[should be **feasible**, but will [require]{.mark} **significant
[effort]{.mark}** [to **develop**]{.mark} and **deploy**]{.underline}.
(Nanotechnology critic Bill Joy, responding to this author, complained
in late 2000 that any nanoshield defense to protect against global
ecophagy "appears to be so outlandishly dangerous that I can't imagine
we would attempt to deploy it." \[12\]) But [**[blocking]{.mark}** the
**development** of defensive systems [would]{.mark} simply
**[insure]{.mark}** that [**offensive** systems]{.mark}, once deployed,
would [**achieve** their]{.mark} intended [objective]{.mark} in the
absence of effective countermeasures]{.underline}. James Hughes \[13\]
concurs: ["[The **only** safe]{.mark} and feasible [approach to]{.mark}
the dangers of **[emerging tech]{.mark}**nology [is to **build**]{.mark}
the social and scientific [**infrastructure** to]{.mark}]{.underline}
monitor, regulate and [**[respond]{.mark}** to their
threats."]{.underline}

We can reasonably conclude that blocking the development of defensive
systems would be an extraordinarily bad idea. Actively encouraging rapid
development of defensive systems by responsible groups while
simultaneously slowing or ~~hindering~~ development and deployment by
less responsible groups ("nations of concern") would seem to be a more
attractive strategy, and is supported by the Foresight Guidelines
\[10\]. As even nanotechnology critic Bill Joy \[14\] finally admitted
in late 2003: "These technologies won't stop themselves, so we need to
do whatever we can to give the good guys a head start."

While [[a 100%]{.mark} effective [ban]{.mark}]{.underline} against
development might theoretically be effective at avoiding the potential
adverse consequences, blocking all groups for all time [[does **not**
appear]{.underline}]{.mark} to be a **[[feasible]{.underline}]{.mark}**
goal. The attempt would strip us of defenses against attack, increasing
rather than decreasing the risks. In addition, [blocking development
would insure that the substantial economic, environmental, and medical
**benefits**]{.underline} \[15\] [of this new technology would not be
available]{.underline}.

Observes Glenn Reynolds \[16\]:

> [To the extent that such efforts \[to ban all development\] succeed,
> the **cure** may be **worse than the disease**. [In 1875]{.mark},
> Great [Britain]{.mark},]{.underline} then the world's sole superpower,
> [was]{.underline} sufficiently [[concerned about]{.underline}]{.mark}
> the [dangers of]{.underline} the new technology of [high
> [explosives]{.mark}]{.underline} that [it]{.underline} passed an act
> **[[bar]{.underline}]{.mark}**ring [all private experimentation in
> explosives]{.underline} and [[rocketry]{.mark}. The [result
> was]{.mark}]{.underline} that [**[German missiles]{.mark}** bombarded
> London rather than the other way around. [Similarly, efforts to
> control **nano**]{.mark}tech]{.underline}nology,
> **[biotech]{.underline}**nology [[or
> **a**]{.underline}]{.mark}rtificial
> **[[i]{.underline}]{.mark}**ntelligence [[are **more likely** to
> **drive**]{.mark} **research [underground]{.mark}** (often [under
> **covert**]{.mark} government [sponsorship]{.mark}]{.underline},
> regardless of international agreement) [[than]{.mark} they are [to
> **prevent**]{.mark} research [entirely]{.mark}. The [research would
> be]{.mark} conducted by **[unaccountable]{.mark}** scientists, often
> [in **rogue regimes**]{.mark}, and often [under **inadequate**]{.mark}
> **safety [precautions]{.mark}**. Meanwhile, **[legit]{.mark}**imate
> [research that]{.mark} might cure disease or [solve]{.mark} important
> environmental [problems]{.mark} would
> **[suffer]{.mark}**]{.underline}.

### 2NC -- Uniqueness \[China\]

#### China's surpassing the US in AI [implementation]{.underline} and [weaponry]{.underline} -- supercharged tech industry and access to data

**Bourne 22** (Jacob Bourne, analyst on the Connectivity & Tech
Briefings team at Inside Intelligence, formerly covering Engineering;
\"Collaborations falter as US, China compete for AI dominance\",
5-31-2022, Insider Intelligence,
https://www.insiderintelligence.com/content/collaborations-falter-us-china-compete-ai-dominance,
DOA: 7-18-2022)//ATJ

Why it's worth watching: [[Despite the US's **early dominance**]{.mark}
and continued academic research lead in AI, [China's **ultra-competitive
tech industry** and]{.mark} greater [access to]{.mark} big [data]{.mark}
to train AI systems [is positioning it as a **global leader** for AI
**implementation**]{.mark}.]{.underline}

AI is a pivotal technology across industries, including defense,
prompting Russian President Vladimir Putin to say in 2017 that AI is
"the future, not only for Russia, but for all humankind. ... Whoever
becomes the leader in this sphere will become the ruler of the world."

Russia previously received AI tech support from China for robotic
weapons development, but war sanctions are dealing a blow to Russia's AI
aspirations. [China is similarly [working]{.mark} [on
**AI**]{.mark}**-fueled autonomous [weapons]{.mark}**, possibly
[**surpassing US progress**]{.mark} in that arena.]{.underline}

#### China's [incredibly close]{.underline} to US AI capability now -- massive increases in spending and civil-military fusion have [propelled]{.underline} it forwards

**Shkurti 19** (Gloria Shkurti, SETA Foundation; \"Artificial
Intelligence Application in the Military: The Case of United States and
China\", June 2019, ResearchGate,
https://www.researchgate.net/publication/340503792_Artificial_Intelligence_Application_in_the_Military_The_Case_of_United_States_and_China,
DOA: 7-18-2022)//ATJ

CHINA AND ARTIFICIAL INTELLIGENCE

[China is following in the steps of the U.S. when it comes to the
research and application of AI technology.]{.underline} Not only is it
using AI in domestic surveillance, but [[China has]{.mark} already
[stated]{.mark} that it aims [to **overtake**]{.mark} the West [in AI
R&D by **2025**]{.mark} and more importantly to be the world leader in
AI by 2030.]{.underline}64 The Chinese leadership, including here Xi
Jinping, on many occasions has clearly stated that the leadership in AI
technology is "critical to the future of global military and economic
power competition."65 It can be said that [for China, the military AI
R&D is seen as a possible and easy way to challenge the American
military hegemony.]{.underline}

[The [Chinese government]{.mark} has [increased]{.mark} their [AI
R&D]{.mark} spending [by **350%**]{.mark} between 2005 and 2015 and [it
is]{.mark} considered to be **[very close to the U.S.
currently]{.mark}**.]{.underline} Furthermore, [in 2017, [**48%** of the
world's]{.mark} total [AI]{.mark} start-up [funding was]{.mark} cov-ered
[by **Chinese companies** and]{.mark} between 2013 and 2018 [China's AI
industry attracted **60% of global funding**]{.mark} for
AI.]{.underline}66 As a result, it is believed among the Chinese
leadership and industry that [[the gap]{.mark} between China and the
U.S. [in AI is **very narrow now**]{.mark}]{.underline} and China sees
"AI as 'a race of two giants,' between itself and the United States."67

While there may be a lot of skepticism re-garding China's challenge to
the American hege-mony, it can be said that [the [systemic]{.mark} and
strategic [advantages]{.mark} can [act in **favor of China**]{.mark} and
help it achieve its goal of be4coming a world leader in AI. This would
[include]{.mark} the potential [human talent]{.mark} re-sources
[and]{.mark} the large amount of [data]{.mark} that China possesses but
most importantly the relation of the government and AI private
sector.]{.underline}68 In this context, [in contrast to the U.S., one of
[China's **strongest strategies**]{.mark} is the **[civil--military
integration]{.mark}**]{.underline}69 (CMI) [and the development of
advanced dual-use tech-ologies.]{.underline}70 [These complementary
strategies are re-lated to the fact that the Chinese government and AI
private sector are working closely together and this makes the
application of AI technologies to the military easier. The main goal
behind [such a policy]{.mark} is to create a strong military and [help
the]{.mark}]{.underline} People's Liberation Army
([[PLA]{.underline}]{.mark}) [to dominate the warfare domains and as a
result **['leapfrog' the U.S.]{.mark}**]{.underline}71

### 2NC -- Generic Link 

#### Constraint of AI cedes the race to \[[Russia/China]{.mark}\]

#### 1 ― PACING ― \*warrant out from cards\*

#### AI regulation [overshoots]{.underline}, destroying [productive]{.underline} applications necessary to prevent [existential catastrophes]{.underline}

Gönenç **Gürkaynak 18**, Founding Partner of ELIG Gürkaynak
Attorneys-at-Law, LL.M. from Harvard Law School, İlay Yılmaz, Partner at
ELIG Gürkaynak Attorneys-at-Law, and Güneş Haksever, LLM from Istanbul
Bilgi University, Attorney at IBM Turkey, "Stifling Artificial
Intelligence: Human Perils", Computer Law & Security Review, Volume 32,
Issue 5, 12/12/2018,
https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3285264

[[Although scientists]{.underline}]{.mark} have [[calculated]{.mark} the
**[significant positive]{.mark} welfare [effects]{.mark}**
[of]{.mark}]{.underline} Artificial Intelligence [[(AI)]{.mark}, **fear
mongering** continues to **hinder** AI **development**. [If
**reg**]{.mark}**ulation[s]{.mark}** in this sector **[stifle]{.mark}**
our active imagination, [we risk **wasting**]{.mark} the **[true
potential]{.mark}** of AIs dynamic efficiencies]{.underline}. Not only
would Schumpeter dislike us for spoiling creative destruction, but the
AI thinkers of the future would also rightfully see our efforts as [[the
**'dark age'** of]{.mark} human [advancement]{.mark}]{.underline}. This
article provides a brief philosophical introduction to artificial
intelligence; categorizes artificial intelligence to shed light on what
we have and know now and what we might expect from the prospective
developments; reflects thoughts of worldwide famous thinkers to broaden
our horizons; provides information on the attempts to regulate
artificial intelligence from a legal perspective; and discusses how the
legal approach needs to be to ensure the balance between artificial
intelligence development and human control over them, and to ensure
friendly artificial intelligence.

Our technology, our machines, is part of our humanity. We created them
to extend ourselves, and that is what is unique about human beings. --
Ray Kurzweil1

1\. Introduction

The Chinese cardboard game "Go" is one of the most complex strategy
games humankind invented. Go was considered so important, there are
myths indicating that ancient kings played Go between their armies in
the battlefield to resolve the conflict in peace. Computers prevailed
against humanities best in many zero-sum, perfect-information, partisan,
deterministic strategy games2 before, with the exception of Go, which
was something to be proud of.

The strategy aspect of Go is very complex and emphasizes the importance
of balance on multiple levels and has internal tensions. A game of Go
cannot be won by using brute force: calculating every possible move,
similar to what IBM®'s then state of the art AI, Deep Blue® used to win
over Gary Kasparov. To manoeuvre through the countless possible moves on
the Go board and chose the most efficient path, one requires
capabilities beyond the conventional computing powers; capabilities only
our minds have (or so we thought), such as extremely accurate image and
pattern recognition and insight, all of which we thought granted us
superiority over the artificial minds we created.

In October 2015, a software called "AlphaGo®" became the first computer
to beat a professional human Go player in an un-handicapped game of Go
(Silver and Hassabis, 2016). AlphaGo's victory is probably one of the
most significant demonstrations of the capabilities of an AI. Firstly,
it shows that AIs are beginning to surpass us at things where success is
dependent on strategy as well as calculation. Things we classify as a
"game", from stock exchange to conflicts, from contract negotiations to
hostage situations. Second, AlphaGo developed strategies on its own,
through playing millions of games against itself. These feats sent the
chills down the spines of those who fear that AIs will overpower us in
the future.

We humans accelerate the future with our minds. This is a strength and a
weakness. [[Often]{.underline}]{.mark}, our [**[predictions]{.mark}** of
the future [are **highly inaccurate**. Based on]{.mark} predictions from
a book called ['The World in 2010']{.mark}, published [in]{.mark}
19[**76**, we]{.mark} should have **[be]{.mark}**en living [**above**
and **below**]{.mark} the surfaces of **[three planets]{.mark}** as of
**[five years ago]{.mark}**. Predictions [regarding]{.mark} the future
of **[AI]{.mark}** are **[equally]{.mark} likely** to be **[off
base]{.mark}**]{.underline}.

[To avoid **premature** regulation over AI, we should]{.underline} be
studying and **[search]{.underline}**ing [for the **meaningful point in
time** when a broader anxiety about AI becomes a genuine
concern]{.underline}. The study of a point of ripeness, a 'threshold
ability test,' asks when AI could really bring about concrete
disadvantages that might counter-balance the demonstrated contribution
to economic efficiency and welfare.

[In the absence of]{.underline} such [an **objective
benchmark**]{.underline} marking the point in time when AI becomes a
competitor with the human mind, [[regulators could]{.mark} easily
**[jump the gun]{.mark}** in regulating AI, [which]{.mark} would [lead
to **irreparable harm**]{.mark} in **total welfare** [of]{.mark} human
[societies]{.mark}]{.underline}.

Most of what we consider AI today is really our own intelligence
re-formatted and re-cycled, with the help of computers lacking any skill
of learning or consciousness of being. [Regulation at this stage would
be **perverse**. The economic efficiency **[potential]{.mark}**s of AI
[should be]{.mark} set **[entirely free]{.mark}** at **this point** in
time, [allowing]{.mark} us to **[active]{.mark}**ly [and
**aggressivel**]{.mark}y [research]{.mark} appropriate goals for them
which would **not** result in the **extinction** of
humankind]{.underline}.

[If you think]{.underline} our [future **robot overlords**
will]{.underline} one day [thank us for ignoring]{.underline} the risks
[and **under regulating**, **think again**]{.underline}. On the one
hand, [**[any issues]{.mark}** we may face [from **AI**]{.mark}s
[will]{.mark} likely [result from]{.mark} humanity [**failure**
to]{.mark} **effectively [direct]{.mark}** AIs [to]{.mark} our
**[needs]{.mark}**, **not** because we switched to a defensive AI
regulation regime **too early**]{.underline}. On the other hand, [[at
**some point**]{.mark} of time]{.underline} in the not too distant
future, [[**natural**, **human**]{.mark}**-related** [or
**external**]{.mark} factors may [**threaten** the **fate of the
Earth**, and we]{.mark} may **[need AI to save]{.mark} the planet and
[us]{.mark}**. One **[hope]{.mark}**s that [society has not **pulled
the**]{.mark} **hand [brakes]{.mark}** on the wheels of AI **[too
early]{.mark}**]{.underline}, fearing our own active imagination.

#### 2 ― DEVELOPER PUSHOUT ― regulation drops developer interest to outside western SOI

Dr. Nell **Watson 21**, **PhD** in Engineering from the University of
Gloucestershire, Degree in AGI Safety Fundamentals from the University
of Cambridge, Senior Scientific Advisor to The Future Society at Harvard
University, Fellow at the British Computing Society and Royal
Statistical Society, "Regulatory Challenges to Catastrophic AI Risk",
ExO Insight, 11/24/2021,
https://insight.openexo.com/regulatory-challenges-to-ai/

Rick Increase Factors:

Obfuscation: [**[Reg]{.mark}**ulation**[s]{.mark}** may [**drive
research underground** where]{.mark} it is [**harder to monitor**, or to
**'flag of convenience' jurisdictions** with **lax
restrictions**]{.mark}, by [**embedding** dangerous
**tech**]{.mark}nologies with**[in]{.mark}** apparently benign **[cover
op]{.mark}eration[s]{.mark}** (multipurpose technologies), [or]{.mark}
by **[obfuscating]{.mark}** the **externalized [effects]{.mark}** of a
system, such as in the **vehicle emissions** scandal]{.underline}
(Wikipedia).

Arms race: Recent advances in machine learning such as multimodal
abstractions models (aka Transformers, Large Language Models, Foundation
Models) such as GPT-3 and DALL-E illustrate that dumping computing
resources (and the funds for them) in colossal models seems to be a
worthy investment. So far, there is no apparent limit or diminishing
return on model size, and so now state and non-state actors are
scrambling to produce the largest models feasible in order to access
thousands of new capabilities never before possible. An arms race is
afoot. Such [arms [races]{.mark} can [lead to **rapid**]{.mark} and
**unexpected** [take-off]{.mark} in terms of AI capability, and the rush
can blindside people to risks, especially when]{.underline} the
[loss]{.underline} of a race [can mean [an **existential
threat**]{.mark}]{.underline} to a nation or organization.

Perverse incentives: Incentives can be powerful forces within
organizations, and financialization, moral panic, or fear of political
danger may cause irrational or incorrigible behavior of personnel within
organizations.

Postmodern Warfare: Inexpensive Drones and other AI-enabled technologies
have tremendous disruptive promise within the realm of warfare,
especially given their asynchronous nature. Control of drone swarms must
be performed using AI technologies, and this may encourage the entire
theatre of war to be increasingly delegating to AI, perhaps including
the interpretation of rules of engagement and grand strategy. (Lsusr,
2021)

Cyber Warfare: Hacking of systems is increasingly being augmented with
machine intelligence (Cisomag, 2021), through GAN-enabled password
crackers (Griffin, 2019) and advanced social engineering tools (Newman,
2021). This is equally the case in the realm of defense, where only
machine intelligence may provide the swift execution required to defend
systems from attack. A lack of international cyberwar regulations, and
poor international policing of organized cybercrimes, may increase the
risk of catastrophic risks to societal systems.

Zersetzung: The human mind is becoming a new theatre of war, through
personalized generative propaganda, which may even extend to gaslighting
attacks on targeted individuals, significantly leading to
destabilization of societies (Williams, 2021). Such technologies are
also plausibly deniable, being difficult to prove who may be
responsible.

[Inflexibility: The [German Military]{.mark} after WW1 [was **not**
allowed to develop]{.mark} their **[artillery]{.mark}** materiel, and
[so developed]{.mark} powerful [**rocket** **tech**]{.mark}nologies
**[instead]{.mark}**, as these were **[not subject to
reg]{.mark}ulation**. [Similarly]{.mark}, inflexible [rules]{.mark} may
[permit]{.mark} **exploitable [loopholes]{.mark}**. They may also **not
be sufficiently adaptive** to allow for the **implementation** of **new
technologies** and even improved **industry standards**.]{.underline}

Limitation of problem spaces: -- [[It]{.mark} may be taboo to allow
machine intelligence to work on sensitive issues or to be exposed to
controversial (if potentially accurate) datasets. This may
[limit]{.mark} the [ability of AI to make sense of]{.mark} out [complex
issues, and]{.mark} thereby **[frustrate]{.mark} finding**
[**solutions** for **crises**]{.mark}]{.underline}.

#### 3 ― R&D ― Regs block [innovative start-ups]{.underline} AND make [advanced neural nets]{.underline} infeasible 

Daniel **Castro 19**, Vice President at the Information Technology and
Innovation Foundation (ITIF) and Director of ITIF\'s Center for Data
Innovation, M.S. in Information Security Technology and Management from
Carnegie Mellon University, B.S. in Foreign Service from Georgetown
University, and Michael McLaughlin, "Ten Ways the Precautionary
Principle Undermines Progress in Artificial Intelligence", Information
Technology & Innovation Foundation, 2/4/2019,
https://itif.org/publications/2019/02/04/ten-ways-precautionary-principle-undermines-progress-artificial-intelligence

HOW POLICIES BASED ON THE PRECAUTIONARY PRINCIPLE IMPACT AI

[Policies]{.underline} based on the precautionary principle can [impact
AI]{.underline} in several ways. [[They]{.underline}]{.mark} can [[make
it]{.mark} **more [expensive]{.mark}** to develop]{.underline} AI,
[[limit]{.mark} the **[testing]{.mark}** and **use** of AI, [and]{.mark}
even [ban]{.mark} certain **[applications]{.mark}**]{.underline}.
Clearly nations have the right to impose any regulations they chose
(assuming they do not violate World Trade Organization rules or other
global treaty obligations). But they should not delude themselves into
believing that [**[reg]{.mark}**ulatory
regime**[s]{.mark}**]{.underline} based on the precautionary principle
[will]{.underline} not [[limit]{.mark} increased
**productivity**]{.underline}, competitiveness, [and
**[innovation]{.mark}**]{.underline}.

To provide a more detailed discussion of the negative effects policies
based on the precautionary principle can have on AI, the following
section analyzes the effects of policies discussed earlier in this
report. In many cases, these policies have multiple negative effects on
AI.

1\. Slower and More Expensive AI Development

[[Policies]{.underline}]{.mark} based on the precautionary principle
both [**[slow]{.mark}** and make]{.underline} the [[development]{.mark}
of AI **more expensive**]{.underline}. For example, if all fifty U.S.
states had laws such as New York's, which requires autonomous vehicle
firms to perform road testing under the paid supervision of police,
testing such vehicles would be more expensive. Moreover, proposals to
require even non-medical algorithms to undergo pre-market trials would
hurt the development of AI because such [[trials are **time-consuming**
and **expensive**]{.underline}]{.mark}. Such [proposals]{.underline} may
also [[make]{.mark} AI systems that use **[m]{.mark}**achine
**[l]{.mark}**earning, and thus]{.underline} may [change frequently and
need more testing, significantly [**less viable** because]{.mark} such
systems could [constantly need]{.mark} to go through a [new
approval]{.mark} process]{.underline}.96 Finally, [policies that
increase]{.underline} the [[cost]{.underline}]{.mark} of developing AI
would likely [discourage innovation]{.underline} in AI [by
**[creat]{.mark}**ing [a]{.mark} substantial **[barrier]{.mark} to
entry** [for **startups** that **lack**]{.mark} **sufficient
[funding]{.mark}** to cover the cost of proving their AI system is
safe]{.underline}. For example, the GDPR has dampened investment in
European technology startups and led to a 30 percent decrease in the
market share of small online advertising firms that lack the resources
to easily comply with the regulation.97

Restrictions on one AI technology can also limit ways to develop another
AI technology. For example, researchers in Germany are using drones
hovering hundreds of meters above highways to record the movements of
vehicles. This data can help develop simulations to test autonomous
vehicles; such simulations are important tools for improving the safety
of autonomous vehicles because otherwise they would need to travel
billions of miles for safety validation.98 While this novel method of
collecting data to validate the safety of autonomous vehicles may or may
not prove valuable, implementing it in the United States would be would
be difficult to do at scale until the FAA implements its new rules that
allow out-of-sight drone flights and flights over people.99

2\. Less Innovation

AI will spur innovation so policies that limit the development of AI
will limit innovation.100 For example, proposals to ban or limit the
introduction of autonomous vehicles would also limit the generation of
new businesses, business models, and ways to do deliver services through
the "passenger economy." The passenger economy, a term coined by Intel
and research firm Strategy Analytics, "is the economic and societal
value that will be generated by fully autonomous...pilotless
vehicles."101 The firms envision a world where a significant portion of
vehicle ownership is replaced by fleets of autonomous vehicles that
provide on-demand transportation. Productivity would also increase as
autonomous vehicles free employees to work during their commutes and
autonomous trucks to operate more efficiently. The firms estimate the
value of this economy could be \$7 trillion by 2050.102 Nations that ban
autonomous vehicles will not experience the benefits of such an economy.

3\. Lower-Quality AI

[There is]{.underline} often [a negative correlation between making an
AI system more **explainable** and its **accuracy**]{.underline}.103 [As
a result, any [policies that require AI]{.mark} to [be
**explainable**]{.mark} could [lead to **less accurate**]{.mark} AI. For
example, researchers at Mount Sinai]{.underline} Hospital in New York
[developed]{.underline} an AI system called [Deep Patient
that]{.underline} can [predict]{.underline} whether a patient is
contracting any of a wide variety of [diseases]{.underline}.104 The
researchers trained Deep Patient on the health data from 700,000
patients, using hundreds of variables, such as test results, which allow
it to predict diseases such as schizophrenia---which doctors struggle to
predict---extremely well.105 [Even though]{.underline} its
[operators]{.underline} can [verify its accuracy]{.underline} by
measuring outcomes, such as if a person is developing a disease, [it is
difficult]{.underline} for its own developers [to know why it made a
particular decision]{.underline}.106

Many sophisticated forms of AI pose a similar problem. Developing an AI
system capable of explaining itself or justifying its decisions is an
incredibly challenging technical feat, so much so that the U.S. Defense
Advanced Research Projects Agency (DARPA) devoted \$75 million in 2017
to research how AI could achieve it.107 Some groups are skeptical that
requiring explainability would chill innovation. They cite DeepMind, a
British company owned by Google parent-company Alphabet, developing an
AI system in 2018 that can analyze eye scans to predict diseases while
also providing doctors a map of the features of disease it sees, such as
hemorrhages.108 However, the fact that one of the world's leading AI
companies could achieve a form of explainability in a system it worked
on for nearly two years is not evidence that all other operators should
or would be able to achieve explainability for their AI easily.109 To be
clear, it is legitimate for companies, such as IBM, to create internal
requirements for AI explainability.110 Requiring all firms to meet such
a standard, however, would create a barrier to adopting AI, because not
all AI systems are alike and not all businesses have a similar level of
expertise.

Nonetheless, it is important for AI operators to continually assess
their AI system's accuracy to ensure it is generating or predicting the
correct outcomes. [The other option is to allow **only** AI applications
that operators can **explain**; [this]{.mark} would [lead to]{.mark} AI
[systems that consider **fewer variables** and]{.mark} that [use
**simpler algorithms**]{.mark} to make decisions. In turn, [this]{.mark}
would **[reduce]{.mark}**]{.underline} the [**[effectiveness]{.mark}**
of AI that can generate **significant impacts**]{.underline} such as
identifying a terminal illness before a doctor can.

#### Which [nukes]{.underline} R&D at the [small business]{.underline} and [individual]{.underline} levels―they're key

Dr. Jeremy **Straub 21**, **PhD**, Assistant Professor in the North
Dakota State University Department of Computer Science and NDSU Challey
Institute Faculty Fellow, "Would Regulation Prevent AI From Becoming an
Evil Overlord?", Dakota Digital Review, 10/1/2021,
https://dda.ndus.edu/ddreview/would-regulation-prevent-ai-from-becoming-an-evil-overlord/

WHO DOES REGULATION REALLY PROTECT?

[Achieving]{.underline} most of these [[benefits]{.mark} will
[**require** a **lot more** **r**]{.mark}]{.underline}esearch [[and
**d**]{.underline}]{.mark}evelopment. [**[Reg]{.mark}**ulation[**s**
that make it]{.mark} **more [expensive]{.mark}** to develop AIs or
prevent certain uses might **delay** or **[forestall]{.mark}** those
[efforts]{.mark}. This is **[particularly]{.mark}** true [for **small
businesses** and **individuals**---**key drivers**]{.mark} of new
technologies---who are **[not]{.mark}** as **well [equipped]{.mark}**
[to deal with]{.mark} regulation **[compliance]{.mark}** as larger
companies]{.underline}.

In fact, the biggest beneficiary of AI regulation may be large companies
that are used to dealing with it, because startups will have a harder
time competing in a regulated environment. Even ambiguity regarding
regulation and what aspects of AI are regulated may be problematic, as
it may cause people to avoid innovation to avoid risking inadvertent
ensnarement by vague regulations and potential penalties.

[Humanity faced]{.underline} a [similar]{.underline} set of [issues in
the early days of the **internet**. But the **U**]{.underline}nited
**[S]{.underline}**tates [actively avoided [regulating]{.mark} the
internet to avoid **[stunt]{.mark}**ing its **[early
growth]{.mark}**]{.underline}.\[39\] Elon Musk's PayPal and numerous
other businesses helped build the modern online world while subject only
to regular human-scale rules, like those preventing theft and fraud.
Similarly, no special rules were rolled out to govern early software
businesses, such as Microsoft, in their burgeoning years, that have gone
on to become industry titans.

## Affirmative

### 2AC -- No Russia AI Lead

#### Non-unique and we'll pre-empt their uniqueness warrants -- the invasion haulted Russia's AI developments -- even if they win that AI is being implemented and developed now -- the war severely changed the trajectory of it

- Agrees that Russia was ahead on Ethical AI and AI implementation in
  2021 and they planned to continue this onto 2022, but the invasion
  stopped that progress that they were making that would've put them
  ahead in the first place

**Bendett, 22**

\[Samuel, Adjunct Senior Fellow at the Center for a New American
Security and an Adviser at the CNA Corporation, April 15, 2022,
"Russia's Artificial Intelligence Boom May Not Survive the War", Defense
One,
<https://www.defenseone.com/ideas/2022/04/russias-artificial-intelligence-boom-may-not-survive-war/365743/>,
accessed 7-18-2022, BB\]

But [[talk of AI has been muted since the Russian invasion]{.mark} of
Ukraine]{.underline}. Apart from the widespread use of UAVs for
reconnaissance and target acquisition and a single display of a
mine-clearing robot---all of which are remote-controlled---[there is no
overt evidence of Russian AI in C4ISR or decision-making among the
Russian military forces, other than a single public deepfake attempt to
discredit the Ukrainian government.]{.underline} That does not mean AI
isn't used, considering how Ukrainians are now utilizing artificial
intelligence in data analysis---but there is a notable absence of larger
discussion about this technology in open-source Russian media.

The [gap between Russian military aspirations for high-tech warfare of
the future and the actual conduct of war today is becoming
clear]{.underline}. In January 2021, Colonel-General Vladimir
Zarudnitsky, the head of the Military Academy of the Russian Armed
Forces General Staff, wrote that the development and use of unmanned and
autonomous military systems, the "robotization" of all spheres of armed
conflict, and [the development of AI for robotics will have the greatest
medium-term effect on the Russian armed forces' ability to meet their
future challenges]{.underline}. Other MOD military experts also debated
the impact of these emerging technologies on the Russian military and
future balance of forces. [Russia continued to upgrade and replace
Soviet-made systems, part of the MOD's drive from
"digitization"]{.underline} (weapons with modern information
technologies for C4ISR) to "intellectualization" (widespread
implementation of AI capable of performing human-like creative thinking
functions). These and other developments were covered in detail during
Russia's "Army-2021" conference, with AI as a key element in C4ISR at
the tactical and strategic levels.

Meanwhile, [Russian military developers and researchers worked on
multiple AI-enabled robotics projects, including the "Marker" concept
unmanned ground vehicle and its autonomous operation in groups and with
UAVs]{.underline}.

[Toward the end of 2021, the state agency responsible for exporting
Russian military technology even announced plans to offer unmanned
aviation, robotics, and high-tech products with artificial intelligence
elements to potential customers this year.]{.underline} The agency
emphasized the equipment is geared toward defensive, border protection,
and counter-terrorism capabilities.

[[Since the invasion, things have changed]{.mark}. [Russia's
defense-industrial complex]{.mark}---especially military high-tech and
AI research and development]{.underline}---may be [affected by the
international sanctions and cascading effects of Russia being cut off
from semi-conductor and microprocessor imports.]{.underline}

Throughout 2021, [[the Russian government was pushing for the adoption
of its AI civilian initiatives across the country]{.mark}, such as
nationwide hackathons aimed at different age groups with the aim of
making artificial intelligence familiar at home, work, and
school]{.underline}. The [[government also pushed for the digital
transformation of science and higher education, emphasizing the
development of AI, big data, and the internet of
things.]{.underline}]{.mark}

[Russian academic AI R&D efforts drove predictive
analytics;]{.underline} development of chat bots that process text and
voice messages and resolve user issues without human intervention; and
technologies for working with biometric data. [Russia's development of
facial recognition technology continued apace, with key efforts
implemented across Moscow and other large cities. AI as a key image
recognition and data analytical tool was used in many medical projects
and efforts dealing with large data sets.]{.underline}

[Russian government officials noted their country's efforts in promoting
the ethics of artificial intelligence, and expressed confidence in
Russia's continued participation in this UN-sponsored work]{.underline}.
The Russian Council for the Development of the Digital Economy has
officially called for a ban on artificial intelligence algorithms that
discriminate against people.

[Russia's Ministry of Economic Development was asked to \"create a
mechanism for assessing the humanitarian impact of the consequences of
the introduction of such \[AI\] technologies, including in the provision
of state and municipal services to citizens,\"]{.underline} and to
prepare a \"road map\" for effective regulation, use, and
implementation. [According to the council, citizens should be able to
appeal AI decisions digitally, and such a complaint should only be
considered by a human. The council also proposed developing legal
mechanisms to compensate for damage caused as a result of AI
use.]{.underline}

In October, [[Russia's leading information and communications companies
adopted the National Code of Ethics in the Field of
A]{.mark}I]{.underline}; the code was recommended for all participants
in the AI market, including government, business, Russian and foreign
developers. [Among the basic principles in the code are a human-centered
approach to the development of this technology and the safety of working
with data.]{.underline}

[[AI workforce development was spelled out as a key requirement when the
government officially unveiled the national AI roadmap in
2019]{.mark}.]{.underline} A 2021 government poll that tried to gauge
the level of confidence in the government's [AI efforts showed that only
about 64 percent of domestic AI specialists were satisfied with the
working conditions in Russia]{.underline}.

[The survey reflected the microcosm of AI research, development,
testing, and evaluation in Russia]{.underline}---lots of government
activity and different efforts [that did not automatically translate
into a productive ecosystem conducive for developing AI, some major
efforts notwithstanding.]{.underline}

[Among some of the reasons in 2021 that Russia was lagging behind in the
development of artificial intelligence technologies were the personnel
shortage and the weakness of the venture capital market]{.underline}.
The [civilian developer community also noted the low penetration of
Russian products into foreign markets, dependence on imports, slow
introduction of products into business and government bodies, and a weak
connection between AI theory and practice.]{.underline}

[[Russia's]{.mark} likely [plans to concentrate on these areas in 2022
were revised or put on hold once Russia invaded
Ukraine]{.mark}]{.underline}. [The sudden [pull-out of major IT and
high-tech companies from Russia]{.mark}, [coupled with a rapid brain
drain of Russia's IT workers, and the ever-expanding high-tech sanctions
against the Russian state may hobble domestic AI research and
development for years to come]{.mark}]{.underline}. [While the Russian
government is trying to prop up its AI and high-tech industry with
subsidies, funding, and legislative support,]{.underline} the
[[impact]{.mark} of the above-mentioned consequences [may be too much
for the still-growing and evolving Russian AI
ecosystem]{.mark}]{.underline}. [That does not mean AI research and
development will stop---on the contrary, many 2021 trends, efforts, and
inventions are being implemented into the Russian economy and society in
2022,]{.underline} [and there are domestic high-tech companies and
public-private partnerships which are trying to fill the void left by
the departed global IT majors.]{.underline} [But [the effects of the
invasion will be felt in the AI ecosystem for a long time, especially
with so many IT workers leaving the country]{.mark}]{.underline}, either
because of the massive impact on the high-tech economy, or because they
disagree with the war, or both.

[One of [the most-felt sanctions aftereffects has been the severing of
international cooperation on AI among Russian universities and research
instructions, which earlier was enshrined as one of the most important
drivers for domestic AI R&D]{.mark}, and reinforced by support from the
Kremlin]{.underline}. For most high-tech institutions around the world,
[the [impact of civilian destruction across Ukraine by the Russian
military greatly outweighs the need to engage Russia on
A]{.mark}I]{.underline}. [At the same time, much of the Russian military
AI R&D took place in a siloed environment---in many cases behind a
classified firewall and without significant public-private
cooperation---so it's hard to estimate just how sanctions will affect
Russian military AI efforts.]{.underline}

[While [many in Russia now look to China as a substitute for departed
global commercial relationships and products, it's not clear if Beijing
could fully replace the software and hardware products and services that
left Russian markets at this point.]{.mark}]{.underline}

[Recent events may not stop Russian civilians and military experts from
discussing how AI influences the conduct of war and peace---but [the
practical implementation of these deliberations may become increasingly
more difficult for a country under global high-tech
isolation.]{.mark}]{.underline}

### 2AC -- No China AI Lead

#### China [doesn't]{.underline} lead in AI ­-- reports use [false metrics]{.underline} (i.e., patents, research publications) -- insert this graph

**Ghi et al. 21** (Trung Ghi; Abhishek Srivastava; Arthur D. Little;
\"The Global AI Arms Race -- How Nations can Avoid being Left Behind\",
January 2021, PRISM,
https://www.adlittle.com/sites/default/files/prism/Global%20AI%20article.pdf,
DOA: 7-18-2022)//ATJ

[[There are several]{.mark} country [rankings of AI strength]{.mark}
across the world. [Those that focus on]{.mark} metrics such as
[patents]{.mark} and research publications tend to [list China first,
followed by the]{.mark} [US]{.mark}]{.underline}, with third place
disputed between European and Asian countries including South Korea,
Japan and India.

[[However]{.mark}, taking [a broader approach using]{.mark} a
[composite]{.mark} AI-readiness index]{.underline} (from Oxford
Insights) [that [factors in]{.mark} governance, [skills]{.mark} and
education, [infrastructure and data]{.mark}, and government/public
services [reveals the top three countries to be Singapore]{.mark}, the
[UK and Germany]{.mark}.]{.underline} (See Figure 1.)

![Chart Description automatically
generated](media/image1.png){width="4.416601049868766in"
height="2.6437948381452316in"}

#### Still pertinent today -- the US leads by a [wide margin]{.underline} above China -- insert this chart

**O.I. 22** (Oxford Insights; \"Government AI Readiness Index 2021\",
January 2022, Oxford Insights,
https://static1.squarespace.com/static/58b2e92c1e5b6c828058484e/t/61ead0752e7529590e98d35f/1642778757117/Government_AI_Readiness_21.pdf,
DOA: 7-18-2022)//ATJ

![: the US firmly leads the AI index, with China coming in at
15th.](media/image2.png){alt="Table Description automatically generated"
width="3.1416141732283465in" height="4.6426071741032375in"}

#### Not unique --- China is not overtaking the US.

**Cooper and Kompella 22** --- [James Cooper]{.underline}, professor of
law and director of International Legal Studies at California Western
School of Law, research fellow at Singapore University of Social
Sciences, J.D. from the University of Toronto (Canada), LL.M. from the
University of Cambridge (U.K), and [Kashyap Kompella]{.underline},
technology industry analyst and CEO of RPA2AI, a global artificial
intelligence advisory firm, Masters in Business Laws from National Law
School of India University (India), 2022 ("No, China is not winning the
AI race," *The Hill*, February 3^rd^, Available Online at
<https://thehill.com/opinion/technology/592270-no-china-is-not-winning-the-ai-race/>,
Accessed 07-19-2022)

[[The global competition between the U]{.mark}nited [S]{.mark}tates [and
China continues]{.mark} apace.]{.underline} Technology is rightly seen
as providing unique leverage to win this geopolitical race. [The U.S.
long has been the global technology powerhouse, but not surprisingly, we
have heard much about the Chinese government's ambition to dominate
high-tech industries such as 5G]{.underline} telecommunications,
[autonomous vehicles, blockchain, and semiconductor chips]{.underline}.

In this light, as a horizontal technology that can be applied across all
sectors, [artificial intelligence ([AI]{.mark}) [has become a strategic
priority]{.mark} and the Chinese focus on superiority in this field is
touted as something about which the U.S. should be concerned.
[Some]{.mark} have gone so far as to [conclude that the West has already
lost the AI race]{.mark}.]{.underline}

[[Don't believe the hype]{.mark}.]{.underline} To be sure, the
availability of large amounts of data is at the heart of AI success. It
is tempting to think that less-democratic regimes that amass huge
amounts of data about their citizens and have scant regard for privacy
can develop better AI systems using that data. However, all other things
being equal, [[better]{.mark} and higher quality AI [systems emerge from
countries]{.mark} [with strong data privacy and]{.mark} data
[protection]{.mark} [regulations]{.mark} because AI systems must undergo
greater scrutiny during their development and deployment]{.underline}.
An example of this can be seen in the United States regarding fair
lending practices and consumer protection from credit bureaus. Further,
the market for AI is global, and such high-quality AI systems find
buyers in other countries as well.

Around the globe, Big Tech's rising power has resulted in calls for more
oversight. In a drastic move that stunned the industry and analysts
alike, the Chinese government recently rewrote the rulebook for the
country's technology industry. In effect, China is vacating entire
swaths of digital and creative industries, arenas that serve as training
grounds and talent factories for other industries. This more restrictive
approach may not bode well for China's AI industry in the long term.
China may find itself constrained on the extent of automation and AI in
its manufacturing sector --- labor-intensive manufacturing remains
China's main strength, and a high degree of automation can result in job
losses, labor unrest, and instability.

Meanwhile, there is bipartisan support for AI in the United States.
Former President Trump proposed increasing funding for AI development
through the National Science Foundation. The National AI Initiative Act
of 2020 signaled a sense of urgency and suggested that several federal
agencies create a national strategy on artificial intelligence. The
Biden administration has formed the Artificial Intelligence Research
Resource Task Force to develop a roadmap to foment AI research and spark
innovation nationwide. There is draft legislation, at both the state and
federal level, to promote responsible use of AI and prevent its misuse.

Strong objections to the use of facial recognition and other AI systems
by law enforcement in the U.S., raised by civil liberties advocates,
have led some local authorities, such as the City of San Francisco, to
ban such systems. To use a Silicon Valley phrase, these debates are "not
a bug, but a feature." They shine a light on the limitations of AI
systems and help to set the "rules of the road" for proper use of AI.
This will establish the U.S. as a global leader in AI regulation, once
lawmakers and regulators do their work. China, meanwhile, has faced
strong global criticism for using facial recognition software to monitor
and surveil Uyghurs in its Xinjiang region. China has outlined a set of
AI ethics principles, but the jury is still out on enforcement and how
they function in practice.

[[The]{.mark} increasing [number of]{.mark} AI research [papers and
patents]{.mark} by Chinese researchers [is]{.mark} often [cited as proof
that China has caught up]{.mark} with the United States in this field.
The increased focus is good for the Chinese AI ecosystem, and it will
help them solve China-specific problems. [But dominance]{.mark} in this
emerging strategic industry [is not guaranteed]{.mark}.]{.underline}
[The U.S. has several strategic advantages, including: the strengths of
its higher education and research institutes]{.underline}, which attract
the best STEM talent from across the world; the largest venture capital
ecosystem; and the largest number of technology unicorns (start-ups with
private valuations greater than \$1 billion).

[[China is not overtaking the U.S.]{.mark} in artificial
intelligence]{.underline}. The current evidence and trajectory paint a
clear picture: [[The conditions for AI to flourish]{.mark}, such as
incentives to experiment, freedom to pursue opportunities without
restrictions, and the coming guardrails to prevent misuse, [favor U.S.
leadership]{.mark}]{.underline}. This is still the United States's game
to lose --- though maybe both countries could win through collaboration.
To solve planet-scale problems such as climate change, we are going to
need AI solutions from both competitors.

#### The US leads China in defense AI --- increased spending.

**Greene 21** --- Tristan Greene, editor and technology reporter at TNW,
2021 ("Here's why the US continues to beat China in the AI race," *TNW*,
June 2^nd^, Available Online at
<https://thenextweb.com/news/heres-why-the-us-continues-to-beat-china-in-the-ai-race>,
Accessed 07-18-2022)

[[The]{.mark} global [AI race was supposed to be a sprint]{.mark}. Back
in 2017 when driverless cars and domestic robots were thought to be just
around the corner, the promise of deep learning made it seem like we
were mere months away from living in an AI-powered utopia.]{.underline}

[As [it turns out]{.mark}, the global AI race is [more of a
marathon]{.mark}. And [the US has a huge lead that'll be difficult to
overcome]{.mark} for any country, but [especially
China]{.mark}.]{.underline}

The setup

It was easy to believe China would pull ahead a few years ago. US big
tech companies such as Microsoft and Apple had always co-existed with
eastern outfits. But, once deep learning exploded in 2014, many experts
believed China would use its government influence to direct the flow of
research in ways the EU and US' respective leaders simply couldn't.

And, for a while, it looked like that was going to be enough to propel
the PRC to the top of the global AI leaderboards.

In the west, a lion's share of AI research ends up patented by
businesses who keep their algorithms in a walled-garden. But in the east
things are different.

Per an article in the Harvard Business Review:

Unlike in Western developed economies where companies are the primary
holders of AI patents, in China, the majority of AI patents are filed by
universities and research institutes, most of which are government owned
or sponsored.

China's big problem

[[The biggest problem China has]{.mark} when it comes to AI [is a lack
of innovation]{.mark}. Consumer demand is at an all-time high for deep
learning technologies in China, but this social trend isn't translating
into breakthroughs.]{.underline}

[In essence, China is still playing catch up. The Chinese government may
be pouring more money into research and producing more of it, but US
tech companies are raising and spending more on research outside of
academia.]{.underline}

[[The US]{.mark} government still [spends more on defense AI]{.mark}
than China, and US [businesses spend more money on]{.mark} cutting-edge
[research]{.mark} than Chinese companies do.]{.underline}

Simply put, [the biggest technology companies in the US can afford to
invest in breakthrough research even when such research leads nowhere.
The profit margins are much leaner at most Chinese firms so the
incentive is typically on producing a profit.]{.underline}

Unfortunately for China, much of its AI position is rooted in developing
Chinese-language versions of language recognition software and creating
surveillance technology -- neither of those are very marketable outside
of places where Chinese is spoken or where privacy laws exist.

What it all means

[Deep learning might not be the best path forward for artificial
intelligence technologies. This is great news for big tech companies in
the US. But it's bad news for China.]{.underline}

In the US, where most of the AI breakthroughs tend to come from big tech
companies with large enough coffers to afford supercomputers and high
enough salaries to lure away academia's brightest, scientists won't miss
a beat if we transition away from deep learning

But China's heavily-saturated market likely won't extend beyond its own
bubble, much less the deep learning bubble that could pop and leave
AI-only companies behind. There's a reason why there's only one Chinese
firm among the top five richest technology companies in the world.

It'll be tough for academia in China to keep up with big tech in the US
no matter how much data it can generate or acquire.

We're more likely to see these kinds of catch-up cycles end in
cooling-off cycles when heavy government investment doesn't pay off.
China could be headed for an AI winter.

### 2AC -- Regulation good

#### AI arms race leads to a [race to the bottom]{.underline} on AI safety, which [undermines]{.underline} international stability

**Scharre 21** (Paul Scharre, holds a **Ph.D.** in War Studies from
King's College London and an M.A. in Political Economy and Public Policy
and a B.S. in Physics, cum laude, from Washington University in St.
Louis, Vice President and Director of Studies at the Center for a New
American Security, previously worked in the Office of the Secretary of
Defense; \"Debunking the AI Arms Race Theory\", Texas National Security
Review, vol. 4, iss. 3, summer 2021, 121-132, DOI: 10.26153/tsw/13985,
DOA: 4-14-2022)//ATJ

[**Race to the Bottom on Safety**]{.underline} A related risk of [a
**"racing" dynamic** among competitors could come from an
acceleration]{.underline}, not of the pace of operations on the
battlefield, but [of the process of fielding new AI systems. [AI
systems]{.mark} today [have]{.mark} a **host** of **safety and [security
problems]{.mark}** [that]{.mark} can [make them]{.mark} brittle,
[**unreliable**, and **insecure**]{.mark}.]{.underline}29 Because
[**[m]{.mark}achine [l]{.mark}earning** in particular can
[create]{.mark} **new [ways in which]{.mark} [systems]{.mark} can
[fail]{.mark}**]{.underline}, militaries face novel challenges in
adopting AI systems.30 Militaries will have to adopt new [methods to
test, evaluate, verify, and validate AI systems]{.underline} (also known
as TEVV).31 Such concerns related to autonomy are well known in the U.S.
defense community,32 although at present [they have **not been solved**
to a satisfactory degree.]{.underline} Machine learning introduces
additional challenges with regard to testing, evaluation, verification,
and validation. [[A rush to field AI]{.mark} systems before they are
fully tested could [result in a **"race to the bottom" on
safety**]{.mark}, with [militaries **field**]{.mark}**ing**
**[accident-prone]{.mark} AI [systems]{.mark}**.]{.underline} There are
strong bureaucratic and institutional imperatives for militaries to
field systems that are robust and secure. Indeed, designing systems to
military specification standards often means making them more robust for
a wider range of environmental conditions and shocks than comparable
commercial systems, even at the expense of other aspects of performance,
such as size, weight, or usability. AI presents novel challenges,
however, in achieving the robustness needed for operating in the
complex, hazardous, and adversarial environments that often characterize
military operations. Certain AI methods today, such as deep learning,
remain relatively immature with significant reliability challenges. A
2017 Department of Defense report by the JASON scientific advisory group
explained that [deep neural networks are immature as regards the
"illities", including reliability, maintainability, accountability,
validation and verification, debug-ability, evolvability, fragility,
attackability, and so forth]{.underline}. ... Further, it is not clear
that the existing AI paradigm is immediately amenable to any sort of
software engineering validation and verification. This is a serious
issue, and is a potential roadblock to DoD's \[Department of Defense's\]
use of these modern AI systems, especially when considering the
liability and accountability of using AI in lethal systems.33 The
Defense Department's 2018 AI strategy calls for building AI systems that
are "resilient, robust, reliable, and secure."34 Yet, the current state
of technology makes achieving this goal particularly difficult for AI
systems that incorporate deep learning, a subfield of AI that has seen
significant growth and attention in recent years. While there is active
research underway to improve AI safety and security, militaries will
have to adapt to the technology as it currently is, at least for the
time being. An ideal process would be for militaries to engage in
experimentation, prototyping, and concept development, but also to
subject AI systems to rigorous TEVV under realistic operational
conditions before deployment. [[Taking **shortcuts**]{.mark} on testing
and evaluation and fielding a system before it is fully tested could
**[lead to]{.mark} [accidents]{.mark}**[, which]{.mark}, in some
settings, could **[undermine international stability]{.mark}**.
[In]{.mark} evaluating [new tech]{.mark}nologies,
[**militaries**]{.mark} may be relatively accepting of the risk of
accidents, which may lead them to **[tolerate]{.mark} the deployment of
systems that have [reliability concerns]{.mark}**.]{.underline} In
building and fielding new capabilities, [militaries have to weigh the
possibility of an accident occurring against other concerns, such as
forgoing valuable military capabilities. The military operational
environment is **[fraught with risk]{.mark}**]{.underline}, in both
training and real-world operations. Military institutions balance
managing this risk with other factors, such as the need for training,
developing new capabilities, or accomplishing the mission. [Military
institutions view casualties from training accidents or testing new
capabilities as a tragic but **unavoidable part of the business** of
preparing for war.]{.underline} Militaries expect high performance from
their forces, often while they are performing dangerous tasks, but
militaries neither demand nor expect accident-free operations in most
settings.35 From 2006 to 2020, over 5,000 U.S. servicemembers were
killed in non-war related accidents, the majority of which occurred
within the United States. Accidents overall accounted for nearly 32
percent of U.S. servicemember deaths during this period, and even
accounted for a significant portion of servicemember deaths in Iraq (19
percent) and Afghanistan (16 percent).36 These [**accident rates are not
unusual** for the U.S. armed forces. This is business as
usual.]{.underline} Accidents draw the attention of senior military and
civilian officials when a spate of accidents occur in a short amount of
time --- such as a series of aircraft crashes,37 ship collisions,38 or
training accidents.39 Yet, as one report on naval accidents from 1945 to
1988 notes, "peacetime naval accidents are a fact of life."40 The same
is true of military air and ground operations. Other nations' militaries
may do an even poorer job of managing risk when it comes to accidents
than the U.S. military. For example, the Soviet/Russian submarine
community has a much higher accident rate than the U.S. submarine
community.41 [[New tech]{.mark}nologies **in particular**
[present]{.mark} an **[increased risk of]{.mark} [accidents]{.mark}**,
yet [militaries]{.mark} may **[press ahead]{.mark} out of a desire to
develop** and field]{.underline} what they perceive to be a valuable
capability. For example, the V-22 Osprey tiltrotor aircraft suffered
four crashes during development, killing 30 U.S. servicemembers in
total, yet the Defense Department continued development.42 The V-22
program manager cited a rush to develop the technology as a factor in
the accidents, stating, "Meeting a funding deadline was more important
than making sure we'd done all the testing we could."43 [**[Taking
shortcuts]{.mark}** on testing in particular appears to have been a
**factor**]{.underline} in at least one fatal crash. According to a
Government Accountability Office investigation of the V-22 program,
"schedule pressures" led the program to conduct only 33 of 103 planned
tests of an aerodynamic phenomenon called a "vortex ring state,"44 a
phenomenon that later caused an April 2000 crash that killed 19
servicemembers.45 Absent competitive dynamics, militaries may be able to
manage the challenges of fielding safe AI systems to a more-or-less
satisfactory degree, albeit with some risk of an accident occurring.
However, [out of a **desire to [field AI]{.mark}** capabilities
[**ahead**]{.mark} of **competitors**, militaries may be **more willing
to [accept risk]{.mark} than they might otherwise be** and to **[field
systems]{.mark} that are [prone to mishaps]{.mark}**.]{.underline}46
[Similar **competitive dynamics** may have played a role in
**accidents** with **self-driving cars** and commercial airline
**autopilot** technology, as **companies rushed to beat others** to
market.]{.underline}47 [These dynamics]{.underline}, while not an arms
race, could [lead militaries to engage in **a "race to the bottom" on
safety**. **This [risk]{.mark} could [become]{.mark} particularly [acute
in wartime]{.mark}**. Managing these risks is challenging because
**assessing them** can be **difficult**, especially when it comes to new
technologies. **Accident rates** may be well-**known for mature
technologies**, but they are **unknown for technologies still in
development**.]{.underline} In the case of the V-22 Osprey development,
for example, it is not as though the Defense Department knew that
developing it would lead to multiple crashes and 30 fatalities but
decided that achieving the capability was worth the cost.
[**[Engineers]{.mark}, testers, and program managers [are flying in the
dark]{.mark}** when it comes to new technologies]{.underline} --- that
is, after all, the point of testing new systems. [The concern is not
only that organizations may take measured risks to field new
capabilities, but also that **[institutional]{.mark} and bureaucratic
[imperatives]{.mark}** may lead organizations to **[distort]{.mark}
their own perceptions of [risk]{.mark}**, further **contributing to
accidents**.]{.underline} [This **sociological phenomenon** has been
cited as a cause in the **1986 Space Shuttle Challenger
explosion**]{.underline}, for example.48

#### Proactive AI regulations [protect]{.underline} consumers and [drive]{.underline} innovation -- squo ex-post measures [cede]{.underline} AI to industry, [decimate]{.underline} public support for emerging tech, and [crushes]{.underline} innovation -- facial recognition proves

- businesses want proactive regs on AI because the public doesn't trust
  the companies because of the lack of regs

- Empirics prove -- credit bureaus

- Facial recognition is hated and the gov is thinking about banning it
  now only because the companies making it in the beginning had no
  regulatory framework to operate in

- Reed says ex-post regs coming now

**MacCarthy 20** (Mark MacCarthy, holds a **PhD** in philosophy from
Indiana University, MA in Economics from Notre Dame, and a BA from
Fordham University, Senior Fellow in Governance Studies at the Center
for Technology Innovation at Brookings, adjunct professor at Georgetown
University; \"AI needs more regulation, not less\", 3-9-2020, Brookings,
https://www.brookings.edu/research/ai-needs-more-regulation-not-less/,
DOA: 4-19-2022)//ATJ

In the early 1970s, the fledgling credit card industry routinely and
shortsightedly held cardholders liable for fraudulent transactions, even
if their cards had been lost or stolen. In response, Congress passed the
1974 Fair Credit Billing Act to limit cardholder liability. This
[**protection** increased **public trust** in the **new** payment
**system** and **spurred growth and innovation**.]{.underline} Because
they could no longer just pass fraud losses on to cardholders, payment
networks devised one of the first commercial applications of neural
networks to detect out-of-pattern card usage and reduce their fraud
losses.

**[Smart [regulation]{.mark}]{.underline}**, like the above example,
[that gets out [**in front** of]{.mark} **emerging [tech]{.mark}**nology
[can **protect consumers**]{.mark} and [drive **innovation**]{.mark}. In
the **last several decades**, however, [policymakers have
**forgotten**]{.mark} this **[beneficial]{.mark} side [effect]{.mark}**
[of **reg**]{.mark}**ulation**, preferring to [give **industry**]{.mark}
players **[free rein]{.mark}**]{.underline} to deploy emerging
technologies as they see fit.

[The [**grim results** of]{.mark} that **[laissez-faire
philosophy]{.mark}** are all around us today in the form of **[a]{.mark}
still-growing [backlash]{.mark}** []{.mark}against tech
companies.]{.underline} The public darkly suspects that these companies
are interested primarily in promoting their own dominance and not
dealing with deleterious ramifications. [[As]{.mark} a [**result**,
**policymakers**]{.mark} at the state and local levels are **beginning**
to [consider]{.mark} **technology [bans]{.mark}** []{.mark}on **AI**
applications]{.underline} such as facial recognition. **[[The path
forward is]{.mark} not deregulation or prohibitions, but smart,
[proactive regulation]{.mark} that establishes a framework for both
public protection and innovation growth.]{.underline}**

THE WHITE HOUSE AI GUIDANCE HAS GOOD AND BAD NEWS

[The [White House]{.mark} **recently** [released **guidance**
for]{.mark} the regulation of **[AI]{.mark}** applications]{.underline},
establishing a framework that future rulemaking or legislation can build
upon. The good news is that the administration is committed to a
sectoral approach. Since AI is just a collection of statistical
techniques that can be used throughout the economy, it makes no sense to
have a federal AI commission to enforce one-size-fits-all rules. The
White House report wisely encourages sectoral regulators to formulate
rules for the AI applications within their jurisdiction. In a recent
op-ed, former White House official R. David Edelman makes a similar
point about not regulating AI as if it were a single thing.

[Unfortunately, [the report]{.mark} also **[perpetuates]{.mark}** the
**out-of-date, [hands-off approach]{.mark}**. It **encourages**
regulators to think of their activity as one which **holds innovation
back**.]{.underline} Regulators are told that they must "avoid
regulatory or non-regulatory actions that needlessly hamper AI
innovation and growth." [**[Reg]{.mark}ulation** is [seen as]{.mark} a
**cost**, a **hindrance**, a **delay**, or a **barrier** which must be
**reluctantly accepted** as [a **last resort**]{.mark} only if
absolutely necessary.]{.underline}

[The idea that measures such as **transparency**, **accountability**,
and **fairness** might **promote AI growth and innovation** is
**foreign** to this **framework**. But in today's world, the **real
task** for **AI regulators** is to create a rules structure that both
**protects the public** and **promotes industry innovation**---not to
trade off one against the other.]{.underline}

**[NEW LEGISLATION IS NEEDED]{.underline}**

[Many AI applications **cry out** for **before-the-fact legislation**,
**not just** application of **existing rules**.]{.underline} When
Illinois passed its Artificial Intelligence Video Interview Act last
year, some commentators thought it was overreacting to science fiction
speculations. But [the [law]{.mark}]{.underline}, which established
requirements for notice, consent, and explanations when employers use AI
to analyze videos of job applicants, [**[is already behind]{.mark} the
curve**.]{.underline} A host of companies, such as HireVue, are already
using AI video analysis to score job applicants.

Employment screening is riddled with insular, clubby judgments that
perpetuate a uniform workplace rather than finding talented or creative
types. Companies are right to look for fairer and more accurate
algorithmic screening techniques.

Still, except for the new Illinois state law, [AI hiring algorithms are
**[devoid of]{.mark} consumer [protections]{.mark}**.]{.underline}
Vendors provide neither validity tests to show that these techniques
detect traits relevant to job performance, nor disparate impact
assessments to reveal potential discriminatory effects. Employers can
turn job applicants down on the basis of these screenings without ever
having to explain the basis for these adverse actions.

[Policymakers **used to** know what to do when faced with such a
**promising emerging technology**: They would [throw a **regulatory
net**]{.mark} around it [to provide]{.mark} for [**growth** and]{.mark}
**[consumer protection]{.mark}**. [When]{.mark} computerized
**[credit]{.mark} bureaus** [began to spread]{.mark} in the **late
1960s**, [Congress]{.mark} **got ahead of the emerging technology** and
[put]{.mark} in place [the]{.mark} 1970 **[F]{.mark}**air
**[C]{.mark}**redit **[R]{.mark}**eporting
**[A]{.mark}**ct]{.underline}, which established consumer-protection
rights and shielded the bureaus from defamation suits. [The [industry
**expanded**]{.mark} rapidly, but [consumers]{.mark} [remained
**safe**]{.mark}. Passing a national [law]{.mark} **now** [to **regulate
AI**]{.mark}-driven employment tests might similarly **[provide win-win
benefits]{.mark} to AI**]{.underline} firms, employers, and job
applicants.

THE BACKLASH AGAINST FACIAL RECOGNITION

[The **troublesome experience** with [facial recognition shows
what]{.mark} can [happen when companies **rush AI**]{.mark} applications
to market **[without a regulatory safety net]{.mark}**.]{.underline}
Tests at the National Institute for Standards and Technology have
demonstrated that [the technology on the market now has **discriminatory
effects**.]{.underline} Nevertheless, with almost no public scrutiny,
local law enforcement agencies have been using the technology. The
latest such story concerns widespread law enforcement access to
Clearview's trove of (illegally obtained!) photos in pursuit of
lawbreakers---apparently oblivious of the civil liberties risks
involved.

[[As a **result**]{.mark} of this rush to market, [facial
recognition]{.mark} technology **[is in trouble]{.mark}** both here and
abroad.]{.underline} Privacy and civil liberties groups have urged a
suspension of federal government use of facial recognition systems,
pending further review. Scholars have called for a ban, and some states
and cities have already implemented partial bans.

[A ban might be **throwing out the baby with the bathwater.** But, if
the only alternative is after-the-fact regulation to correct whatever
mistakes turn up, a ban or moratorium might make sense.]{.underline} In
a welcome, if belated, development, [key **[industry]{.mark}
participants** have come out in **[favor]{.mark}** of [a proactive
regulatory framework]{.mark}.]{.underline}

**[PROACTIVE REGULATION IS NEEDED]{.underline}**

[Machine learning is the "most important general-purpose technology of
our era." The [calls for **modest regulation**]{.mark} that **lets
industry take the lead** [are]{.mark} part of [a **failed**]{.mark}
**[regulatory philosophy]{.mark}**, one that saw its natural experiment
over the past several decades come up **lacking**. **[AI is too
important]{.mark} and too promising [to be governed]{.mark} in a
[hands-off]{.mark} fashion, [waiting for problems]{.mark} to develop and
[then]{.mark} trying to [fix]{.mark} them [after the
fact.]{.mark}**]{.underline}

It is time to return to the way we used to regulate emerging
technologies. Industry leaders like Google CEO Sundar Pichai have
recently recognized the advantages of proactive, sector-by-sector
regulation of AI applications. [Thoughtful, **far-sighted
[policymakers]{.mark}**, like those in the 1970s who regulated and
jump-started new payment systems and credit bureaus, [need to
**set**]{.mark} **the [rules]{.mark} and priorities [for this vital
tech]{.mark}**nology in a way that **protects consumers** and
**provides** for **innovation** and **growth**.]{.underline}

#### Absent the plan, companies have [free reign]{.underline} in AI -- that leads to [demoware proliferation]{.underline} that produces [useless applications]{.underline} of AI (i.e., deepfakes/image generation vs military/defense uses)

- It's not a question of "who implements AI first" if American AI is bad

**Marcus 22** (Gary Marcus, holds a **PhD** from MIT, founded Geometric
Intelligence, a machine learning company purchased by Uber two years
later in 2016; \"Artificial General Intelligence Is Not as Imminent as
You Might Think\", 6-6-2022, Scientific American,
https://www.scientificamerican.com/article/artificial-general-intelligence-is-not-as-imminent-as-you-might-think1/,
DOA: 7-18-2022)//ATJ

[[To the **average person**, it must seem as if]{.mark} the field of
**[a]{.mark}**rtificial **[i]{.mark}**ntelligence [is making]{.mark}
immense [progress]{.mark}. According to the press releases, and some of
the more gushing media accounts, OpenAI's DALL-E 2 can seemingly create
spectacular images from any text; another OpenAI system called GPT-3 can
talk about just about anything; and a system called Gato that was
released in May by DeepMind]{.underline}, a division of Alphabet,
[seemingly worked well on every task the company could throw at
it.]{.underline} One of DeepMind's high-level executives even went so
far as to brag that in the quest for artificial general intelligence
(AGI), AI that has the flexibility and resourcefulness of human
intelligence, "The Game is Over!" And Elon Musk said recently that he
would be surprised if we didn't have artificial general intelligence by
2029.

[**[Don't be fooled]{.mark}**.]{.underline} Machines may someday be as
smart as people, and perhaps even smarter, but [the game is far from
over. [There is still an **immense** ]{.mark}**amount of [work to be
done]{.mark}** in making machines that truly can comprehend and reason
about the world around them. What [we]{.mark} really [need]{.mark} right
now is [**less posturing** and more **basic
research**]{.mark}.]{.underline}

To be sure, [there are indeed some ways in which AI truly is making
progress---synthetic images look more and more realistic, and speech
recognition can often work in noisy environments---but [we are]{.mark}
still **[light-years away from general]{.mark} purpose, human-level
[AI]{.mark}** that can understand the true meanings of articles and
videos, or deal with unexpected obstacles and interruptions. We are
still stuck on precisely the same challenges that academic
scientists]{.underline} (including myself) [having been pointing out for
years: getting AI to be reliable and getting it to cope with unusual
circumstances.]{.underline}

Take the recently celebrated [[Gato]{.underline}]{.mark}, an alleged
jack of all trades, and how it [captioned an image of a pitcher hurling
a baseball. The system returned three different answers: "A baseball
player pitching a ball on top of a baseball field," "A man throwing a
baseball at a pitcher on a baseball field" and "A baseball player at bat
and a catcher in the dirt during a baseball game." The first response is
correct, but the other two answers include hallucinations of other
players that aren't seen in the image. The system [has no idea what
is]{.mark} [actually in the picture]{.mark} as opposed to what is
typical of roughly similar images.]{.underline} Any baseball fan would
recognize that this was the pitcher who has just thrown the ball, and
not the other way around---and although we expect that a catcher and a
batter are nearby, they obviously do not appear in the image.

Likewise, [[DALL-E]{.mark} 2 [couldn't tell the difference between a
red]{.mark} cube on top of a blue cube [and]{.mark} a [blue cube]{.mark}
on top of a red cube.]{.underline} A newer version of the system,
released in May, couldn't tell the difference between an astronaut
riding a horse and a horse riding an astronaut.

When systems like DALL-E make mistakes, the result is amusing, but
[other AI errors create serious problems. To take another example, a
Tesla on autopilot recently drove directly towards a human worker
carrying a stop sign in the middle of the road]{.underline}, only
slowing down when the human driver intervened. [The system could
recognize humans on their own (as they appeared in the training data)
and stop signs in their usual locations (again as they appeared in the
trained images), but failed to slow down when confronted by the unusual
combination of the two, which put the stop sign in a new and unusual
position.]{.underline}

Unfortunately, [the fact that these [systems]{.mark} still [**fail to be
reliable** and **struggle with novel circumstances**]{.mark} is usually
buried in the fine print. Gato worked well on all the tasks DeepMind
reported, but rarely as well as other contemporary systems. GPT-3 often
creates fluent prose but still struggles with basic arithmetic, and it
has so little grip on reality it is prone to creating sentences like
"Some experts believe that the act of eating a sock helps the brain to
come out of its altered state as a result of meditation," when no expert
ever said any such thing. A cursory look at recent headlines wouldn't
tell you about any of these problems.]{.underline}

[The subplot here is that the biggest teams of [researchers in AI are
**no longer**]{.mark} **to be found [in the academy]{.mark}**, where
peer review used to be coin of the realm, [but in
**corporations**]{.mark}.]{.underline} And
[[corporations]{.underline}]{.mark}, unlike universities, [[have **no
incentive to play fair**]{.mark}.]{.underline} Rather than submitting
their splashy new papers to academic scrutiny, [they have taken to
[publication by **press**]{.mark} **release**, seducing journalists and
**[sidestepping]{.mark}** the [peer review]{.mark} process. We know only
what the companies want us to know.]{.underline}

[In the software industry, [there's a word for this]{.mark} kind of
[strategy]{.mark}: **[demoware]{.mark}**, software designed to look good
for a demo, but not necessarily good enough for the real
world.]{.underline} Often, [[demoware becomes **vaporware**]{.mark},
[announced for **shock**]{.mark} **and awe** in order to discourage
competitors, [but **never released**]{.mark} **at all.**]{.underline}

Chickens do tend to come home to roost though, eventually. Cold fusion
may have sounded great, but you still can't get it at the mall. [The
cost in [AI is likely to be a **winter of deflated
expectations**]{.mark}. Too many products, like driverless cars,
automated radiologists and all-purpose digital agents, have been demoed,
publicized---and never delivered.]{.underline} For now, the investment
dollars keep coming in on promise (who wouldn't like a self-driving
car?), but [if the core problems of reliability and coping with outliers
are not resolved, investment will dry up. We will be [left with]{.mark}
powerful **[deepfakes]{.mark}**, enormous networks that emit **immense
amounts of carbon**, [and]{.mark} solid advances in **[machine
translation]{.mark}**, speech recognition and object recognition,
[but]{.mark} too **[little else to show]{.mark}** for all the
**premature hype.**]{.underline}

[[Deep learning]{.mark} has advanced the ability of machines to
recognize patterns in data, but it [has]{.mark} three **[major
flaws]{.mark}**. The patterns that it learns are, ironically,
**superficial**, not conceptual; the results it creates are **difficult
to interpret**; and the results are difficult to use in the **context of
other processes**]{.underline}, such as memory and reasoning. As Harvard
computer scientist Les Valiant noted, "The central challenge \[going
forward\] is to unify the formulation of ... learning and reasoning."
You can't deal with a person carrying a stop sign if you don't really
understand what a stop sign even is.

[For now, **[we]{.mark} are trapped [in a "local minimum"]{.mark} in
which [companies pursue benchmarks, rather than foundational
ideas]{.mark}**, eking out **small improvements** with the technologies
they already have rather than pausing to ask more **fundamental
questions**.]{.underline} Instead of pursuing flashy
straight-to-the-media demos, we need more people asking basic questions
about how to build systems that can learn and reason at the same time.
Instead, current engineering practice is far ahead of scientific skills,
working harder to use tools that aren't fully understood than to develop
new tools and a clearer theoretical ground. This is why basic research
remains crucial.

#### AI regulation is key to development -- developers only get on board if they can ensure that AI won't become a threat

**Stepken, 21**

\[Axel, chairman of the board of management, October 27, 2021, "AI
regulation -- why it will boost innovation", LinkedIn,
<https://www.linkedin.com/pulse/ai-regulation-why-boost-innovation-axel-stepken>,
accessed 7-18-2022, BB\]

Most people tend to flinch automatically at the mention of regulation.
However, [[regulatory agreements]{.mark} and normative requirements [are
what enable us to benefit from today's global trade
networks]{.mark}.]{.underline} I firmly believe that [an assured
[regulatory framework enhances, rather than impairs, innovation and
economic opportunities]{.mark}]{.underline}[.]{.mark}

[When it comes to innovative technologies, regulation frequently lags
behind market development]{.underline}. This is nothing new. [Regulatory
oversight, particularly for disruptive technologies such as AI
applications]{.underline}, is expected to provide a reliable framework
for both users and companies, [while keeping efforts and expenses at a
reasonable level]{.underline}. But at the same time, it is expected to
support the dynamic development of these technologies and steer them
safely towards the greater and common good.

Complex fields of regulation

The [[fundamental principles of good AI regulation include legal
compliance, interoperability, IT security and data protection]{.mark},
but also the ethical principles of the European Union.]{.underline}
[Technology- and process-related requirements can be drawn up based on
previous regulations and easily operationalised]{.underline}. [A more
difficult aspect in the case of AI applications involves ethical
considerations and how to weigh them against technological
aspects]{.underline}. In addition to the above, AI applications
including their tasks and results may have enormous implications for the
realities of people's lives. This certainly does not make regulatory
oversight any easier.

What is more important, [the performance of an AI
application,]{.underline} which I can improve by feeding it a
significantly greater quantity of more detailed data, or protection of a
patient's personal data?

This is one of the questions that regularly comes up for AI applications
in the medical field.

EU draft legislation: new dimension of risk assessment

[In April 2021, the European Parliament took a first significant step in
this direction by publishing the world's first draft legislation for
categorising AI systems in risk classes]{.underline}. The [[proposal for
a regulation provides for four AI risk classes and is aimed at building
an "ecosystem of trust" towards AI applications]{.mark}.]{.underline} A
new and important aspect is that the proposal looks not only at the
potential risks, but also at the options of individuals affected by AI
decisions -- in other words, how they can understand, doubt or even, if
necessary, reverse these decisions. This is a completely new dimension
of risk assessment and goes far beyond risk management as practised
today.

Regulatory oversight welcome

These [[efforts cannot come too soon]{.mark}. International competition
in the [development and application of AI systems is progressing
rapidly, with stakeholders asking more and more often which of the many
possibilities of AI systems can be reconciled with European
value]{.mark}s]{.underline}. The need for information and regulation in
this area is demonstrated by the increasing number of companies
publishing codes of AI ethics, designed to provide guidance for the
companies' actions and inspire consumer trust. Various surveys among
consumers and companies alike show a demand for regulatory oversight and
certification of AI applications One examples are the surveys (in
German) Unternehmer-Studie 2020 or Verbraucher-Studie 2021 conducted by
TÜV-Verband among companies or consumers respectively.

Regulation brings benefits

While many of us associate the concept of regulatory oversight with
negative connotations, it has tangible benefits when implemented in a
moderate and practical manner.

"[AI quality made in Germany" or "Made in the EU" enables companies and
their AI systems to stand out from their market companions from other
countries.]{.underline}

[[Regulation establishes transparency and trust, enabling faster market
penetration and thus growth in sales revenue.]{.underline}]{.mark}

Conclusive [[regulation provides a framework for the development of AI
systems which may act as a catalyst for high
quality.]{.underline}]{.mark}

It will provide companies with a stable, secure and certain legal
framework for their business operations.

Regulation creates transparency and trust

[[Regulated and verified AI applications create transparency and trust
for consumers and companies alike, which in turn enable faster market
penetration.]{.underline}]{.mark}

[[AI will only be able to unfold its full potential if people are
reassured that AI applications will not disrupt our societal and
economic principles.]{.underline}]{.mark} Given this, I am certain that
clear and [[reliable regulatory oversight of AI applications with a
healthy sense of perspective will generate competitive edge for Germany
and Europe.]{.underline}]{.mark}

#### Regulated AI is key to hamper innovations and get developers on board

**Maliha, 21**

\[George, MD, is a second-year internal medicine resident at the
University of Pennsylvania Health System, 7-13-2021, "To Spur Growth in
AI, We Need a New Approach to Legal Liability", Harvard Business Review,
<https://hbr.org/2021/07/to-spur-growth-in-ai-we-need-a-new-approach-to-legal-liability>,
accessed 7-18-2022, BB\]

[[A]{.mark}rtificial [i]{.mark}ntelligence (]{.underline}AI) [[is
sweeping through industries]{.mark} ranging from cybersecurity to
environmental protection]{.underline} --- [and the Covid-19 pandemic has
only accelerated this trend]{.underline}. [[AI may improve the lives of
millions, but it also will inevitably cause accidents that injure people
or parties]{.underline}]{.mark} --- indeed, it already has through
incidents like autonomous vehicle crashes. [[An outdated liability
system in the United States and other countries, however, is unable to
manage these risks]{.mark}, which is a problem because those risks can
impede AI innovations and adoption]{.underline}. Therefore, [[it is
crucial that we reform the liability system. Doing so will help speed AI
innovations and adoption]{.underline}.]{.mark}

[[Misallocated liability can hamper innovation in several
ways]{.mark}.]{.underline} All else being equal, an [A[I designer
looking to implement a system in one of two industries will avoid the
industry that places more liability on the
designer]{.underline}]{.mark}[.]{.underline} Similarly, [the [end users
of an AI system will resist adoption if an AI algorithm carries further
liability risk without some compensation]{.mark}]{.underline}[.]{.mark}
[Liability reforms are needed to address these issues]{.underline}.
[Many of the changes we advocate involve rebalancing liability among the
players]{.underline} --- from end users (physicians, drivers, and other
consumers of AI) to more upstream actors (e.g., designers,
manufacturers).

#### All their link evidence assumes [reactionary]{.underline} regulation, not [liability]{.underline} regulations

**Maliha, 21**

\[George, MD, is a second-year internal medicine resident at the
University of Pennsylvania Health System, 7-13-2021, "To Spur Growth in
AI, We Need a New Approach to Legal Liability", Harvard Business Review,
<https://hbr.org/2021/07/to-spur-growth-in-ai-we-need-a-new-approach-to-legal-liability>,
accessed 7-18-2022, BB\]

Granted[, [a regulatory scheme that attempts to specify an AI system
completely will almost certainly hamper innovation]{.mark}.]{.underline}
But [those costs may be acceptable in particular areas such as drug
development, where comprehensive Food and Drug Administration regulatory
schemes can replace liability completely.]{.underline}

[[Given the tremendous innovation engendered by AI]{.mark}, [it is often
easy to ignore liability concerns until the offering makes it to market.
Policymakers]{.mark}, designers[, and end users of AI should develop a
balanced liability system to facilitate AI]{.mark}]{.underline} ---
rather than merely react to it. [[Building this 21st century liability
system will ensure that 21st century AI will
flourish.]{.underline}]{.mark}

### 2AC -- R&D turn

#### Non-unique and link turn -- DoD is losing the tech race now -- investing more in R&D is key to combine the private and military sectors to maintain their competitive edge

**Hoffman, 20**

\[Daniel, retired clandestine services officer and former chief of
station with the Central Intelligence Agency. His combined 30 years of
government service included high-level overseas and domestic positions,
7-13-2020, "The US cannot compete with China if our military doesn't
invest in R&D", The Hill,
<https://thehill.com/opinion/national-security/506991-the-us-cannot-compete-with-china-if-our-military-doesnt-invest-in/>,
accessed 7-18-2022, BB\]

The Department of [Defense ([DOD]{.mark}) last month [declared]{.mark}
that 20 [Chinese companies]{.mark}]{.underline}, including
telecommunications firm Huawei and video surveillance company
Hikvision[, [are a threat to U.S. national security because of their
relationship with the]{.mark} Chinese [military]{.mark}. DOD emphasized
that Chinese president Xi Jinping's military-civilian fusion strategy of
exploiting high-technology is the blueprint for "China's global 'return'
to military preeminence."]{.underline}

[[China is relentlessly harnessing artificial
intelligence]{.underline}]{.mark}, neuroscience and quantum
communication to support military research and development and
ubiquitous spying on its citizens and adversaries. The [Trump
administration has taken steps to strengthen our defense against China,
but [the U.S. will not outcompete China without dominating this
century's revolution in technology.]{.mark}]{.underline}

China's ruthless communist autocracy imposes its will on China's
businesses through dictatorial fiat. The [[U.S. is poised to win the
competition to develop and deploy high-technology under the power of
free markets]{.mark} and innovative defense acquisition
policies]{.underline}. [[The key to ensuring American success is for DOD
and private industry to turbo-boost their
collaboration.]{.mark}]{.underline}

[[U.S. private industry now spends more]{.mark} [on]{.mark} research and
development ([R&D) than the U.S. military]{.mark}]{.underline}, which,
according to the most recently released budget, [[calls for increasing
research, development, testing and evaluation of spending by 8.7 percent
to \$104 billion]{.underline}]{.mark}. [[DOD is less influential today
as a purchaser of high-technology than in the past]{.underline}]{.mark}.
[Defense dominates fewer U.S. industries. In 1965, DOD accounted for
over 75 percent of all U.S. semiconductor purchases. By 2012, all
governments worldwide represented less than 2 percent of the
semiconductor market.]{.underline}

In order to excel in the high-technology domain[, [DOD must attract
companies to participate in the defense marketplace or risk losing its
military advantage]{.mark}.]{.underline} [[DOD needs to incentivize
private industry to invest its own resources into military-relevant
R&D]{.mark}.]{.underline} Private industry, in turn, will benefit from
access to DOD capital, expertise and facilities.

[When [encountering challenges working with DOD, companies naturally
diversify their revenue streams]{.mark}]{.underline}. Some companies
choose not to compete for defense contracts because of excessive and
constantly changing regulations, increased costs, auditing requirements,
and instability of funding caused by sequestration, continuing
resolutions and lapses in appropriations.

The [COVID-19 pandemic has highlighted the importance of maintaining a
domestic manufacturing base and being able to speed up the acquisition
process, both of which are critical to the drive for technological
superiority]{.underline}. The [U.S. must strengthen supply chains to
ensure fast, reliable access to critical parts, especially in the event
of a national emergency]{.underline}. Even the most technologically
advanced capabilities will prove ineffective if we do not have the
domestic manufacturing capabilities to manufacture, operate and maintain
these systems.
