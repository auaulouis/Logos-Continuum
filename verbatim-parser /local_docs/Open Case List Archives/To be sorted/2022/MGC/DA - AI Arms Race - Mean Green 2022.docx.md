### 1nc

#### Incorporating new ethics standards on AI causes the US to scale back development and transition to [autonomous weapons]{.underline} -- that makes every conflict worse

**Yoo 17**

(John, Emanuel S. Heller Professor of Law, University of California,
Berkeley, School of Law; Visiting Scholar, American Enterprise
Institute, "Embracing the Machines: Rationalist War and New Weapons
Technologies", California Law Review, Vol. 105, Issue 2) DB

**[Governments and scholars are not always clear about when such attacks
meet the legal standards for an armed attack or something
less]{.underline}**. [For example, the new United States Law of War
Manual, issued by the Department of Defense in 2015, declares that the
existing laws of war should apply to what it calls "cyber
operations."]{.underline}11 [But it then concedes that the rules here
are "not well-settled" and are "likely to continue to
develop."]{.underline}12 [The United States even takes the position that
it may not have a position]{.underline}, as the manual declares that it
does not "preclude the \[Defense\] Department from subsequently changing
its interpretation of the law."13

This Essay argues that **[efforts to constrain new military technologies
with ex ante per se rules, rather than ex post reasonable regulation,
are not only doomed, but dangerous]{.underline}**. [History is littered
with proposals to stop advances in weapons]{.underline}. [Medieval
leaders tried to ban crossbows, early artillery, and firearms because
they violated chivalry and honor]{.underline}.14 During World War I,
nations argued over whether international law prohibited airplanes from
bombing targets or submarines from sinking ships without warning.15 But
by the end of World War II, the United States used atomic bombs to end
the war against Japan.16 International agreements, such as the League of
Nations and the Kellogg-Briand Pact, failed to stop the Axis. History
tells us that restraint arrives through deterrence, not law or morality.
In World War II, the Allies and Axis stockpiled ample arsenals of
chemical weapons, but did not use them for fear of retaliation.17 During
the Cold War, mutually assured destruction ultimately led both
superpowers to agree to limits on, and then reductions of, nuclear
weapons.

[International law, rightly understood, does not prohibit the use of
these new weapons]{.underline}. **[The United States will not stop China
from stealing its government personnel databases by appealing to common
values, but by deploying equally effective offensive and defensive cyber
weapons]{.underline}**. [Europeans will not force the United States to
limit its drone campaign against terrorist leaders through legal
arguments, but they could pressure Washington by refusing to cooperate
with intelligence sharing and joint operations]{.underline}. **[In fact,
limiting, and especially prohibiting, the use of robotic and cyber
weapons could have perverse effects on the very goals of international
law]{.underline}**. **[If nations cannot employ new, more precise
weapons, they will have to resort to traditional conventional warfare,
using human soldiers and pilots in larger numbers with more destructive
weapons]{.underline}**. [In addition to causing greater destruction,
limits on new weapons will discourage nations from using force when the
international system needs it most: to stop terrorism, human rights
disasters, nuclear proliferation, and aggression]{.underline}.
**[Perversely, banning new weapons out of a vague desire to make war
harder to start will make war more destructive and harmful to the
innocent---the very antithesis of the laws of war]{.underline}**.

#### Particularly, European regulatory standards into US AI policy hampers innovation and development of military AI

**Castro and McLaughlin 19**

(Daniel, and Michael, "Ten Ways the Precautionary Principle Undermines
Progress in Artificial Intelligence",
<https://itif.org/publications/2019/02/04/ten-ways-precautionary-principle-undermines-progress-artificial-intelligence/>)
DB

PRECAUTIONARY PRINCIPLE VS. INNOVATION PRINCIPLE

While some people advocate for an almost completely hands-off approach
to regulating new technologies, those who recognize that there is a
legitimate role for government take two distinct approaches toward
action: the precautionary principle and the innovation principle.

[The **precautionary principle** is the idea that if a technological
innovation may carry a risk of harming the public or the environment,
then those proposing the technology should bear the burden of proving it
will not]{.underline}. [If they cannot, governments should limit the use
of the new technology until proven safe]{.underline}. [Those who support
the precautionary principle, which calls for government intervention
even when there is no clear evidence of tangible and imminent threats of
harm, adhere to the cliché it is "better to be safe than
sorry."]{.underline}4 For some technologies, such as nuclear power, the
principle makes sense, because the risk of getting it wrong can be
catastrophic. [However, for most areas of innovation, the precautionary
principle leads to more harm than good because **it generates
hypothetical worst-case scenarios that incorrectly suggest technological
advancement presents severe and irreversible threats**]{.underline}.5

[In contrast, the innovation principle holds that because the
overwhelming majority of technological innovations benefit society and
pose modest and not irreversible risks, government's role should be to
pave the way for widespread innovation while building guardrails, where
necessary, to limit harms]{.underline}. The innovation principle
recognizes that market forces, tort law, existing laws and regulations,
or light-touch targeted interventions can usually manage the risks new
technologies pose. The principle does not, however, argue for a ban on
regulation of new technologies. Instead, it advocates for a case-by-case
approach, suggesting regulations only in those cases where there is a
reasonable expectation that other forces will not suffice and where the
potential harms are more than minor. Moreover, in cases where
regulations are needed, it stresses the importance of designing
regulatory interventions and structuring regulatory enforcement in ways
that minimize the harm to innovation, while still achieving the
regulatory goals. Finally, it focuses more on ensuring that penalties
punish bad actors who cause harm than creating regulations that limit
beneficial and benign uses.6 In other words, speculative concerns should
not hold back concrete benefits.

Perhaps more so than any government, the U.S. federal government adhered
to the innovation principle in its early regulation of the Internet, and
this approach fostered a successful era of innovation and growth in the
U.S. digital economy.7 In contrast, Europe's more heavy-handed approach
limited and continues to limit digital innovation. For example, many
jurisdictions in Europe have restricted the use of ride-sharing apps
like Uber because of concerns about the impact on the local taxi
industry.8

**[Given AI's nascent state of adoption---less than half of businesses
worldwide have embedded even one AI-enabled capability into their
business process---it is crucial that public policy in all nations spur
its development and adoption instead of unnecessarily hindering
it]{.underline}**.9 [Consequently, if policymakers want their nations to
achieve the full benefits of AI, they should base their actions on the
**innovation principle** to foster it rather than use the precautionary
principle to limit, delay, and constrain its progress]{.underline}.10

Unfortunately, **[concerns about potential AI harms lead some
individuals and groups to advocate for public policies based on the
precautionary principle]{.underline}**. As a case in point, Elon Musk in
2017 told the world that AI \"is a fundamental risk to the existence of
civilization" that represents \"a rare case where we need to be
proactive about regulation instead of reactive.\"11 He also warned that
adopting AI is "summoning the demon" and predicted that these advances
could create "an immortal dictator from which we can never escape."12
Recently Musk has since dialed back his warnings, predicting that AI
will not kill us, but only cage us in zoos.13

It is troubling that some people take Musk seriously, but because they
do, it is important to rebut such nonsense: Musk is completely wrong. As
Max Versace, CEO of the robotics and computing company Neurala and
founding director of the Boston University Neuromorphics Lab has
explained, "The likelihood of an AI scientist building Skynet is the
same as someone accidentally building the space station from Legos."14
Likewise, University of Washington AI researcher Pedro Domingos has
stated that "The Terminator scenario, where a super-AI becomes sentient
and subdues mankind with a robot army, has no chance of coming to
pass..."15 Unfortunately, the public is often bombarded with hyperbolic
and incorrect statements decrying AI, which make it more difficult for
policymakers to oppose policies that would hurt AI adoption and to
support policies to enable it.

[Thus, it is not surprising that **several governing bodies embrace the
precautionary principle**]{.underline}. [The **European Parliament**
adopted a resolution in 2017 that research and commercialization of **AI
and robotics** "**should be conducted in accordance with the
precautionary principle**\..."]{.underline}16 And Loubna Bouarfa, a
member of the European Union High-Level Expert Group on Artificial
Intelligence, has even argued that cultural resistance to AI is a
"blessing in disguise."17 [After all, if AI is an existential threat to
our species, policymakers should be unrelentingly focused on limiting
this horror]{.underline}.

[Policies based on the precautionary principle are not cost-free
propositions, however]{.underline}. **[In seeking to eliminate potential
risks, they can reduce potential benefits and create new problems and
unintended consequences]{.underline}**.18 For example, some countries
have implemented bans on importing or cultivating genetically modified
organisms (GMOs)---plants or animals that have altered genetic
code---over fears about their safety.19 This is despite a virtually
unanimous scientific consensus that GMOs are perfectly safe.20 Bans on
GMOs can not only cause higher food prices but also increased greenhouse
gas emissions as more forests become farmland to compensate for the
lower yields of non-GMO crops.21 Moreover, research suggests GMOs could
have saved thousands of lives that perished from malnourishment in
African nations that delayed the approval of GMOs.22 Lastly, the ban on
GMOs by many European nations has severely limited incomes for many
small-scale African farmers.23

Policies based on the precautionary principle almost always stand in the
way of innovations that can help the public, and this report identifies
11 policies that would limit the benefits of AI. The remainder of this
report provides an overview of AI, lists policies based on the
precautionary principle that threaten AI, and analyzes ten detrimental
impacts of such policies. To close, it discusses what governments should
do to reduce and rectify cases where AI use could be harmful.

WHAT IS ARTIFICIAL INTELLIGENCE?

AI is a field of computer science devoted to creating computer systems
that perform operations characteristic of human intelligence, such as
learning and decision making. The term does not imply human-level
intelligence and the level of intelligence in any implementation of AI
can vary greatly. For example, the intelligence level needed for Roomba
vacuum cleaners is significantly lower than what is needed for
autonomous vehicles.24 Regardless, the development of better hardware,
including faster processors and more abundant storage, large data sets,
and more capable algorithms in the last decade have helped AI make
significant advancements and unlocked new applications.25

AI's functions include: a) monitoring, such as rapidly analyzing large
amounts of data to detect abnormalities and patterns in transactions; b)
discovering, including extracting insights from datasets such as the
link between a gene and a disease, and through simulations; c)
predicting, e.g., using forecasting models to analyze trends to make
predictions or recommendations, such as future crop yields; d)
interpreting, such as making sense of patterns in unstructured data such
as images, video, audio, and text; and e) interacting, both with helping
machines interact with one another and also helping humans more easily
interact with computer systems.26

There are a vast and diverse array of uses for AI.27 Early adopters
include parts manufacturers using AI to invent new metal alloys for 3D
printing; pharmaceutical companies using AI to discover lifesaving
drugs; mining companies using AI to predict the location of mineral
deposits; credit card companies using AI to reduce fraud; and farmers
using AI to increase automation. As the technology progresses, AI will
continue to bring significant benefits to individuals and societies.

[AI is a "general purpose technology," meaning, among other things, that
it will affect most functions in the economy]{.underline}. In some
cases, AI will automate work, thereby boosting productivity. [By
increasing the level of automation in virtually every sector, leading to
more efficient processes and higher-quality outputs, AI is poised to
boost per-capita incomes]{.underline}. AI can also complete tasks that
it is not worth paying a human to do but that still create value, such
as writing newspaper articles to summarize Little League games. In other
cases, AI adds a layer of analytics that uncovers insights human workers
would be incapable of providing. In many cases, it boosts both quality
and efficiency. For example, researchers at Stanford have used machine
learning techniques to develop software that can analyze lung tissue
biopsies faster and more accurately than a top human pathologist can.28
AI is also delivering social benefits, such as rapidly analyzing the
deep web to crack down on human trafficking, fighting harassment online,
helping development organizations better target impoverished areas, and
reducing the influence of gender bias in hiring decisions. Finally,
**[AI will be an increasingly important technology for defense and
national security]{.underline}**.

AI POLICIES BASED ON THE PRECAUTIONARY PRINCIPLE

[Too often policies based on the precautionary principle fail to strike
the balance between addressing actual harms posed by AI and not
hindering innovation]{.underline}. [This failure not only harms the
development and adoption of AI but also distracts policymakers from
focusing on more important issues, including both legitimate areas of
concern and ways in which policy can proactively support the development
and adoption of AI]{.underline}. Such misguided policies treat AI in one
of three ways: too dangerous to allow (i.e. bans specific uses of AI);
too dangerous unless proven safe (i.e. prohibits the technology without
special approval from the government); and too dangerous without strict
regulatory interventions (i.e. requires the technology to jump through
unnecessary and costly hoops before operators can use the technology).
**[These policies are misguided not because they create regulation, but
because they create unnecessary barriers to developing and adopting AI
due to exaggerated fears of AI or failures to recognize that existing or
more nuanced regulation would address potential issues]{.underline}**.
For example, it is completely legitimate for policymakers to regulate
autonomous vehicles to ensure their safe use. But it is another matter
for policymakers to limit autonomous vehicles because of possible job
losses. We list 11 examples below of unwise policies based on the
precautionary principle---that have either become law or have generated
support---and we group them into the aforementioned three categories.

Policies That Treat AI as Too Dangerous to Allow

[While many critics advocate that the public should fear future uses of
AI, or at least carefully plan their use, the most extreme form of the
precautionary principle leads to **bans on certain uses of
AI**]{.underline}.29 [Various groups and individuals have called for
bans on various AI applications, including lethal autonomous weapons,
facial recognition, autonomous vehicles, and delivery
robots]{.underline}.30 While bans harm innovation and progress, calls
for banning new technology have a long history. In the late 19th and
early 20th centuries, there were numerous calls to ban automobiles in
towns across the United States and Europe. Some individuals lamented the
loss of horse-and-carriage jobs, while others complained that
automobiles were stirring dust up and causing illnesses. Others called
for a ban on automobiles because they opposed the expense of paving
roads or because they wanted to preserve the sanctity of the Sunday
stroll.31 And in 1982, one New Jersey town even banned pedestrians from
using Sony Walkman audio devices "while crossing a street or jogging
along a municipal or county thoroughfare."32 The town created the ban
for safety reasons but ignored that individuals could both listen to
music and cross streets safely.

In the early 2000s, privacy advocates called for bans of radio frequency
identification (RFID) chips, which use radio waves to transmit data, in
several use cases, including on government identification documents.33
These advocates warned that stores, governments, and even terrorists
would use RFID to track the movements of individuals. For example, the
Electronic Frontier Foundation (EFF) argued that a 2005 U.S. State
Department proposal to require RFID chips in passports would turn
passports into "terrorist beacons," stating "that\'s precisely what
they\'ll become if we allow the State Department to move ahead with this
plan.34 While the fears of stores, governments, or terrorists tracking
individuals with RFID never materialized, RFID tags are helping
manufacturers and retailers increase sales and reduce theft and labor
costs. They are also in U.S. passports, expediting the scanning of
passports.35 Policies that ban technologies do not allow society to gain
the technologies' potential benefits, and most people understand in
hindsight that bans only held back progress.

Banning Lethal Autonomous Weapons

[Many groups have started movements to **ban lethal autonomous
weapons**---autonomous robotics systems that can independently identify
and engage targets based on programmed constraints---due to fears that
they will lead to armed conflict on a scale greater and faster than ever
before]{.underline}. For example, 116 founders of mostly small robotics
and AI companies, including Elon Musk, signed a letter to the United
Nations (UN) in 2017 that urges the body to ban lethal autonomous
weapons.36 In 2018, the UN Secretary-General António Guterres stated
that "machines that have the power and the discretion to take human
lives are politically unacceptable, are morally repugnant, and should be
banned by international law."37 [Also in 2018, members of the **European
Parliament** adopted a resolution asking member states and the European
Council for "the start of international negotiations on a legally
binding instrument prohibiting lethal autonomous weapons
systems."]{.underline}38 **[If policymakers enacted such a ban, it would
slow research into AI, as historically, at least in the United States,
defense agencies have been a source of significant funding for
technology advancement, such as the Internet]{.underline}**. **[And much
of the research to support autonomous weapons would yield dual-use
technology that could be used for commercial purposes]{.underline}**.
**[For example, a fully autonomous tank will likely rely on large
portions of the same algorithms and data used to develop a fully
autonomous military transport vehicle]{.underline}**.39 **[These same
algorithms would be relevant to developing autonomous vehicles for
civilian use]{.underline}**.

#### US leadership in emerging tech prevents war with Russia and China -- goes nuclear

**Kroenig and Gopalaswamy 18**

(Matthew, Associate Professor of Government and Foreign Service at
Georgetown University and Deputy Director for Strategy in the Scowcroft
Center for Strategy, and Bharath, director of the South Asia Center at
the Atlantic Council, "Will disruptive technology cause nuclear war?",
The Bulletin of the Atomic Scientists, 11/12,
<https://thebulletin.org/2018/11/will-disruptive-technology-cause-nuclear-war/>)
DB

Recently, analysts have argued that **[emerging technologies with
military applications may undermine nuclear stability]{.underline}**
(see here, here, and here), [but the logic of these arguments is
debatable and overlooks a more straightforward reason why **new
technology might cause nuclear conflict: by upending the existing
balance of power among nuclear-armed states**]{.underline}. This latter
concern is more probable and dangerous and demands an immediate policy
response. [For more than 70 years, the world has avoided major power
conflict, and many attribute this era of peace to nuclear
weapons]{.underline}. In situations of mutually assured destruction
(MAD), neither side has an incentive to start a conflict because doing
so will only result in its own annihilation. **[The key to this model of
deterrence is the maintenance of secure second-strike capabilities---the
ability to absorb an enemy nuclear attack and respond with a devastating
counterattack]{.underline}**. Recently analysts have begun to worry,
however, that **[new strategic military technologies may make it
possible for a state to conduct a successful first strike on an
enemy]{.underline}**. For example, Chinese colleagues have complained to
me in Track II dialogues that the United States may decide to launch a
sophisticated cyberattack against Chinese nuclear command and control,
essentially turning off China's nuclear forces. Then, Washington will
follow up with a massive strike with conventional cruise and hypersonic
missiles to destroy China's nuclear weapons. Finally, if any Chinese
forces happen to survive, the United States can simply mop up China's
ragged retaliatory strike with advanced missile defenses. China will be
disarmed and US nuclear weapons will still be sitting on the shelf,
untouched. [If the United States, or any other state acquires such a
first-strike capability, then the logic of MAD would be undermined.
Washington may be tempted to launch a nuclear first strike. Or China may
choose instead to use its nuclear weapons early in a conflict before
they can be wiped out---the so-called "use 'em or lose 'em"
problem.]{.underline} According to this logic, therefore, the
appropriate policy response would be to ban outright or control any new
weapon systems that might threaten second-strike capabilities. This way
of thinking about new technology and stability, however, is open to
question. [Would any US president truly decide to launch a massive,
bolt-out-of-the-blue nuclear attack because he or she thought s/he could
get away with it]{.underline}? [And why does it make sense for the
country in the inferior position, in this case China, to intentionally
start a nuclear war that it will almost certainly lose]{.underline}?
More important, [this conceptualization of how new technology affects
stability is too narrow, focused exclusively on how new military
technologies might be used against nuclear forces directly]{.underline}.
Rather, **[we should think more broadly about how new technology might
affect global politics, and, for this, it is helpful to turn to
scholarly international relations theory]{.underline}**. **[The dominant
theory of the causes of war in the academy is the "bargaining model of
war."]{.underline}** This theory identifies **[rapid shifts in the
balance of power as a primary cause of conflict]{.underline}**.
[International politics often presents states with conflicts that they
can settle through peaceful bargaining, but **when bargaining breaks
down, war results**]{.underline}. **[Shifts in the balance of power are
problematic because they undermine effective bargaining]{.underline}**.
After all, [why agree to a deal today if your bargaining position will
be stronger tomorrow?]{.underline} And, [a clear understanding of the
military balance of power can contribute to peace]{.underline}. (Why
start a war you are likely to lose?) But **[shifts in the balance of
power muddy understandings of which states have the
advantage]{.underline}**. You may see where this is going. **[New
technologies threaten to create potentially destabilizing shifts in the
balance of power]{.underline}**. For decades, stability in Europe and
Asia has been supported by US military power. In recent years, however,
the balance of power in Asia has begun to shift, as China has increased
its military capabilities. Already, Beijing has become more assertive in
the region, claiming contested territory in the South China Sea. And the
results of Russia's military modernization have been on full display in
its ongoing intervention in Ukraine. Moreover, [China may have the lead
over the United States in emerging technologies that could be decisive
for the future of military acquisitions and warfare, including 3D
printing, hypersonic missiles, quantum computing, 5G wireless
connectivity, and artificial intelligence]{.underline} (AI). And Russian
President Vladimir Putin is building new unmanned vehicles while
ominously declaring, "**[Whoever leads in AI will rule the
world]{.underline}**." **[If China or Russia are able to incorporate new
technologies into their militaries before the United States, then this
could lead to the kind of rapid shift in the balance of power that often
causes war]{.underline}**. **[If Beijing believes emerging technologies
provide it with a newfound, local military advantage over the United
States, for example, it may be more willing than previously to initiate
conflict over Taiwan]{.underline}**. And **[if Putin thinks new tech has
strengthened his hand, he may be more tempted to launch a Ukraine-style
invasion of a NATO member]{.underline}**. **[Either scenario could bring
these nuclear powers into direct conflict with the United States, and
once nuclear armed states are at war, there is an inherent risk of
nuclear conflict through limited nuclear war strategies, nuclear
brinkmanship, or simple accident or inadvertent
escalation]{.underline}**. This framing of the problem leads to a
different set of policy implications. [The concern is not simply
technologies that threaten to undermine nuclear second-strike
capabilities directly, but, rather, any **technologies that can result
in a meaningful shift in the broader balance of power**]{.underline}.
And [the solution is not to preserve second-strike capabilities, but to
preserve prevailing power balances more broadly]{.underline}. When it
comes to new technology, this means that **[the United States should
seek to maintain an innovation edge]{.underline}**. Washington should
also work with other states, including its nuclear-armed rivals, to
develop a new set of arms control and nonproliferation agreements and
export controls to deny these newer and potentially destabilizing
technologies to potentially hostile states. These are no easy tasks,
**[but the consequences of Washington losing the race for technological
superiority to its autocratic challengers just might mean nuclear
Armageddon]{.underline}**.

## Uniqueness

### Military ai high

#### Military AI development is high -- it's being integrated into all aspects of the military 

**Nurkin and Konaev 22**

(Tate and Margarita, "Eye to eye in AI: Developing artificial
intelligence for national security and defense",
<https://www.atlanticcouncil.org/in-depth-research-reports/report/eye-to-eye-in-ai/#defining-ai>)
DB

Overview of US military progress in AI

[The Pentagon's interest and urgency related to AI is due both to the
accelerating pace of development of technology and, increasingly, the
transformative capabilities it can enable]{.underline}. Indeed, **[AI is
poised to fundamentally alter how militaries think about, prepare for,
carry out, and sustain operations]{.underline}**. Drawing on a previous
Atlantic Council report outline, the "Five Revolutions" framework for
classifying the potential impact of AI across five broad capability
areas, Figure 3 below illustrates the different ways in which AI could
augment human cognitive and physical capabilities, fuse networks and
systems for optimal efficiency and performance, and usher in a new era
of cyber conflict and chaos in the information space, among other
effects.38

[The DoD currently has more than six hundred AI-related efforts in
progress, with a vision to integrate AI into every element of the DoD's
mission---from warfighting operations to support and sustainment
functions to the business operations and processes that undergird the
vast DoD enterprise]{.underline}.39 A February 2022 report by the US
Government Accountability Office (GAO) has found that [the DoD is
pursuing AI capabilities for warfighting that predominantly focus on
"(1) recognizing targets through intelligence and surveillance analysis,
(2) providing recommendations to operators on the battlefield (such as
where to move troops or which weapon is best positioned to respond to a
threat), and (3) increasing the **autonomy of uncrewed
systems**."]{.underline}40 Most of the DoD's AI capabilities, especially
the efforts related to warfighting, are still in development, and not
yet aligned with or integrated into specific systems. And, despite
notable progress in experimentation and some experience with deploying
AI-enabled capabilities in combat operations, there are still
significant challenges ahead for wide-scale adoption.

#### The DoD's focused on AI integration and development now

Horowitz and Kahn 22

(Michael C and Lauren, "Why DoD's New Approach to Data and Artificial
Intelligence Should Enhance National Defense",

[The ability of the United States to compete in the 21st century depends
on U.S. leadership in data and artificial intelligence
(AI).]{.underline} In response, the Department of Defense (DoD) is
taking a new and much-needed approach to U.S. defense efforts in data
and AI. David Spirk, the departing Chief Data Officer of the Pentagon,
made clear yesterday that the office of the Chief Digital and AI Officer
(CDAO), in addition to its other functions, will be the successor
organization for and replace DoD's much-touted Joint Artificial
Intelligence Center (JAIC). [While the JAIC symbolized DoD's efforts to
get smart on AI beginning in 2018, the integration of data and AI
represents a maturation of the U.S. AI approach---one that elevates the
importance of AI in national defense]{.underline}. The JAIC itself was
not as important as what the JAIC stood for---DoD's commitment to U.S.
defense AI leadership. In paving the way forward and getting AI on the
agenda, the JAIC succeeded. [From this point on, a more cohesive
approach to AI and data through the CDAO is more likely to accelerate AI
adoption throughout the U.S. military because it links DoD's AI efforts
with data, the fuel AI requires]{.underline}. **[For U.S. defense AI
adoption, in particular, aligning these organizations could be
game-changing]{.underline}**. [Addressing DoD's siloed data,
standardizing and improving its quality and access, is a precondition to
having the data necessary to train algorithms for many defense uses, and
any future technologies that rely on collecting, processing, and using
information]{.underline}. Implementation will be critical and heavily
dependent on two things. First, to catalyze AI adoption, the CDAO will
need to develop close relationships with the military services and
combatant commands. Second, the CDAO will need to coordinate with DoD's
research and development organizations, such as the Defense Innovation
Unit, leading on AI experimentation and research. [There is hard work
ahead, but the new organizational design is promising]{.underline}.

The office of the CDAO brings together previously independent components
of DoD: the JAIC, the office of the Chief Data Officer, the Defense
Digital Service (DDS), and the Office of Advancing Analytics (ADVANA).
The office of the Chief Data Officer is in charge of data management and
coordination, DDS finds digital solutions for internal data and security
issues, and ADVANA aggregates data and conducts data analytics. The
combination of these offices raised questions about whether an
independent JAIC was necessary for U.S. defense AI leadership.
[Departing CDO Spirk says that the CDAO will be "taking the best parts
of all the organizations it is overseeing and redistributing them for
faster and better decision-making." We agree.]{.underline} [At present,
not only is DoD's data siloed but its AI efforts and initiatives are as
well]{.underline}. According to the company Govini, in FY21, fifteen
separate departments and organizations funded and worked on AI and
AI-adjacent technologies, often without formal coordination or
throughlines. This has led to redundancies, gaps, inconsistencies in
application and access to data and resources, and an overall hodge-podge
of AI efforts. [DoD has acknowledged this and is making organizational
changes necessary to accelerate AI adoption even more by restructuring
its AI approach from the ground up]{.underline}. [Now, CDAO will have
teams working on policy and governance, technology development, and
rolling out data and AI for the Pentagon and the military services, to
avoid bureaucratic duplication and confusion that could undermine the
CDAO's overall authority]{.underline}. [In particular, bringing the data
and AI teams together will improve the data DoD needs for AI
development]{.underline}.

## Link

### General -- ai coop

#### AI cooperation undermines innovation due to diverging perspectives

**HEIKKILÄ 21**

(MELISSA, "NATO wants to set AI standards. If only its members agreed on
the basics.",
<https://www.politico.eu/article/nato-ai-artificial-intelligence-standards-priorities/>)
DB

[On paper, NATO is the ideal organization to go about setting standards
for military applications of artificial intelligence]{.underline}.
**[But the widely divergent priorities and budgets of its 30 members
could get in the way]{.underline}**.

[The Western military alliance has identified artificial intelligence as
a key technology needed to maintain an edge over adversaries, and it
wants to **lead the way in establishing common ground rules** for its
use]{.underline}.

"We need each other more than ever. No country alone or no continent
alone can compete in this era of great power competition," NATO Deputy
Secretary-General Mircea Geoană, the alliance's second in command, said
in an interview with POLITICO.

[The standard-setting effort comes as **China is pressing ahead with AI
applications in the military largely free of democratic
oversight**]{.underline}.

David van Weel, NATO's assistant secretary general for emerging security
challenges, said **[Beijing\'s lack of concern with the tech\'s ethical
implications has sped along the integration of AI into the military
apparatus]{.underline}**.

\"I\'m \... not sure that they\'re having the same debates on principles
of responsible use or they\'re definitely not applying our democratic
values to these technologies," he said.

[Meanwhile, **the EU** --- which has pledged to roll out the world\'s
first binding rules on AI in coming weeks --- is **seeking closer
collaboration** with Washington to oversee emerging technologies,
including artificial intelligence]{.underline}. **[But those efforts
have been slow in getting off the ground]{.underline}**.

For Geoană, that [collaboration will happen at NATO, which is working
closely with the European Union as it prepares AI regulation focusing on
"high risk" applications]{.underline}.

The pitch

[NATO does not regulate, but]{.underline} "**[once NATO sets a standard,
it becomes in terms of defensive security the gold standard in that
respective field]{.underline}**," Geoană said.

[The alliance\'s own AI strategy, to be released before the summer, will
identify ways to operate AI systems responsibly, identify military
applications for the technology, and provide a "platform for allies to
test their AI to see whether it\'s up to NATO standards,"]{.underline}
van Weel said.

[**The strategy will also set ethical guidelines around how to govern AI
systems**, for example by ensuring systems can be shut down by a human
at all times, and to maintain accountability by ensuring a human is
responsible for the actions of AI systems]{.underline}.

"If an adversary would use autonomous AI powered systems in a way that
is not compatible with our values and morals, it would still have
defense implications because we would need to defend and deter against
those systems," van Weel said.

"We need to be aware of that and we need to flag legislators when we
feel that our restrictions are coming into the realm of \[being
detrimental to\] our defense and deterrence," he continued.

Mission impossible?

**[The problem is that NATO\'s members are at very different stages when
it comes to thinking about AI in the military context]{.underline}**.

**[The U.S., the world\'s biggest military spender, has prioritized the
use of AI in the defense realm]{.underline}**. [But in **Europe**,
**most countries**]{.underline} --- France and the Netherlands excepting
--- [**barely mention the technology's defense and military
implications** in their national AI strategies]{.underline}.

"It's absolutely no surprise that [the U.S. had a military AI strategy
before it has a national AI strategy]{.underline},\" but the Europeans
\"did it exactly the other way around,\" said Ulrike Franke, a senior
policy fellow at the European Council on Foreign Relations, said:

That echoes familiar transatlantic differences --- and previous U.S.
President Donald Trump\'s complaints --- over defense spending, but also
highlights the **[different approaches to AI regulation more
broadly]{.underline}**.

[The EU\'s AI strategy takes a cautious line, touting itself as
\"human-centric,\" focused on taming corporate excesses and keeping
citizens\' data safe]{.underline}. **[The U.S., which tends to be light
on regulation and keen on defense, sees things
differently]{.underline}**.

[There are also divergences over what technologies the alliance ought to
develop, including **lethal autonomous weapons systems** --- often
dubbed "killer robots" --- programmed to identify and destroy targets
without human control]{.underline}.

[Powerful NATO members including France, the U.K., and the U.S. have
developed these technologies and oppose a treaty on these weapons, while
others like Belgium and Germany have expressed serious concerns about
the technology]{.underline}.

[These weapons systems have also faced **fierce public opposition from
civil society and human rights groups**]{.underline}, including from
United Nations Secretary-General António Guterres, who in 2018 called
for a ban.

Geoană said the alliance has "retained autonomous weapon systems as part
of the interests of NATO." The group hopes that its upcoming
recommendations will allow the ethical use of the technology without
"stifling innovation."

Staying relevant

**[These issues threaten to hamper NATO\'s standard-setting
drive]{.underline}**. \"I think there's a certain danger that **[if NATO
doesn't take this on as a real challenge, that it may be marginalized by
other such efforts]{.underline}**," Franke said.

### Cyber

#### Cyber ambiguity is prevents attacks -- clarifying cyber redlines undermines deterrence 

**Davis 19**

(Susan, US General Rapporteur, "NATO IN THE CYBER AGE: STRENGTHENING
SECURITY & DEFENCE, STABILISING DETERRENCE", 148 STC 19 E rev. 1,
October,
<https://www.nato-pa.int/download-file?filename=sites/default/files/2019-10/REPORT%20148%20STC%2019%20E%20rev.%201%20fin%20%20-%20NATO%20IN%20THE%20CYBER%20AGE.pdf>)
DB

28\. Although cyber security and defence capabilities continue to
improve, most experts argue that the **[offence has the advantage in
cyber space and that this is unlikely to change soon]{.underline}**.
**[Given sufficient time, skills, and resources, attackers can
perpetrate a cyber attack, finding the targeted system's
vulnerabilities, gaining access, and delivering their
payload]{.underline}**. This is a key reason why **[the Alliance must
complement dissuasion with strategies of deterrence by
punishment]{.underline}**. In other words, they must try "**[to prevent
an attack by threatening unacceptable damage so that in the attacker's
cost-benefit calculations the best choice is not to
attack]{.underline}**" (Morgan, 2009). It should be noted some experts
would argue that offence is not as dominant. For example, the more
sophisticated cyber weapons are, the more opportunities the defender has
to stop an attacker and the more errors the attacker is likely to make.
Additionally, continued organisational deficiencies could be a key
reason why attackers have had the advantage thus far (Slayton, 2017).
29. **[NATO maintains a cyber deterrence policy of
ambiguity]{.underline}**. First, **[it does not draw a clear line for
when a cyber attack is sufficiently harmful to cross the threshold to an
armed attack]{.underline}**. Second, **[it does not currently have an
operational definition of what the collective response would be if that
threshold were to be crossed]{.underline}**. **[Such a cyber deterrence
policy offers several advantages]{.underline}**. **[If the Alliance were
to set a clear threshold, the opponent would better understand how to
stay below that threshold]{.underline}**. **[This would strengthen
deterrence of threats above the threshold but would encourage the
opponent to increase attacks just below the threshold]{.underline}**.
**[A certain degree of ambiguity is beneficial because it could make
opponents wary of going too far in their cyber attacks]{.underline}**.
**[The opponent always fears stepping over the invisible line, and thus
prefers to tread lightly]{.underline}**. **[A similar deterrence posture
arguably worked well during the Cold War]{.underline}**. 30. However,
ambiguity on where the threshold lies could indeed lead an opponent who
is sufficiently comfortable with taking risks to continuously exploit
the "grey zones", test the defender's resolve, and conduct ever more
daring cyber attacks. Arguably, **[the solution for such attacks cannot
be found in deterrence alone, but rather in a clearly defined policy
response for hybrid operations]{.underline}**. **[Allied nations,
individually and collectively, continue to develop such
options]{.underline}**. This is where the United States military saw a
need to shift to an innovative strategy of Persistent Engagement. The
Rapporteur encourages Allies to explore if and how such a strategy can
be most effectively embraced together. 31. **[NATO's ambiguity also
extends to the type of punishment it threatens were it to suffer a cyber
attack]{.underline}**. **[The Alliance has made clear that it neither
limits punishment to similar cyber attacks nor excludes
them]{.underline}**. Instead, **[it keeps the option open to use the
full range of Allied capabilities to deter and counter cyber
attacks]{.underline}**. Once again, **[this introduces useful doubt in
an opponent's mind]{.underline}**. A more technical reason for the
difficulty of restricting retaliation to cyber attacks is that it is
hard to credibly threaten the assets of the attacker in a similar
fashion. If an attacker shuts down a power plant, would the Alliance
have cyber options to attack an opponent's power plants or similar
infrastructure? Would NATO even want to if it could, as it complies with
the principle of proportionality and international law in all its
activities? **[NATO's ambiguity on the type of retaliation serves a
convincing purpose]{.underline}**. **[It produces doubts in the would-be
attacker's mind and presents more options to tailor and scale a response
to re-establish deterrence]{.underline}**. 32. **[A key feature of a
stable deterrence situation is the ability to signal retaliatory
capabilities and resolve to enforce the deterrence
threat]{.underline}**. However, such **[signalling is difficult when it
comes to cyber deterrence]{.underline}**. **[States can hardly display
malicious codes at a military parade or a defence
exhibition]{.underline}**. **[They must, therefore, find different ways
to signal capabilities and resolve impending conflicts]{.underline}**.
For example, demonstrating capabilities in real-world situations
typically makes deterrence threats more plausible (Nye, 2017). Indeed,
many experts argue that recent, limited cyber attacks should, at least
in part, be seen as such demonstrations (Lewis, 2018). Additionally
investing in cyber capabilities in a way visible to an opponent
"generally can help to signal resolve" (Lindsay, 2015). In other words,
transparency on cyber security and defence measures also serves as a
deterrence signal. In the limited way they can signal their cyber
security and defence capabilities, NATO and individual Allies appear to
be making progress. In the public realm, NATO should therefore remain as
transparent as possible when it comes to its cyber capabilities. In
areas where public disclosure is not an option, communicating with
potential opponents through non-public channels should happen as
frequently as possible.

### Norms/regulation

#### Setting ethical standards for use of military AI undermines US competitiveness and causes a switch to worse weapons

**Yoo 17**

(John, Emanuel S. Heller Professor of Law, University of California,
Berkeley, School of Law; Visiting Scholar, American Enterprise
Institute, "Embracing the Machines: Rationalist War and New Weapons
Technologies", California Law Review, Vol. 105, Issue 2) DB

THE DANGERS OF OVERREGULATING NEW WEAPONS TECHNOLOGIES These new types
of weapons allow nations to coerce and pressure each other in novel
ways. This Part argues that **[banning these new weapons will prove
futile]{.underline}**. [It will first base its analysis on a realist
approach to international relations, which assumes that nations pursue
their interest in security above all other interests and that
cooperation will prove difficult due to the anarchic nature of the
international system]{.underline}. It then shows why **[nations will not
agree on the regulation of new weapons systems]{.underline}**. It
follows these predictions about international agreements by arguing that
these **[new weapons may actually reduce the harm to both combatants and
civilians in armed conflict]{.underline}**. **[Counterintuitively, heavy
regulation of drones or cyberweapons may have the unforeseen consequence
of making war more dangerous and destructive]{.underline}**. A. Realism
and Weapons Before we can develop a sensible approach to regulation, we
first must present a theory about international conflict. [Without a
theory, the international system might adopt standards without any
understanding whether it is making the problems of war better or
worse]{.underline}. **[Excessive regulation of new weapons technologies
might unintentionally exacerbate the harms of war by forcing combatants
to use more destructive weapons]{.underline}**. [Unduly lax standards
might have the unintended consequence of allowing both sides to inflict
grievous harms on each other to no military advantage]{.underline}.
Without a theory of war, we do not know what values our legal rules
should maximize and what costs it should minimize. We are not applying a
philosopher's attitude to war. Some believe that nations have followed
laws during armed conflict out of a sense of morality that can trace its
origins to Christian and, before that, Roman just war theories.81 We do
not discount the intellectual tradition of just war theory or its recent
appearances in the works of Michael Walzer, John Rawls, and others.82
But **[the history of war does not reveal a universal morality that
imposes consistent restraints on warfare]{.underline}**.83 [This should
come as no surprise in light of the diversity of the world's religions
and political, economic, and moral belief systems]{.underline}. [Nations
have often rejected moral appeals when they can achieve military
advantage in combat]{.underline}. Instead, we follow a more instrumental
approach. Like realist scholars in the field of international relations,
we assume that **[the international system fundamentally suffers from
anarchy]{.underline}**. [Anarchy does not mean that anything
goes]{.underline}. **[Anarchy does not mean that the world lacks order,
but only that no world authority can enforce international rules in the
same way that domestic governments maintain law and
order]{.underline}**. [The world is composed of independent sovereign
states with no higher sovereign above them]{.underline}. Under realism,
states are obsessed with their own security because they cannot rely
upon a supranational body to guarantee it. Nor can nations hold 100
percent certainty about other nations' motives. As a result, **[states
will fear each other's military capabilities and political
intentions]{.underline}**. They will take self-help measures and act
strategically to guarantee their own survival, regardless of their
internal politics.84 As Kenneth Waltz famously argued, a state's place
within the anarchical international system, rather than its domestic
nature, will dictate national policy.85 Realists believe that
international organizations and international law can do little to
affect the pursuit of power and security. States may sign treaties, but
the legal effect of the agreement does not affect their calculations.
They will always pursue a course of conduct that maximizes their
interests regardless of the presence or absence of a treaty. "Realists
maintain that institutions are basically a reflection of the
distribution of power in the world," John Mearsheimer argued during the
heady days of the end of the Cold War. "[They are based on the
self-interested calculations of the great powers, and they have no
independent effect on state behavior]{.underline}."86 [Realism does not
prevent nations from cooperating for mutual benefit]{.underline}. But
the competition of states will make agreements difficult. First, because
nations worry about their security from attack by other nations, they
will be concerned by the relative gains of cooperation. Two nations
might improve their situations in a deal, but if one of the two
increases its relative advantage as a result, the balance of power might
shift enough to sink the deal. In other words, [states may refuse to
cooperate, even if they were to enjoy absolute gains, if other nations
gain even more]{.underline}. Second, nations may not cooperate even if
they were to gain because they will worry that their partners will
cheat. Without a world government, no higher authority can force
sovereign states to keep their promises.87 These challenges make it
unlikely that nations can commit to end the use of force in their
relations. The 1928 Kellogg-Briand Pact, which purported to ban all war,
stands as a failed symbol of such utopian efforts.88 Other forms of
international cooperation may employ soaring rhetoric but---by design---
produce little real world effect, such as the many resolutions of the UN
General Assembly. But nations can cooperate in limited areas, so long
that the resulting treaties or laws mirror the existing balance of
power. Trade agreements, for example, allow comparative advantage to
improve the economies of all signatories. Nations can cooperate to solve
problems that cross borders, such as pollution or drug trafficking, or
to divide valuable resources, such as underground oil fields or
fisheries. States may even cooperate on matters of war if they can both
benefit without either gaining a military or political advantage. State
practice in waging armed conflicts produces customary practice that
coalesces, over time, into the laws of war. Rules that will come to have
the force of law must leave states better off in pursuing their
interests. **[Nations will reject rules---no matter how noble their
intent or humanitarian their goal---that leave them worse
off]{.underline}**. NATO allowed the United States and Western Europe to
band together to contain the Soviet Union. NATO, however, did not alter
the balance of power between Washington and Moscow. Instead, the
institution served as a mechanism for the United States to better
organize the forces on its side.89 NATO did not alter the balance of
powers within Europe, but allowed the United States to rebuild Germany
without reigniting security fears on the part of France and the United
Kingdom.90 International agreements might also regulate wartime tactics,
operations, and strategy. Nations at war, for example, might treat
prisoners humanely if they can expect that their opponents will behave
similarly.91 They might agree to foreswear chemical weapons if their use
inflicts great suffering without giving either side an advantage.92
War's high stakes, however, will tempt nations to cheat when battlefield
conditions may make reneging harder to detect. It is also important to
recognize that the success of a legal regime on war will depend on the
international context. In a certain period of history, for example, a
few great powers might protect their own security not just by building
defenses, but also by maintaining a rough equality of power with their
rivals. War might occur when nations band together to prevent a rising
nation from upsetting the balance of power. In 1849, Great Britain and
France dispatched troops across Europe to fight the Russian Empire in
Crimea. Even though they had few interests there, London and Paris
believed that Moscow's move into Ottoman territory would upset the
European balance of power. Using machine guns and trench warfare for the
first time, the combatants fought to a stalemate. Under conditions that
produced the static defenses of trench warfare, a balanced distribution
of power and restrained national goals might make limitations on arms
possible. No state would enjoy an advantage either before or after an
agreement. It should be no surprise that the first treaties regulating
warfare, the Hague Regulations of 1899, hail from the highpoint of
Europe's classic balance of power. Different circumstances, however,
might not yield to noble goals. In the wake of World War I's carnage,
for example, the great powers sought to limit the naval arms race that
they thought had caused British-German antagonism. If nations had built
great fleets simply to keep up with their neighbors, a common limit on
dreadnoughts might have restrained competition, thereby saving resources
and reducing conflict. In the Washington Naval Treaty of 1920, the major
western powers and Japan agreed to permanent limits on battleships, with
the United States and Great Britain permitted to maintain the largest
fleets, followed by Japan, France, and Italy.93 The Treaty of Versailles
subjected Germany to far stricter limits on naval building.94 Wracked by
the Great Depression and the rise of fascism, however, the Axis powers
embarked on a campaign of territorial expansion. They circumvented the
Washington and Versailles Treaties, either by concealing vessels (such
as the Bismarck), building weapons that skirted the rules (Germany's
"pocket battleships"), or developing new naval weapons unforeseen by the
drafters (aircraft carriers). As history suggests, [changes in the
international system and in war will have critical consequences for the
nature and success of international rules. Several developments have set
the environment that will surround the new military
technologies]{.underline}. Perhaps the most important is that **[the
destructiveness of war has rapidly declined since World War
II]{.underline}**. [This seems counterintuitive in light of the
deployment of vastly more destructive weapons by the superpowers and
their allies]{.underline}. The United States used two fission bombs to
destroy Hiroshima and Nagasaki, a level of destructive power that
rivaled entire air campaigns of the day. A single modern fusion bomb
would exceed the total destructive power of all of the conventional
bombs dropped in World War II. Ballistic missile technology allows
nations to deliver nuclear weapons anywhere in the world without having
to deploy vast naval or air fleets. Throughout the 1980s, the United
States and the Soviet Union fielded more than twenty thousand nuclear
weapons each and thousands of air, sea, and ground missiles capable of
delivering them. During the Cold War, both nations kept their nuclear
forces at high levels of alert and could have destroyed the world many
times over. But they did not. [Though still present, conventional war
has become less harmful]{.underline}. By some counts, the period from
the end of World War II to the present day has seen the level of armed
conflict between nations fall by an order of magnitude from the Peace of
Westphalia to the mid-twentieth century.95 **[When corrected for the
higher number of nations, wars between states have dropped both in their
frequency and destructiveness]{.underline}**. [War no longer
characterized the relations between European states]{.underline}.
Indeed, for the first time in centuries, no major war between the great
powers broke out in Europe or Asia. Historians now appreciate the Cold
War as "the Long Peace."96 [But deaths from war have not
disappeared]{.underline}. [They have not even significantly
declined]{.underline}. [They have only dropped in **wars between the
great powers**]{.underline}. While the number of conflicts between
nations has steeply fallen, it has jumped inside states.97 Civil wars
have replaced and even exceeded the scourge of great power war in terms
of their casualties and frequency. By some estimates, post-World War II
conflicts have killed forty million, including both combatants and
civilians who have died on the battlefield or from related starvation
and disease.98 Studies report that between 70 and 80 percent of these
casualties occurred in civil wars.99 Conflict has become less global and
more local. Globe-spanning wars between broad alliances of great powers,
such as the Allies against the Axis in World War II, have receded. No
great power has directly fought a war with another for the last seventy
years. Instead, wars have remained limited to specific areas, such as
the Balkans or the Middle East, with Africa generating the greatest
share of conflicts and deaths. Realists account for the decline in war
in two ways. First, and most important, the postwar world soon divided
into a contest between two superpowers. Despite the arms race and proxy
wars in Korea, Vietnam, and Afghanistan, this "bipolar" system had the
counterintuitive effect of producing global stability. With only two
superpowers, calculations of war and peace became simpler, less friction
occurred, and the interest in superpower deterrence suppressed national
interests that might have once caused war. According to this view, the
multipolarity between the seventeenth and twentieth centuries made war
more likely because of the heightened opportunities for conflict between
more great powers. Second, the emergence of nuclear weapons only
reinforced the balancing effect of bipolarity. The great powers could
achieve greater security with a nuclear deterrent and the possibility of
a nuclear exchange caused them to exercise greater caution in the use of
force. The United States and the Soviet Union did not come into direct
military conflict, it is argued, because of the fear that conventional
war could escalate quickly into a nuclear conflict with devastation for
both sides.100 As Waltz observed near the end of the Cold War, "\[t\]he
probability of major war among states having nuclear weapons approaches
zero."101 The decline in interstate wars has accompanied greater
international cooperation on nonsecurity areas, such as commerce, trade,
and the environment. Nations have increased global welfare by lowering
trade barriers within regional free-trade areas, such as the European
Union and the North American Free Trade Agreement, and internationally
through the World Trade Organization. These developments have encouraged
scholars to see greater opportunities for international institutions and
law to help build stability and peace. Such benefits usually arise from
a hegemonic power, like Great Britain in the nineteenth century or the
United States in the twentieth century, which can establish an
international order built on free navigation and trade. Realist theory,
however, would predict disorder when a dominant great power declines.
Nevertheless, institutional scholars, as they are sometimes known in
political science, argue that self-interested nations can cooperate to
create international regimes that can maintain global order in the
absence of a hegemon. In After Hegemony, for example, Robert Keohane
argues, "[Realist assumptions about world politics are consistent with
the formation of institutionalized arrangements]{.underline}."102 In
other words, [self-interested states that worried only about their
security might still cooperate if they can realize benefits that leave
them better off than before]{.underline}. There is little doubt that
cooperation has advanced in the world economy even as the absolute and
relative power of the United States has declined after the end of the
Cold War.103 International agreements have successfully lowered tariffs
and trade barriers and improved coordination in economics, science,
environment, and health. Political economists theorize that these
regimes help nations escape "the prisoner's dilemma." In this stylized
game, the optimal outcome for two suspects is to remain silent under
police questioning, but because they cannot communicate with each other
there is a strong incentive to bargain for a reduced sentence and
implicate the other criminal. The prisoner's dilemma has become a
shorthand in the social sciences for situations in which two parties, in
pursuit of their rational self-interest, will do themselves short-term
harm, when they could have achieved longer-term benefits if they had
cooperated. Nations might engage in ruinous trade wars or arms races
because of the prisoner's dilemma. A classic example would be the
nuclear arms race between the superpowers during the Cold War. Even
though the United States and Russia apparently believe today that they
need no more than 2,500 warheads each, during the 1950s and 1960s their
nuclear stockpiles reached more than 25,000 weapons. Only decades of
negotiation, verification, and trust-building deals on smaller
weapons---like the 1988 intermediate-range missile ban---gave the
superpowers the confidence to agree to reductions in their strategic
arsenals. Liberal institutionalists argue that international agreements
can help overcome the prisoner's dilemma through repeated interaction.
They believe that regimes offer states the chance to communicate and
learn about each other over time in order to gain information and
generate trust. As Andrew Guzman argues, a history of contact allows
nations to develop reputations for keeping promises or retaliating
against cheaters.104 Regimes can also improve the opportunities for
cooperation by linking more issues together, which increase the benefits
of performance and the tools for tit-for-tat responses to shirking.
Permanent institutions can enhance these effects by spreading reliable
information on the compliance of states and reducing transaction costs
to future negotiations and deal making. [Leading political scientists
argue that the successful record in trade and finance should set an
example to regulate other areas of global concern]{.underline}. **[There
are several reasons to doubt, however, whether the structure of
cooperation in areas such as the international economy will transfer
easily to global security]{.underline}**. First, and most important,
**[cheating in a security agreement could inflict greater harms on a
nation]{.underline}**. [If a nation suffers a surprise attack, loses a
military advantage, or is left without its promised allies, it may
encounter a long-term, permanent drop in its territory, population,
resources, and ultimately power]{.underline}. Germany's surprise 1941
invasion of the Soviet Union, and the breaking of their nonaggression
pact, led to grievous losses and almost knocked Moscow out of the war.
Withdrawal from a trade or financial agreement, by contrast, may cause
economic pain, but likely cannot equal the severity and immediacy of a
wartime defeat.105 Second, **[nations may have much greater concerns
about "relative gains."]{.underline}** 106 In economic affairs, nations
may remain content with gains in their GDP growth, increases in exports,
and drops in the cost of imports, even if their trade partners do
better. But **[in security affairs, as Joseph Grieco has argued, nations
may refuse to cooperate if doing so would benefit a competitor in a
relative sense]{.underline}**.107 [Cooperation is not impossible, but it
is most likely in situations where both the use of force is not a
serious threat between the nations concerned and they can engage in a
long practice of tit-for-tat reciprocity to encourage
cooperation]{.underline}.108 **[Cooperation itself does not disappear
during armed conflict, but it is doubtful that formal legalization
produces lasting rules of war]{.underline}**. [Nations have sought to
follow some basic norms in combat, such as eliminating weapons that
cause unnecessary human suffering]{.underline}. Despite the recent
deterioration in practice, nation-states have generally refrained from
using chemical weapons since the end of World War I. They have followed
the Geneva Conventions on prisoners of war, though not consistently.
Nations have observed others norms in the breach, such as the immunity
of the civilian population and resources from attack. World War II
included the aerial bombing of cities and the nuclear attacks on Japan,
while the years since have seen precision targeting of terrorists off
the battlefield, attacks on urban infrastructure, and the acceptance of
high levels of collateral damage among civilians. International lawyers
and diplomats may proclaim that nations follow universal rules on
combat, either because of morality or a sense of legal obligations, but
the record of practice tells a far different story. [We should also not
mistake temporary restraint in combat for a nations' lack of military
capacity]{.underline}. Human rights activists, for example, often hold
up as an example of progress the 1998 Ottawa Convention banning land
mines.109 Nongovernmental organizations (NGOs) led a decades-long
campaign to persuade nations to give up antipersonnel mines, which can
kill large numbers of civilians years after fighting has ended. In 1997,
the Nobel Prize committee ultimately awarded the peace prize to Jody
Williams, the director of the international campaign. The apparent
success of the treaty, which now includes about 80 percent of the
world's nations, led to visions of a new approach to the laws of war in
which groups and individuals, rather than states, would mobilize to
force states to control war. Ottawa "spawned a new politics, new
partnerships, new ways of thinking about the international environment.
It was the forerunner of a clear notion of global citizenship," writes
Lloyd Axworthy, a former Canadian foreign minister and current
university president. 110 "It challenged conventional notions of
sovereignty and set in motion a form of coalition politics at the global
level that could be used to shift power and political relationships."111
NGOs and international activists have sought to duplicate the perceived
success of the Ottawa Convention by extending its scope to regulate
cluster munitions and small arms.112 Such efforts, however,
unintentionally reaffirm the enduring importance of power. [While the
great majority of the world's nations foreswear antipersonnel mines,
most of them do not field large militaries or fight major conventional
wars]{.underline}. Latin American states, for example, signed Ottawa in
large numbers, but they have not fought a major war against each other
for many decades. **[Nations with power projection capabilities, such as
the United States, Russia, China, and India, did not commit to the
agreement]{.underline}**. **[Smaller states, such as Korea and Israel,
faced with aggressive neighbors and local instability, also
declined]{.underline}**. [Eliminating mines would place these nations at
a serious disadvantage because they would reduce their ability to deter
invasion or maintain a regional status quo]{.underline}. Despite their
costs on civilians, mines actually may enhance stability and advance
peace---they maintain borders by raising the cost of a ground invasion
and have no offensive capability. **[Nations that live under threat of
conflict will have little reason to agree to abandon such a defensive
advantage that does not produce a corresponding reduction for offensive
weapons]{.underline}**. [For similar reasons, the "Ottawa Process" will
have little impact on other weapons]{.underline}. Nations with small
militaries, peaceful neighbors, or large allies, such as many in Europe
and Latin America, may promote new arms control agreements. But because
the impact on their military performance is negligible and the chances
that they will engage in war are small, these states experience very
little loss in expected benefits---such as security gains by deploying
these weapons---by signing on. [Nations with large militaries and
greater possibilities for conflict will not join agreements that could
force a significant shift in their ability to prevail]{.underline}.
Instead, nations will reach agreements whose reciprocal reductions in
arms will leave no nation with an advantage. As James Morrow has
observed, the laws of war treaties succeed when they guarantee minimum
standards of treatment for captured soldiers that leave both sides to a
conflict better off without giving either an advantage.113 A similar
dynamic gives the agreements on chemical and biological weapons their
strength: these weapons cause undue suffering and are difficult to
control, and eliminating them leaves nations in the same relative power
position as before. But if a nation can narrow a large gap in military
power with a competitor by developing these weapons, it may violate an
agreement banning their possession. Thus, nations at a disadvantage in
conventional or nuclear forces have turned to biological and chemical
weapons because reciprocity no longer holds. Nations, of course, will
also follow humanitarian treaties even when they do not depend on
reciprocity. But they will do so when compliance still advances their
national interests. In the Korean and Vietnam Wars, for example, the
United States followed the Geneva Conventions even though its opponents
had not adopted the agreements. Rather than reciprocate, North Korea and
the Viet Cong engaged in the systematic mistreatment of American
prisoners of war. The United States, however, could have benefited from
following Geneva anyway. Treating prisoners humanely may increase the
willingness of enemy fighters to surrender, while a history of cruel
treatment might cause opposing soldiers to fight to the death. These
objectives may have even greater force if the contending armies are
fighting for the support of the local populations. Extending Geneva
protections to local guerrillas might help persuade the local populace
to side with the government, for example. We should not overstate the
influence of reputation on the decisions of nations at war. Human rights
advocates, for example, might believe that nations should follow the
Geneva Conventions to build a track record of compliance that will lead
to better treatment for their soldiers in the next war. Under this
argument, the United States should extend Geneva protections to al-Qaeda
terrorists, even if the latter execute U.S. prisoners and civilians,
because its soldiers will benefit in a future conflict with Iran or
China. This claim, however, runs counter to the self-interest of nations
at war. China would have little interest in punishing U.S. soldiers
captured in a United States-China conflict because of American conduct
toward terrorists in a separate contest. Such incentives might even
cause a nation to conduct itself differently depending on the
corresponding policies of its enemies. In World War II, for example,
Germany generally followed the Geneva Conventions on the western front,
where the United States and the United Kingdom treated prisoners of war
well. The eastern front followed completely different rules: Germany and
the Soviet Union descended into the barbaric treatment of each other's
troops and civilians. This detour into international humanitarian
treaties here underscores the workings of reciprocity. [Nations will
follow the human rights treaties when they gain a benefit that outweighs
the cost]{.underline}. Their captured troops will receive good treatment
at the hands of the enemy even though Geneva will require them to bear
costs in maintaining security, providing housing, and keeping prisoners
in good health. **[Limitations on the use of weapons will follow a
similar logic]{.underline}**. **[Nations will refrain from using new
weapons technologies only when they provide little benefit or their use
by both sides leaves no one better off]{.underline}**. **[Robots and
cyberweapons do not bear the same features as the weapons where the laws
of war have succeeded]{.underline}**. [They do not inflict unnecessary
suffering out of proportion to their military advantages, as do poisoned
bullets or blinding lasers]{.underline}. Rather, **[these weapons
improve the precision of force such that they reduce human death and
destruction in war]{.underline}**. **[Nor do these new weapons
technologies yet engage nations in a useless arms race]{.underline}**.
[Nuclear weapons eventually became opportune for arms control because
larger stockpiles provided marginal, if any, benefits due to the
destructive potential of each weapon and the deterrence provided by even
a modest arsenal]{.underline}. Mutual reductions could leave both sides
in the same position as they were before the agreement. Today, the
marginal cost of nuclear weapons for the United States and Russia so
outweigh their marginal benefit that it is not even clear that a binding
international agreement is needed to reduce their arsenals. Russia, for
example, reduced its arsenal below New START's ceilings of 1,550 nuclear
warheads and 700 strategic launchers even before the United States
approved the deal.114 Some experts estimate that the United States
requires less than even those numbers to maintain an adequate deterrent,
and it is possible that future presidents may reduce the nuclear arsenal
without any international agreement.115 **[Cyberweapons and drones do
not yet appear to bear these characteristics]{.underline}**. **[The
marginal gains in deploying these weapons will not be even across
nations, but instead may be asymmetric]{.underline}**. [Some nations
will experience much greater gains in military capability by developing
cyber and drone technology]{.underline}. Or put differently, prohibition
or regulation of these new weapons will not have equal impacts on rival
nations. **[Chinese military doctrine, for example, emphasizes using
such technologies to offset U.S. advantages in conventional sea and air
power]{.underline}**. 116 [Drones might be used to overwhelm the
defenses for U.S. carrier groups in the Western Pacific, while cyber
weapons could interfere with U.S. command-and-control, communications,
or even military readiness]{.underline}. Just as the Germans resorted to
submarines to offset British and American naval superiority in World
Wars I and II, the **[Chinese may well turn to robotics and network
attacks in a future contest]{.underline}**. **[Reciprocity will not hold
because international limitations on new weapons would have the effect
of favoring the United States' existing advantage in conventional
arms]{.underline}**. [Even if there might be some mutual advantage to a
universal limit on these new weapons technologies, it is still doubtful
that nations would accept a legal agreement]{.underline}. Nations would
still develop these weapons because they improve their military
capabilities at lower cost. [Aerial drones, for example, can achieve the
objective of attacking an enemy target at lower cost]{.underline}. An
F-35 Lightning II stealth fighter/bomber costs \$85 million under the
best estimates, while Predator and Reaper drones cost about \$4 million
and \$16.9 million respectively.117 The U.S. Air Force can not only
purchase twenty Predators for the cost of a single F-35, but it can
operate them at a far lower cost per hour, keep them on station for far
longer (a predator can fly over a target area for up to fourteen hours)
but also not risk the lives of pilots who may be captured or killed.118
**[Militaries will have an incentive to replace expensive manned
aircraft with fleets of drones, regardless of whether their rivals
choose to match them or not]{.underline}**. **[Cyber and robotic weapons
today resemble the emergence of the new weapons borne of the industrial
age]{.underline}**. [At the time of their emergence, the impact of
high-capacity firearms, long-range artillery, warplanes, submarines, and
motorized vehicles was not well understood]{.underline}. World War I's
carnage can be attributed to the failure of nineteenth century tactics
and strategy, still influenced by Napoleonic theories, to adapt to the
greater firepower of modern weapons. [Although nations and experts
proposed the regulation of aerial and submarine warfare, their
asymmetric benefits and the uncertainty of their effects on the balance
of power precluded any agreement among the combatants]{.underline}.
[Nations may prove even more reluctant to adopt arms control schemes for
weapons that have strong defensive qualities, such as the machine gun
and artillery, because upgrading defensive systems may pose little
offensive threat to neighbors]{.underline}.

## Impact

### Turns case

#### The thesis of the da turns case -- other countries will develop new weapons regardless of international norms -- that only makes the US's position worse

**Yoo 17**

(John, Emanuel S. Heller Professor of Law, University of California,
Berkeley, School of Law; Visiting Scholar, American Enterprise
Institute, "Embracing the Machines: Rationalist War and New Weapons
Technologies", California Law Review, Vol. 105, Issue 2) DB

Cyberweapons and drones do not yet appear to bear these characteristics.
[The marginal gains in deploying these weapons will not be even across
nations, but instead may be asymmetric]{.underline}. [Some nations will
experience much greater gains in military capability by developing cyber
and drone technology]{.underline}. Or put differently, **[prohibition or
regulation of these new weapons will not have equal impacts on rival
nations]{.underline}**. [Chinese military doctrine, for example,
emphasizes using such technologies to offset U.S. advantages in
conventional sea and air power]{.underline}. 116 Drones might be used to
overwhelm the defenses for U.S. carrier groups in the Western Pacific,
while cyber weapons could interfere with U.S. command-and-control,
communications, or even military readiness. Just as the Germans resorted
to submarines to offset British and American naval superiority in World
Wars I and II, the Chinese may well turn to robotics and network attacks
in a future contest. **[Reciprocity will not hold because international
limitations on new weapons would have the effect of favoring the United
States' existing advantage in conventional arms]{.underline}**.

**[Even if there might be some mutual advantage to a universal limit on
these new weapons technologies, it is still doubtful that nations would
accept a legal agreement]{.underline}**. [Nations would still develop
these weapons because they improve their military capabilities at lower
cost]{.underline}. Aerial drones, for example, can achieve the objective
of attacking an enemy target at lower cost. An F-35 Lightning II stealth
fighter/bomber costs \$85 million under the best estimates, while
Predator and Reaper drones cost about \$4 million and \$16.9 million
respectively.117 The U.S. Air Force can not only purchase twenty
Predators for the cost of a single F-35, but it can operate them at a
far lower cost per hour, keep them on station for far longer (a predator
can fly over a target area for up to fourteen hours) but also not risk
the lives of pilots who may be captured or killed.118 Militaries will
have an incentive to replace expensive manned aircraft with fleets of
drones, regardless of whether their rivals choose to match them or not.

[Cyber and robotic weapons today resemble the emergence of the new
weapons borne of the industrial age]{.underline}. At the time of their
emergence, the impact of high-capacity firearms, long-range artillery,
warplanes, submarines, and motorized vehicles was not well understood.
World War I's carnage can be attributed to the failure of nineteenth
century tactics and strategy, still influenced by Napoleonic theories,
to adapt to the greater firepower of modern weapons. [Although nations
and experts proposed the regulation of aerial and submarine warfare,
their asymmetric benefits and the uncertainty of their effects on the
balance of power precluded any agreement among the
combatants]{.underline}. [Nations may prove even more reluctant to adopt
arms control schemes for weapons that have strong defensive qualities,
such as the machine gun and artillery, because upgrading defensive
systems may pose little offensive threat to neighbors]{.underline}.

### Turns heg

#### Integrating AI into military tech makes heg sustainable

**Etzioni 17**

(Amitai, "Pros and Cons of Autonomous Weapons Systems",
<https://www.armyupress.army.mil/Journals/Military-Review/English-Edition-Archives/May-June-2017/Pros-and-Cons-of-Autonomous-Weapons-Systems/>)
DB

Military advantages. [Those who call for further development and
deployment of autonomous weapons systems generally point to several
military advantages]{.underline}. First, **[autonomous weapons systems
act as a force multiplier]{.underline}**. That is, fewer warfighters are
needed for a given mission, and the efficacy of each warfighter is
greater. Next, advocates credit autonomous weapons systems with
expanding the battlefield, allowing combat to reach into areas that were
previously inaccessible. Finally, [autonomous weapons systems can reduce
casualties by removing human warfighters from dangerous
missions]{.underline}.1

The Department of Defense's Unmanned Systems Roadmap: 2007-2032 provides
additional reasons for pursuing autonomous weapons systems. These
include that robots are better suited than humans for "'dull, dirty, or
dangerous' missions."2 An example of a dull mission is long-duration
sorties. An example of a dirty mission is one that exposes humans to
potentially harmful radiological material. An example of a dangerous
mission is explosive ordnance disposal. Maj. Jeffrey S. Thurnher, U.S.
Army, adds, "\[lethal autonomous robots\] have the unique potential to
operate at a tempo faster than humans can possibly achieve and to
lethally strike even when communications links have been severed."3

In addition, [the **long-term savings** that could be achieved through
fielding an army of military robots have been highlighted.]{.underline}
In a 2013 article published in The Fiscal Times, David Francis cites
[Department of Defense figures showing that "each soldier in Afghanistan
costs the Pentagon roughly \$850,000 per year."]{.underline}4 [Some
estimate the cost per year to be even higher]{.underline}. Conversely,
according to Francis, "[the TALON robot---a small rover that can be
outfitted with weapons, costs \$230,000]{.underline}."5 According to
Defense News, Gen. Robert Cone, former commander of the U.S. Army
Training and Doctrine Command, suggested at the 2014 Army Aviation
Symposium that [by relying more on "support robots," the Army eventually
could reduce the size of a brigade from four thousand to three thousand
soldiers without a concomitant reduction in effectiveness]{.underline}.6

Air Force Maj. Jason S. DeSon, writing in the Air Force Law Review,
notes the potential advantages of autonomous aerial weapons systems.7
According to DeSon, the [physical strain of high-G maneuvers and the
intense mental concentration and situational awareness required of
fighter pilots make them very prone to fatigue and exhaustion; robot
pilots, on the other hand would not be subject to these physiological
and mental constraints]{.underline}. Moreover, fully autonomous planes
could be programmed to take genuinely random and unpredictable action
that could confuse an opponent. More striking still, Air Force Capt.
Michael Byrnes predicts that [a single unmanned aerial vehicle with
machine-controlled maneuvering and accuracy could, "with a few hundred
rounds of ammunition and sufficient fuel reserves," take out an entire
fleet of aircraft, presumably one with human pilots]{.underline}.8

### Magnitude

#### We outweigh on [magnitude]{.underline} -- wars using new AI weapons prevent conventional weapons, which are worse

**Yoo 17**

(John, Emanuel S. Heller Professor of Law, University of California,
Berkeley, School of Law; Visiting Scholar, American Enterprise
Institute, "Embracing the Machines: Rationalist War and New Weapons
Technologies", California Law Review, Vol. 105, Issue 2) DB

Such [mistaken arguments confuse jus in bello and jus ad
bellum]{.underline}. [The means of fighting a war do not bear on the
justifications for the war]{.underline}. A war that rests on just
grounds need not be fought in an evenhanded fashion. One might want a
war that has a just purpose to come to a quick and rapid conclusion,
with the least loss of life as possible, which should be more likely if
one side has a great advantage over the other. Again, the Kosovo War
provides a good example. [If we agree with the goal of stopping Serbia's
ethnic cleansing of the provinces of the former Yugoslavia, we should
want NATO to execute the war with the maximum dispatch and
effectiveness]{.underline}. [A faster war, fought with **precision
weapons** and reduced combat casualties, could end the human rights
catastrophe earlier and save more lives]{.underline}. **[Similarly, if a
nation were defending itself, we should want it to have the most
advanced weapons possible]{.underline}**. **[Superiority itself may not
just deter an attack; it could also inflict such high costs in an actual
war that the attacker may give up]{.underline}**.

In fact, [the **critics of autonomous systems support a rule that could
just as well produce more death and destruction**]{.underline}.
**[Preemptively banning such advanced weapons systems would establish a
technological parity among many nations]{.underline}**. **[We could
witness wars that grind on for years between nations with similarly
matched weaponry and tactics]{.underline}**. [The world witnessed such a
conflict a century ago in the trench warfare of World War
I]{.underline}. Even World War II, which favored offensive weaponry and
tactics, lasted for six years and killed millions more than the Great
War. [Technological discoveries, such as the atomic bomb, shortened the
war and saved millions of lives]{.underline}. If today's human rights
advocates had persuaded Roosevelt, Truman, and Churchill to preemptively
ban nuclear weapons research, the invasion of Japan would have taken an
estimated one million Allied lives and eight to nine million
Japanese.136 Of course, we would not have wanted Nazi Germany or
Imperial Japan to develop advanced weapons first, but that merely
confirms that whether we want nations to enjoy superiority in war
depends on whether we agree with why those nations employ force.

[The real-world consequences of military technology point to a rejection
of a categorical ban on new weapons]{.underline}. [As with drones,
autonomous weapons systems no doubt often allow nations at war to wage
hostilities with greater lethality for the enemy, but at a lower cost to
their soldiers]{.underline}. [A weapon's improved effectiveness in
cost-benefit terms should not prompt efforts at bans; rather, **they
should be welcomed for reducing the destructiveness of
war**]{.underline}. [Technology that makes war more effective by making
targeting more precise, reducing combatant and civilian deaths, and
ultimately shortening conflicts will improve overall global welfare, a
result that any system of rules on war should seek]{.underline}.

#### It makes conflicts [more peaceful]{.underline} -- regulating weapons fails 

**Yoo 17**

(John, Emanuel S. Heller Professor of Law, University of California,
Berkeley, School of Law; Visiting Scholar, American Enterprise
Institute, "Embracing the Machines: Rationalist War and New Weapons
Technologies", California Law Review, Vol. 105, Issue 2) DB

[A rationalist approach to war also provides an answer to the broader
critique of the new weapons technologies as facilitating
war]{.underline}. Recall that some UN officials and scholars share the
concern that drones and cyberweapons will encourage states to wage war
more often. Critics argue that these weapons remove a nation's soldiers
from the battlefield, thereby emboldening leaders to choose force more
frequently. But, [understanding war as a bargaining failure reveals the
importance of **signaling to resolving international
disputes**]{.underline}. **[New weapons create more opportunities for
signaling, which allows nations to communicate their intentions and
capabilities more effectively]{.underline}**. **[Greater signaling
should allow nations to share more information, which on the margins
will lead to more international deals and therefore an overall reduction
of major wars]{.underline}**. [Ironically, an effort to ban new weapons
may well produce more war, not less]{.underline}.

CONCLUSION

The rise of robotics and cyberwarfare has blurred our understanding of
what constitutes "war." Indeed, within weeks, both blanketed American
news media. First, according to news reports, the Central Intelligence
Agency concluded that Russia had attempted to influence the 2016
presidential elections by hacking into the computer systems of the
Democratic National Committee and the campaign of candidate Hillary
Clinton. Then, China captured a U.S. submersible drone in the South
China Sea, in violation of international legal right of free navigation.
Perhaps few see these actions as "war," in the traditional sense. And
yet, many would feel differently if Russian spies entered the United
States to tamper with election result or if Chinese marines had boarded
an American warship. These examples, however, only begin to introduce
the complexity of robotics and cyberwarfare, because they involve
powerful and identifiable state actors. When nonstate groups adopt these
measures, understanding "war" seems less and less feasible.

[The technology that has created aerial drones will produce unmanned
versions of many other weapon systems, from tanks to submarines to
sentries]{.underline}. [Criticism of this advance misses the
mark]{.underline}. **[Rather than an unknown to be feared, new weapons
technology will bring a greater precision in the use of force that will
reduce casualties and destruction]{.underline}**. **[Allowing the use of
robotic weapons against a broader range of targets promises to contain
the harm of international disputes and help lead to peaceful
settlements]{.underline}**. [Concerns about autonomous weapons are
equally mistaken]{.underline}. [Such systems promise to increase the
precision and decrease the harms of attack]{.underline}. [In a world
beset by fresh challenges to international security, reducing the costs
and increasing the accuracy of force may reduce the obstacles to action
to stop weapons of mass destruction proliferation, terrorist groups,
humanitarian disasters, or revanchist powers]{.underline}.

### Yes impact

#### Causes [extinction]{.underline} through uncontrolled risks from emerging tech.

**Jain '20** \[Ash; 2020; Senior fellow with the Scowcroft Center for
Strategy and Security; Strategic Studies Quarterly; "Present at the
Re-Creation: A Global Strategy for Revitalizing, Adapting, and Defending
a Rules-Based International System,"
<https://www.atlanticcouncil.org/wp-content/uploads/2019/10/Present-at-the-Recreation.pdf>\]

[The system must]{.underline} also [be adapted to deal with]{.underline}
new issues that were not envisioned when the existing order was
designed. Foremost among these issues is [emerging and disruptive
technology, including **AI**, **additive manufacturing**]{.underline}
(or **[3D printing]{.underline}**), [quantum computing]{.underline},
[**genetic engineering**, **robotics**, **directed energy**,
the]{.underline} Internet of things ([**IOT**), **5G**, **space**,
**cyber**]{.underline}, and many others. Like other disruptive
technologies before them, [these innovations promise great benefits, but
also carry **serious downside risks**]{.underline}. For example, AI is
already resulting in massive efficiencies and cost savings in the
private sector. Routine tasks and other more complicated jobs, such as
radiology, are already being automated. In the future, autonomous
weapons systems may go to war against each other as human soldiers
remain out of harm's way.

Yet, [AI is]{.underline} also transforming economies and societies, and
[generating new security challenges]{.underline}. Automation will lead
to widespread unemployment. The final realization of driverless cars,
for example, will put out of work millions of taxi, Uber, and long-haul
truck drivers. Populist movements in the West have been driven by those
disaffected by globalization and technology, and mass unemployment
caused by automation will further grow those ranks and provide new fuel
to grievance politics. Moreover, some fear that [autonomous
weapons]{.underline} systems will become "killer robots" that select and
engage targets without human input, and [could eventually **turn on
their creators, resulting in human extinction**. The other technologies
on this lisgt similarly balance great potential upside with great
downside risk. 3D printing]{.underline}, for example, can be used to
"make anything anywhere," reducing costs for a wide range of
manufactured goods and encouraging a return of local manufacturing
industries.61 At the same time, advanced 3D printers [can]{.underline}
also [be used by revisionist and rogue states to print component parts
for advanced weapons systems or]{.underline} even [WMD]{.underline}
programs, **[spurring arms races and weapons
proliferation]{.underline}**.62 [Genetic engineering can]{.underline}
wipe out entire classes of disease through improved medicine, or [wipe
out entire classes of people through genetically engineered superbugs.
Directed-energy missile defenses may defend against incoming missile
attacks, while]{.underline} also **[undermining global strategic
stability]{.underline}**.

Perhaps [the greatest risk to global strategic stability from new
technology]{.underline}, however, [comes from the risk]{.underline} that
**[revisionist autocracies may win the new tech arms
race]{.underline}**. Throughout history, states that have dominated the
commanding heights of technological progress have also dominated
international relations. The United States has been the world's
innovation leader from Edison's light bulb to nuclear weapons and the
Internet. Accordingly, [stability has been maintained in Europe and
Asia]{.underline} for decades [because the **U**]{.underline}nited
**[S]{.underline}**tates [and its democratic allies possessed a
favorable economic and military balance of power]{.underline} in those
key regions. Many believe, however, that China may now have the lead
[in]{.underline} the new technologies of the twenty-first century,
including [AI, quantum, 5G, **hypersonic missiles**, and others. If
China succeeds in mastering the technologies of the future before the
democratic core]{.underline}, then [this could lead to a drastic and
rapid shift in the balance of power, upsetting global strategic
stability, and the call for a democratic- led, **rules-based
system**]{.underline} outlined in these pages.63

[The **U**]{.underline}nited **[S]{.underline}**tates [and its
democratic allies need to work with other major powers to develop a
framework for **harness**ing emerging **tech**nology in a way that
**maximizes**]{.underline} its [**upside potential**, while
**mitigating**]{.underline} against its **[downside
risks]{.underline}**, and also [contributing to the maintenance of
global stability]{.underline}. The existing international order contains
a wide range of agreements for harnessing the technologies of the
twentieth century, but they need to be updated for the twenty-first
century. The world needs an entire new set of arms-control,
nonproliferation, export-control, and other agreements to exploit new
technology while mitigating downside risk. These agreements should seek
to maintain global strategic stability among the major powers, and
prevent the proliferation of dangerous weapons systems to hostile and
revisionist states.

## Aff

### +Uniqueness

#### We're losing the ai race now -- cooperation is key 

**Wodecki 22**

(Ben, "NATO at risk of losing AI innovation race to Russia, China",
<https://aibusiness.com/document.asp?doc_id=777260>) DB

**[The North Atlantic Treaty Organization (NATO) should standardize and
regulate AI to keep up with rivals]{.underline}**, according to findings
published by the U.S. think tank, Center for European Policy Analysis
(CEPA).

CEPA's comments came as it published a series of AI-related
recommendations for NATO amid growing geopolitical tensions with the
likes of Russia, China and North Korea.

[Its recommendations include AI standardization, encouraging and
improving AI literacy and spurring private sector
innovation]{.underline}.

**[Such undertakings would allow NATO allies to better scale and deploy
AI -- and keep pace with rivals]{.underline}**.

"**[These new capabilities will revolutionize NATO's military and
strategic affairs, thus strengthening NATO's ability to fulfill its
essential core tasks of collective defense, crisis management and
cooperative security]{.underline}**," CEPA's Nicholas Nelson and Nico
Luzum wrote.

The pair cited AI projects being undertaken by adversaries, including
China's attempts to develop purported mind-controllable drones and AI
assistants for fighter pilots.

But [NATO allies have their own capabilities -- including U.S.-developed
autonomous tanks and British-made systems that provide ground troops
with information on the surrounding terrain]{.underline}.

The think tank's study suggests that at present, **[NATO]{.underline}**
is leading the AI race -- but **[risks losing its competitive advantage
to peer competitors]{.underline}** "competitors if allies fail to
leverage the private sector, coordinate implementation and engage with
the public."

CEPA suggests that NATO allies should accelerate AI adoption and
actively encourage private sector innovation.

"Ultimately, we hope that [these recommendations enable NATO allies to
better innovate, scale, deploy, and integrate AI and autonomy-based
technologies to form agile, system-wide solutions]{.underline}.

### turn

#### AI cooperation is key to Nato's threat response -- turns the da

**van der Merwe 21**

(Joanna, "NATO Leadership on Ethical AI is Key to Future
Interoperability",
<https://cepa.org/nato-leadership-on-ethical-ai-is-key-to-future-interoperability/>)
DB

In October 2020, Deputy Secretary General of NATO Mircea Geoană
highlighted the benefits of establishing a "transatlantic community
cooperating on Artificial Intelligence (AI)." The Deputy Head of NATO's
Innovation Unit followed with a commitment to its responsible use. The
US Department of Defense (DoD) adopted Ethical Principles for AI in 2020
and has committed to bringing together NATO member and partners to
operationalize these principles. [Despite these statements and
developments, more work is required to tackle the very real challenge
that **ethical AI** will pose to **future interoperability** within
NATO]{.underline}.

[Without a NATO-led initiative focused on aligning these ethical
principles across the Alliance, the interoperability risk of nations
fielding AI-based systems that hinder joint operations is
high]{.underline}. **[As the foremost security framework for Europe and
North America, as well as the leading defense alliance for promoting and
protecting democratic values, NATO is able to facilitate alignment on
this issue]{.underline}**. [As part of a broader strategy on emerging
and disruptive technologies, NATO must prioritize ethical AI if it
wishes to promote the shared values upon which it was founded, play a
key role in facilitating innovation across the Atlantic, and ultimately
retain the ability of its members to undertake joint
operations]{.underline}.

**[Establishing NATO ethical AI principles is the first step toward both
technical and political alignment, in turn enhancing and fostering
interoperability, which is the foundation for NATO to respond to
emerging threats as an Alliance, in a flexible and timely
manner]{.underline}**.

A key challenge for NATO is raising awareness that the answers to
ethical questions can no longer be left to later stages of the
development and procurement cycle. [Decisions made at the political and
legal level will have a significant impact on the engineering practices
used to develop AI, as well as the technical characteristics of the
AI-based systems]{.underline}. The answers to questions such as
respecting human dignity, human control, and accountability will be the
foundation upon which many technical elements are programed. Systems
developers need to make a number of calls throughout the development
cycle informed by the answers to key questions, including:

how to label data

what data to use, and

what is an acceptable outcome?

These answers will also impact how AI systems are evaluated and
ultimately deployed.

If individual nations or groups are left to develop their own ethical
principles without wider alignment to NATO, the result will be a number
of AI-based systems with varying technical specifications based on the
legal and policy decisions made by individual governments when answering
the key questions. As has been demonstrated in areas such as facial
recognition and policing algorithms, the assumptions made by those
developing the tools and answering the key questions have a significant
impact on the real-world functioning of the tool and societal acceptance
of its ethics. The risk of tools failing to gain acceptance depends on
the legal and ethical decisions made by governments. [For the military,
this may mean one state using an AI-based system that is seen as
unacceptable by another, and in a joint operation one state fielding a
system that cannot be used by another]{.underline}. Or worse yet, [this
could render a joint operation impossible]{.underline}. **[Without the
ability to interoperate across NATO, the inability to effectively and
efficiently respond to future threats would undermine the
Alliance]{.underline}**.

The role of the private sector is another aspect of ethical AI
development that has proved a challenge to governments and the
transatlantic relationship. Within states, governments have struggled to
adequately regulate Big Tech firms, which has led to these companies
encroaching on government responsibilities to protect and uphold the
public interest. This encroachment permeates all aspects of government,
including defense and security. As Deputy Secretary of Defense Kathleen
Hicks discussed during her confirmation hearings, the lack of
competition is also a challenge to innovation in the private defense
industry. This, along with a lack of regulation, feeds into the power
imbalance between the sectors. Consequently, private sector companies
building the AI and AI systems that are or will be deployed on the
battlefield are deciding the ethics policies for themselves.

The transatlantic partnership must focus on coordinating these core
principles and systematic governance to ensure AI systems development
aligns with the rule of law and democracy. In particular, this must
ensure answering questions about human dignity, human control, and
accountability. **[NATO is the ideal defense and security forum for this
alignment]{.underline}**. [Given the US lead on adopting ethical
principles for the entire DoD and the EU's drive to assert checks and
balances for private-sector tech companies, **NATO remains the
organization that can bring these two together and establishes the
ethical bottom line**]{.underline}. [These will then ensure the
diverging legal and ethical stances towards Big Tech do not lead to an
interoperability barrier in the future]{.underline}. [If developments
surrounding the General Data Protection Regulation (GDPR) and the
challenges it brought for U.S.-based, data-driven companies are any
indication, a strong transatlantic led initiative is needed in order to
ensure the same challenges do not hinder NATO]{.underline}.

[The solution to the challenge that ethical AI poses for the future of
interoperability within NATO is for the Alliance to establish shared
transatlantic ethical principles, informed by the US DoD, the EU, and
others]{.underline}. **[Establishing these principles will not only
strengthen transatlantic political relations; more technically, it will
allow for the establishment of standardization agreements and inform
training and education initiatives of the Alliance in the
future]{.underline}**.

### No link

#### AI coop doesn't undermine US innovation 

**Lawrence and Cordey 20**

(Christie and Sean, "The Case for Increased Transatlantic Cooperation on
Artificial Intelligence",
<https://www.belfercenter.org/publication/case-increased-transatlantic-cooperation-artificial-intelligence>)
DB

**[The Case for Transatlantic Cooperation]{.underline}**

[There are three critical, interconnected arguments for transatlantic
cooperation to ensure AI innovation protects the security, values, and
economic interests of the United States and the European
Union]{.underline}.

1.**[Global Good]{.underline}**: [Transatlantic AI partnerships and
cooperation encourages innovation and applications that enhance human
welfare, strengthen the economies of the US and the EU, and advance
global security]{.underline}.

2.**[Great Power Competition]{.underline}**: [US-EU leadership of
like-minded nations is needed in this age of great power competition to
tip the scales against efforts by authoritarian
governments---particularly, China and Russia---to undermine
democracies]{.underline}.

3.**[Shared Values]{.underline}**: [The US and the EU share fundamental
values and would benefit from joint efforts to establish AI norms that
would more effectively advance their common vision of AI and ripple
throughout the global AI ecosystem]{.underline}.

Although the US consistently sounds the alarm bells around China's AI
aspirations and the EU urges international efforts against AI that
violates fundamental rights, increasingly noting China's actions with
concern,8 little concrete international action has taken place. [The
United States and the European Union's ongoing reassessment of their
respective AI strategies and legislation]{.underline}9 [provides a
window of opportunity to align and collaborate]{.underline}.
[Transatlantic AI cooperation is at a critical juncture and the United
States and the European Union should seize this opportunity to take
concrete actions]{.underline}.

The Current State

The United States and the European Union are separately assessing and
updating their AI strategies. However, it is a myth to assume they are
not collaborating at all to advance their AI-related goals.
[Transatlantic cooperation on AI norms, standards, research and
development, and data sharing should increase, but the United States and
the European Union can build upon an existing foundation for a stronger
alliance]{.underline}.

United States:The United States views American leadership in AI as
necessary to safeguard American values and maintain defense and economic
superiority. Recognizing the need to develop a national AI approach and
reclaim the AI R&D global leadership position from China, which had
already surpassed the US in several research output metrics by 2016,10
the Obama Administration developed an AI R&D prioritization in October
2016.11 Building on this urgency, the Trump Administration has
prioritized AI and established the American AI Initiative in February
2019.12 This Initiative identified the need for a whole-of-government
approach to prioritize AI R&D and deployment throughout the entire
federal government. The Initiative also identifies the need to grow the
US AI workforce, set national and global norms and standards, and work
with industry and allies to promote an AI environment favorable to the
United States.13

The United States' federal government has made key strategic and
tactical changes to achieve these goals. Federal AI R&D and the American
AI Initiative are coordinated by several committees and subcommittees
within the Executive Office. President Trump pledged to more than double
non-defense AI R&D to \$2 billion by 2022.14 Federal AI R&D, guided by
the National AI R&D Strategic Plan, must now be reported annually for
each federal entity.15 [The United States has taken a "light-touch"
approach to regulation, fearing overly burdensome laws will stifle
innovation]{.underline}. However, guidance is not completely absent. The
Office of Management and Budget released a memo to guide Federal
agencies as they develop regulatory and non-regulatory approaches to
non-government applications of AI and the Department of Defense
published five AI principles to guide AI design, deployment, and
adoptions in defense.16

[Obstacles to the US realizing its goal of global AI leadership exist,
despite the government's prioritization of it.]{.underline} [Key
obstacles include the need to bolster its private sector AI landscape;
address regulatory or standards gaps to safeguard American values;
repair the breakdown of funding and information sharing relationships
between academia, industry, and government; grow its AI workforce; and
further increase its federal AI R&D funding]{.underline}.

European Union: [The European Union, like the United States, intends to
leverage AI's potential as a strategic and transformative
technology]{.underline}.17 **[However, the EU has positioned itself as a
leader in trustworthy, human-centric, ethical, and values-based AI,18 in
comparison to the US government's emphasis on the need for AI innovation
to protect American values, civil liberties, and privacy]{.underline}**.
The EU recognizes that it trails behind the US and China in terms of
volume of investment and maturity of its tech industry.19 [Nonetheless,
the EU believes it can capitalize on its underlying structural strengths
(e.g., academic and innovation record) and on its values to compete
globally and reaffirm its digital and technological
sovereignty]{.underline}.20 Starting with its 2018 Communication:
Artificial Intelligence for Europe,21, 22 the European Commission (EC)
has launched a coordinated effort promoting AI.23 Policies include
increasing public and private investments from \$5.6 billion to \$22
billion annually;24 coordinating research and innovation across Europe;
devising ethical guidelines; fostering digital skills in its workforce;
and promoting public and private sector adoption of AI.25 To support and
counsel these efforts, the EC has established the High-Level Expert
Group on AI (AI HLEG) comprising 52 experts who advise the Commission on
policy and regulatory changes.

The European Union's Juncker26 Commission (2014-2019) actively avoided
regulating AI, causing the European Parliament to increase their efforts
as a proactive voice in favor of stronger AI regulation. However, since
the beginning of Ursula von der Leyen's tenure, the Commission has
initiated efforts to adopt stronger regulation for AI applications
(i.e., differentiating regulation of AI based on defined "high-risk" and
"low-risk" sectors") and associated data spaces.27,28 These legislative
proposals and their associated discussions are planned to be completed
by the end of 2020. During the strategic planning and budgeting process
of its R&D programs, the EU committed to providing at least EUR10.7
billion29 for AI-related research conducted between 2021 and 2027.30
[Despite these financial and political efforts, the EU still remains
technologically dependent on the US and China and suffers from a lack of
capital and private funding, decentralized and uncoordinated AI
expertise, severe brain drain (including to the US), and slow adoption
of AI programming in its education and public sectors]{.underline}.

Transatlantic Cooperation: [Despite over 40 years of scientific
relationships and projects between the United States and the European
Union, AI-specific collaboration has been fraught with varying degrees
of political and academic skepticism on both side of the Atlantic,
notably within the European Commission and the governments of some
Member States]{.underline} (e.g., France and Germany).31 Such a dynamic
is aggravated, in part, by the ever-deteriorating transatlantic
relationship spurred by policy and trade disagreements, public spats,
and increasing American isolationism. Despite such explicit omissions
and stand-offs at the highest levels, transatlantic collaboration for AI
does happen, most notably in various multilateral forums working on
standards (e.g., ISO, IEC, IEEE, G7, G20) or on ethics and norms (e.g.,
OECD, GPAI32).33 In recent months, however, interests and political
support for greater transatlantic coordination on AI seems to be
increasing. This trend was notably demonstrated by a visit from Lt. Gen.
Jack Shanahan---then Director of the US Department of Defense's Joint
Artificial Intelligence Center (JAIC)---to Brussels in January 2020 and
a visit by the European Parliament's delegation to Washington D.C in
February 2020. Both visits included discussions on AI with a variety of
key stakeholders, such as NATO, representatives from the US Congress,
State Department, Federal Transit Administration (FTA), Federal Bureau
of Investigation (FBI), and Privacy and Civil Liberties Oversight Board
(PCLOB).34

[Transatlantic collaboration for AI-related research is taking place at
varying levels although these projects are relatively ad hoc and
materialize within existing scientific and technological research
agreements and roadmaps]{.underline}. For instance, the current Roadmap
for US-EU Science & Technology prioritizes four areas for transatlantic
cooperation, most of which leverage AI (e.g., health, transportation,
bioeconomy, marine and arctic research) or promote institutions that do
(e.g., European Organization for Nuclear Research or CERN).35, 36 These
collaborative links are supported and promoted through a variety of
arrangements and initiatives, such as BILAT 4.0, EURAXES37 or the
European Network of Research and Innovation Centers and Hubs (ENRICH).
In general, and despite challenges to systematically integrating US
entities into European research programs, the US remains the leading
non-EU ("third country") participant in Horizon 2020,38 with over 60
participations and 1,200 partnerships.39 US funding contributions to
Horizon 2020 and participation in AI-related projects, however, is
meager than its broader research involvement in Horizon 2020. For
instance, US collaborative links with Horizon 2020 projects can only be
found in 2% of AI-related projects, 12% of deep learning projects, and
4% of machine learning-related projects.40 [Accordingly, there is still
plenty of room for improvement]{.underline}.41

### No impact

#### No emerging tech impact. 

**Sechser** et al. **19**, \*Todd S., Pamela Feinour Edmonds and
Franklin S. Edmonds, Jr. Discovery Professor of Politics and Public
Policy at the University of Virginia and Senior Fellow at the Miller
Center of Public Affairs, \*\*Neil Narang, Associate Professor of
Political Science at the University of California, Santa Barbara,
\*\*\*Caitlin Talmadge, Associate Professor of Security Studies in the
School of Foreign at Georgetown University. ( "Emerging technologies and
strategic stability in peacetime, crisis, and war", *Journal of
Strategic Studies*, 42:6, pg. 728-729)

Yet [the **history of technological revolutions** counsels **against**
alarmism]{.underline}. Extrapolating from current technological trends
is problematic, both because [technologies often **do not live up to
their promise**, and because technologies often have **countervailing**
or **conditional effects** that can **temper** their **negative
consequences**]{.underline}. Thus, [the fear that **emerging
technologies** will necessarily cause **sudden** and **spectacular
changes** to international politics should be treated with
**caution**]{.underline}. There are at least two reasons to be
circumspect. First, [very **few** technologies fundamentally reshape the
dynamics of international conflict.]{.underline} Historically, [most
technological innovations have amounted to **incremental
advancements,**]{.underline} [and]{.underline} some have **[disappeared
into irrelevance]{.underline}** despite widespread hype about their
promise. For example, [the introduction of **chemical
weapons**]{.underline} [was widely expected to immediately change the
nature of warfare]{.underline} and deterrence after the British army
first used poison gas on the battlefield during World War I. [Yet
chemical weapons quickly turned out to be less **practical**, easier to
**counter**, and **less effective**]{.underline} than conventional
high-explosives in inflicting damage and disrupting enemy operations.6
Other technologies have become important only after advancements in
other areas allowed them to reach their full potential: until armies
developed tactics for effectively employing firearms, for instance,
these weapons had little effect on the balance of power. And [even when
technologies do have significant strategic consequences, they often take
**decades to emerge**]{.underline}, as the invention of airplanes and
tanks illustrates. In short, [it is easy to **exaggerate** the strategic
effects of nascent technologies]{.underline}.7 Second, even if today's
emerging technologies are poised to drive important changes in the
international system, they are likely to have variegated and even
contradictory effects. Technologies may be destabilising under some
conditions, but stabilising in others. Furthermore, [other factors are
likely to **mediate** the effects of **new technologies** on the
international system, including **geography**, the **distribution of
material power, military strategy**, **domestic** and **organisational
politics**, and social and cultural variables, to name **only a
few**]{.underline}.8 Consequently, [the strategic effects of new
technologies often **defy** simple classification.]{.underline} Indeed,
more than 70 years after [nuclear weapons]{.underline} emerged as a new
technology, their [consequences for stability **continue** to be
debated]{.underline}.9

#### AI won't undermine nuclear stability

**Sankaran 19**

(Jaganath, assistant professor at the Lyndon B. Johnson School of Public
Affairs at the University of Texas at Austin, "A DIFFERENT USE FOR
ARTIFICIAL INTELLIGENCE IN NUCLEAR WEAPONS COMMAND AND CONTROL", War on
the Rocks, 4/25,
<https://warontherocks.com/2019/04/a-different-use-for-artificial-intelligence-in-nuclear-weapons-command-and-control/>)
DB

**[Decision-makers who stand guard at the various levels of the nuclear
weapons chain of command face two different forms of
stress]{.underline}**. [The first form of stress is information
overload, shortage of time, and chaos in the moment of a
crisis]{.underline}. [The second is more general, emerging from moral
tradeoffs and the fear of causing loss of life on an immense
scale]{.underline}. **[AI and big data analysis techniques have already
been applied to address the first kind of stress]{.underline}**. [The
current U.S. nuclear early warning system employs a "**dual
phenomenology**" mechanism designed to ensure speed in detecting a
threat and in streamlining information involved in the decision-making
process]{.underline}. [The early warning system employs advanced
satellites and radars to confirm and track an enemy missile almost
immediately after launch]{.underline}. [In an actual nuclear attack, the
various military and political personnel in the chain of command would
be informed progressively as the threat is analyzed, until finally the
president is notified]{.underline}. **[This structure substantially
reduces information overload and chaos for decision-makers in a
crisis]{.underline}**. However, as Richard Garwin writes, **[the system
also reduces the role of the decision-maker "simply to endorse the claim
of the sensors and the communication systems that a massive raid is
indeed in progress."]{.underline}** [While the advanced technologies and
data processing techniques used in the early warning system reduces the
occurrence of false alerts, it does not completely eliminate the chances
of one occurring]{.underline}. In order to address decision-makers' fear
of inadvertently starting a nuclear war, **[future applications of AI to
nuclear command and control should aspire to create an algorithm that
could argue in the face of overwhelming fear of an impending attack that
a nuclear launch isn't happening]{.underline}**. [Such an algorithm
could verify the authenticity of an alert from other diverse
perspectives, in addition to a purely technological
analysis]{.underline}. **[Incorporating this element into the nuclear
warning process could help to address the second form of stress,
reassuring decision-makers that they are sanctioning a valid and
justified course of action]{.underline}**. Command and Control During
the Cold War: The Importance of Big Data In the world of nuclear command
and control, the pursuit of speed and analysis of big data is old news.
In the early 1950s, before the advent of nuclear intercontinental
ballistic missiles (ICBMs), the United States began developing the SAGE
supercomputer. SAGE, which was built at approximately three times the
cost of the Manhattan Project, was the quintessential big data
processing machine. It used the fastest and most expensive computers at
the time -- the Whirlwind II (AN/FSQ-7) IBM mainframe computers -- at
each of 24 command centers to receive, sort, and process data from the
many radars and sensors dedicated to identifying incoming Soviet
bombers. The SAGE supercomputer then coordinated U.S. and Canadian
aircraft and missiles to intercept those bombers. Its goal was to
supplement "the fallible, comparatively slow-reacting mind and hand of
man" in anticipating and defending against a nuclear bomber campaign.
The proliferation of ICBMs in the 1960s, however, made the SAGE command
centers "extraordinarily vulnerable." The U.S. Air Force concluded that
Soviet ICBMs could destroy "the SAGE system long before the first of
their bombers crossed the Arctic Circle." In 1966, speaking at a
congressional hearing, Secretary of Defense Robert McNamara argued that
"the elaborate defenses which we erected during the 1960s no longer
retain their original importance. Today with no defense against the
major threat, Soviet ICBMs, our anti-bomber defense alone would
contribute very little..." The SAGE command centers were shut down.
McNamara formed a National Command and Control Task Force, informally
referred to as the Partridge Commission, to study the problem of nuclear
command and control in the early days of the ICBM era. The commission
concluded "that the capabilities of US \[nuclear\] weapon systems had
outstripped the ability to command and control them" using a
decentralized military command and control structure. The commission
recommended streamlining and centralizing command and control with much
stronger civilian oversight. The commission also advocated the formation
of the modern-day North American Aerospace Defense Command, better known
as NORAD, with its advanced computer and communication systems, early
warning satellites, and forward-placed radars designed to track any
missile launch on the planet before it could reach the continental
United States. [NORAD and its computer and communication systems were
designed to resolve the stress from information overload by
compartmentalizing and automating the process of evaluating a
threat]{.underline}. [Depending on its particular trajectory, **an enemy
nuclear missile may take anywhere between 35 minutes to just eight
minutes to reach its target**]{.underline}. **[When the launch of an
enemy missile occurs, it is first picked up by early warning satellite
sensors within seconds]{.underline}**. [The satellites track these
missiles while the engines are still ignited]{.underline}. Once the
missile comes over the horizon, forward-deployed radars independently
track them. The data from the two systems is then assessed in the
context of the prevailing geostrategic intelligence by NORAD. NORAD
would then pass the assessment up the military and political chain of
command. [This sequence of steps ensures that senior decision-makers are
not overwhelmed with information]{.underline}. [By the time
decision-makers are notified, the decision to retaliate to an apparent
attack "must be made in minutes." **Future advances in AI might only add
incremental improvements to the speed and quality of information
processing to this already advanced nuclear early warning
system**]{.underline}. Using AI to Prevent Inadvertent Nuclear War These
advances in nuclear command and control still do not directly address
the second form of stress, one that emerges from the fear of a nuclear
war and the accompanying moral tradeoffs. [How can AI mitigate this
problem]{.underline}? [History reminds us that technological
sophistication cannot be relied upon to avert accidental nuclear
confrontations]{.underline}. [Rather, these confrontations have been
prevented by individuals who, despite having state-of-the-art technology
at their disposal, proffered alternate explanations for a nuclear
warning alert]{.underline}. **[Operating under the most demanding
conditions, they insisted on a "gut feeling" that evidence of an
impending nuclear war alert was misleading]{.underline}**. [They chose
to disregard established protocol, fearing that a wrong choice would
lead to accidental nuclear war]{.underline}. Consider for example a
declassified President's Foreign Intelligence Advisory Board report
investigating the decision by Leonard Perroots, a U.S. Air Force
lieutenant general, not to respond to incoming nuclear alerts. The
incident occurred in 1983 when NATO was conducting a large simulated
nuclear war exercise code-named Able Archer. The report notes that
Perroots' "recommendation, made in ignorance, not to raise US readiness
in response" was "a fortuitous, if ill-informed, decision given the
changed political environment at the time." The report also states: the
military officers in charge of the Able Archer exercise minimized this
risk by doing nothing in the face of evidence that parts of the Soviet
armed forces were moving to an unusual level of \[nuclear\] alert. But
these [officers acted correctly out of instinct, not informed
guidance]{.underline}. Perroots later complained in 1989, just before
retiring as head of the U.S. Defense Intelligence Agency, "that the U.S.
intelligence community did not give adequate credence to the possibility
that the United States and Soviet Union came unacceptably close to
\[accidental\] nuclear war." In the same year, Stanislav Petrov, a
commanding officer involved in Soviet nuclear operations, also dismissed
a nuclear alert from his country's early warning system. In the face of
data and analysis that confirmed an incoming American missile salvo,
Petrov decided the system was wrong. Petrov later said, "that day the
satellites told us with the highest degree of certainty these rockets
were on the way." Still, he decided to report the warning as a false
alert. His decision was informed by fears that he "didn't want to be the
one responsible for starting a third world war." Later recalling the
incident, he said: "I had a funny feeling in my gut. I didn't want to
make a mistake. I made a decision, and that was it. When people start a
war, they don't start it with only five missiles." Both, Perroots and
Petrov feared the moral consequences of a nuclear war, particularly one
initiated accidentally. They distrusted the data and challenged
protocol. Conclusion Fred Iklé once remarked, "if any witness should
come here and tell you that a totally reliable and safe launch on
warning posture can be designed and implemented that man is a fool." If
that is true, [how close can AI get us to reliable and safe nuclear
command and control]{.underline}? [AI-enabled systems may aspire to
reduce some of the mechanical and human errors that have occurred in
nuclear command and control]{.underline}. **[Prior instances of false
alerts and failures in early warning systems should be used as a
training dataset for an AI algorithm to develop benchmarks to quickly
test the accuracy of an early warning alert]{.underline}**. [The goal of
integrating AI into military systems should not be speed and accuracy
alone]{.underline}. **[It should also be to help decision-makers
exercise judgment and prudence to prevent inadvertent
catastrophes]{.underline}**.
