## Aff

### +Uniqueness

#### We're losing the ai race now -- cooperation is key 

**Wodecki 22**

(Ben, "NATO at risk of losing AI innovation race to Russia, China",
<https://aibusiness.com/document.asp?doc_id=777260>) DB

**[The North Atlantic Treaty Organization (NATO) should standardize and
regulate AI to keep up with rivals]{.underline}**, according to findings
published by the U.S. think tank, Center for European Policy Analysis
(CEPA).

CEPA's comments came as it published a series of AI-related
recommendations for NATO amid growing geopolitical tensions with the
likes of Russia, China and North Korea.

[Its recommendations include AI standardization, encouraging and
improving AI literacy and spurring private sector
innovation]{.underline}.

**[Such undertakings would allow NATO allies to better scale and deploy
AI -- and keep pace with rivals]{.underline}**.

"**[These new capabilities will revolutionize NATO's military and
strategic affairs, thus strengthening NATO's ability to fulfill its
essential core tasks of collective defense, crisis management and
cooperative security]{.underline}**," CEPA's Nicholas Nelson and Nico
Luzum wrote.

The pair cited AI projects being undertaken by adversaries, including
China's attempts to develop purported mind-controllable drones and AI
assistants for fighter pilots.

But [NATO allies have their own capabilities -- including U.S.-developed
autonomous tanks and British-made systems that provide ground troops
with information on the surrounding terrain]{.underline}.

The think tank's study suggests that at present, **[NATO]{.underline}**
is leading the AI race -- but **[risks losing its competitive advantage
to peer competitors]{.underline}** "competitors if allies fail to
leverage the private sector, coordinate implementation and engage with
the public."

CEPA suggests that NATO allies should accelerate AI adoption and
actively encourage private sector innovation.

"Ultimately, we hope that [these recommendations enable NATO allies to
better innovate, scale, deploy, and integrate AI and autonomy-based
technologies to form agile, system-wide solutions]{.underline}.

### turn

#### AI cooperation is key to Nato's threat response -- turns the da

**van der Merwe 21**

(Joanna, "NATO Leadership on Ethical AI is Key to Future
Interoperability",
<https://cepa.org/nato-leadership-on-ethical-ai-is-key-to-future-interoperability/>)
DB

In October 2020, Deputy Secretary General of NATO Mircea Geoană
highlighted the benefits of establishing a "transatlantic community
cooperating on Artificial Intelligence (AI)." The Deputy Head of NATO's
Innovation Unit followed with a commitment to its responsible use. The
US Department of Defense (DoD) adopted Ethical Principles for AI in 2020
and has committed to bringing together NATO member and partners to
operationalize these principles. [Despite these statements and
developments, more work is required to tackle the very real challenge
that **ethical AI** will pose to **future interoperability** within
NATO]{.underline}.

[Without a NATO-led initiative focused on aligning these ethical
principles across the Alliance, the interoperability risk of nations
fielding AI-based systems that hinder joint operations is
high]{.underline}. **[As the foremost security framework for Europe and
North America, as well as the leading defense alliance for promoting and
protecting democratic values, NATO is able to facilitate alignment on
this issue]{.underline}**. [As part of a broader strategy on emerging
and disruptive technologies, NATO must prioritize ethical AI if it
wishes to promote the shared values upon which it was founded, play a
key role in facilitating innovation across the Atlantic, and ultimately
retain the ability of its members to undertake joint
operations]{.underline}.

**[Establishing NATO ethical AI principles is the first step toward both
technical and political alignment, in turn enhancing and fostering
interoperability, which is the foundation for NATO to respond to
emerging threats as an Alliance, in a flexible and timely
manner]{.underline}**.

A key challenge for NATO is raising awareness that the answers to
ethical questions can no longer be left to later stages of the
development and procurement cycle. [Decisions made at the political and
legal level will have a significant impact on the engineering practices
used to develop AI, as well as the technical characteristics of the
AI-based systems]{.underline}. The answers to questions such as
respecting human dignity, human control, and accountability will be the
foundation upon which many technical elements are programed. Systems
developers need to make a number of calls throughout the development
cycle informed by the answers to key questions, including:

how to label data

what data to use, and

what is an acceptable outcome?

These answers will also impact how AI systems are evaluated and
ultimately deployed.

If individual nations or groups are left to develop their own ethical
principles without wider alignment to NATO, the result will be a number
of AI-based systems with varying technical specifications based on the
legal and policy decisions made by individual governments when answering
the key questions. As has been demonstrated in areas such as facial
recognition and policing algorithms, the assumptions made by those
developing the tools and answering the key questions have a significant
impact on the real-world functioning of the tool and societal acceptance
of its ethics. The risk of tools failing to gain acceptance depends on
the legal and ethical decisions made by governments. [For the military,
this may mean one state using an AI-based system that is seen as
unacceptable by another, and in a joint operation one state fielding a
system that cannot be used by another]{.underline}. Or worse yet, [this
could render a joint operation impossible]{.underline}. **[Without the
ability to interoperate across NATO, the inability to effectively and
efficiently respond to future threats would undermine the
Alliance]{.underline}**.

The role of the private sector is another aspect of ethical AI
development that has proved a challenge to governments and the
transatlantic relationship. Within states, governments have struggled to
adequately regulate Big Tech firms, which has led to these companies
encroaching on government responsibilities to protect and uphold the
public interest. This encroachment permeates all aspects of government,
including defense and security. As Deputy Secretary of Defense Kathleen
Hicks discussed during her confirmation hearings, the lack of
competition is also a challenge to innovation in the private defense
industry. This, along with a lack of regulation, feeds into the power
imbalance between the sectors. Consequently, private sector companies
building the AI and AI systems that are or will be deployed on the
battlefield are deciding the ethics policies for themselves.

The transatlantic partnership must focus on coordinating these core
principles and systematic governance to ensure AI systems development
aligns with the rule of law and democracy. In particular, this must
ensure answering questions about human dignity, human control, and
accountability. **[NATO is the ideal defense and security forum for this
alignment]{.underline}**. [Given the US lead on adopting ethical
principles for the entire DoD and the EU's drive to assert checks and
balances for private-sector tech companies, **NATO remains the
organization that can bring these two together and establishes the
ethical bottom line**]{.underline}. [These will then ensure the
diverging legal and ethical stances towards Big Tech do not lead to an
interoperability barrier in the future]{.underline}. [If developments
surrounding the General Data Protection Regulation (GDPR) and the
challenges it brought for U.S.-based, data-driven companies are any
indication, a strong transatlantic led initiative is needed in order to
ensure the same challenges do not hinder NATO]{.underline}.

[The solution to the challenge that ethical AI poses for the future of
interoperability within NATO is for the Alliance to establish shared
transatlantic ethical principles, informed by the US DoD, the EU, and
others]{.underline}. **[Establishing these principles will not only
strengthen transatlantic political relations; more technically, it will
allow for the establishment of standardization agreements and inform
training and education initiatives of the Alliance in the
future]{.underline}**.

### No link

#### AI coop doesn't undermine US innovation 

**Lawrence and Cordey 20**

(Christie and Sean, "The Case for Increased Transatlantic Cooperation on
Artificial Intelligence",
<https://www.belfercenter.org/publication/case-increased-transatlantic-cooperation-artificial-intelligence>)
DB

**[The Case for Transatlantic Cooperation]{.underline}**

[There are three critical, interconnected arguments for transatlantic
cooperation to ensure AI innovation protects the security, values, and
economic interests of the United States and the European
Union]{.underline}.

1.**[Global Good]{.underline}**: [Transatlantic AI partnerships and
cooperation encourages innovation and applications that enhance human
welfare, strengthen the economies of the US and the EU, and advance
global security]{.underline}.

2.**[Great Power Competition]{.underline}**: [US-EU leadership of
like-minded nations is needed in this age of great power competition to
tip the scales against efforts by authoritarian
governments---particularly, China and Russia---to undermine
democracies]{.underline}.

3.**[Shared Values]{.underline}**: [The US and the EU share fundamental
values and would benefit from joint efforts to establish AI norms that
would more effectively advance their common vision of AI and ripple
throughout the global AI ecosystem]{.underline}.

Although the US consistently sounds the alarm bells around China's AI
aspirations and the EU urges international efforts against AI that
violates fundamental rights, increasingly noting China's actions with
concern,8 little concrete international action has taken place. [The
United States and the European Union's ongoing reassessment of their
respective AI strategies and legislation]{.underline}9 [provides a
window of opportunity to align and collaborate]{.underline}.
[Transatlantic AI cooperation is at a critical juncture and the United
States and the European Union should seize this opportunity to take
concrete actions]{.underline}.

The Current State

The United States and the European Union are separately assessing and
updating their AI strategies. However, it is a myth to assume they are
not collaborating at all to advance their AI-related goals.
[Transatlantic cooperation on AI norms, standards, research and
development, and data sharing should increase, but the United States and
the European Union can build upon an existing foundation for a stronger
alliance]{.underline}.

United States:The United States views American leadership in AI as
necessary to safeguard American values and maintain defense and economic
superiority. Recognizing the need to develop a national AI approach and
reclaim the AI R&D global leadership position from China, which had
already surpassed the US in several research output metrics by 2016,10
the Obama Administration developed an AI R&D prioritization in October
2016.11 Building on this urgency, the Trump Administration has
prioritized AI and established the American AI Initiative in February
2019.12 This Initiative identified the need for a whole-of-government
approach to prioritize AI R&D and deployment throughout the entire
federal government. The Initiative also identifies the need to grow the
US AI workforce, set national and global norms and standards, and work
with industry and allies to promote an AI environment favorable to the
United States.13

The United States' federal government has made key strategic and
tactical changes to achieve these goals. Federal AI R&D and the American
AI Initiative are coordinated by several committees and subcommittees
within the Executive Office. President Trump pledged to more than double
non-defense AI R&D to \$2 billion by 2022.14 Federal AI R&D, guided by
the National AI R&D Strategic Plan, must now be reported annually for
each federal entity.15 [The United States has taken a "light-touch"
approach to regulation, fearing overly burdensome laws will stifle
innovation]{.underline}. However, guidance is not completely absent. The
Office of Management and Budget released a memo to guide Federal
agencies as they develop regulatory and non-regulatory approaches to
non-government applications of AI and the Department of Defense
published five AI principles to guide AI design, deployment, and
adoptions in defense.16

[Obstacles to the US realizing its goal of global AI leadership exist,
despite the government's prioritization of it.]{.underline} [Key
obstacles include the need to bolster its private sector AI landscape;
address regulatory or standards gaps to safeguard American values;
repair the breakdown of funding and information sharing relationships
between academia, industry, and government; grow its AI workforce; and
further increase its federal AI R&D funding]{.underline}.

European Union: [The European Union, like the United States, intends to
leverage AI's potential as a strategic and transformative
technology]{.underline}.17 **[However, the EU has positioned itself as a
leader in trustworthy, human-centric, ethical, and values-based AI,18 in
comparison to the US government's emphasis on the need for AI innovation
to protect American values, civil liberties, and privacy]{.underline}**.
The EU recognizes that it trails behind the US and China in terms of
volume of investment and maturity of its tech industry.19 [Nonetheless,
the EU believes it can capitalize on its underlying structural strengths
(e.g., academic and innovation record) and on its values to compete
globally and reaffirm its digital and technological
sovereignty]{.underline}.20 Starting with its 2018 Communication:
Artificial Intelligence for Europe,21, 22 the European Commission (EC)
has launched a coordinated effort promoting AI.23 Policies include
increasing public and private investments from \$5.6 billion to \$22
billion annually;24 coordinating research and innovation across Europe;
devising ethical guidelines; fostering digital skills in its workforce;
and promoting public and private sector adoption of AI.25 To support and
counsel these efforts, the EC has established the High-Level Expert
Group on AI (AI HLEG) comprising 52 experts who advise the Commission on
policy and regulatory changes.

The European Union's Juncker26 Commission (2014-2019) actively avoided
regulating AI, causing the European Parliament to increase their efforts
as a proactive voice in favor of stronger AI regulation. However, since
the beginning of Ursula von der Leyen's tenure, the Commission has
initiated efforts to adopt stronger regulation for AI applications
(i.e., differentiating regulation of AI based on defined "high-risk" and
"low-risk" sectors") and associated data spaces.27,28 These legislative
proposals and their associated discussions are planned to be completed
by the end of 2020. During the strategic planning and budgeting process
of its R&D programs, the EU committed to providing at least EUR10.7
billion29 for AI-related research conducted between 2021 and 2027.30
[Despite these financial and political efforts, the EU still remains
technologically dependent on the US and China and suffers from a lack of
capital and private funding, decentralized and uncoordinated AI
expertise, severe brain drain (including to the US), and slow adoption
of AI programming in its education and public sectors]{.underline}.

Transatlantic Cooperation: [Despite over 40 years of scientific
relationships and projects between the United States and the European
Union, AI-specific collaboration has been fraught with varying degrees
of political and academic skepticism on both side of the Atlantic,
notably within the European Commission and the governments of some
Member States]{.underline} (e.g., France and Germany).31 Such a dynamic
is aggravated, in part, by the ever-deteriorating transatlantic
relationship spurred by policy and trade disagreements, public spats,
and increasing American isolationism. Despite such explicit omissions
and stand-offs at the highest levels, transatlantic collaboration for AI
does happen, most notably in various multilateral forums working on
standards (e.g., ISO, IEC, IEEE, G7, G20) or on ethics and norms (e.g.,
OECD, GPAI32).33 In recent months, however, interests and political
support for greater transatlantic coordination on AI seems to be
increasing. This trend was notably demonstrated by a visit from Lt. Gen.
Jack Shanahan---then Director of the US Department of Defense's Joint
Artificial Intelligence Center (JAIC)---to Brussels in January 2020 and
a visit by the European Parliament's delegation to Washington D.C in
February 2020. Both visits included discussions on AI with a variety of
key stakeholders, such as NATO, representatives from the US Congress,
State Department, Federal Transit Administration (FTA), Federal Bureau
of Investigation (FBI), and Privacy and Civil Liberties Oversight Board
(PCLOB).34

[Transatlantic collaboration for AI-related research is taking place at
varying levels although these projects are relatively ad hoc and
materialize within existing scientific and technological research
agreements and roadmaps]{.underline}. For instance, the current Roadmap
for US-EU Science & Technology prioritizes four areas for transatlantic
cooperation, most of which leverage AI (e.g., health, transportation,
bioeconomy, marine and arctic research) or promote institutions that do
(e.g., European Organization for Nuclear Research or CERN).35, 36 These
collaborative links are supported and promoted through a variety of
arrangements and initiatives, such as BILAT 4.0, EURAXES37 or the
European Network of Research and Innovation Centers and Hubs (ENRICH).
In general, and despite challenges to systematically integrating US
entities into European research programs, the US remains the leading
non-EU ("third country") participant in Horizon 2020,38 with over 60
participations and 1,200 partnerships.39 US funding contributions to
Horizon 2020 and participation in AI-related projects, however, is
meager than its broader research involvement in Horizon 2020. For
instance, US collaborative links with Horizon 2020 projects can only be
found in 2% of AI-related projects, 12% of deep learning projects, and
4% of machine learning-related projects.40 [Accordingly, there is still
plenty of room for improvement]{.underline}.41

### No impact

#### No emerging tech impact. 

**Sechser** et al. **19**, \*Todd S., Pamela Feinour Edmonds and
Franklin S. Edmonds, Jr. Discovery Professor of Politics and Public
Policy at the University of Virginia and Senior Fellow at the Miller
Center of Public Affairs, \*\*Neil Narang, Associate Professor of
Political Science at the University of California, Santa Barbara,
\*\*\*Caitlin Talmadge, Associate Professor of Security Studies in the
School of Foreign at Georgetown University. ( "Emerging technologies and
strategic stability in peacetime, crisis, and war", *Journal of
Strategic Studies*, 42:6, pg. 728-729)

Yet [the **history of technological revolutions** counsels **against**
alarmism]{.underline}. Extrapolating from current technological trends
is problematic, both because [technologies often **do not live up to
their promise**, and because technologies often have **countervailing**
or **conditional effects** that can **temper** their **negative
consequences**]{.underline}. Thus, [the fear that **emerging
technologies** will necessarily cause **sudden** and **spectacular
changes** to international politics should be treated with
**caution**]{.underline}. There are at least two reasons to be
circumspect. First, [very **few** technologies fundamentally reshape the
dynamics of international conflict.]{.underline} Historically, [most
technological innovations have amounted to **incremental
advancements,**]{.underline} [and]{.underline} some have **[disappeared
into irrelevance]{.underline}** despite widespread hype about their
promise. For example, [the introduction of **chemical
weapons**]{.underline} [was widely expected to immediately change the
nature of warfare]{.underline} and deterrence after the British army
first used poison gas on the battlefield during World War I. [Yet
chemical weapons quickly turned out to be less **practical**, easier to
**counter**, and **less effective**]{.underline} than conventional
high-explosives in inflicting damage and disrupting enemy operations.6
Other technologies have become important only after advancements in
other areas allowed them to reach their full potential: until armies
developed tactics for effectively employing firearms, for instance,
these weapons had little effect on the balance of power. And [even when
technologies do have significant strategic consequences, they often take
**decades to emerge**]{.underline}, as the invention of airplanes and
tanks illustrates. In short, [it is easy to **exaggerate** the strategic
effects of nascent technologies]{.underline}.7 Second, even if today's
emerging technologies are poised to drive important changes in the
international system, they are likely to have variegated and even
contradictory effects. Technologies may be destabilising under some
conditions, but stabilising in others. Furthermore, [other factors are
likely to **mediate** the effects of **new technologies** on the
international system, including **geography**, the **distribution of
material power, military strategy**, **domestic** and **organisational
politics**, and social and cultural variables, to name **only a
few**]{.underline}.8 Consequently, [the strategic effects of new
technologies often **defy** simple classification.]{.underline} Indeed,
more than 70 years after [nuclear weapons]{.underline} emerged as a new
technology, their [consequences for stability **continue** to be
debated]{.underline}.9

#### AI won't undermine nuclear stability

**Sankaran 19**

(Jaganath, assistant professor at the Lyndon B. Johnson School of Public
Affairs at the University of Texas at Austin, "A DIFFERENT USE FOR
ARTIFICIAL INTELLIGENCE IN NUCLEAR WEAPONS COMMAND AND CONTROL", War on
the Rocks, 4/25,
<https://warontherocks.com/2019/04/a-different-use-for-artificial-intelligence-in-nuclear-weapons-command-and-control/>)
DB

**[Decision-makers who stand guard at the various levels of the nuclear
weapons chain of command face two different forms of
stress]{.underline}**. [The first form of stress is information
overload, shortage of time, and chaos in the moment of a
crisis]{.underline}. [The second is more general, emerging from moral
tradeoffs and the fear of causing loss of life on an immense
scale]{.underline}. **[AI and big data analysis techniques have already
been applied to address the first kind of stress]{.underline}**. [The
current U.S. nuclear early warning system employs a "**dual
phenomenology**" mechanism designed to ensure speed in detecting a
threat and in streamlining information involved in the decision-making
process]{.underline}. [The early warning system employs advanced
satellites and radars to confirm and track an enemy missile almost
immediately after launch]{.underline}. [In an actual nuclear attack, the
various military and political personnel in the chain of command would
be informed progressively as the threat is analyzed, until finally the
president is notified]{.underline}. **[This structure substantially
reduces information overload and chaos for decision-makers in a
crisis]{.underline}**. However, as Richard Garwin writes, **[the system
also reduces the role of the decision-maker "simply to endorse the claim
of the sensors and the communication systems that a massive raid is
indeed in progress."]{.underline}** [While the advanced technologies and
data processing techniques used in the early warning system reduces the
occurrence of false alerts, it does not completely eliminate the chances
of one occurring]{.underline}. In order to address decision-makers' fear
of inadvertently starting a nuclear war, **[future applications of AI to
nuclear command and control should aspire to create an algorithm that
could argue in the face of overwhelming fear of an impending attack that
a nuclear launch isn't happening]{.underline}**. [Such an algorithm
could verify the authenticity of an alert from other diverse
perspectives, in addition to a purely technological
analysis]{.underline}. **[Incorporating this element into the nuclear
warning process could help to address the second form of stress,
reassuring decision-makers that they are sanctioning a valid and
justified course of action]{.underline}**. Command and Control During
the Cold War: The Importance of Big Data In the world of nuclear command
and control, the pursuit of speed and analysis of big data is old news.
In the early 1950s, before the advent of nuclear intercontinental
ballistic missiles (ICBMs), the United States began developing the SAGE
supercomputer. SAGE, which was built at approximately three times the
cost of the Manhattan Project, was the quintessential big data
processing machine. It used the fastest and most expensive computers at
the time -- the Whirlwind II (AN/FSQ-7) IBM mainframe computers -- at
each of 24 command centers to receive, sort, and process data from the
many radars and sensors dedicated to identifying incoming Soviet
bombers. The SAGE supercomputer then coordinated U.S. and Canadian
aircraft and missiles to intercept those bombers. Its goal was to
supplement "the fallible, comparatively slow-reacting mind and hand of
man" in anticipating and defending against a nuclear bomber campaign.
The proliferation of ICBMs in the 1960s, however, made the SAGE command
centers "extraordinarily vulnerable." The U.S. Air Force concluded that
Soviet ICBMs could destroy "the SAGE system long before the first of
their bombers crossed the Arctic Circle." In 1966, speaking at a
congressional hearing, Secretary of Defense Robert McNamara argued that
"the elaborate defenses which we erected during the 1960s no longer
retain their original importance. Today with no defense against the
major threat, Soviet ICBMs, our anti-bomber defense alone would
contribute very little..." The SAGE command centers were shut down.
McNamara formed a National Command and Control Task Force, informally
referred to as the Partridge Commission, to study the problem of nuclear
command and control in the early days of the ICBM era. The commission
concluded "that the capabilities of US \[nuclear\] weapon systems had
outstripped the ability to command and control them" using a
decentralized military command and control structure. The commission
recommended streamlining and centralizing command and control with much
stronger civilian oversight. The commission also advocated the formation
of the modern-day North American Aerospace Defense Command, better known
as NORAD, with its advanced computer and communication systems, early
warning satellites, and forward-placed radars designed to track any
missile launch on the planet before it could reach the continental
United States. [NORAD and its computer and communication systems were
designed to resolve the stress from information overload by
compartmentalizing and automating the process of evaluating a
threat]{.underline}. [Depending on its particular trajectory, **an enemy
nuclear missile may take anywhere between 35 minutes to just eight
minutes to reach its target**]{.underline}. **[When the launch of an
enemy missile occurs, it is first picked up by early warning satellite
sensors within seconds]{.underline}**. [The satellites track these
missiles while the engines are still ignited]{.underline}. Once the
missile comes over the horizon, forward-deployed radars independently
track them. The data from the two systems is then assessed in the
context of the prevailing geostrategic intelligence by NORAD. NORAD
would then pass the assessment up the military and political chain of
command. [This sequence of steps ensures that senior decision-makers are
not overwhelmed with information]{.underline}. [By the time
decision-makers are notified, the decision to retaliate to an apparent
attack "must be made in minutes." **Future advances in AI might only add
incremental improvements to the speed and quality of information
processing to this already advanced nuclear early warning
system**]{.underline}. Using AI to Prevent Inadvertent Nuclear War These
advances in nuclear command and control still do not directly address
the second form of stress, one that emerges from the fear of a nuclear
war and the accompanying moral tradeoffs. [How can AI mitigate this
problem]{.underline}? [History reminds us that technological
sophistication cannot be relied upon to avert accidental nuclear
confrontations]{.underline}. [Rather, these confrontations have been
prevented by individuals who, despite having state-of-the-art technology
at their disposal, proffered alternate explanations for a nuclear
warning alert]{.underline}. **[Operating under the most demanding
conditions, they insisted on a "gut feeling" that evidence of an
impending nuclear war alert was misleading]{.underline}**. [They chose
to disregard established protocol, fearing that a wrong choice would
lead to accidental nuclear war]{.underline}. Consider for example a
declassified President's Foreign Intelligence Advisory Board report
investigating the decision by Leonard Perroots, a U.S. Air Force
lieutenant general, not to respond to incoming nuclear alerts. The
incident occurred in 1983 when NATO was conducting a large simulated
nuclear war exercise code-named Able Archer. The report notes that
Perroots' "recommendation, made in ignorance, not to raise US readiness
in response" was "a fortuitous, if ill-informed, decision given the
changed political environment at the time." The report also states: the
military officers in charge of the Able Archer exercise minimized this
risk by doing nothing in the face of evidence that parts of the Soviet
armed forces were moving to an unusual level of \[nuclear\] alert. But
these [officers acted correctly out of instinct, not informed
guidance]{.underline}. Perroots later complained in 1989, just before
retiring as head of the U.S. Defense Intelligence Agency, "that the U.S.
intelligence community did not give adequate credence to the possibility
that the United States and Soviet Union came unacceptably close to
\[accidental\] nuclear war." In the same year, Stanislav Petrov, a
commanding officer involved in Soviet nuclear operations, also dismissed
a nuclear alert from his country's early warning system. In the face of
data and analysis that confirmed an incoming American missile salvo,
Petrov decided the system was wrong. Petrov later said, "that day the
satellites told us with the highest degree of certainty these rockets
were on the way." Still, he decided to report the warning as a false
alert. His decision was informed by fears that he "didn't want to be the
one responsible for starting a third world war." Later recalling the
incident, he said: "I had a funny feeling in my gut. I didn't want to
make a mistake. I made a decision, and that was it. When people start a
war, they don't start it with only five missiles." Both, Perroots and
Petrov feared the moral consequences of a nuclear war, particularly one
initiated accidentally. They distrusted the data and challenged
protocol. Conclusion Fred Iklé once remarked, "if any witness should
come here and tell you that a totally reliable and safe launch on
warning posture can be designed and implemented that man is a fool." If
that is true, [how close can AI get us to reliable and safe nuclear
command and control]{.underline}? [AI-enabled systems may aspire to
reduce some of the mechanical and human errors that have occurred in
nuclear command and control]{.underline}. **[Prior instances of false
alerts and failures in early warning systems should be used as a
training dataset for an AI algorithm to develop benchmarks to quickly
test the accuracy of an early warning alert]{.underline}**. [The goal of
integrating AI into military systems should not be speed and accuracy
alone]{.underline}. **[It should also be to help decision-makers
exercise judgment and prudence to prevent inadvertent
catastrophes]{.underline}**.
