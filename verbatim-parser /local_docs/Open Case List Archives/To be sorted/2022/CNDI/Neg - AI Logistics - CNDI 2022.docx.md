# AI Projects Neg

## Inherency Answers

### SQ Solves

#### US and NATO tech already counters malicious AI 

Sanur **Sharma** May 30, 20**22** \[ Associate Fellow at Manohar
Parrikar Institute for Defence Studies and Analyses.; NATO's AI Push And
Military Implications -- Analysis; EurasiaReview\]

The technological [advancements in]{.underline} Artificial Intelligence
([AI]{.underline}), machine learning, big data analytics, robotics,
quantum computing and virtual reality have [led to the rise in use of
autonomous systems in military applications]{.underline}. This is
changing the face of the battlefield by enabling new forms of military
functions, over and above the conventional systems, thus enabling the
execution of higher coercive actions. The North Atlantic Treaty
Organization (NATO) countries are also adopting such emerging
technologies to maintain their strategic advantage and to mitigate
transnational threats. [Russia's offensive cyber hostilities and China's
military adoption of AI]{.underline} for augmenting its high-tech
warfare mechanisms [have emerged as the contributing factors for NATO to
upscale its technological efforts in Emerging and Disruptive
Technologies]{.underline} (EDTs). [NATO is making ambitious investments
in EDTs to ensure interoperability and standardisation among member
states.]{.underline} This Issue Brief takes stock of the current
strategic surge by NATO in AI adoption and its ongoing efforts to
exploit EDTs for defence innovation and adoption. It discusses the role
of AI in contemporary conflicts, specifically NATO's response to the
Russia--Ukraine conflict, and explores the vulnerabilities in the AI
systems as well as the challenges and limitations in AI adoption by
NATO. The US National Security Commission Report of 2021 states that
China is leapfrogging to new technologies by investing in
intelligentised warfare like swarm drones and using AI for
reconnaissance, electromagnetic countermeasures and coordinated
firepower strikes.1 The US is jointly working with its allies on the
policy implications of such new technology. It is also partnering with
countries like Canada, Denmark, Estonia, the UK, France and Norway, to
work on military standards on AI.2 In October 2021, NATO formally
adopted the first AI strategy on the responsible military use of AI with
three core tasks: collective defence, crisis management and cooperative
security.3 [NATO]{.underline}'s strategy [aims]{.underline} to
accelerate the uptake of AI for military systems.4 The six principles of
the NATO's AI strategy include: Lawfulness, Responsibility and
Accountability, Explainability and Traceability, Reliability,
Governability and Bias Mitigation.5 This strategy aims [to protect,
monitor and innovate AI and related disruptive technologies in a phased
manner to establish political support for AI military projects. The
strategic surge in EDTs is driven by the accelerated investment towards
the military adoption and innovation of EDTs and maintaining a
sustainable innovation ecosystem that can be achieved through
civil--military collaboration]{.underline}. In 2021, NATO endorsed the
strategy on EDTs that included AI and machine learning among the seven
identified key technologies (Data, AI, Autonomy, Quantum, Space,
Biotechnology, and Hypersonic).6 The strategy plans to invest US\$ 1
billion in building test centres across Europe and North America,
focusing on emerging technologies like AI, Quantum and hypersonics.7 In
the NATO Summit held at Brussels in 2021, as a part of the NATO 2030
Agenda, NATO's new Defence Innovation Accelerator for the North Atlantic
[(DIANA)]{.underline} was launched. It [aims to maintain NATO's
technological edge]{.underline} compared to nations like China and
Russia, which are challenging the West with their accelerated
investments to build technological capacity and use offensive subversive
measures. DIANA has been assigned to manage the NATO Innovation Fund[,
receiving]{.underline} a funding of US[\$ 82.6 million a year for 15
years]{.underline}.8 It [will]{.underline} [explore the]{.underline}
future roadmap of [implementation of advanced technologies
and]{.underline} competition to [foster transatlantic
cooperation]{.underline}.9 [At present, there are 10 accelerator sites
with more than 50 test centres in technological hubs across the
states]{.underline}.10 The NATO advisory group on EDTs is an external
body that advises NATO on the optimisation of its innovation efforts.
This group provides recommendations on improving collaboration and
partnerships with the private sector, industry, and academia. In
addition, there are other bodies like the NATO Advisory board, Allied
Command Transformation (ACT), NATO's Science and Technology Organisation
(STO), and NATO Communication and Information Agency (NCIA) that support
the alliance's adoption of deep technologies and EDTs. AI has been a
contributing agent in weaponising cyberspace and augmenting cyberwarfare
to the next level in modern battlefield scenarios. While some of its
uses such as in scaling of data analytics, data fusion, deep fakes,
cyber defence have matured, its use in autonomous weapon systems and
other complex operational applications are at a nascent stage. [AI has
been]{.underline} aggressively [used to spread disinformation in the
Russia--Ukraine War.]{.underline} Machine learning algorithms have been
used to amplify misleading and fake content on social media platforms,
like doctored videos of invading forces and fake live streams. [On the
other hand, it has also been used for anomaly detection, identification
of disinformation and for cybersecurity]{.underline}. AI uses natural
language processing algorithms, machine learning and deep learning to
identify anomalies in the text data, images and videos.
[Russia]{.underline} is said to have [used AI-enabled
systems]{.underline} not only on the battlefield but also [in
cyberspace, targeting the critical infrastructures]{.underline} of
Ukraine.11 Russian troll farms have been [alleged to have used
AI-enabled systems]{.underline} to generate human faces [for fake
propagandist personas on social media]{.underline} platforms like
Twitter, Instagram and Facebook.12 NATO countries have also used AI to
help Ukraine counter such AI-based attacks. Private companies are also
playing a role in the unfolding AI battlespace. [US-based
companies]{.underline} like Snorkel AI, a data science platform, has
[made]{.underline} its [services accessible to federal
authorities]{.underline} for the detection of anomalous signals and
adversary communications in order [to access high-value information for
better decision-making]{.underline}.13 Similarly, [Ukraine has been
given free access to Clearview AI facial recognition
software]{.underline}, which has a database of 2 billion photos crawled
from Russian social media platforms. This software is being used [for
the detection of Russian forces and to identify the dead and gauge the
spread of disinformation in cyberspace]{.underline}.14 AI's analytical
potential has been tapped by companies even before the Russia--Ukraine
war started. In December 2021, a geospatial data firm, SpaceKnow,
claimed to have detected a military presence in Yelna, a Russian town.
The Russia--Ukraine conflict has become a test case for AI adoption in
modern warfare. The US is using the conflict as a test-bed for many of
its AI projects with the Pentagon's 'Maven' project having contributed
to the detection and classification of objects of interest from various
drone footage through AI and Machine Learning (ML) algorithms. It has
been reported that the Pentagon has been using AI and ML tools to
collect a vast amount of data on the Russia--Ukraine war and analyse it
to learn and generate battlefield intelligence about the Russian command
and control strategies.15 The advanced AI-enabled systems with the US
Department of Defense (DoD) are said to have been used for overseeing
the battlefield and collecting and archiving signals intelligence. It
was stated at the Defense One's Genius AI Summit in April 2022 that all
this information will be fed into systems for training of machine
learning algorithms to support future decision-making processes.16 It is
believed that the [US and NATO allies have already built such AI-enabled
cyber weapons and defences]{.underline}, information about which is said
to be highly classified.17 [The US DoD and its allies have taken
advantage of these advanced tools to gather critical
information]{.underline} from the publically available image data [to
thwart Russian attacks]{.underline} in Ukraine. This war data will also
[help NATO allies anticipate adversary attacks, their behaviour, and the
use of advanced technologies in the real world]{.underline} by countries
like China and Russia. This intelligence will also augment multifactor
analysis and modelling changes dynamically by integrating different
technological platforms. [Due to the sanctions imposed on Russia as a
result of the Russia--Ukraine war, its AI development is expected to
slow down.]{.underline} The ongoing conflict highlights the constraints
around the use of AI. [Despite AI-enabled cyber-attacks and
misinformation campaign by Russia, Ukraine has mounted effective
counter-cyber operations.]{.underline}18 Russia's limited use of AI in
the conflict can be explained through the existing vulnerabilities in
the AI systems that can be exploited in many ways. One hypothesis for
Russia's limited use of AI could be the trust in such systems where it
is a matter of lives and military objectives at stake.19 The
vulnerabilities in the AI systems can include data poisoning and input
attacks, attacking the supply pipelines by simply crafting data and
feeding it to public resources, white-box and black-box attacks.20 There
is always a chance of orchestrated and conflicting data in the face of
AI models to derail them and to exploit the vulnerabilities in the
algorithms, and active manipulation by the adversaries can be induced.
Defense Advanced Research Projects Agency ([DARPA) has
launched]{.underline} a Guaranteeing AI Robustness against Deception
([GARD]{.underline}) programme. Under this programme, [development
efforts are being made to establish a theoretical foundation for
defensible ML and the creation and testing of such
systems]{.underline}.21 The Army Research Laboratory ([ARL) is working
with]{.underline} the Internet of Battlefield Things Collaborative
Research Alliance ([IoBT-CRA]{.underline}) to explore the use of ML and
intelligent technology on the battlefield and strengthen the
collaboration between autonomous actors and human soldiers in combat.
They are also working on methods [to understand]{.underline} the
challenges of [AI]{.underline}-enabled systems employed [on the
battlefield and to make them less susceptible to attacks]{.underline}.22
AI technology in modern warfare will be an intractable weapon in future
conflicts beyond Ukraine. Countries trying to achieve a technological
edge over others have started considerable investments in AI technology
to strengthen their militaries. [NATO has invested US\$ 1 billion to
develop new AI defence technologies. The US DoD has also planned to
invest US\$ 874 million in AI-]{.underline}related technologies as a
part of their army research and development budget (federal fiscal year
2022 DoD budget).23 [The UK DoD is]{.underline} funding suppliers to
work with Defence Science & Technology Lab (Dstl) on AI projects which
were [£7million for the year 2021/22 and]{.underline} is supposed to
[increase to £29 million in the next year]{.underline}.

## 

## Competition Advantage Answers

### US Winning/Can't Lose

#### US winning -- China's authoritarian ascendence will cause its downfall -- the US will outcompete

**Kroenig 20** (Matthew Kroenig is a professor of government and foreign
service at Georgetown University, and the deputy director of the
Scowcroft Center for Strategy and Security at the Atlantic Council, "Why
the U.S. Will Outcompete China," The Atlantic, 4/3/2020,
<https://www.theatlantic.com/ideas/archive/2020/04/why-china-ill-equipped-great-power-rivalry/609364/)->
MP

[National-security analysts see China as one of the greatest threats
facing the United States]{.underline} and its allies. According to an
emerging conventional wisdom, [China has the leg up on the
U.S]{.underline}. in part [because its authoritarian government can
strategically plan for the long term, unencumbered by competing branches
of government, regular elections, and public opinion. Yet this faith in
autocratic ascendance and democratic decline is contrary to historical
fact]{.underline}. China may be able to put forth big, bold plans---the
kinds of projects that analysts think of as long term---but [the
visionary projects of autocrats don't usually pan out]{.underline}. Yes,
democratic governments are obligated to answer to their citizens on
regular intervals and are sensitive to public opinion---that's actually
democracies' greatest source of strength. Democratic leaders have a
harder time advancing big, bold agendas, but the upside of that
difficulty is that the plans that do make it through the system have
been carefully considered and enjoy domestic support. Historically
speaking, once a democracy comes up with a successful strategy, it
sticks with the plan, even through a succession of leadership.
Washington has arguably followed the same basic, three-step geopolitical
plan since 1945. First, [the [United States
built](https://www.amazon.com/Present-Creation-Years-State-Department/dp/0393304124)
the current, rules-based international system by providing security in
important geopolitical regions, constructing international institutions,
and promoting free markets and democratic politics within its sphere of
influence]{.underline}. Second, it welcomed into the club any country
that played by the rules, even former adversaries, like Germany and
Japan. And, third, the U.S. worked with its allies to defend the system
from those countries or groups that would challenge it, including
competitors such as Russia and China, rogue states such as Iran and
North Korea, and terrorist networks. [America can pursue long-term
strategy in part because it enjoys domestic political
stability]{.underline}. While new politicians seek to improve on their
predecessor's policies, [the United States is unlikely to see the
drastic shifts in strategy that come from the fall of one political
system and the rise of another]{.underline}. Democratic elections may be
messy, but they're not as messy as coups or civil wars. [Open
societies]{.underline} have many other advantages as well. They
[facilitate innovation, trust in financial markets, and economic
growth.]{.underline} Because [democracies tend to be more reliable
partners, they are typically skillful alliance builders, and they can
accumulate resources without frightening their neighbors.
They]{.underline} tend to [make thoughtful, informed decisions on
matters of war and peace, and to focus their security forces on external
enemies]{.underline}, not their own populations[. Autocratic systems
simply cannot match this impressive array of economic, diplomatic, and
military attributes]{.underline}. David Leonhardt [recently
wrote](https://www.nytimes.com/2020/01/16/opinion/sunday/china-economy-trade.html)
in The New York Times, "Chinese leaders stretching back to Deng Xiaoping
have often thought in terms of decades." Commonly cited examples of that
long-term thinking include the Belt and Road Initiative, a program that
invests in infrastructure overseas; Made in China 2025, an effort to
subsidize China's giant tech companies to become world leaders in
21st-century technologies, such as artificial intelligence; and
Beijing's promise to be a global superpower by 2049. Since putting in
place sound economic reforms in the 1970s, China has seen its economy
expand at eye-popping rates, to become the world's second largest. Many
[economists
predict](https://www.newsweek.com/worlds-largest-economy-2030-will-be-china-followed-india-us-pushed-third-1286525)
that China could even surpass the United States within the decade, and
[some have
suggested](http://content.time.com/time/world/article/0,8599,2043235,00.html)
that China's model of state-led capitalism will prove more successful,
in terms of economic growth, than the U.S. template of free markets and
open politics. I doubt these predictions. Because [autocratic leaders
are unconstrained and do not have to contend with a legislature or
courts, they have an easier time taking their countries in new and
radically different directions]{.underline}. Then, when the dictator
changes his mind, he can do it again. Mao's autocratic China ricocheted
from one failed policy to another: the Great Leap Forward, then the
Hundred Flowers Campaign, then the Cultural Revolution. Mao [aligned
with the Soviet Union in
1950](https://www.fmprc.gov.cn/mfa_eng/ziliao_665539/3602_665543/3604_665547/t18011.shtml)
only to nearly [fight a nuclear
war](https://nsarchive2.gwu.edu/NSAEBB/NSAEBB49/index2.html) with Moscow
in the next decade. Beginning in the time of Deng Xiaoping, China
pursued a fairly constant strategy of liberalizing its economy at home
and ["hiding its capabilities and biding its
time"](https://www.ft.com/content/05cd86a6-b552-11e7-a398-73d59db9e399)
abroad. But President [Xi Jinping]{.underline} abandoned these dictums
when he took over. As the most powerful leader since Mao---he has
[changed China's constitution to set himself up as dictator for
life---he could once again jerk China in several new directions,
according to his whims]{.underline}, and back again. [According to the
Asia Society](https://aspi.gistapp.com/winter-2020/page/overview), he
has stalled or reversed course on eight of 10 categories of economic
reform promised by the Chinese Communist Party (CCP) itself. Moreover,
Xi is baring China's teeth militarily, [taking contested
territory](https://www.nytimes.com/2018/02/08/world/asia/south-china-seas-photos.html)
from neighbors in the South China Sea and [conducting military
exercises](https://www.nytimes.com/2017/07/25/world/europe/china-russia-baltic-navy-exercises.html)
with Russia in Europe. The problem for Beijing is that [stalled reforms
will stymie its economic potential and its confrontational policies are
provoking an international coalition to contain them]{.underline}. The
[2017 U.S. National Security
Strategy](https://www.whitehouse.gov/wp-content/uploads/2017/12/NSS-Final-12-18-2017-0905-2.pdf)
declared great-power competition with China the foremost security threat
to the U.S.; the European Union labeled China a "systemic rival"; and
Japan, Australia, India, and the United States have formed a new "quad"
of powers to balance China in the Pacific. Furthermore, the plans often
cited as evidence of China's farsighted vision, the Belt and Road
Initiative and Made in China 2025, were announced by Xi only in 2013 and
2015, respectively. Both are way too recent to be celebrated as
brilliant examples of successful, long-term strategic planning. A
certain level of domestic political stability is a prerequisite for
charting a steady strategic course in foreign and domestic affairs. But
autocratic regimes are notoriously brittle. While institutionalized
political successions in democracies typically lead to changes of
policy, political successions in autocracies are likely to result in
regime collapse and war. China's "5,000 [years of
history](https://camphorpress.com/5000-years-of-history/)" were
pockmarked by rebellion, revolution, and new dynasties. Fearing internal
threats to domestic political stability---consider the [protests this
year in Hong Kong](https://www.bbc.com/news/world-asia-china-49317695)
and Xinjiang---[the CCP [spends more on domestic
security](https://www.wsj.com/articles/china-spends-more-on-domestic-security-as-xis-powers-grow-1520358522)
than on its national defense. If you follow the money, the CCP is
demonstrating that the government is more afraid of its own people than
of the Pentagon]{.underline}. This [domestic fragility will frustrate
China's efforts to design and execute farsighted plans]{.underline}. If
threats to Chinese domestic stability were to materialize and the CCP
were to collapse tomorrow, for example, **[Chinese grand strategy could
undergo another seismic shift, including possibly opting out of
competition with the United States altogether]{.underline}**. [Shadi
Hamid: China Is Avoiding Blame by Trolling the
World](https://www.theatlantic.com/ideas/archive/2020/03/china-trolling-world-and-avoiding-blame/608332/)
Autocracies have other vulnerabilities as well. State-led planning has
never produced high rates of economic growth over the long term.
[Autocrats are poor alliance builders who fight with their supposed
allies more than with their enemies]{.underline}. And the highest
priority of autocratic security forces is repressing their own people,
not defending the country. The world has undergone drastic changes in
just the past few years, but these enduring patterns of international
affairs have not. Some fear that Trump's nationalist tendencies will
erode the U.S. position, but the momentum of America's successful grand
strategy has kept the country on a fairly steady course. Despite Trump's
criticism of NATO, for example, two new countries have joined the
alliance on his watch, including [North Macedonia this
week](https://www.nytimes.com/reuters/2020/04/02/world/europe/02reuters-nato-northmacedonia.html).
The coronavirus has upended a sense of security in the U.S., leading
many people into the familiar trap of [lauding autocratic China's firm
response](https://www.nytimes.com/2020/03/19/world/asia/coronavirus-china-united-states.html)
in contrast to the halting and patchwork measures in the United States.
But there is good reason to believe that this assessment will be updated
in America's favor with the benefit of hindsight. Already we are seeing
evidence that conditions are much worse in China than CCP officials are
letting on and that China's attempts at international "disaster
diplomacy" are backfiring. It has been revealed that the CCP has
continually
[misrepresented](https://time.com/5813628/china-coronavirus-statistics-wuhan/)
the numbers of COVID-19 infections and
[deaths](https://www.bloomberg.com/news/articles/2020-03-27/stacks-of-urns-in-wuhan-prompt-new-questions-of-virus-s-toll)
in China, and European nations have
[rejected](https://www.bbc.com/news/world-europe-52092395) and returned
faulty Chinese coronavirus testing kits. The great political theorist
Niccolò Machiavelli considered a similar line of thinking in the 16th
century, about whether republics or dictators charted a more stable
course. He wrote, "I, therefore, disagree with the common opinion that a
populace in power is unstable \[and\] changeable ... The prince ...
unchecked by laws, will be more ... unstable, and imprudent than a
populace." The U.S. political system certainly has problems. But
democracy is the best machine ever invented for generating enormous
power, wealth, and prestige on the international stage.

#### US leading AI now. 

**NIST 22** (National Institute of Standards and Technology, agency of
the United States Department of Commerce, April 5, 2022,
https://www.nist.gov/artificial-intelligence/ai-standards-federal-engagement)

[Federal agencies engaged in developing standards for artificial
intelligence (AI) either because these activities are part of their
assigned responsibilities or because AI is essential to their current or
evolving missions.]{.underline} Executive Order (EO) 13859 directed
agencies to ensure that \"technical standards minimize vulnerability to
attacks from malicious actors and reflect federal priorities for
innovation, public trust, and public confidence in systems that use AI
technologies\" and to \"develop international standards to promote and
protect those priorities.\" [NIST]{.underline} involved stakeholders
from the private and public sectors in developing the U.S. Leadership in
AI: A Plan for Federal Engagement in Developing Technical Standards and
Related Tool, which was released in August 2019. The Plan provided
guidance regarding priorities and appropriate levels of engagement in
AI-standards-related matters. It also [recommended that the \"[Federal
Government should commit to]{.mark} deeper, [consistent]{.mark},
long-term [engagement]{.mark} [in AI standards dev]{.mark}elopment
activities to help the United States to speed the pace of reliable,
robust, and trustworthy AI technology development.\"]{.underline} Since
then, agencies which develop or use AI have made progress in bolstering
AI standards-related knowledge, leadership, and coordination; promoted
focused research on the trustworthiness of AI systems; supported and
expanded public-private partnerships; and engaged internationally.
Notable steps by agencies include: Established the role of Federal AI
Standards Coordinator with responsibility to gather and share AI
standards-related needs, strategics, roadmaps, terminology, use cases,
and best practices in support of reliable, robust, and trustworthy AI in
government operations. This responsibility resides with NIST. Created
the AI Standards Coordination Working Group (AISCWG) to facilitate
agency activities related to development and use of AI standards.
Working under the charter of the Interagency Committee on Standards
Policy (ICSP), and aligning its activities with the Federal AI Standards
Coordinator, the AISCWG is responsible for promoting effective and
consistent federal policies leveraging AI standards cited in the AI
Standards Plan Multiple agencies are reviewing options to better
position the Federal Government to gain access to new employees and to
develop current employees to meet rapidly growing AI-capable workforce
needs. That includes aiming to develop and provide a clear career
development and promotion path that values and encourages participation
in and expertise in AI standards and standards development. The National
Defense Authorization Action of 2021 (NDAA) explicitly authorized NIST
to carry out a wide range of AI standards-related functions. Over the
past two years, NIST has expanded and made noteworthy progress in
carrying out research that specifically addresses standards-oriented
research recommendations in the AI Standards Plan. The National Science
Foundation (NSF) is supporting several grant programs related to AI
trustworthiness. Among them is the National AI Research Institutes
program, which includes a growing number of partnerships with federal
agencies and private companies. Strategic engagement in international AI
standards was the focus of the U.S. Department of State\'s submission to
Congress of \"A Plan to Establish Exchanges and Partnerships between the
United States and Its Allies to Create Standards for Artificial
Intelligence Technologies.\" [[The US championed]{.mark} development of
the first [international principles]{.mark} [for the responsible use of
AI at]{.mark} the Organisation for Economic Co-operation and Development
([OECD]{.mark}). [Also]{.mark}, [the US became a founding member of
the]{.mark} Global Partnership on AI ([GPAI]{.mark}). Through engagement
in GPAI, the United States seeks to complement the more policy-oriented
work of the OECD by increasing coordination on research and development
and scaling up practical projects for implementing trustworthy
AI.]{.underline}

### China Not a Threat

#### US-China AI competition is not zero-sum -- collaboration with clear goals solves.

**Sullivan 21** (Lieutenant Colonel Ryan Sullivan is an Army pilot by
trade, who lived and studied at the prestigious Fudan University in
Shanghai, China, as an Olmsted Scholar. He was one of just five Army
officers selected that year. Ryan has taken his experience in and
knowledge of China and combined that with graduate-level work in the
field of Artificial Intelligence to deliver an in-depth study of the
critical elements of U.S.-China competition in Artificial Intelligence.
"US China AI Competition Factors," 10/04/21
<https://www.airuniversity.af.edu/Portals/10/CASI/documents/Research/Cyber/2021-10-04%20US%20China%20AI%20Competition%20Factors.pdf?ver=KBcxNomlMXM86FnIuuvNEw%3D%3D>,
p9)ZK

[The U.S. and China remain]{.underline} the two nations [best positioned
to benefit from the increasing adoption of AI]{.underline} across
society[. [Competition over AI is not zero-sum,]{.mark} in that **both
nations will derive value** from AI growth]{.underline}, but the
benefits are not shared equally. China's substantial advantages in data
and the size of its market lead experts to predict that China will
account for nearly half of the estimated \$15.7 trillion that AI will
add to the global gross domestic product (GDP) by 2030, and almost
double the expected growth in North America.23 AI's economic benefits
are undeniable, but AI competition with China goes beyond GDP growth and
centers on the global diffusion of Chinese values, norms, and standards
for AI utilization. [That environment is very much undecided, as the
international community struggles with challenges over data privacy, the
role of AI-empowered multinational companies, and critical choke points
in supply chains that impact the AI industry and threaten national
security interests.]{.underline} This paper seeks to recognize that
[[collaboration occurs between the U.S. and]{.mark} China and identifies
efforts [to address security concerns]{.mark}]{.underline},
[[collaborations, and even cooperation]{.underline}]{.mark} in areas of
trans-national challenges that benefit all of humankind. The
deteriorating bilateral relationships and mistrust likely prohibit
bilateral cooperation in many areas. However, in working with
like-minded nations with shared values, the U.S. can promote
multilateral collaboration and cooperation opportunities for AI[. Such
[engagements should not exclude China by design.
However]{.mark}]{.underline}, [the criteria for engagement [must clarify
that the values shared by democratic nations]{.mark} will help address
the ethical concerns [over AI]{.mark}]{.underline} and the adoption of
technical standards. The current political environment and U.S. actions
taken against universities and companies with ties to the PLA leave very
little room for military-to-military (mil-to-mil) collaboration on AI,
nor did research for this paper discover any indications from Chinese
sources of a desire for mil-to-mil engagements. Nevertheless, in
focusing on our allies and promoting interoperability, the Department of
Defense ([DoD) entities must remain aware that collaboration
opportunities might arise in various areas of
responsibility]{.underline} (AOR) [and prepare appropriate
guidance]{.underline} and safeguards for units based on current
policies.

#### China and the US will collaborate on AI research even through strategic tensions

**Andrews 3/16** (Edmund L. Andrews is a former economics reporter for
The New York Times who served as a technology reporter in Washington,
European economics correspondent and Washington economics correspondent,
"China and the United States: Unlikely Partners in AI," Stanford
University Human-Centered Artificial Intelligence, 3/16/2022,
<https://hai.stanford.edu/news/china-and-united-states-unlikely-partners-ai>)
-- MP

Despite both rivalry and rising tensions between [the United States and
China]{.underline}, the two nations [have become the world's leading
collaborators in research on artificial intelligence]{.underline}. The
newly released AI Index Report, which tracks AI trends on a host of
fronts and is published by the Stanford Institute for Human-Centered
Artificial Intelligence, finds that [U.S. and Chinese AI researchers
teamed up on far more published articles than collaborators between any
other two nations]{.underline}. Overall, U.S.-China collaborations on AI
research have quintupled since 2010 and totaled 9,660 papers in
2021---much faster than the increase in collaborations between any other
two nations. Collaborations between the United States and United
Kingdom, the second most prolific source of cross-border research,
increased almost threefold to 3,560 papers. Read the 2022 AI Index The
startling trend highlights a paradox. [Even as China and the U.S. race
for leadership in what they view as a strategically important
technology, researchers on both sides appear to see benefits in sharing
expertise and working together]{.underline}. "What's clear is that the
amount of [collaboration between the United States and China has gone up
dramatically, and it has gone up much more than collaborations between
any two other countries]{.underline}," says Raymond Perrault,
Distinguished Computer Scientist at SRI International in Menlo Park and
co-chair of the AI Index Steering Committee. To some extent, the surge
[in U.S.-China research simply reflects the fact that both nations have
poured vast resources into artificial intelligence and produce huge
amounts of research]{.underline}. On top of that, many Chinese
researchers were trained in the United States and retain close
professional ties to their American colleagues. But the practice is
consistent with patterns observed during previous technological
revolutions in textiles, steel, and chemical engineering. Research by
Jeffrey Ding, a postdoctoral fellow at Stanford HAI, has shown that [the
full economic impact of historic tech advances stemmed less from which
nation pioneered a technology than from which ones were best at applying
it across a broad range of industries]{.underline}. That dispersion of
technology requires sharing information across industries as well as
borders, much as the United States catapulted applied British advances
in steel machinery to develop manufacturing approaches that catapulted
it to economic dominance. That said, [the collaboration in AI comes at a
time of growing friction between the United States and China over trade,
human rights, and strategic power in the Pacific Rim]{.underline}.
Former President Donald Trump villainized China over its trade
practices, and President Joe Biden imposed a diplomatic boycott of the
Beijing Olympics over China\'s human rights abuses. Indeed, the volume
of published U.S.-China collaborations in AI has declined slightly from
its peak in 2019. Perrault says it's unclear whether the recent dip
reflects a lag in data, a temporary disruption, or a more fundamental
change. Collaborations have also declined slightly between most other
nations, such as those between researchers in the United States, Canada,
and Europe. Globally, AI research has soared over the past decade. The
volume of peer-reviewed AI journal articles has more than doubled since
2015, hitting a new record of 172,000 papers in 2021. On top of that,
researchers posted 56,000 pre-print articles in repositories
specializing in AI. Chinese researchers have been the most prolific for
the past several years, publishing 27.5% of all AI journal articles
worldwide. American researchers accounted for 12%. Chinese journal
articles also led those of every other nation in citations, an indicator
of their scientific importance. [And although the United States
continues to receive more AI patents than any other nation, China is now
filing more than half of all the world's patent applications in the
field]{.underline}. Image Graph showing increase in patent filings in
China (51.69% of total) while patent filings in the U.S. fall (16.92%)
AI research has also intensified in most other parts of the world,
notably in the European Union, Canada, and Japan. The one notable
exception is Russia, which has largely gone its own way. "Russia is way,
way smaller in this area,'' says Perrault. In the United States, 1,200
institutions published roughly 3,000 cross-border AI collaborations in
2021. In China, 500 institutions published 2,000 cross-border projects.
By contrast only 60 Russian institutions teamed up on 600 AI projects.

#### AI collaboration between US-China are vital, and a translated Chinese report confirms development is NOT zero-sum. 

**Sullivan 21** (Lieutenant Colonel Sullivan is an Army pilot by trade,
who lived and studied at the prestigious Fudan University in Shanghai,
China, as an Olmsted Scholar. He was one of just five Army officers
selected that year. Ryan has taken his experience in and knowledge of
China and combined that with graduate-level work in the field of
Artificial Intelligence to deliver an in-depth study of the critical
elements of U.S.-China competition in Artificial Intelligence.) "The
U.S., China, and Artificial Intelligence Competition Factors" 2021-10-04
<https://www.airuniversity.af.edu/Portals/10/CASI/documents/Research/Cyber/2021-10-04%20US%20China%20AI%20Competition%20Factors.pdf?ver=KBcxNomlMXM86FnIuuvNEw%3D%3D>
// ZX

[The purpose of this research is to examine the critical elements of
U.S.-China competition over Artificial Intelligence (AI) norms and
determine if the resulting contest is inherently zero-sum]{.underline}.
The paper explores SinoAmerican AI competition through the lenses of
values, cohesion, influence, and legitimacy to better understand each
nation's positional advantages and identify possibilities for
cooperation in bilateral or multilateral engagements. [Those engagements
will be part of the overall Sino-American struggle for influence within
international institutions and standard-setting
organizations]{.underline} (SSO) [over]{.underline} technical standards
and, more importantly, **[values, norms and ethics guiding AI
applications.]{.underline}**In the new world order, **[competition over
the establishment of AI norms is not zero-sum]{.underline}**; [however,
the continued deterioration in mutual trust and bilateral relations will
test both the U.S. and China to avoid transactional
engagements]{.underline} or challenges to core interests which could
escalate AI competition into AI conflict. Hal Brands and Zack Cooper
argue that ["many American conceptions of the competition with China
rest on the false premise that this contest will be neatly bipolar
--]{.underline} a replay of the East-West standoff in Europe during the
Cold War. A much messier world is taking shape."2 This messiness stems
from 21st-century competition, which [a report from RAND views as mixed
sum, as shared interest and objectives converge in an international
system that currently displays elements of unipolarity, bipolarity, and
multipolarity.]{.underline}3 AI will play an instrumental role in
reshaping the world order and the degrees to which each polarity element
exists. Neither the U.S. or China can achieve its objectives alone, and
both must seek support from other nations to achieve their desired ends.
AI and other emerging technology will empower developing nations and
middle powers to play a more prominent role in constructing a world
order. [Those middle powers, or "third countries, such as India,
Indonesia and Turkey," no longer feel the need "to align entirely with
the United States, nor with China,]{.underline} when they can gain by
playing Washington and Beijing against each other to produce a
"multipolar competition, not a bipolar one."4 This paper assumes that
the liberal framework remains intact, and that AI competition will occur
in an increasingly multipolar world order. [Despite breakdowns in
bilateral relations, zero-sum conflict is not predestined, as Shanghai
scholar Wu Xinbo notes, "promoting pragmatic cooperation and
constructive competition, effective management, control of risks, and
prevention of major conflicts between China and the United States
remains the basic direction of China's diplomacy]{.underline} with the
United States."11 Several U.S. scholars and leaders such as Elizabeth
Economy and Graham Allison also advocate cooperative approaches to
competition by promoting coopetition, coevolution, or rivalry
partnerships.12 [Such strategies are hard to imagine in a period of
increasingly strained bilateral relations and diverging values, and are
likely to fail if the leaders of both nations maintain a binary approach
to engagement]{.underline}. This paper explores competition through the
lens of values, cohesion, influence, and legitimacy to better understand
each nation's positional advantages and identify possibilities for
cooperation in bilateral or multilateral engagements. The U.S. and China
remain the two nations best positioned to benefit from the increasing
adoption of AI across society. Competition over AI is not zero-sum, in
that both nations will derive value from AI growth, but the benefits are
not shared equally. China's substantial advantages in data and the size
of its market lead experts to predict that China will account for nearly
half of the estimated \$15.7 trillion that AI will add to the global
gross domestic product (GDP) by 2030, and almost double the expected
growth in North America. [Values competition is not zero-sum, but
diverging AI values represent the most significant challenge to
cooperation and limits the extent to which Sino-American collaboration
on AI]{.underline} can continue in some areas. The synergy of economic,
political, and technological challenges that AI competition creates
demands action. Our nation's gravest risk is to remain on the sidelines
of global leadership and allow the CCP to promote a values system that
prioritizes the Party over the nation and its people as a viable
alternative to democracy. [Failure to address federal data privacy and
consumer rights concerns leaves China and Europe as the only two models
offering "guardrails against invasive data collection]{.underline}."106
Challenges and opportunities in forming alliances and relying on the
collective to balance other nations' interests and needs present
opportunities and risks. Regarding China, Graham Allison offers that
"while U.S. planners must consider all reasonable contingencies, basing
our strategy to meet the China challenge on the expectation that the
Chinese economy or political system fails would be a mistake."107
**[Finding ways to cooperate or collaborate would prove beneficial to
the collective]{.underline}**, but such choices on a state-to-state
basis could very well lead to conflict**[. Attempting to challenge or
contain China without allies seems unlikely to succeed in the long
term.]{.underline}**

#### Despite Advantages on both sides, US-China relations could solve AI MAD

**Allison '20** (Graham Allison, Douglas Dillon Professor of Government,
Harvard Kennedy School Member of the Board, Belfer Center Former
Director, Belfer Center Faculty Affiliate, Future of Diplomacy
Project,August 2020,
<https://www.belfercenter.org/publication/china-beating-us-ai-supremacy>)Roho

Clues for a Winning Strategy Is AI a race China is destined to win? With
a population four times the size of the United States, there is no
question that China will have the largest domestic market for AI
applications. With many multiples of the United States in data,
substantially larger numbers of computer scientists and a government for
which there is a first-order priority, [we can understand colleagues who
are pessimistic. Indeed, it is our best judgment that on the current
trajectory, while the United States will maintain a narrow lead over the
next five years, China will then catch up and pass us quickly
thereafter.]{.underline} Nonetheless, we believe that this is an arena
in which the United States can compete---and win. Congress recently
established the "National Security Commission on Artificial
Intelligence," with Eric Schmidt as its chair, and Bob Work, who served
as Deputy Secretary of Defense under both Obama and Trump, as Vice
Chair. Its mission is to develop that strategy "to ensure America's
national security enterprise has the tools it needs to maintain U.S.
global leadership."55 In the hope of being helpful to that effort, we
conclude with five pointers toward a winning strategy. First, Americans
must wake up to the challenge. Recognition that that the United States
faces a serious competitor in a contest in which the outcome will be
decisive for our future is necessary to get our competitive juices
flowing. The Olympics offers an instructive analogy for thinking about a
competitive strategy for AI. It also reminds us that competition is
inherently a good thing. Competition produces superior performance.
Participants in a marathon run faster than they do when running alone.
Indeed, competition is a core American value. Free markets organize a
competitive process that produces better products at cheaper prices.
Science and its applications advance as research teams compete to better
understand the world.56 Second, in this competition, the United States
cannot hope to be the biggest---in that category, China wins by default
due to the size of its population. However, what the United States can
be is the smartest. In the seeking to improve and advance the most
advanced of technologies, the brightest 0.0001 percent of individuals
make the difference. [The U]{.underline}nited [S]{.underline}tates [can
succeed by recruiting talent from]{.underline} all [7.7 billion
people]{.underline} on Earth and enabling these individuals to realize
their full potential.57 In fact, U.S. companies have now recruited more
than half of the top 100 recognized AI geniuses. In sharp contrast,
[China is a closed society---limited essentially to 1.4 billion Chinese
speakers]{.underline}. Just 1000 foreign born individuals became Chinese
citizens last year. So while the United States will not win competitions
in which bulk numbers are the dominant factor, where brilliance,
creativity and innovation matter most, the United States has a decisive
advantage.58 Third, platforms matter. Here the United States begins with
a huge sustainable competitive advantage: [English is the universal
language for science, business and the web.]{.underline} Chinese face
the choice of either speaking English, or simply talking to themselves.
Not only do the Chinese, but also the French and others often complain
that this is unfair---and it may be. But it is a fact. To transform
Singapore from a third-world city into one of the world's most
successful and prosperous global trading hubs, Lee Kuan Yew insisted on
making English its first language. (Indeed, at one point in counseling
Chinese leaders, he suggested that China make English its first
language.) Today, [more than half of the 7.5 billion people on Earth
speak English---and another billion are seeking to learn.]{.underline}
Fourth, [American companies have a significant first mover advantage in
the establishment of the major platforms in AI, including operating
systems (Android and Apple), design of advanced semiconductors (arm),
and killer apps]{.underline}---including Instagram, YouTube and
Facebook. Instagram has 1 billion monthly active users; Facebook more
than 2.4 billion. While Chinese competitors will certainly attempt to
displace the current leaders in both platforms and applications, if
American companies are smart enough to continue enlarging their users'
opportunities, improving their experiences, and expanding the number of
people using their platforms and applications, Chinese and others who
want to speak to the world could have to continue relying on
U.S.-dominated platforms. Fifth[, while competing vigorously with the
intention of sustaining U.S. leadership, [we]{.mark} [must
recognize]{.mark} at the same time [the necessity of coop]{.mark}eration
in areas where neither the United States nor China can secure its own
minimum vital national interests without the help of the
other]{.underline}. The consequences of human energy consumption on the
climate offers a vivid illustration. If either the United States or
China keeps emitting greenhouse gases at the current rate, in one
hundred years, this could produce a biosphere in which neither nation
can survive. [Thus [there is no viable alternative to
coop]{.mark}eration.]{.underline} The same is true in other realms
including preventing third party provocations---for example, in North
Korea or Taiwan---from dragging the United States and China into a
catastrophic war; and cooperation to prevent recurring financial crises
like the Great Recession of 2008 from cascading into another Great
Depression. We suspect there may be an analog in limiting the
unconstrained advance of AI. [The possibility that nations could
simultaneously compete ruthlessly, on the one hand, while cooperating
intensely, on the other, sounds to diplomats like a contradiction. In
the world of business, however, it is called life.]{.underline} While no
one has yet developed a felicitous term for what is sometimes called
"coopetition," Apple and Samsung offer a powerful example. The two are
ruthless rivals in the global market for smartphones (where, in fact,
over the past five years Samsung has become number one). But who is
Apple's largest supplier of components for smartphones? Samsung.
Managing a relationship that is simultaneously competitive and
cooperative requires vigilance, judgment and agility in adapting. But
if, as we believe the evidence shows, [technologies on a small globe
have left the United States and China with two---and only two---options,
[we believe they can find ways to coexist, however uncomfortably, if
their only alternative is mutual destruction]{.mark}.]{.underline}

### No Arms Race

#### We are not in an AI arms race -- everything is gradual. 

**Horowitz and Scharre, 2021** - Director of the Emerging Capabilities
Policy Office in the Office of the Under Secretary of Defense for Policy
and  Vice President and Director of Studies at CNAS \[Michael, Paul,
January 12 2021, "AI and International Stability: Risks and Confidence-
Building Measures",
<https://www.cnas.org/publications/reports/ai-and-international-stability-risks-and-confidence-building-measures>,
Acc 6/18/22. M.A.\]

AI is a general-purpose technology akin to computers or the internal
combustion engine, not a discrete technology like missiles or aircraft.
Thus, while [concerns of an "AI arms race" are overblown,]{.underline}
real risks
exist.[2](https://www.cnas.org/publications/reports/ai-and-international-stability-risks-and-confidence-building-measures#fn2)
[Additionally, despite the rhetoric of many national leaders, military
spending on AI is relatively modest to date]{.underline}. [Rather than a
fervent arms race, militaries' pursuit of AI looks more like routine
adoption of new technologies and a continuation of the multi-decade
trend of adoption of computers, networking, and other information
technologies. Nevertheless, the incorporation of AI into national
security applications and warfare poses genuine risks]{.underline}.
Recognizing the risks is not enough, however. [Addressing them requires
laying out suggestions for practical steps states can take to minimize
risks stemming from military AI
competition.[3](https://www.cnas.org/publications/reports/ai-and-international-stability-risks-and-confidence-building-measures#fn3)
One approach states could take is adopting confidence-building
measures]{.underline} (CBMs): [unilateral]{.underline}, bilateral,
[and]{.underline}/or [multilateral actions that states can take to build
trust and prevent inadvertent military conflict]{.underline}. CBMs
generally involve using transparency, notification, and monitoring to
attempt to mitigate the risk of
conflict.[4](https://www.cnas.org/publications/reports/ai-and-international-stability-risks-and-confidence-building-measures#fn4)
There are challenges involved in CBM adoption due to differences in the
character of international competition today versus during the Cold War,
when CBMs became prominent as a concept. However, considering
possibilities for CBMs and exploring ways to shape the dialogue about AI
could make the adoption of stability-promoting CBMs more likely.

### Escalation Turn

#### Treating AI as an arms race causes crisis stability.

**Scharre 2021** -- Director of the Technology and National Security
Program at the Center for a New American Security \[Paul,
6/28/21,https://tnsr.org/2021/06/debunking-the-ai-arms-race-theory/,
"Debunking the AI Arms Race Theory", 6/18/22, LND\]

[Even if military AI spending does not rise to the level of an "arms
race,]{.underline}" [many nations are]{.underline} nevertheless [engaged
in a security competition in the adoption of military AI,]{.underline} a
competition [that does pose risks]{.underline}. The situation that
[states find themselves]{.underline} in [with regard to AI competition
is]{.underline} much more [accurately described as a security
dilemma]{.underline},[16](https://tnsr.org/2021/06/debunking-the-ai-arms-race-theory/#_ftn16)
a more generalized competitive dynamic between states than the more
narrowly defined "arms race." In his 1978 article, "Cooperation Under
the Security Dilemma," [Robert Jervis defined the security dilemma
as]{.underline} follows: ["\[M\]any of the means by which a state tries
to increase its security decrease the security of
others]{.underline}."[17](https://tnsr.org/2021/06/debunking-the-ai-arms-race-theory/#_ftn17)
As Charles Glaser has pointed out, it is not obvious from this
definition why it would be intrinsically bad for an increase in one
state's security to come at the expense of another's
security.[18](https://tnsr.org/2021/06/debunking-the-ai-arms-race-theory/#_ftn18)
In fact, decreasing the security of other states could have beneficial
effects in enhancing deterrence and reducing the risks of aggression or
achieving a favorable balance of power in a region, which could lead to
greater political influence. The problem comes in the second- and
third-order effects that could develop when another state reacts to
having its security reduced. Responses could include counterbalancing
with a net effect of no change in security (or worsening security).
Glaser argues that there are some situations in which security
competition is a rational strategy for a state to pursue even if
competitors will arm in response. In other situations, arming may be a
suboptimal strategy for a state, which would be better served by
restraint or pursuing arms
control.[19](https://tnsr.org/2021/06/debunking-the-ai-arms-race-theory/#_ftn19)
[Security competition could even leave both states worse off than
before.]{.underline} This can occur [during a]{.underline} traditional
[arms race]{.underline} if [nations expend vast sums of money in an
unsuccessful attempt to gain an advantage over one another, with the
result that both nations divert funds from non-defense
expenditures.]{.underline} If the outcome of a security competition is
the same relative military balance as before, the balance of power may
not have meaningfully changed, but both nations could face diminished
economic and social well-being at home relative to if they had avoided a
security competition. Even absent this "guns vs. butter" tradeoff,
however, [there are other ways in which security competition can lead to
a net negative outcome for both state]{.underline}s. [One way this could
occur is if]{.underline} military innovation and [the development of new
capabilities alter the character of warfare in a manner that is more
harmful, more destructive, less stable]{.underline}, or otherwise less
desirable than before. In his 1997 article, "The Security Dilemma
Revisited," Glaser gave the example of military capabilities that
shifted warfare to a more offense-dominant
regime.[20](https://tnsr.org/2021/06/debunking-the-ai-arms-race-theory/#_ftn20)
There are other ways in which warfare could evolve in a net negative
direction as well. For example, in World War I, Germany's interest in
developing and deploying chemical weapons was spurred in part due to
fears about France's developments in poison
gas.[21](https://tnsr.org/2021/06/debunking-the-ai-arms-race-theory/#_ftn21)
The result was the introduction of a weapon that increased combatant
suffering on both sides, without delivering a significant military
advantage to either. The same could occur with [AI]{.underline}: It
[could alter the character of warfare in a way that would be a net
negative for all participants. One possibility for how AI]{.underline}
could alter warfare in a manner that [would leave all states worse off
would be if it accelerated the tempo of war past the point of human
control, making warfare faster, more violent, and less
controllable.]{.underline} There are advantages to adding intelligence
into machines, but given the limitations of AI systems today, the
optimal model for achieving the highest quality decision-making would be
a joint human-machine architecture that combines human and machine
decision-making. One way in which machines outperform humans, however,
is in speed. [It is possible to envision a competitive dynamic in which
countries feel compelled to automate increasing amounts of their
military operations in order to keep pace with adversaries.]{.underline}
Then-Deputy Secretary of Defense Robert O. Work summed up the dilemma
when he asked, "If our competitors go to Terminators and we are still
operating where the machines are helping the humans and it turns out the
Terminators are able to make decisions faster, even if they're bad, how
would we
respond?"[22](https://tnsr.org/2021/06/debunking-the-ai-arms-race-theory/#_ftn22)
[This is a classic security dilemma. One state's pursuit of greater
automation and faster reaction times undermines other states' security
and leads them to similarly pursue more automation just to keep
up.]{.underline} If states fall victim to this trap, [it could lead to
all states being less secure]{.underline}, since the pursuit of greater
automation would not merely be an evolution in weapons and
countermeasures that simply leads to [the creation of new weapons in the
future warfare could shift to a qualitatively different regime in which
humans have less control over lethal force as decisions become more
automated and the accelerating tempo of operations pushes humans "out of
the loop" of decision-making. Some]{.underline} Chinese scholars have
hypothesized about a battlefield "singularity," in which the pace of
combat eclipses human
decision-making.[23](https://tnsr.org/2021/06/debunking-the-ai-arms-race-theory/#_ftn23)
U.S. scholars have used the term "hyperwar" to refer to a similar
scenario.[24](https://tnsr.org/2021/06/debunking-the-ai-arms-race-theory/#_ftn24)
While the speed of engagement necessitates automation in some limited
areas today, such as immediate localized defense of ships, bases, and
vehicles from rocket and missile attack, expanding this zone of machine
control into broader areas of war would be a significant development.
Less human control over warfare could lead to wars that are less
controllable and that escalate more quickly or more widely than humans
intend. Similarly[, limiting escalation or terminating conflicts could
be more challenging if the pace of operations on the battlefield exceeds
human decision-making.]{.underline} Political leaders would have a
command-and-control problem in which their military forces are operating
"inside" (i.e., faster than) their own decision cycle. The net effect of
the quite rational desire for nations to gain an edge in speed could
lead to an outcome that is worse for all. Yet, competitive dynamics
could nevertheless drive such a result. [One state's pursuit of greater
automation and faster reaction times undermines other states'
security]{.underline} and leads them to similarly pursue more automation
just to keep up. Financial markets provide an example of this dynamic in
a non-military competitive environment. Automation introduced into
financial markets, especially high-frequency trading in which trades are
executed at super-human speeds in milliseconds, has contributed to
unstable market conditions that can lead to "flash crashes," in which
prices rapidly and dramatically
shift.[25](https://tnsr.org/2021/06/debunking-the-ai-arms-race-theory/#_ftn25)
Financial regulators have responded by employing "circuit breakers" that
automatically halt trading for a pre-determined period of time if the
price moves too
quickly.[26](https://tnsr.org/2021/06/debunking-the-ai-arms-race-theory/#_ftn26)
Financial markets have the benefit of a regulator who can force
cooperative measures on competitors to address suboptimal outcomes.
Under conditions of anarchy in the international security environment,
any such cooperation would have to come from state v s themselves.

### Deterrence Turn

#### Quick adoption of AI applications degrades nuclear [deterrence]{.underline} and causes [miscalculation]{.underline} and war.

**Johnson '20** (Dr. James Johnson is a postdoctoral research fellow at
the James Martin Center for Nonproliferation Studies (CNS) at the
Middlebury Institute of International Studies, Monterey. He holds a PhD
in politics and international relations from the University of
Leicester, where he is also an honorary visiting fellow with the School
of History and International Relations. Dr. Johnson is fluent in
Mandarin and has published widely in the fields of security and
strategic studies, Sino-American security relations, nuclear
nonproliferation and arms control, emerging technology (especially AI),
Chinese foreign policy, and East Asian security) "Artificial
Intelligence: A Threat to Strategic Stability" Feb 25, 2020
[https://www.airuniversity.af.edu/Portals/10/SSQ/documents/Volume-14_Issue-1/Johnson.pdf
//](https://www.airuniversity.af.edu/Portals/10/SSQ/documents/Volume-14_Issue-1/Johnson.pdf%20//)
ZX

AI-augmented conventional capabilities might affect strategic stability
between great military powers. [The nuanced, multifaceted possible
intersections of this emerging technology with a range of advanced
conventional weapons can compromise nuclear capabilities, thus
amplifying the potentially destabilizing effects of these
weapons.]{.underline} This article argues that **[a [new generation of
artificial intelligence--enhanced]{.mark} conventional [capabilities
will exacerbate the risk of inadvertent
escalation]{.mark}]{.underline}** caused by the commingling of nuclear
and nonnuclear weapons. **[[The increasing speed of warfare will also
undermine strategic stability and increase the risk of nuclear
confrontation]{.underline}]{.mark}**. The hyperbole surrounding
artificial intelligence (AI) makes it easy to overstate the
opportunities and understate the challenges posed by the development and
deployment of AI in the military sphere.1 Commingling and entangling
nuclear and nonnuclear capabilities and the increasing speed of warfare
may well undermine strategic stability.2 From what we know today about
emerging technology, new iterations of AI augmented advanced
conventional capabilities will compound the risk of military
escalation,3 especially inadvertent and accidental escalation.4 While
the potential escalation risks posed by advances in military technology
have been discussed lightly in the literature, the potential of military
AI to compound the risk and spark inadvertent escalation is missing.5
This article addresses how and why AI could affect strategic stability
between nuclear-armed great powers (especially China and the United
States) and the multifaceted possible intersections of this disruptive
technology with advanced conventional capabilities.6 Toward this end,
the article conceptualizes and defines military-use AI and identifies a
broad portfolio of nonnuclear weapons with "strategic effects"7 along
with their attendant enabling systems, including specific AI innovations
that pose the greatest risks to nuclear stability.8 Rather than provide
a net assessment of all of the possible ways AI could influence
strategic stability, the article instead examines the possible stability
enhancing and destabilizing effects in the nuclear domain using two
examples: swarming autonomous weapon systems (AWS) and hypersonic
weapons.9 Four core themes help conceptualize military-relevant AI.10
[First, [AI does not exist in a vacuum]{.mark}.That is, in isolation AI
will unlikely be a strategic game changer. Instead, it will mutually
reinforce the destabilizing effects of existing advanced
capabilities]{.underline}, [thereby increasing the speed of warfare and
compressing the decision-making time frame]{.underline}. Second, [AI's
impact on stability, deterrence, and escalation will likely be
determined as much by a state's perception of its
functionality]{.underline} than what it is capable of doing. [In the
case of nuclear policy, deterrence, and strategic calculations more
broadly, [**the perception of an adversary's capabilities** and
intentions **is as important as its actual
capabilit**]{.mark}**y**]{.underline}. In addition to the importance of
military force postures, capabilities, and doctrine, the effects of AI
will therefore also have a strong cognitive element, increasing the risk
of inadvertent escalation as a result of misperception and
misunderstanding. For the foreseeable future, military AI will include a
fair degree of human agency, especially in the safety-critical nuclear
domain. Thus, strategic calculations on the use of force made in
collaboration with machines at various levels will continue to be
informed and shaped by human perceptions. Third, [[the increasingly
competitive and contested nuclear multipolar world order will compound
the destabilizing effects of AI and, in turn, increase escalation
risks]{.mark} in future warfare between great military
powers---especially China and the United States]{.underline}. [Moreover,
the potential operational and strategic advantages offered by
AI-augmented capabilities could prove irresistible to nuclear-armed
strategic rivals. Thus motivated, adversaries could eschew the
limitations of AI, compromising safety and verification standards to
protect or attempt to capture technological superiority on the future
digitized battlefield]{.underline}.11 Finally, and related, against this
inopportune geopolitical backdrop, [the perceived strategic benefits of
AI powered weapons will likely attract states as a means to sustain or
capture the technological upper hand over rivals.]{.underline} **[[The
most pressing risk]{.underline}]{.mark}** posed to nuclear **[[security
is, therefore, the premature adoption of unsafe, errorprone, unverified,
and unreliable AI technology]{.underline}]{.mark}** in the context of
nuclear weapons, which could have catastrophic implications.12 Military
AI applications can be broadly categorized into those that have utility
at a predominately operational or strategic level of warfare.13 At the
operational level, applications include autonomy14 and robotics
(especially drone swarming); multi-actor interaction during red teaming
and war gaming; big data--driven modeling;15 and intelligence analysis
to locate and monitor mobile missiles, submarines, mines, and troops
movement.16 At a strategic level, applications include (1) intelligence,
surveillance, and reconnaissance (ISR) and command, control,
communications, and intelligence (C3I) systems (especially in complex,
adversarial, and cluttered environments);17 (2)  enhanced missile
defense with machine learning-augmented automatic target recognition
(ATR) technology (i.e., improving target acquisition, tracking, guidance
systems, and discrimination);18 conventional precision missile munitions
(including but not limited to hypersonic variants) able to target
strategic weapons; (3)  increased speed and scope of the observation,
orientation, decision, and action (OODA) loop decision-making to augment
air defense and electronic warfare (especially in antiaccess/area-denial
\[A2/AD\] environments); and (4) AI-enhanced offensive and defensive
cyber capabilities (e.g., machine learning techniques to infiltrate and
uncover network vulnerabilities and to manipulate, spoof, and even
destroy these networks).19 While the potential strategic effects of
military AI are not unique or exclusive to this technology[, the
confluence of several trends weighs heavily on the pessimistic side of
the instability]{.underline}-[stability ledger: the rapid technological
advancements and diffusion of military AI; the inherently destabilizing
characteristics of AI technology (especially heightened speed of
warfare, explainability, and vulnerability to cyberattack); the
multifaceted possible intersections of AI with nuclear weapons; the
interplay of these intersections with strategic nonnuclear capabilities;
and the backdrop of a competitive multipolar nuclear world order, which
may entice states to prematurely deploy unverified, unreliable, and
unsafe AI-augmented weapons into combat situations.]{.underline} The
historical record demonstrates that security competition---motivated by
the desire to control warfare---tends to be ratcheted up because of the
complexity of military technology and operations over time.20 As a
result, the Clausewitzian conditions of "fog and friction" will likely
become a ubiquitous outcome of the uncertainties created by increasingly
complex and inherently escalatory technologies. From this perspective,
the acceleration of modern warfare, the shortening of the
decision-making time frame, and the commingling of military systems have
occurred within the broader context of the computer revolution (e.g.,
remote sensing, data processing, acoustic sensors, communications, and
cyber capabilities).21 These overarching trends do not rely on AI and
would have likely occurred whether AI were involved or not. **[AI is
best understood, therefore, as a potentially powerful force multiplier
of these developments.]{.underline}** [Put another way, military AI, and
the advanced capabilities it enables, is a natural
manifestation---rather than the cause or origin---of an established
trend, potentially leading states to adopt destabilizing launch
postures]{.underline} due to the increasing speed of war and
commingling.22 The following three case studies ground the discussion of
the core themes related to AI and the risk of inadvertent escalation to
illustrate how and why military AI applications fused with nonnuclear
weapons might cause or exacerbate escalation risks in future warfare.
They also illuminate how these AI-augmented capabilities would work and,
despite the risks associated with the deployment of these systems, why
militaries might deploy them nonetheless. Because military commanders
are concerned with tightly controlling the rungs on the "escalation
ladder," they should, in theory, be against delegating too much
decision-making authority to machines---especially involving nuclear
weapons.23 Competitive pressures between great military powers and fear
that others will gain the upper hand in the development and deployment
of military AI (and the advanced weapon systems AI could empower) might
overwhelm these concerns, however. By way of a caveat, the cases do not
assume that militaries will necessarily be able to implement these
augmented weapon systems in the near term. Disagreements exist among AI
researchers and analysts about the significant operational challenges
faced by states in the deployment of AI-augmented weapon systems. [The
proliferation of a broad range of AI-augmented autonomous weapon systems
(most notably drones used in swarming tactics) could have far-reaching
strategic implications for nuclear security and escalation in future
warfare]{.underline}.24 [Several observers anticipate that sophisticated
AIaugmented AWSs will soon be deployed for a range of ISR and strike
missions]{.underline}.25 Even if AWSs are used only for conventional
operations, their proliferation could nonetheless have destabilizing
implications and increase the risk of inadvertent nuclear escalation.
For example, AIaugmented drone swarms may be used in offensive sorties
targeting ground-based air defenses and by nuclear-armed states to
defend their strategic assets (i.e., launch facilities and their
attendant C3I and earlywarning systems), exerting pressure on a weaker
nuclear-armed state to respond with nuclear weapons in a
use-them-or-lose-them situation. Recent advances in AI and autonomy have
substantially increased the perceived operational value that military
great powers attach to the development of a range of AWSs,26 potentially
making the delegation of lethal authority to AWSs an increasingly
irresistible and destabilizing prospect.27 That is, in an effort to
defend or capture the technological upper hand in the possession of
cutting-edge war-fighting assets vis-à-vis strategic rivals'
traditionally conservative militaries, states may eschew the potential
risks of deploying unreliable, unverified, and unsafe AWS. [Today, the
main risk for stability and escalation is the technical limitations of
the current iteration of AI machine learning software (i.e.,
brittleness, explainability, unpredictability of machine learning,
vulnerability to subversion or "data poisoning," and the fallibility of
AI systems to biases)]{.underline}.28 To be sure, immature deployments
of these nascent systems in a nuclear context would have severe
consequences.29 [Conceptually speaking, autonomous systems will
incorporate AI technologies such as visual perception, speech, facial
recognition, and decisionmaking tools to execute a range of core air
interdiction, amphibious ground assaults, long-range strike, and
maritime operations independent of human intervention and
supervision]{.underline}.**[30Currently, only a few weapon systems
select and engage their targets without human
intervention]{.underline}**. Loitering attack munitions (LAM)---also
known as "loitering munitions" or "suicide drones"---pursue targets
(such as enemy radars, ships, or tanks) based on preprogrammed targeting
criteria and launch an attack when their sensors detect an enemy's air
defense radar.31 Compared to cruise missiles (designed to fulfill a
similar function), LAMs use AI technology to shoot down incoming
projectiles faster than a human operator ever could and can remain in
flight (or loiter) for much longer periods. This attribute could
complicate the ability of states to reliably and accurately detect and
attribute autonomous attacks.32 A low-cost lone-wolf unmanned aerial
vehicle (UAV) would, for example, not pose a significant threat to a US
F-35 stealth fighter, [but hundreds of AI machine learning autonomous
drones in a swarming sortie may potentially evade and overwhelm an
adversary's sophisticated defense capabilities]{.underline}---even in
heavily defended regions such as China's east and coastal regions.33
Moreover, stealth variants of these systems34---[coupled with
miniaturized electromagnetic jammers and cyberweapons---may be used to
interfere with or subvert an adversary's targeting sensors and
communications systems, undermining its multilayered air
defenses]{.underline} in preparation for drone swarms and long-range
stealth bomber offensive attacks.35 In 2011, for example, MQ-1 and MQ-9
drones in the Middle East were infected with hard-to-remove malicious
malware, exposing the vulnerability of US subset systems to offensive
cyber.36 This threat might, however, be countered (or mitigated) by the
integration of future iterations of AI technology into stealth fighters
such as the F-35.37 Manned F-35 fighters will soon be able to leverage
AI to control small drone swarms in close proximity to the aircraft
performing sensing, reconnaissance, and targeting functions, including
countermeasures against swarm attacks.38 In the future, extended
endurance of UAVs and support platforms could potentially increase the
ability of drone swarms to survive these kinds of countermeasures.39
Several prominent researchers have opined that, notwithstanding the
remaining technical challenges as well as the legal and ethical
feasibility,40 we can expect to see operational AWSs in a matter of
years.41 According to former US deputy secretary of defense Robert Work,
the United States "will not delegate lethal authority to a machine to
make a decision" in the use of military force. 42 Work adds, however,
that such self-restraint could be tested if a strategic competitor
(especially China and Russia) "is more willing to delegate authority to
machines than we are and, as that competition unfolds, we'll have to
make decisions on how we can best compete" (emphasis added).43 In short,
pre-delegating authority to machines, and taking human judgment further
out of the crisis decision-making process, might severely challenge the
safety, resilience, and credibility of nuclear weapons in future
warfare.44 The historical record is replete with examples of near
nuclear misses, demonstrating the importance of human judgment in
mitigating the risk of miscalculation and misperception (i.e., of
another's intentions, redlines, and willingness to use force) between
adversaries during crises.45 Despite these historical precedents, the
risks associated with unpredictable AIaugmented autonomous systems
operating in dynamic, complex, and possibly a priori unknown
environments remain underappreciated by global defense communities.46
Eschewing these risks, China and Russia plan to incorporate AI into
unmanned aerial and undersea vehicles for swarming missions infused with
AI machine learning technology.47 [Chinese strategists have reportedly
researched data-link technologies for "bee swarm" UAVs]{.underline},
particularly emphasizing network architecture, navigation, and
anti-jamming military operations for targeting US aircraft carriers.48
Drones used in swarms are conceptually well suited to conduct preemptive
attacks and nuclear ISR missions against an adversary's nuclear and
nonnuclear mobile missile launchers and nuclear-powered ballistic
missile submarines (SSBN), along with their attendant enabling
facilities (e.g., C3I and early warning systems, antennas, sensors, and
air intakes).49 The Defense Advanced Research Projects Agency (DARPA),
for example, is developing an autonomous surface vehicle (ASV) double
outrigger, Sea Hunter, currently being tested by the US Navy to support
antisubmarine warfare operations (i.e., submarine reconnaissance).50
Some observers have posited that autonomous systems like Sea Hunter may
render the underwater domain transparent, thereby eroding the
second-strike deterrence utility of stealthy SSBNs. The technical
feasibility of this hypothesis is highly contested, however.51 On the
one hand, several experts argue that deployed in large swarms, these
platforms could transform antisubmarine warfare, rendering at-sea
nuclear deterrence vulnerable. On the other hand, some consider such a
hypothesis technically premature because (1) it is unlikely that sensors
on board AWSs would be able to reliably detect deeply submerged
submarines; (2) the range of these sensors (and the drones themselves)
would be limited by battery power over extended ranges;52 and (3) given
the vast areas traversed by SSBNs on deterrence missions, the chance of
detection is negligible even if large numbers of autonomous swarms were
deployed.53 Thus, significant advances in power, sensor technology, and
communications would be needed before these autonomous systems have a
gamechanging strategic impact on deterrence.54 However, irrespective of
the veracity of this emerging capability, the mere perception that
nuclear capabilities face new strategic challenges would nonetheless
elicit distrust between nuclear-armed adversaries---particularly where
strategic force asymmetries exist. Moreover, DARPA's Sea Hunter
demonstrates how [the emerging generation of autonomous weapons is
expediting the completion of the iterative targeting cycle to support
joint operations, thus increasing the uncertainty about the reliability
and survivability of states' nuclear second-strike
capability]{.underline} **[and [potentially triggering
use-them-or-lose-them situations]{.mark}]{.underline}**. Conceptually
speaking, **[[the most destabilizing impact of AI on nuclear deterrence
would be the synthesis of autonomy with a range of
machinelearning-augmented sensors, undermining states' confidence in the
survival of their second-strike capabilities]{.underline}]{.mark}** and
in extremis triggering a retaliatory first strike.55 [Enhanced by the
exponential growth in computing performance and coupled with advances in
machine learning techniques that can rapidly process data in real time,
AI will empower drone swarms to perform increasingly complex
missions]{.underline}, such as hunting hitherto hidden nuclear
deterrence forces.56 In short, the ability of future iterations of AI
able to predict based on the fusion of expanded and dispersed data sets
and then to locate, track, and target strategic missiles such as mobile
ICBM launchers in underground silos, on board stealth aircraft, and in
SSBNs is set to grow.57 [Combining speed, persistence, scope,
coordination, and battlefield mass, AWSs will offer states attractive
asymmetric options to project military power within contested A2/AD
zones]{.underline}.73 **[Enhanced by sophisticated machine learning
neural networks, China's manned and unmanned drone teaming operations
could potentially impede future US freedom of navigation operations in
the South China Seas.]{.underline}**74 Its air- and sea-based drones
linked to sophisticated neural networks could, for example, support the
People's Liberation Army's manned and unmanned teaming operations. Were
China to infuse its cruise missiles and hypersonic glide capabilities
with AI and autonomy, close-range encounters in the Taiwan Straits and
the East and South China Seas would become more complicated, accident-
prone, and destabilizing---at both a conventional and nuclear level.75
China is reportedly developing and deploying UUVs to bolster its
underwater monitoring and antisubmarine capabilities as part of a
broader goal to establish an "underwater Great Wall" to challenge US
undersea military primacy. US AI-enhanced UUVs could, for example,
theoretically threaten China's nuclear ballistic and nonnuclear attack
submarines.76 A new generation of AI-augmented advanced conventional
capabilities will exacerbate the risk of inadvertent escalation caused
by the commingling of nuclear and strategic nonnuclear weapons (or
conventional counterforce weapons) and the increasing speed of warfare,
thereby undermining strategic stability and increasing the risk of
nuclear confrontation. This conclusion is grounded in the overarching
findings that relate to how and why AI could affect strategic stability
between great military powers--- especially China and the United States.
If a state perceives that the survivability of its nuclear forces were
at risk, advanced conventional capabilities (e.g., autonomous drone
swarms and hypersonic weapons) augmented with AI machine learning
techniques will have a destabilizing impact at a strategic level of
conflict. AI's effect on strategic stability will likely be determined
by states' perceptions of its operational utility rather than actual
capability. If an adversary underestimated the potential threat posed by
nascent and especially poorly conceptualized accident-prone autonomous
systems, the consequences would be severely destabilizing. Despite the
speed, diverse data pools, and processing power of algorithms compared
to humans, complex AI-augmented systems will still depend on the
assumptions encoded into them by human engineers to simply extrapolate
inferences---potentially erroneous or biased---from complexity,
resulting in unintended outcomes. One of the most significant escalatory
risks caused by AI is likely to be, therefore, the perceived pressure
exerted on nuclear powers in the use of AI-augmented conventional
capabilities to adopt unstable nuclear postures (such as launch on
warning, rescinding no-first-use pledges, or nuclear war fighting), or
even to exercise a preemptive first nuclear strike during a crisis. In
extremis, human commanders might lose control of the outbreak, course,
and termination of warfare. Further, a competitive and contested
multipolar nuclear environment will likely exacerbate the potentially
destabilizing influence of AI, increasing that risk of inadvertent
escalation to a nuclear level of conflict between great military powers.
In today's multipolar geopolitical order, therefore, relatively low-risk
and low-cost AI-augmented AWS capability---with ambiguous rules of
engagement and absent a robust normative and legal framework---will
become an increasingly enticing asymmetric option to erode an advanced
military's deterrence and resolve. By disrupting effective and reliable
flows of information and communication between adversaries and allies
and within military organizations, AI-augmented conventional weapon
systems (i.e., C3I, early warning systems, and ISR) could complicate
escalation management during future crisis or conflict--- especially
involving China and the United States. [A prominent theme that runs
through the scenarios in this article---and central to understanding the
potential impact of AI for strategic stability and nuclear security---is
the concern that AI systems operating at machine speed will push the
pace of combat to a point where machine actions surpass the cognitive
and physical ability of human decision-makers to control or even
comprehend events.]{.underline} Effective deterrence depends on the
clear communication of credible threats and consequence of violation
between adversaries, which assumes the sender and recipient of these
signals share a common context allowing for mutual interpretation.103
For now, it remains axiomatic that human decisions escalate a situation;
however, [military technology like AI that enables offensive
capabilities to operate at higher speed, range, and lethality will move
a situation more quickly up the escalation rungs, crossing thresholds
that can lead to a strategic level of conflict. These escalatory
dynamics would be greatly amplified by the development and deployment of
AI-augmented tools functioning at machine speed. Military AI could
potentially push the pace of combat to a point where the actions of
machines surpass the cognitive and physical ability of human
decision-makers]{.underline} to control (or even fully understand)
future warfare. Thus, until experts can unravel some of the
unpredictable, brittle, inflexible, unexplainable features of AI, this
technology will continue to outpace strategy, and human error and
machine error will likely compound one another---with erratic and
unintended effects.

#### Autonomous nuclear command-and-control makes miscalc inevitable\-\--false alarms and automation bias

**Horowitz 19** \[Michael C. Horowitz (United States) is professor of
political science at the University of Pennsylvania and the associate
director of its Perry World House. May 2019 "Artificial intelligence and
nuclear stability" THE IMPACT OF ARTIFICIAL INTELLIGENCE ON STRATEGIC
STABILITY AND NUCLEAR RISK Volume I Euro-Atlantic Perspectives pp. 80-81
<https://www.sipri.org/sites/default/files/2019-05/sipri1905-ai-strategic-stability-nuclear-risk.pdf>\]
-os-

I. AI and nuclear command and control Excluding a first strike, [the
first step in the process leading up to the possible use of nuclear
weapons is how a nuclear-armed state attempts to detect whether another
country is launching nuclear weapons]{.underline} and how it responds.
Many countries already automate parts of their nuclear weapon
infrastructure, especially advanced nuclear powers such as the United
States.3 This includes early warning, command and control, and missile
targeting. [Advances in AI could lead to the **expansion of the use of
autonomous systems in command and control**.]{.underline} For example,
[states could decide to automate **additional components** of early
warning because autonomous systems can detect patterns and changes in
patterns **faster than humans**]{.underline}. This [could have potential
benefits for nuclear security]{.underline} and stability, because
well-functioning algorithms could give decision makers more time in a
complex environment. Moreover, autonomous systems could represent
another form of redundancy that helps to ensure the dissemination of
launch orders in the worst case. However, [the 1983 Petrov incident
illustrates a clear downside to **fully automating command and
control**]{.underline}. In this case, [the Soviet]{.underline} Oko
satellite-based [early-warning system reported a false
alarm]{.underline}---the launch of five US intercontinental ballistic
missiles (ICBMs). No missiles had been launched. Lieutenant Colonel
Stanislav [Petrov was the watch officer on duty]{.underline}. It was his
job to alert Soviet leadership of a US attack. While the automated
systems reported the 'highest' confidence that a missile strike was
occurring, Petrov stated that he 'had a funny feeling in \[his\] gut'.
[He instead reported a system malfunction, rather than a nuclear
strike]{.underline}.4 [The risk is that a future incident could lead to
escalation, instead of a malfunction report, for two reasons**. First,**
a decision to fully automate early warning would mean that **there was
no human operator**]{.underline}---no Petrov---[to prevent a false alarm
from escalating]{.underline}. To be fair, however, it seems unlikely
that a country would cut humans entirely out of the early-warning
process. [**Second, automation bias** could mean that a future Petrov
**trusts the algorithm** and instead reports that an attack is under
way]{.underline}.5 While also unlikely, **[academic research on
automation bias suggests that this is a real risk]{.underline}**.6

### Brittle AI Turn

#### The Pressure to deploy AI Rapidly makes accidents inevitable -- the technology is brittle

**Arnold and Toner, 2021** -- Center for Security and Emerging Threats
\[Zachary and Helen, July, CSET Policy Brief. "AI Accidents: An Emerging
Threat What Could Happen and What to Do" https://cset.
georgetown.edu/wp-content/uploads/CSET-AI-Accidents-An-Emerging-Threat.pdf
Acc 6/7/22 TA\]

Despite these problems, [AI systems are becoming integrated into the
real world at a pace that is only expected to accelerate in the next
decade]{.underline}.9 [These systems may be fragile, but as]{.underline}
companies, governments, and [militaries decide when and how to deploy
them, their huge potential benefits will often overshadow uncertain
risks]{.underline}. [Leaders]{.underline} in these organizations also
[may not be fully aware of these risks, and may face pressure from
competitors willing to move quickly.1]{.underline}0 To be sure, some
industries are already deploying AI much faster than others, and a few
sensitive sectors may remain "walled off" for some time.11 But
[eventually, the powerful incentives driving the spread of AI today are
likely to make it pervasive. As our]{.underline} economy,
[security]{.underline}, and health [become more and more dependent on AI
systems, these systems' fragilities will put lives at
stake.]{.underline} Today, many are worried about AI being misused
intentionally. An adversary could attack with swarms of drones;
authoritarian governments are already using AI algorithms to
discriminate on the basis of race or ideology. These risks are real, and
they deserve attention. But [unintended, accidental AI disasters are
also an urgent concern. AI-related accidents are already making
headline]{.underline}s, from inaccurate facial recognition systems
causing false arrests to unexpected racial and gender discrimination by
machine learning software.12 [This is especially striking since AI has
so far mostly been deployed in seemingly lower-stakes
settings]{.underline}, such as newsfeed rankings, ad targeting, and
speech recognition, with less deployment in higher stakes areas such as
autonomous driving. [Despite these initial accidents]{.underline},
governments, businesses, and [militaries are preparing to use today's
flawed, fragile AI technologies in critical systems around the world.
Future versions of AI technology may be less accident-prone, but there
is no guarantee---and regardless, if rollout continues as expected,
prior versions of the technology may already have been deployed at
massive scale. The machine learning models of 2020 could easily still be
in use decades in the future]{.underline}, just as airlines, stock
exchanges, and federal agencies still rely today on COBOL, a programming
language first deployed in 1960.13 In retrospect, even the most extreme
technological accidents, from the Challenger disaster to the meltdown at
Chernobyl, can seem both predictable and preventable.14 [History is full
of accidents that seem obvious in retrospect, but "no one could have
seen coming" at the time.]{.underline} In other cases, known risks are
brushed aside, or obvious fixes go unmade. [Unless we act, there is no
reason to think that the advent of AI will be any
different]{.underline}. In fact, there are reasons to think [AI could
cause more accidents than other technologies that have caused
high-profile disasters]{.underline}. Unlike the space shuttle or nuclear
power plants, for example, AI will be pervasive throughout society,
creating endless opportunities for things to go awry. What's more,
modern AI is so good at some tasks that even sophisticated users and
developers can come to trust it implicitly.15 This degree of trust,
placed in pervasive, fallible systems without any common sense, could
have terrible consequences.

### Leaks/Theft Turn

#### Any innovation would be stolen by Russia and China -- takes out advantage. 

**Christie '22** (Edward Hunter Christie; Researcher, consultant,
economist, EU affairs professional, former NATO official, public policy
expert; "Defence cooperation in artificial intelligence: Bridging the
transatlantic gap for a stronger Europe," Sage Journals, March 31 2022;
https://journals.sagepub.com/doi/full/10.1177/17816858221089372#)-amc

[Both EU nations and the US are exposed to the same global environment
and to similar strategic concerns]{.underline}, at the confluence of
**[rapid technological change and global power shifts]{.underline}**.
Starting from around 2018, policy discourse in the US became
particularly focused on fears of being overtaken by China
technologically and militarily. A good illustration of these fears is a
2020 statement by the Director of the Federal Bureau of Investigation,
who accused the [**Chinese government** of '**fighting**]{.underline} a
generational fight [**to surpass our country in economic and
technological leadership**' and of 'taking an all-tools and all-sectors
approach . . . that **demands our own all-tools and all-sectors approach
in response'**]{.underline} ([Wray
2020](https://journals.sagepub.com/doi/full/10.1177/17816858221089372)).
[For military AI, China poses the greatest challenge to Western
nations]{.underline} ([Kania
2019](https://journals.sagepub.com/doi/full/10.1177/17816858221089372)).
However, [**Russia is** also actively **pursuing such
capabilities**]{.underline} ([Zysk
2021](https://journals.sagepub.com/doi/full/10.1177/17816858221089372);
[Engvall
2021](https://journals.sagepub.com/doi/full/10.1177/17816858221089372)),
including **[through espionage]{.underline}**, for example [against the
Netherlands]{.underline} ([AIVD
2020](https://journals.sagepub.com/doi/full/10.1177/17816858221089372)[)
and France]{.underline} ([Follorou
2021](https://journals.sagepub.com/doi/full/10.1177/17816858221089372)).
Nations on both sides of the Atlantic have recognised [the rising
challenge of Chinese and Russian government-sponsored industrial
espionage **aimed at** the **illegitimate acquisition of** cutting-edge
**Western technologies**]{.underline}. And both the US and the EU have
adopted strengthened legislation in several key areas, including on the
protection of trade secrets, on export controls for dual-use items and
on the screening of foreign direct investment ([Christie
2021a](https://journals.sagepub.com/doi/full/10.1177/17816858221089372)).
Another relevant area of work is measures to better protect the
university and research sector from espionage. A new toolkit of
recommendations now exists at EU level ([European Commission
2022](https://journals.sagepub.com/doi/full/10.1177/17816858221089372)).

#### The US and allies need to reform methods of technology transfer to minimize Chinese threats and patch up vulnerabilities. 

**Imbrie et al. '20** (Andrew Imbrie, Senior Fellow at Georgetown\'s
Center for Security and Emerging Technology; Ryan Fedasiuk, Research
Analyst at Georgetown\'s Center for Security and Emerging Technology;
Catherine Aiken, Director of Data Science and Research at Georgetown\'s
Center for Security and Emerging Technology; Tarun Chhabra, nonresident
fellow with the Center for Security, Strategy, and Technology at the
Brookings Institution; Husanjot Chahal, Research Analyst at Georgetown
University\'s Center for Security and Emerging Technology; February
2022; "HOW THE UNITED STATES AND ITS ALLIES CAN DELIVER A DEMOCRATIC WAY
OF AI"; CSET;
<https://cset.georgetown.edu/publication/agile-alliances/)//akg>

The following 10 initiatives provide a roadmap for how the United States
and its allies can defend against threats, network to seize
opportunities, and project influence to safeguard democracy in the age
of AI. Initiative 1: Prevent the transfer of sensitive technical
information. The **[Chinese]{.underline}** government
[undertakes]{.underline} multiple, [coordinated **efforts to obtain
sensitive information from U.S. AI**]{.underline} researchers. Many of
these pathways and [**access points** for technology transfer **are
legal**]{.underline} or extralegal [and]{.underline} therefore [**poorly
understood** or monitored]{.underline} **[by Western
intelligence]{.underline}** agencies.28 Common vectors include
technology transfer centers and forums, copyright infringement, and
grant and funding opportunities for Chinese undergraduate, graduate, and
post-doctoral researchers to study abroad and collaborate with foreign
universities, research labs, and companies.29 [**International
partners** share U.S. **concerns about** the **transfer of sensitive
technology**]{.underline}: just over half of survey respondents
indicated that their [government has concerns about foreign talent
studying or working in fields with military or national security
applications]{.underline}, and a majority of officials (60 percent)
stated that their governments have policies in place to counter the
transfer of sensitive technologies.30 A third of respondents did not
know if their governments shared such concerns, indicating an
opportunity for U.S. leadership on this issue. The United States could
improve coordination with allies and partners to counter technology
transfer in several ways. Officials from each surveyed [**country**
indicated interest in **coordinating with the United States to prevent
the transfer of sensitive technology**]{.underline}. This initiative
received the second highest level of agreement, just after coordinated
AI norms and standards. Respondents from Japan, Australia, Italy, and
France were particularly interested in collaboration around tech
transfer policies. The [**United State**s]{.underline} should work with
its [**allies** and partners]{.underline} to build an [empirical base of
knowledge]{.underline} on this issue, supported by [**robust data
collection** and analysis]{.underline}. Survey results suggest that
allies believe **[international management]{.underline}** is [required
to **counter cyber exploitation**]{.underline}, with nearly 75 percent
of officials noting it as a trend requiring international coordination.
By [launching a **multilateral cyber defense initiative**, the United
States]{.underline} [and]{.underline} its [allies could **strengthen**
the **capabilities of small- to medium-sized enterprises at risk of
intellectual property theft and industrial espionage.**]{.underline}31

## 

## Cooperation Advantage Answers

### Democratic Norms Impossible 

#### Democratic AI cannot be created due to cultural differences and data bias. 

**Rainie and Anderson 21** (Lee Rainie is the director of internet and
technology research at Pew Research Center. Under his leadership, the
Center has issued more than 650 reports based on its surveys that
examine people's online activities and the internet's role in their
lives. Digital futures consultant and researcher and full professor of
emerging media and digital journalism at Elon University. Contract
researcher for the Pew Research Center\'s Internet Project since 2003.
Leader of Imagining the Internet, a massive research project revealing
people\'s expectations for the future of communications networks.)
"[EXPERTS DOUBT ETHICAL AI DESIGN WILL BE BROADLY ADOPTED AS THE NORM
WITHIN THE NEXT
DECADE](https://www.pewresearch.org/internet/2021/06/16/experts-doubt-ethical-ai-design-will-be-broadly-adopted-as-the-norm-within-the-next-decade/)"
JUNE 16, 2021
<https://www.pewresearch.org/internet/2021/06/16/1-worries-about-developments-in-ai/>
// ZX

[It would be quite difficult -- some might say impossible -- to design
broadly adopted ethical AI systems. A]{.underline} share of the experts
responding noted that ethics are hard to define, implement and enforce.
They said context matters when it comes to ethical considerations. [Any
attempt to fashion ethical rules generates countless varying scenarios
in which applications of those rules can be messy]{.underline}. The
nature and relative power of the actors in any given scenario also
matter. [Social standards and norms evolve and can become wholly
different as cultures change.]{.underline} Few people have much
education or training in ethics. Additionally, good and bad actors
exploit loopholes and gray areas where ethical rules aren't crisp, so
workarounds, patches or other remedies are often created with varying
levels of success. The experts who expressed worries also invoked
governance concerns[. They asked: Whose ethical systems should be
applied? Who gets to make that decision? Who has responsibility to care
about implementing ethical AI? Who might enforce ethical regimes once
they are established? How?]{.underline} A large number of respondents
argued that geopolitical and economic competition are the main drivers
for AI developers, while moral concerns take a back seat. [A share of
these experts said creators of AI tools work in groups that have little
or no incentive to design systems that address ethical
concerns]{.underline}. Some respondents noted that, even if workable
ethics requirements might be established, they could not be applied or
governed because most AI design is proprietary, hidden and complex[. How
can harmful AI "outcomes" be diagnosed and addressed if the basis for AI
"decisions" cannot be discerned]{.underline}? Some of these experts also
note that existing AI systems and databases are often used to build new
AI applications[. That means the biases and ethically troubling aspects
of current systems are being designed into the new systems. They say
diagnosing and unwinding the pre-existing problems may be difficult if
not impossible to achieve]{.underline}. A portion of these experts
infused their answers with questions that amount to this overarching
question: [How can ethical standards be defined and applied for a
global, cross-cultural, ever-evolving, ever-expanding universe of
diverse black-box systems in which bad actors and misinformation
thrive]{.underline}? A selection of respondents' comments on this broad
topic is organized over the next 20 pages under these subheadings: 1) It
can be hard to agree as to what constitutes ethical behavior. 2) Humans
are the problem: Whose ethics? Who decides? Who cares? Who enforces? 3)
Like all tools, AI can be used for good or ill, which makes
standards-setting a challenge. 4) Further AI evolution itself raises
questions and complications. Stephen Downes, senior research officer for
digital technologies with the National Research Council of Canada,
observed, "[The problem with the application of ethical principles to
artificial intelligence is that there is no common agreement about what
those are.]{.underline} While it is common to assume there is some sort
of unanimity about ethical principles, this unanimity is rarely broader
than a single culture, profession or social group. This is made manifest
by the ease with which we perpetuate unfairness, injustice and even
violence and death to other people. No nation is immune. ["Ultimately,
our AI will be an extension of ourselves, and the ethics of our AI will
be an extension of our own ethics**. To the extent that we can build a
more ethical society, whatever that means, we will build more ethical
AI, even if only by providing our AI with the models and examples it
needs in order to be able to distinguish right from
wrong**]{.underline}. I am hopeful that the magnification of the ethical
consequences of our actions may lead us to be more mindful of them; I am
fearful that they may not." "Assuming that we could effectively regulate
it, we face another major hurdle: What do we mean by 'ethical?' Putting
aside philosophical debates, we face practical problems in defining
ethical AI. We do not have to look far to see similar challenges.
[During the past few years, what is or is not ethical behavior in U.S.
politics has been up for debate.]{.underline} Other countries have faced
similar problems. "Even if we could decide on a definition \[for
ethics\] in the U.S., it would likely vary from the definitions used in
other countries. [Given AI's ability to fluidly cross borders,
regulating AI would prove troublesome. We also will find that ethical
constraints may be at odds with other self-interests]{.underline}.
Situational ethics could easily arise when we face military or
intelligence threats, economic competitive threats, and even political
threats. An architect of practice specializing in AI for a major global
technology company said, "The European Union has the most concrete
proposals, and I believe we will see their legislation in place within
three years. My hope is that we will see a ripple effect in the U.S.
like we did from GDPR -- global companies had to comply with GDPR, so
some good actions happened in the U.S. as a result. ... [We may be more
likely to see a continuation of individual cities and states imposing
their own application-specific laws]{.underline} (e.g.,
facial-recognition technology limits in Oakland, Boston, etc.). The
reasons I am doubtful that the majority of AI apps will be
ethical/benefit the social good are: [Even the EU's proposals are
limited in what they will require; China will never limit AI for social
benefit over the government's benefit]{.underline}; The ability to
create a collection of oversight organizations with the budget to audit
and truly punish offenders is unlikely. A researcher in bioinformatics
and computational biology observed, ["Take into account the actions of
the CCP]{.underline} \[Chinese Communist Party\] in China. [They have
been leading the way recently in demonstrating how these tools can be
used in unethical ways]{.underline}. **[And the United States has failed
to make strong commitments to ethics in AI, unlike EU
nations]{.underline}**. AI and the ethics surrounding its use could be
one of the major ideological platforms for the incoming next Cold War. I
am most concerned about the use of AI to further invade privacy and
erode trust in institutions. I also worry about its use to shape policy
in nontransparent, noninterpretable and nonreproducible ways. **[There
is also the risk that some of the large datasets that are the
fundamental to a lot of decision-making]{.underline}** -- from facial
recognition, to criminal sentencing, to loan applications -- **[being
conducted using AI that are critically biased]{.underline}** and will
continue to produce biased outcomes if they are used without undergoing
severe audits -- issues with transparency compound these problems.
[Advances to medical treatment using AI run the risk of not being fairly
distributed as well."]{.underline} Gus Hosein, executive director of
Privacy International, observed, "[Unless AI becomes a competition
problem and gets dominated by huge American and Chinese companies, then
the chances of ethical AI are low, which is a horrible
reality]{.underline}. If it becomes widespread in deployment, as we've
seen with facial recognition, then the only way to stem its deployment
in unethical ways is to come up with clear bans and forced transparency.
This is why AI is so challenging. Equally, it's quite pointless, but
that won't stop us from trying to deploy it everywhere. The underlying
data quality and societal issues mean that AI will just punish people in
new, different and the same ways. If we continue to be obsessed with
innovators and innovation rather than social infrastructure, then we are
screwed."
<https://www.pewresearch.org/internet/2021/06/16/2-hopes-about-developments-in-ethical-ai/>

### Regulations Fail 

#### Multilateral regulation on autonomous weapons fails -- allies and adversaries proliferate and no compliance

**Anderson and Waxman 13** \[Ken Anderson, Professor of Law at AUWCL, is
a leading international and national security law scholar who has
recently been focusing extensively on the regulation of emerging
technologies, especially automation, robotics, and AI. Matthew C. Waxman
is a nationally known authority on national security law, cybersecurity,
terrorism, intelligence, and armed conflict. He brings the perspective
of a former senior government official to his scholarship on war powers,
the regulation of military technology, counterterrorism, surveillance,
and cybersecurity. "Law and Ethics for Autonomous Weapon Systems: Why a
Ban Won\'t Work and How the Laws of War Can," 4/10/13,
https://scholarship.law.columbia.edu/cgi/viewcontent.cgi?article=2804&context=faculty_scholarship\]//PJ

In any case, [ambitions for a **multilateral treaty regulating** or
prohibiting **autonomous weapon systems are misguided**]{.underline} for
several reasons. For starters, [limitations on autonomous military
technologies,]{.underline} although quite likely to find wide
superficial acceptance among some states and some non-governmental
groups and actors, [will have **little traction** among those most
likely to develop and use them]{.underline}. [Some states may want the
United States to be more aggressive in adopting the latest
technologies]{.underline}, [given that possible **adversaries are likely
to have far fewer compunctions about their own autonomous weapon
systems**]{.underline}, and others are likely to favor any technological
development that extends the reach and impact of U.S. and [allied forces
or enhances their own ability to counter adversaries'
capabilities]{.underline}. [Even states and groups inclined to support
treaty prohibitions or limitations will find it difficult to reach
agreement on scope or definitions because lethal autonomy will be
introduced incrementally]{.underline}---as battlefield machines become
smarter and faster, and the real-time human role in controlling them
gradually recedes, [agreeing on what constitutes a prohibited autonomous
weapon will be unattainable]{.underline}. Even assuming agreement could
be reached, [there are the general **challenges of
compliance**]{.underline}: the [collective action problems of failure
and defection that afflict all such treaty regimes, especially when
dealing with dual-use (civilian and military) underlying
technologies.]{.underline} Finally, there are serious humanitarian risks
to prohibition, given the possibility that autonomous weapons systems
could in the long run be more discriminating and ethically preferable to
alternatives[. If all such systems are prohibited, and particularly if
even research and development]{.underline} of relevant technologies is
also prohibited, [one never gets the benefits that might come from new
technologies]{.underline}--- and future generations will not even be
aware of the potential benefits that were given up, because these
prohibitions on development meant they were never even pursued.
[Prohibition precludes the possibility of such benefits, and proponents
of it must acknowledge and bear responsibility for this
risk.]{.underline}

### No Democracy Impact

#### Democracy is [resilient]{.underline}, but it solves [nothing]{.underline}.

**Doorenspleet 19** Renske Doorenspleet, Politics Professor at the
University of Warwick. \[Rethinking the Value of Democracy: A
Comparative Perspective, Palgrave Macmillan, p. 239-243\]

[The value of democracy has been **taken for granted**]{.underline}
until recently, but this assumption seems to be under threat now more
than ever before. As was explained in Chapter 1, democracy's claim to be
valuable does not rest on just one particular merit, and scholars tend
to distinguish three different types of values (Sen 1999). This book
focused on the instrumental value of democracy (and hence not on the
intrinsic and constructive value), and investigated the value of
democracy for peace (Chapters 3 and 4), control of corruption (Chapter
5) and economic development (Chapter 6). [This study was based on a
search of **an enormous academic database**]{.underline} for certain
keywords,6 [then pruned **the thousands of articles**]{.underline} down
to a few hundred articles (see Appendix) [which **statistically
analysed** the connection between the democracy and the four expected
outcomes]{.underline}. The frst fiding is that [[**a reverse wave away
from democracy** has not happened]{.underline}]{.mark} (see Chapter 2).
Not yet, at least. [Democracy is not doing worse than
before]{.underline}, at least not [in comparative
perspective]{.underline}. While it is true that there is a dramatic
decline in democracy in some countries,7 [[a general trend downwards
cannot yet be detected]{.underline}]{.mark}. It would be better to talk
about 'stagnation', as not many dictatorships have democratized
recently, while democracies have not yet collapsed. Another fnding is
that the instrumental value of democracy is very questionable. The feld
has been deeply polarized between researchers who endorse a link between
democracy and positive outcomes, and those who reject this optimistic
idea and instead emphasize the negative effects of democracy. [There has
been **'[no consensus']{.mark}** [in **the quantitative literature** on
whether democracy has]{.mark}]{.underline} instrumental value which
leads some [**[beneficial general outcomes]{.mark}**. Some scholars
claim there is a consensus, but they only do so by ignoring **a huge
amount of literature** which rejects their own point of
view]{.underline}. After undertaking a large-scale analysis of carefully
selected articles published on the topic (see Appendix), [this book can
conclude that the connections between democracy and expected benefts are
**not**]{.underline} as **[strong]{.underline}** as they seem. Hence, we
should not overstate the links between the phenomena. [[**The overall
evidence** is **weak**]{.underline}]{.mark}. Take the expected impact of
democracy on peace for example. As Chapter 3 showed, the study of
democracy and interstate war has been a fourishing theme in political
science, particularly since the 1970s. However, there are four reasons
why democracy does not cause peace between countries, and why the
empirical support for the popular idea of democratic peace is quite
weak. [**Most statistical studies** have not found **a strong
correlation** between democracy and **interstate war** at **the dyadic
level**]{.underline}. They show that [[there are other---**more
powerful**---explanations for war]{.underline}]{.mark} and peace, and
even that [the impact of democracy is **a spurious one**]{.underline}
(caveat 1). Moreover, [[the theoretical foundation of the democratic
peace hypothesis is **weak**]{.underline}]{.mark}, [and [the causal
mechanisms are **unclear**]{.underline}]{.mark} (caveat 2). In addition,
[democracies are not]{.underline} necessarily **[more peaceful in
general]{.underline}**, and [the evidence]{.underline} for the
democratic peace hypothesis [at **the monadic level** is
**inconclusive**]{.underline} (caveat 3). Finally, the process of
[democratization is **dangerous**]{.underline}. Living in a
democratizing country means living in a less peaceful country (caveat
4). With regard to peace between countries, we cannot defend the idea
that democracy has instrumental value. [Can the]{.underline}
(instrumental) [value]{.underline} of democracy [be found in the
prevention of civil war?]{.underline} Or is the evidence for the
opposite idea more convincing, and does democracy have a 'dark side'
which makes civil war more likely? The findings are confusing, which is
exacerbated by the fact that different aspects of civil war (prevalence,
onset, duration and severity) are mixed up in some civil war studies.
Moreover, defining civil war is a delicate, politically sensitive issue.
Determining whether there is a civil war in a particular country is
incredibly diffcult, while measurements suffer from many weaknesses
(caveat 1). Moreover, [there is **no linear link**: civil wars are
**just as unlikely** in democracies as in dictatorships]{.underline}
(caveat 2). Civil war is most likely in times of political change.
[Democratization is **a very unpredictable**, **dangerous process**,
increasing the chance of civil war significantly]{.underline}. Hybrid
systems are at risk as well: the chance of civil war is much higher
compared to other political systems (caveat 3). More specifcally, both
the strength and type of political institutions matter when explaining
civil war. However, [the type of political system (e.g. democracy or
dictatorship) is **not the decisive factor at all**]{.underline} (caveat
4). Finally, democracy has only limited explanatory power (caveat 5).
[Economic factors are far more significant than political
factors]{.underline} (such as having a democratic system) when
explaining the onset, duration and severity of civil war. To prevent
civil war, it would make more sense to make poorer countries richer,
instead of promoting democracy. [Helping countries]{.underline} to
[democratize would **even be a very dangerous idea**]{.underline}, as
countries with changing levels of democracy are most vulnerable, making
civil wars most likely. It is true that there is evidence that the
chance of civil war decreases when the extent of democracy increases
considerably. The problem however is that most countries do not go
through big political changes but through small changes instead; those
small steps---away or towards more democracy---are dangerous. Not only
is the onset of civil war likely under such circumstances, but civil
wars also tend to be longer, and the confict is more cruel leading to
more victims, destruction and killings (see Chapter 4). A more
encouraging story can be told around the value for democracy to control
corruption in a country (see Chapter 5). Fighting corruption has been
high on the agenda of international organizations such as the World Bank
and the IMF. Moreover, the theme of corruption has been studied
thoroughly in many different academic disciplines---mainly in economics,
but also in sociology, political science and law. Democracy has often
been suggested as one of the remedies when fghting against high levels
of continuous corruption. So far, the statistical evidence has strongly
supported this idea. As Chapter 5 showed, dozens of studies with broad
quantitative, cross-national and comparative research have found
statistically signifcant associations between (less) democracy and
(more) corruption. However, there are vast problems around
conceptualization (caveat 1) and measurement (caveat 2) of 'corruption'.
Another caveat is that [democratizing countries are **the poorest
performers** with]{.underline} regard to [controlling
corruption]{.underline} (caveat 3). Moreover, it is not democracy in
general, but particular political institutions which have an impact on
the control of corruption; and a free press also helps a lot in order to
limit corruptive practices in a country (caveat 4). In addition,
democracies seem to be less affected by corruption than dictatorships,
but at the same time, there is clear evidence that economic factors have
more explanatory power (caveat 5). In conclusion, more democracy means
less corruption, but we need to be modest (as other factors matter more)
and cautious (as there are many caveats). The perceived impact of
democracy on development has been highly contested as well (see Chapter
6). Some scholars argue that democratic systems have a positive impact,
while others argue that high levels of democracy actually reduce the
levels of economic growth and development. Particularly since the 1990s,
statistical studies have focused on this debate, and [the empirical
evidence is **clear**: [there is **no direct impact** of
democracy]{.mark} on development]{.underline}. Hence, both approaches
cannot be supported (see caveat 1). The indirect impact via other
factors is also questionable (caveat 2). Moreover, [[there is **too much
variation**]{.mark} in]{.underline} levels of economic
[growth]{.underline} and development [among the dictatorial
systems]{.underline}, and there are huge regional differences (caveat
3). Adopting a one-size-ftsall approach would not be wise at all. In
addition, in order to increase development, it would be better to focus
on alternative factors such as improving institutional quality and good
governance (caveat 4). There is not suffcient evidence to state that
democracy has instrumental value, at least not with regard to economic
growth. However, future research needs to include broader concepts and
measurements of development in their models, as so far studies have
mainly focused on explaining cross-national differences in growth of GDP
(caveat 5). Overall, [[**the instrumental value of democracy**
is]{.underline}]{.mark}---at best---tentative, or---if being less
mild---[**[simply non-existent]{.mark}**. Democracy is **not necessarily
better** than **any alternative form of government**. With]{.underline}
regard to [many]{.underline} of the expected [benefts---such as less
war, less corruption and more economic development---democracy **does
deliver, but so do nondemocratic systems**]{.underline}. High or low
levels of democracy do not make a distinctive difference. Mid-range
democracy levels do matter though. Hybrid systems can be associated with
many negative outcomes, while this is also the case for democratizing
countries. Moreover, other explanations---typically certain favourable
economic factors in a country---are much more powerful to explain the
expected benefts, at least compared to the single fact that a country is
a democracy or not. The impact of democracy fades away in the powerful
shadows of the economic factors.8

#### Democracy doesn't solve war \-\-- increases hostility. 

**Ghatak et al. 17**---Sam Ghatak is a Lecturer in Political Science at
the University of Tennessee Knoxville; Aaron Gold is a PhD Student in
Political Science at UT Knoxville; Brandon C. Prins is a Professor and
Director of Graduate Studies of Political Science at UT Knoxville
\["External threat and the limits of democratic pacifism," *Conflict
Management and Peace Science*, Vol. 34, No. 2, p. 141-159, Emory
Libraries\]

[Conclusion]{.underline}

[It has become a **stylized fact** that dyadic democracy lowers the
hazard of armed conflict]{.underline}. While [the Democratic Peace has
faced many challenges]{.underline}, we believe [the most significant
challenge has come from the argument that [the pacifying effect of
democracy is **epiphenomenal to territorial issues**]{.mark},
specifically the external threats that they pose]{.underline}. [This
argument sees the lower hazards of armed conflict among democracies
**not** as a product of shared norms or institutional structures, but as
a **result of settled borders**]{.underline}. Territory, though, remains
only one geo-political context generating threat, insecurity, and a
higher likelihood of armed conflict. Strategic rivalry also serves as an
environment associated with fear, a lack of trust, and an expectation of
future conflict. [[Efforts to assess democratic pacifism]{.mark} have
largely **[ignored rivalry]{.mark}** as a context conditioning the
behavior of democratic leaders]{.underline}. To be sure, research
demonstrates rivals to have higher probabilities of armed conflict and
democracies rarely to be rivals. But [fundamental to the Democratic
Peace is the notion that even in the face of difficult security
challenges and salient issues, dyadic democracy will associate with a
lower likelihood of militarized aggression. But the presence of an
**external threat**, be that threat disputed territory or strategic
rivalry, may be the key mechanism by which [democratic leaders]{.mark},
owing to **audience costs**, **resolve** and **electoral pressures**,
**[fail to resolve problems nonviolently]{.mark}**]{.underline}.

[This study has sought a ''hard test'' of the Democratic Peace by
testing the conditional effects of joint democracy on armed conflict
when external threat is present]{.underline}. We test three measures of
threat: territorial contention, strategic rivalry, and a threat index
that sums the first two measures. [For robustness checks, we use two
additional measures of our dependent variable: fatal MID onset, and
event data from the Armed Conflict Database]{.underline}, which can be
found in our Online Appendix. As most studies report, democratic dyads
are associated with less armed conflict than mixed-regime and autocratic
dyads. In every one of our models, when we control for each measure of
external threat, joint democracy is strongly negative and significant
and each measure of threat is strongly positive and significant. Here,
liberal institutions maintain their pacific ability and [external
threats clearly increase conflict propensities]{.underline}. However,
[[when we test the **interactive relationship** between democracy
and]{.mark} our measures of [external threat, the pacifying
effect]{.mark} of democracy [is **less visible**]{.mark}]{.underline}.
Park and James (2015) find some evidence that when faced with an
external threat in the form of territorial contention, the pacifying
effect of joint democracy holds up. This study does not fully support
the claims of Park and James (2015). [Using a longer timeframe, [we
find]{.mark} more [**consistent evidence** that]{.mark} when faced with
an external threat, be it territorial contention, strategic rivalry, or
a combination, **democratic [pacifism does not
survive]{.mark}**]{.underline}. What are the implications of our study?
First, while it is clear that we do not observe a large amount of armed
conflict among democratic states, if we organize interstate
relationships along a continuum from highly hostile to highly friendly,
we are probably observing what Goertz et al. (2016) and Owsiak et al.
(2016) refer to as ''lesser rivalries'' in which ''both the frequency
and severity of violent interaction decline. Yet, the sentiments of
threat, enmity, and competition that remain---along with the persistence
of unresolved issues---mean that lesser rivalries still experience
isolated violent episodes (e.g., militarized interstate disputes),
diplomatic hostility, and non-violent crises'' (Owsiak et al., 2016).
Second, our findings show that [[the pacific benefits of **liberal
institutions** or]{.mark} externalized [**norms** are **not**]{.mark}
always [able to lower the likelihood of]{.mark} armed [conflict]{.mark}
when faced with external threats, whether those hazards are disputed
territory, strategic rivalry, or a combination of the two]{.underline}.
The structural environment clearly influences democratic leaders in
their foreign policy actions more than has heretofore been appreciated.
[[**Audience costs**, **resolve**, and **electoral pressures**]{.mark},
produced from external threats, [are **powerful forces**]{.mark} that
are [present even in jointly democratic
relationships]{.mark}]{.underline}. [[These]{.mark} forces [make it
difficult for leaders to **trust one another**, which **inhibits
conflict resolution** and facilitates persistent
**hostility**]{.mark}]{.underline}. [It does appear]{.underline}, then,
[that there is a **limit** to the Democratic Peace]{.underline}.

### Democracy Bad Turn -- Disease 

#### Democracy makes [disease control]{.underline} impossible

Zhifa **Zhou 21**, Associate Professor at the Institute of African
Studies at Zhejiang Normal University and Pan Qu, Postgraduate at the
Institute of African Studies at Zhejiang Normal University, "The Root
Cause of the Failure of American COVID-19 Governance Based on the
Criticism of Liberal Democracy From Error-Tolerant Democracy",
Philosophy Study, Volume 11, Number 7, July 2021,
https://www.davidpublisher.com/Public/uploads/Contribute/60ff9cfb4589c.pdf

Introduction

Whether [liberal **democracy** contributed to]{.underline} the
**[COVID]{.underline}**-19 [governance]{.underline} was a hot topic in
2020 ("Democracy and Rise of Authoritarianism in COVID-19 World", 2020).
At the end of January, 2020, [when **COVID**]{.underline}-19 [witnessed
the lockdown of Wuhan]{.underline} City, [the West]{.underline}
generally [agreed that China lacked **freedom**]{.underline} of speech
and the inertia of a rigid bureaucratic structure, and the national
censorship system kept the whistle blower Dr. Wenliang Li silent, which
led to the disease out of control (Mérieau, 2020). Democracies'
confidence mainly came from Amartya Sen's research on the famine.
[Sen]{.underline} (1999) has [claimed]{.underline} that no substantial
famine has ever occurred in any independent and democratic country with
a relatively free press and there is no exception to this rule. Citizens
in [democracies]{.underline} can [expect governments to be]{.underline}
more [**candid**, transparent, and **responsible**]{.underline} in
dealing with all kinds of crises, which authoritarian countries usually
cannot (Berengaut, 2020; Bollyky & Kickbusch, 2020). So Steve
[Bloomfield]{.underline} (2020) has [regarded that if China had a free
press and transparent government, the pandemic could be brought under
control before the outbreak. In conclusion,]{.underline} freedom plus
[democracy equals the **COVID**]{.underline}-19
**[antidote]{.underline}** according to Western standards, although
Wilson and Wisongye have found that social media rumors can exploit the
right to freedom of speech and erode people's health benefits (New York
Times, 2021; Bollyky & Kickbusch, 2020). [However]{.underline}, since
March, 2020, [with Western democracies]{.underline} seriously [affected
by **COVID**]{.underline}-19, [their superiority of the political system
has begun to **expose** its untrue and **fatal defects**]{.underline}.
Especially when Wuhan began to lift its blockade on April 8, 2020
(People.cn, 2020), [scholars]{.underline} and journalists [began to
question whether democracies had the **ability** to **deal with the
crisis**]{.underline} better than China (Mérieau, 2020). [Liberal
democracy]{.underline} in the United States [has not proved]{.underline}
that it is [more conducive]{.underline} to the COVID-19 governance [than
authoritarianism]{.underline} since 2020. [From a global perspective,
not only do **most democracies fail to contain**]{.underline} the
**[spread]{.underline}** of COVID-19, but almost [**all of the 10 most
affected** countries are **liberal democracies**]{.underline}
(Coronavirus Resource Center, 2021). [Their **policy responses** have a
**poor effect** in reducing the **death toll** in **early stages** of
the crisis, as shown that democratic political institutions may be at a
**disadvantage** in **responding quickly**]{.underline} to COVID-19
(Cepaluni, Dorsch, & Branyiczki, 2020). More surprising is that the
COVID-19 pandemic is so serious in the United States, yet no government
officials have been removed from office because of their inactivity in
fighting against the corona-virus. People doubt whether American
accountability mechanism is still working. However, two impeachments
against President Trump indicate that it seems to function quite well
(Valenta & Valenta, 2017; Herb, Raju, Fox, & Mattingly, 2021). The
direct loss to the United States caused by Russiagate and incitement of
insurrection is far less than the pain caused by the failure of the
COVID-19 governance, but no any official in the United States is
responsible for it. [If it **again** faces infectious diseases
**similar** to **COVID**]{.underline}-19, [will it **repeat** this
**unprecedented tragedy**]{.underline}? Can liberal democracy and the
separation and balance of powers push American president to act more
aggressively? [Error-tolerantism explains that the fundamental reason
for the failure of]{.underline} American [COVID]{.underline}-19
[governance is a **serious misunderstanding** of the concept of
**freedom**]{.underline} (Zhou, 2018; 2019; Zhou, Tan, & Liu, 2020).
Liberalism has witnessed a rare scene: In the context of COVID-19, the
president, governors, magistrates, and the public (Emery, Schwebke, &
Park, 2020; Sullum, 2020; Behrmann, 2020; Kenton, 2020; Strano, 2020)
have severe misunderstanding of freedom [that cost **more
than**]{.underline} American **[600,000 lives]{.underline}**
(Coronavirus Resource Center, 2021). In response to the above
phenomenon, error-tolerantism as the development of liberalism defines
liberty from a new perspective and shows a stronger explanatory power
than liberalism (Zhou et al., 2020). The right paradigm of
error-tolerantism, the right to be wrong (right to trial and error) as
an original right and mutual empowerment theory, instead of natural
rights theory and social contract theory, divides liberty into the right
to liberty in innovative fields, right to be wrong as an original right,
and the right to be right in non-innovative fields as sub-rights. The
lockdown of Wuhan means that Chinese government has excised the power to
be wrong as an original power, but the West criticized it with the right
to liberty at the level of sub-rights, which is the first error in
understanding liberty during American COVID-19 governance; after Wuhan
effectively controlled COVID-19, its governance has transformed from an
innovative field to a non-innovative one. Then, liberties in
non-innovative fields as the sub-rights level, such as wearing face
masks, keeping social distancing, showing health codes, are formed
definitely (Zhou et al., 2020). However, [**wearing masks** has been
regarded as a sign of **political oppression** rather than a simple
hygienic measure by the **U**]{.underline}nited **[S]{.underline}**tates
(Kahanel, 2021). Since liberalism has a major misunderstanding of the
concept of liberty, [liberal democracy]{.underline} based on the
philosophy of liberalism [should be]{.underline} deeply reflected or
even **[reconstructed]{.underline}**, and it is very reasonable for
error-tolerant democracy constructed based on error-tolerantism [to
explore the defects]{.underline} of liberal democracy [in]{.underline}
American [COVID]{.underline}-19 [governance]{.underline}. Therefore, we
first review scholars' relevant research on American democracy and the
COVID-19 governance, and then based on the theory of error-tolerant
democracy, discuss [the **defects** of]{.underline} liberal
**[democracy]{.underline}** and American political system that [are
**unable to cope** with the **crisis of the century**]{.underline}.

#### Future pandemics are [inevitable]{.underline}\-\--extinction

Dr. Matt **Boyd 21**, Research Director at Adapt Research Ltd, PhD in
Philosophy of Evolution & Cognition from the Victoria University of
Wellington, BA from Massey University, and Nick Wilson, Research
Professor in the Department of Public Health at the University of Otago,
"Optimizing Island Refuges Against global Catastrophic and Existential
Biological Threats: Priorities and Preparations", Risk Analysis: An
International Journal, Wiley Online Library

1 INTRODUCTION [Our world is vulnerable to global catastrophic
risks]{.underline} (GCRs) [or existential risks]{.underline} (Bostrom,
2019; Ord, 2020). [**GCRs** are so disastrous because they affect one or
more systems **critical to humanity**, and **spread** to affect the
**entire planet**]{.underline} (Avin et al., 2018). Existential risks
threaten to eliminate humanity or permanently curtail its potential
(Ord, 2020). Some of these [risks are **natural**, for
example]{.underline} asteroid or comet impact, supervolcanic eruption,
[naturally occurring **pandemic**]{.underline}, or various cosmic events
(Bostrom & Cirkovic, 2008; Ord, 2020). Many others are the result of
human activities, for example nuclear war, anthropogenic climate change,
nonaligned artificial intelligence, engineered biological threats,
geoengineering, or inescapable totalitarianism (Bostrom & Cirkovic,
2008; Ord, 2020). There are three phases to an existential catastrophe:
origin, scale up, and reaching every last human (Cotton-Barratt, Daniel,
& Sandberg, 2020). Following any near miss, there would be a period
where recovery of humanity\'s long-term potential may or may not be
realized (Baum et al., 2019). [Failure to]{.underline} anticipate or
[**mitigate** these threats risks **undesirable trajectories** for
**human civilization**]{.underline} (Baum et al., 2019). In addition to
the present generation\'s obvious self-interest in continuing to exist,
the perspective of long-termism suggests that humanity ought to mitigate
these risks due to the potential immense value of future human
generations (Beckstead, 2013), a desire to see aspects of the human
project continue across time and perhaps the universe (Bostrom, 2003;
Scheffler, 2013), and the potential cosmic significance of preserving
intelligent life on Earth (Ord, 2020). A number of philosophical
defenses of long-termism have been published (Beckstead, 2013; Greaves &
MacAskill, 2019). Importantly, these long-term outcomes are largely
under human control because most of the risk is probably anthropogenic
(Beard & Torres, 2020; Ord, 2020). 1.1 Mitigating Existential Threats It
is too simplistic to think of existential risks as mere causes that are
followed by a sequence of effects. We should think of risks as the
product of hazards, vulnerabilities, and exposures (Liu, Lauta, & Maas,
2018). [Hazards are the **precipitating cause** of a
**catastrophe**]{.underline}, vulnerabilities are the inability of
critical systems to withstand hazards, and exposures are the features of
human society that turn this system damage into harm to populations
(Beard & Torres, 2020). [Mitigation of existential threats involves
preventing their emergence, **responding** if the threat spreads, and
building **resilience** so the threat does **not** lead to the death of
**every last human** or leave humanity with **permanently curtailed**
prospects]{.underline} (Cotton-Barratt et al., 2020). After a threat has
passed, there may also be a series of limiters that might prevent the
reemergence of a flourishing humanity (Baum et al., 2019). One such
limiting factor could be the loss of technological society and know-how.
In order to achieve immunity from existential threat, humanity will need
a period where it preserves its potential and protects itself from risks
(Ord, 2020). Various methods have been proposed to address
vulnerabilities and hence shift the probability of existential risk.
These suggestions include: improved international focus, governance, and
cooperation such as through the United Nations (Boyd & Wilson, 2020),
imitating existing frameworks such as the Sendai framework for disaster
risk reduction (Avin et al., 2018), achieving the United Nations
Sustainable Development Goals (Cernev & Fenner, 2020), or extreme
surveillance for threats (Bostrom, 2019). Toby Ord lists 38 specific
measures across eight existential threats, and an additional 12 avenues
to explore that address risks in general terms (Ord, 2020). 1.2
Biological Threats [**Pandemic viruses** with **high case fatality**
could potentially infect a **majority** of the population. Deliberate
biological events (DBEs) have occurred before]{.underline} (Millet &
Snyder-Beattie, 2017a), [will **likely occur again**, and could pose a
**threat** to humans as great as **nuclear war**]{.underline} (Kosal,
2020). [New **tech**]{.underline}nologies [such as
**a**]{.underline}rtificial **[i]{.underline}**ntelligence [could
**amplify biothreats** in a number of ways]{.underline} (O\'Brien &
Nelson, 2020). [These risks are **increased** because the]{.underline}
Biological Weapons Convention [(BWC) has no verification
system]{.underline} (Dando, 2016), [and has been violated in the
past]{.underline} (Gronvall, 2018). [It would **only** take **one**
unanticipated or **accidental event** for a **bioweapon** (or
**lab**]{.underline}oratory [accident) to **be**come a **catastrophic
threat**]{.underline}. The U.S. National Academies of Sciences
specifically warns against synthetic biology and xenobiology
(Gomez-Tatay & Hernandez-Andreu, 2019) and it is argued that [a
state-sponsored bioweapon attack is the **greatest current
threat**]{.underline} (Sandberg & Nelson, 2020). See the Supporting
Information for further details on biological threats. Global
preparedness through the One Health approach, global health security
projects, and the need to integrate health and the GCR field (Millet &
Snyder-Beattie, 2017b) are important. But as the COVID-19 pandemic has
shown, there may be important overlooked aspects or misunderstood risks
that could make any suite of general preparation inadequate. Therefore,
last lines of defense may be required, such as refuges.

#### 

## AI Bad

### AI Hurts Alliances

#### AI hampers military operations -- compressing negotiation timelines and hurting alliances

**Greenberg '20** (Erik Lin-Greenberg, , "Article Title,"
Journal/Magazine/etc, "Allies and artificial intelligence: Obstacles to
operations and decision making", Texas National Security Review,
3(2), 56--76, March 05 2020,
https://tnsr.org/2020/03/allies-and-artificial-intelligence-obstacles-to-operations-and-decision-making/)-amc

Drawing from theories of alliance politics and analysis of emerging AI
technologies, I map out two areas where [AI could hamper multinational
military operations]{.underline}. First, AI could pose challenges to
operational coordination by [complicating burden-sharing and the
interoperability of multinational forces]{.underline}. Not all alliance
or coalition members will possess AI capabilities, [raising barriers to
military cooperation as AI-enabled warfare becomes increasingly
commo]{.underline}n. States with [AI technologies will]{.underline} also
[need to overcome **political barriers** to sharing]{.underline} the
[sensitive data required to develop and operate AI-enabled
systems]{.underline}. At the same time, rivals can stymie multinational
coordination by using AI to launch deception campaigns aimed at
interfering with an alliance's military command-and-control processes.
Second, [AI could hamper alliance and coalition decision-making by
straining]{.underline} the processes and relationships [that undergird
decisions on the use of force]{.underline}. By increasing the speed of
warfare, [AI could decrease the time leaders, from the tactical to
strategic levels, have to debate policies and make
decisions]{.underline}. These [**compressed timelines may not allow
for** the **complex negotiations** and compromises that are **defining
characteristics of alliance
politics**]{.underline}.[10](https://tnsr.org/2020/03/allies-and-artificial-intelligence-obstacles-to-operations-and-decision-making/#_ftn10)
[Decision-making may be further hampered if]{.underline} the "black box"
and [unexplainable nature of AI causes leaders to lack confidence in
AI-enabled systems]{.underline}. And, just as [adversaries could use AI
to]{.underline} interfere with command and control, they could also use
AI to [launch misinformation campaigns that sow discord among
allies]{.underline} and heighten fears that allies will renege on their
commitments.

### AI Causes Miscalc

#### AI systems operate in an opaque "black box" -- they're hard to understand and correct, contributing to the fog of war

**Greenberg '20** (Erik Lin-Greenberg, , "Article Title,"
Journal/Magazine/etc, "Allies and artificial intelligence: Obstacles to
operations and decision making", Texas National Security Review,
3(2), 56--76, March 05 2020,
<https://tnsr.org/2020/03/allies-and-artificial-intelligence-obstacles-to-operations-and-decision-making/)-amc>

[AI can also strain alliance decision-making by fueling uncertainty
about information and military actions. Unlike human
analysts]{.underline} or military personnel who can be asked to explain
and justify their findings or decisions, [AI generally operates in a
**"black
box."**]{.underline} [97](https://tnsr.org/2020/03/allies-and-artificial-intelligence-obstacles-to-operations-and-decision-making/#_ftn97) The
[neural networks that underpin]{.underline} many cutting-edge [AI
systems are opaque and offer little insight into how they arrive
at]{.underline} their
[conclusions]{.underline}.[98](https://tnsr.org/2020/03/allies-and-artificial-intelligence-obstacles-to-operations-and-decision-making/#_ftn98) These
networks rely on deep learning, a process that passes information from
large data sets through a hierarchy of digital nodes that analyze data
inputs and make predictions using mathematical rules. As data flows
through the neural network, the net makes internal adjustments to refine
the quality of outputs. [Researchers are]{.underline} often [unable to
explain how neural nets make]{.underline} these [internal
adjustments]{.underline}. [Because of this lack of "explainability,"
**users of AI systems** may **have difficulty understanding failures**
and correcting
errors]{.underline}.[99](https://tnsr.org/2020/03/allies-and-artificial-intelligence-obstacles-to-operations-and-decision-making/#_ftn99)
Policymakers have called for the development of more transparent AI
systems, and researchers are working to develop explainable AI tools
that peer inside the AI black
box.[100](https://tnsr.org/2020/03/allies-and-artificial-intelligence-obstacles-to-operations-and-decision-making/#_ftn100)
Yet, many [decision-makers remain uncomfortable with]{.underline} the
[uncertainty surrounding AI-enabled systems]{.underline}. [The commander
of the U.S. Air Force's Air Combat Command]{.underline}, for instance,
publicly explained that he [was not]{.underline} yet [willing to rely on
AI programs to analyze the full-motion video collected by reconnaissance
drones]{.underline}. He argued that although [systems are]{.underline}
improving, they are still [unable to consistently provide accurate
analysis]{.underline}.[101](https://tnsr.org/2020/03/allies-and-artificial-intelligence-obstacles-to-operations-and-decision-making/#_ftn101)
So long as the decisions and analysis of AI systems remain opaque,
military commanders may be reluctant to trust AI-enabled systems. And
[if used, **AI may contribute to the fog of war**]{.underline}, rather
than reduce it, [making it difficult to make decisions using information
delivered by AI technologies]{.underline}.

### AI Causes Escalation

#### Unregulated AI weapons directly undermine humanitarian law, contributes to escalation

**Marijan 3/30** (Branka Marijan leads the research on the military and
security implications of emerging technologies at Ploughshares,
"AI-Influenced Weapons Need Better Regulation," Scientific American,
3/30/2022,
https://www.scientificamerican.com/article/ai-influenced-weapons-need-better-regulation/)-MP

Data fed into AI-based systems can teach remote weapons what a target
looks like, and what to do upon reaching that target. While similar to
facial recognition tools, [AI technologies for military use have
different implications, particularly when they are meant to destroy and
kill, and as such, experts have raised concerns about their introduction
into dynamic war contexts]{.underline}. And while Russia may have been
successful in thwarting real-time discussion of these weapons, it isn't
alone. [The U.S., India and Israel are all fighting regulation of these
dangerous systems.]{.underline} AI might be more mature and well-known
in its use in cyberwarfare, including to supercharge malware attacks or
to better impersonate trusted users in order to access to critical
infrastructure, such as the electric grid. But, major powers are using
it to develop physically destructive weapons. Russia has already made
important advances in autonomous tanks, machines that can run without
human operators who could theoretically override mistakes, while [the
United States has demonstrated a number of capabilities, including
munitions that can destroy a surface vessel using a swarm of
drones.]{.underline} AI is employed in the development of swarming
technologies and loitering munitions, also called kamikaze drones.
Rather than the futuristic robots seen in science-fiction movies, [these
systems use previously existing military platforms that leverage AI
technologies.]{.underline} Simply, a few lines of code and new sensors
can make a difference in whether a military system is functioning
autonomously or under human control. Crucially, **[introducing AI into
decision-making by militaries could lead to overrealiance on the
technology, shaping military decision-making and potentially escalating
conflicts]{.underline}**. AI-based warfare might seem like a video game,
but last September, according to Secretary of the Air Force Frank
Kendall, [the U.S. Air Force]{.underline}, for the first time, [used AI
to help to identify a target or targets in "a live operational kill
chain]{.underline}." Presumably, this means AI was used to identify and
kill human targets. Little information was provided about the mission,
including whether any casualties that occurred were the intended
targets. What inputs were used to identify such individuals and could
there have been possible errors in identification? [AI technologies have
been shown to be biased, particularly against women and people in
minority communities]{.underline}. False identifications
disproportionately impact already marginalized and racialized groups. If
recent social media discussions among the AI community are any
indication, the developers, largely from the private sector, who are
creating the new technologies that some militaries are already deploying
are largely unaware of their impact. Tech journalist Jeremy Kahn argues
in Fortune that a dangerous disconnect exists between developers and
leading militaries, including U.S. and Russian, which are using AI in
decision-making and data analysis. The developers seem to be unaware of
the general-purpose nature of some of the tools they are building and
how militaries could use them in warfare, including to target civilians.
Undoubtedly, lessons from the current invasion will also shape the
technology projects the militaries pursue. At the moment, the United
States is at the head of the pack, but a joint statement by Russia and
China in early February notes that they aim to "jointly build
international relations of a new type," and specifically points to their
aim to shape governance of new technologies, including what I believe
will be military uses of AI. Independently, [the U.S. and its allies are
developing norms on responsible military uses of AI, but generally are
not talking with potential adversaries]{.underline}. In general, states
with more technologically advanced militaries have been unwilling to
accept any constraints on the developments of AI technology. This is
where international diplomacy is critical: there must be constraints on
these types of weapons, and everyone has to agree to shared standards
and transparency in use of the technologies. The war in Ukraine should
be a wake-up call regarding the use of technology in warfare, and the
need to regulate AI technologies to ensure civilian protection.
[Unchecked and potentially hasty development of military applications of
artificial intelligence will continue to undermine international
humanitarian law and norms regarding civilian protection]{.underline}.
Though the international order is in disarray, the solutions to current
and future crises are diplomatic, not military, and the next gathering
of the U.N. or another group needs to rapidly address this new era of
warfare.

### AI Unethical

#### AI is widely accepted as an ethical nightmare

Casey-Maslin 21(Stuart Casey Maslin 12-21, 2 international lawyer
specialising in the use of force and the protection of civilians,
<https://www.cambridge.org/core/books/right-to-life-under-international-law/832ED196FB17B46DA1AE50143290F038>)
Roho

introduction 19.01 This chapter considers whether the autonomous use of
force is compatible with respect for and protection of the right to
life. The development of artificial intelligence has given rise to
autonomy from human beings in the decision to target individuals with
force and the ability to kill them. While weapons systems incorporating
features of automaticity of action and reaction have existed for many
years, the ever-increasing sophistication -- and phenomenal speed -- of
decision-making by machine mean that both as a means of warfare and in
law enforcement, autonomous weapons systems may become commonplace in
years to come. Indeed, the informal architect of the US Department of
Defense's first policy on autonomous weapons systems has claimed that
the rise of artificial intelligence 'will transform warfare'.1 Whether
that is desirable or ethical2 is not considered hereunder; this chapter
focuses on whether the use of autonomous weapons systems can comply with
the right to life, and, if so, under which circumstances. 19.02 In this
regard, with respect to the right to life under the 1966 International
Covenant on Civil and Political Rights3 (ICCPR), the [UN Human Rights
Committee]{.underline} has [affirmed]{.underline} that 'the development
of [[autonomous weapon]{.mark} s]{.underline}ystems lacking in human
compassion and judgment [[raise]{.underline}s]{.mark}
[[difficult]{.mark} legal and [ethical questions]{.mark} concerning the
right to life]{.underline}, including questions relating to legal
responsibility for their use'.4 The Committee affirmed that 'such weapon
systems should not be developed and put into operation, either in times
of war or in times of peace, unless it has been established that their
use conforms with article 6 and other relevant norms of international
law'.5 19.03 Thus the Committee did not consider autonomous weapon
systems to be inherently ROHO unlawful; rather, it noted the requirement
that their use be confirmed to comply with, in particular, international
human rights law and international humanitarian law. The duty to review
new weapons for their potential compliance with international
humanitarian law and other salient branches of international law is a
customary rule6 that was first codified in the 1977 Additional Protocol
I to the four Geneva Conventions of 1949.7 19.04 In August 2020, Human
RightsWatch asserted that 'the challenge of killer robots \[fully
autonomous lethal weapons systems\], like climate change, is widely
regarded as a grave threat to humanity that deserves urgent multilateral
action'.8 In support of this assertion, it refers to the endorsement by
the Minister for Foreign Affairs of both France and Germany of an
'Alliance for Multilateralism'9 declaration concerning lethal autonomous
weapons systems. Presented during theUNGeneral Assembly on 26 September
2019, with the support of several other nations including Canada, Chile,
Ghana, Mexico, and Singapore, the declaration's explicit support for the
'Eleven Principles' elaborated by the Group of Governmental Experts
(GGE) created under the UN Convention on Certain Conventional Weapons10
(CCW) marked the first time such a high-level group has acknowledged
concerns' over fully autonomous weapons systems.11 The Eleven Principles
include the stipulation that '[human responsibility for decisions on the
use of weapons systems must be retained since accountability cannot be
transferred to machines.]{.underline} This should be considered across
the entire life cycle of the weapons system.'12 19.05 In September 2020,
[the influential Non-Aligned Movement ([NAM]{.mark})]{.underline}13
[[expressed]{.underline}]{.mark} its view, in a Working Paper submitted
on its behalf by Venezuela in the context of the discussions under CCW
auspices, that 'Lethal Autonomous Weapon Systems [([LAWS]{.mark})
[raise]{.mark} [several]{.mark} [ethical, legal, moral and
technical,]{.mark} as well as international peace [and security related
questions]{.mark}]{.underline} which should be thoroughly deliberated
and examined in the context of conformity to international law including
international humanitarian law and international human rights law.'14
The NAM further declared that it was 'pleased' that, while divergences
remained, a 'general sense has developed' among States Parties to the
CCW 'that all weapons, including those with autonomous functions, must
remain under the direct control and supervision of humans at all times
and must comply with international law, including International
Humanitarian Law and International Human Rights Law'.15 19.06 With
respect to the right to life under the 1981 African Charter on Human and
Peoples' Rights,16 the General Comment issued by the [[African
Commission on Human and Peoples' Rights]{.mark} in 2015]{.underline} was
similarly normative. The [General Comment declares that 'any machine
autonomy in the selection of human targets or the use of force should be
subject to meaningful human control]{.underline}. The use of such new
technologies should follow the established rules of international
law.'17 [This [effectively outlaw]{.mark}s [the use of fully autonomous
weapons systems]{.mark}, deeming them incompatible with the right to
life.]{.underline} There has also been opposition from a great number of
artificial intelligence (AI) and robotics researchers. In an Open Letter
published on [the Future of Life Institute]{.underline}, the signatories
[declare]{.underline} that 'we believe that AI has great potential to
benefit humanity in many ways, and that the goal of the field should be
to do so. [Starting a military [AI arms race is a bad idea]{.mark}, and
should be prevented by a ban on offensive autonomous weapons beyond
meaningful human control]{.underline}.'18 19.07 There are four main
concerns about autonomous use of force under the right to life. First,
it is questioned [whether an autonomous weapons system is, or will ever
be, capable of accurately identifying as a target]{.underline} only
those individuals who may be lawfully targeted with force in the
circumstances prevailing at the time. Second, it is interrogated whether
a decision to use potentially lethal force may lawfully be left to a
computer algorithm rather than to a human being. Third, it is unclear
whether such a system is able to function in a manner that enables a
person to be safely arrested with the minimum necessary use of force in
a law enforcement operation or allows the acceptance of a surrender by a
person participating directly in hostilities during an armed conflict,
as the law of armed conflict/ international humanitarian law requires.
Fourth, if a machine violates the applicable law, who is to be held
accountable and on what legal basis? 19.08 Discussions on the issue of
regulation of lethal autonomous weapons systems (deliberately, if
ironically, attracting the acronym 'LAWS') have been ongoing for several
years in the context of the CCW. But these discussions have not
crystallised in an agreement to negotiate a treaty to at least
constrain, and possibly prohibit, such weapons systems as a means of
warfare beyond the elaboration of the Eleven Principles.19 Moreover, the
CCW has never been used as a forum to regulate the use of force in law
enforcement. Indeed, the Eleven Guiding Principles, published in an
annex to the report on the 2019 Session of the GGE considering the
question under the auspices of the CCW, mention neither human rights nor
situations of law enforcement.20 19.09 This chapter moves next to
consider the definitions of autonomy and a fully autonomous
weapon/lethal autonomous weapons system. The chapter then looks briefly
at machine autonomy and the types of autonomous weapons systems that
have either been procured and deployed or which are known to be under
development. It considers their legality both under the right to life
during peacetime or in law enforcement during armed conflict and as a
means of warfare in the conduct of hostilities during and in connection
with an armed conflict. key definitions 19.10 There is not -- or at
least not yet -- an agreed definition under international law of either
a fully autonomous weapon or a lethal autonomous weapons system. In
November 2019, the report on discussions in the CCWnoted that no
consensus existed among States Parties as to even the desirability of
agreeing upon a definition. The report stated that 'the concept of
autonomy being a spectrum, . . . the difficulty of defining a clear
point between semi- and fully autonomous systems was underlined'.21
19.11 Autonomy ordinarily means 'freedom from external control or
influence'.22 Aptly enough, as the Oxford English Dictionary recalls,
the etymology of the word is from the Greek αυτο ομος, which means
'having its own laws'.23 Or, in the rather wordier definition of
autonomy proposed by the US Department of Defense in the context of
weapons systems: 'Autonomy is the computational capability for
intelligent behavior that can perform complex missions in challenging
environments with greatly reduced need for human intervention, while
promoting effective man-machine interaction.'24 19.12 More
straightforward was [the D]{.underline}epartment [o]{.underline}f
[D]{.underline}efense['s]{.underline} [proposed definition in 2012 of a
(fully) autonomous weapons system: 'a weapon system that, once
activated, can select and engage targets without further intervention by
a human operator']{.underline}.25 The definition, the Department of
Defense confirmed, 'includes human-supervised autonomous weapon systems
that are designed to allow human operators to override operation of the
weapon system, but can select and engage targets without further human
input after activation'.26 The definition, 'elegant in its simplicity',
was praised by Brazil as being of 'great usefulness'27 (although it
wrongly ascribed the origin of the definition to a joint report of June
2020 by the Stockholm International Peace Research Institute (SIPRI) and
the International Committee of the Red Cross (ICRC)).28 Brazil went on
to offer its own, more 'comprehensive' definition of a lethal autonomous
weapons system in its paper on definitions of August 2020: An
intelligent weapon system with autonomous operation mode (i.e., without
human input after activation) capable of recognizing patterns in combat
environments, and of learning to operate and make decisions regarding
the critical functions of target identification, tracking, locking-on
and engaging based on uploaded databases, acquired experiences and its
own calculations and conclusions. It is not certain, however, that this
definition adds great clarity. In particular, are these capabilities to
be regarded as alternatives or as cumulative prerequisites? 19.13 The
United Kingdom's definition of an autonomous weapons system is different
from that of most other States and has not evolved over time. In 2011,
the then UK Ministry of Defence doctrine on unmanned aerial systems
declared that autonomous systems 'will, in effect, be self-aware and
their response to inputs indistinguishable from, or even superior to,
that of a manned aircraft. As such, they must be capable of achieving
the same level of situational understanding as a human.'29 Without that
higher level of awareness, the United Kingdom regarded a system as
'automated'. As Scharre observed, the differing [UK]{.underline} stance
was 'not a product of sloppy language', it was 'a deliberate choice'. It
enabled the United Kingdom to [claim that autonomous weapons systems 'do
not, and may never, exist']{.underline}.30 19.14 In 2017, in its later
Joint Doctrine Publication, the Ministry of Defence similarly
distinguished between automated and autonomous systems. Reiterating the
glossary incorporated in its 2011 doctrine, an automated (or automatic)
system was one that, 'in response to inputs from one or more sensors, is
programmed to logically follow a predefined set of rules in order to
provide an outcome. Knowing the set of rules under which it is operating
means that its output is predictable.'31 In contrast, an autonomous
system is capable of understanding higher-level intent and direction.
From this understanding and its perception of its environment, such a
system is able to take appropriate action to bring about a desired
state. It is capable of deciding a course of action, from a number of
alternatives, without depending on human oversight and control, although
these may still be present. [Although the overall activity of an
autonomous unmanned aircraft will be predictable, individual actions may
not be.]{.underline}32

#### When AI is fed real life data it inherently perpetuates prejudices already found in society

**Franke '21,** (Ulrike Esther Franke, senior policy fellow at the
European Council on Foreign Relations, "ARTIFICIAL DIVIDE: HOW EUROPE
AND AMERICA COULD CLASH OVER AI," ECRF, January 2021,
<https://ecfr.eu/wp-content/uploads/Artificial-divide-How-Europe-and-America-could-clash-over-AI.pdf>)

Machine-learning systems are those that use computing power to execute
algorithms that learn from data. This means that [[AI is only as good
as]{.mark} the algorithm it uses and [the data it is being trained
on]{.mark}]{.underline}. If, for example, the data is incomplete or
biased, the AI trained on it will be equally biased. [[AI
researchers]{.mark} around the world, and especially researchers from
minority groups[, have raised the alarm about this]{.mark} particular
risk, which has already materialised in several cases.]{.underline} In
the US, a risk assessment tool used in [[Florida's criminal justice
system labelled African-American defendants as "high risk" at]{.mark}
nearly [twice the rate as white defendants]{.mark}. [A]{.mark} hiring
[algorithm used at Amazon penalised applicants from women's
colleges,]{.mark} while [a chatbot trained on Twitter]{.mark}
interactions [started to post racist tweets]{.mark}. The concern is
[that real-life data fed into machine-learning systems perpetuate
existing human biases]{.mark}]{.underline}, and that -- as humans tend
to consider computers to be rational -- these biases will effectively be
sanctioned, [thereby [entrenching prejudice further in
society]{.mark}]{.underline}. Furthermore, AI trained on datasets
collected in one cultural context and deployed in another cultural
context might effectively enable cultural imperialism. In response to
these concerns, big tech firms have developed principles and guidelines,
and created research groups and divisions, on ethical AI. More recently,
however, scandals Artificial divide: How Europe and America could clash
over AI -- ECFR/367 4 have emerged over big tech employees reportedly
being forced to leave their jobs for being too critical, heightening
concerns that these companies are not taking the issue seriously enough.
Related to concerns about bias are those about the transparency of how
AI works. [Employing machine-learning methods means that systems are no
longer programmed -- namely, told what to do by human beings -- but
instead learn how to behave either by themselves or under human
supervision. [It is difficult for a human to understand and track how an
AI-enabled system has reached a conclusion. This makes it hard to
challenge AI-enabled decisions]{.mark}, and to tell whether malicious
actors have exploited the vulnerabilities of AI systems]{.underline}.
Problematic context Even if AI-enabled systems were proven to be
perfectly reliable and unbiased, there are contexts in which delegating
decisions to machines may be inherently problematic. This includes using
AIenabled systems to make decisions that have fundamental implications
for an individual's life, such as in a judicial or military context. In
the military context, lethal autonomous weapon systems able to exert
force without meaningful human control or supervision are particularly
controversial. The concern is a moral one: should a machine -- no matter
how intelligent -- be allowed to make decisions about the physical
wellbeing, or indeed life and death, of a human being? The European
Parliament answered this question in the negative, passing a resolution
in 2018 that urged the EU and its member states "to work towards the
start of international negotiations on a legally binding instrument
prohibiting" such weapons.

#### Racism in AI is present and developing. Further implementation exacerbates structural racism and eliminates colored bodies. (really good structural impact card for soft left version, but might need to run with "AI norms spills over" type cards to be completely effective)

**Asaro 19** (Peter M. Asaro (M'10) Dr. Asaro received his PhD in the
history, philosophy and sociology of science from the University of
Illinois at UrbanaChampaign, Urbana, Illinois, USA, where he also earned
a Master of Computer Science degree. He has held research positions at
the Center for Information Technology and Policy at Princeton
University, the Center for Cultural Analysis at Rutgers University, the
HUMlab of Umeå University in Sweden, and the Austrian Academy of
Sciences in Vienna. He has also developed technologies in the areas of
virtual reality, data visualization and sonification, human-computer
interaction, computer-supported cooperative work, artificial
intelligence, machine learning, robot vision, and neuromorphic robotics
at the National Center for Supercomputer Applications (NCSA), the
Beckman Institute for Advanced Science and Technology, and Iguana
Robotics, Inc., and was involved in the design of the natural language
interface for the Wolfram\|Alpha computational knowledge engine for
Wolfram Research) October 17, 2019 Racism and Fully Autonomous Weapons
[https://www.ohchr.org/sites/default/files/Documents/Issues/Racism/SR/Call/campaigntostopkillerrobots.pdf
//](https://www.ohchr.org/sites/default/files/Documents/Issues/Racism/SR/Call/campaigntostopkillerrobots.pdf%20//)
ZX

[The rise of artificial intelligence is largely due to an increase in
power, memory and speed of computers, and the availability of large
quantities of data]{.underline} about many aspects of our lives.
[Through the commercial application of big-data, we are increasingly
being sorted into different classifications and
stereotypes]{.underline}. In its most benign form, this stereotyping is
being used to sell us products via targeted advertising, however, **[in
its most egregious application, we see the weaponization of new
information technologies utilize similar classifications based on biased
algorithms]{.underline}**, to which the consequences for certain
communities could be deadly. [In this paper I focus on fully autonomous
weapons that are currently being developed for military and law
enforcement purposes; and their potential threat to the human rights of
marginalized communities]{.underline}, in particular persons of color
intersectionally. This paper will also consider the systemic nature of
racism and how racism would be reinforced and perpetuated by fully
autonomous weapons. [Fully autonomous weapons can select and attack
targets without meaningful human control]{.underline}, they operate
based on algorithms and data analysis programming. In essence, this
means that machines would have the power to make life-and-death
decisions over human beings. [The trend towards more autonomy in
weaponry without adequate human oversight is alarming especially when we
know that digital **technologies are not racially
neutral**]{.underline}. Moreover, [when it comes to artificial
intelligence (AI) there is an increasing body of evidence that shows
that racism operates at every level of the design process and continues
to emerge in the production, implementation, distribution and
regulation]{.underline}. In this regard AI not only embodies the values
and beliefs of the society or individuals that produce them but acts to
amplify these biases and the power disparities.iii [One example of
racism manifesting in AI is the under-representation problem in science,
technology, engineering and mathematics]{.underline} (STEM) fields,
[which in itself is a manifestation of structural racism and patriarchy
in western society]{.underline}. Technologies in the west are mostly
developed by white males, and thus perform better for this group[. A
2010 study]{.underline} by researchers at the National Institute of
Standards and Technology (NIST) and the University of Texas, [found that
algorithms designed and tested in East Asia are better at recognizing
East Asians, while those designed in Western countries are more accurate
at detecting Caucasians]{.underline}. Similarly, sound detecting devices
perform better at detecting male, Anglo-American voices and accents, as
opposed to female voices, and non-Anglo-American accents. [**Research**
by Joy Buolamwini,v **reveals that race, skin tone and gender are
significant when it comes to facial recognition**]{.underline}.
[Buolamwini demonstrates that facial recognition software recognizes
male faces far more accurately than female faces]{.underline},
especially when these faces are white. For darker-skinned people however
the error rates were over 19%, and [unsurprisingly the systems performed
especially badly when presented with the intersection between race and
gender]{.underline}, **[evidenced by a 34.4% error margin when
recognizing dark-skinned women]{.underline}**. Despite the concerning
error rates in these systems[, commercially we already see adaptations
of faulty facial recognition systems]{.underline} being rolled out in a
variety of ways from soap dispensers to self-driving cars. **[The issue
here is what happens if law enforcement and national security become
reliant on a system that can recognize white males with just 1% error
rate yet fails to recognize dark-skinned women more than one-third of
the time]{.underline}**? These types of applications of new information
technology fail people of color intersectionally at a disturbing rate.
The fact that these systems are commercially available reveals a blatant
disregard for people of color, it also positions \"whiteness\" as the
norm, the standard for objectivity and reason. [These applications of
new information technology including their weaponization favors
whiteness at the expense of all others, it is not merely a
disempowerment but an empowerment.]{.underline} In real terms, racism
bolsters white people\'s life chances. As we all grew up in a
white-dominated world it is not surprising that [the vast majority of
white people operate within, benefit from and reproduce a system that
they barely notice. This is a long-held reality and it is a fundamental
problem that we now see infiltrate technology.]{.underline} Historical
or latent bias in data is another issue, this is created by frequency of
occurrence, for example in 2016 an MBA student named Rosaliaviii
discovered that [googling \"unprofessional hairstyles for work\" yielded
images of mainly black women with afro-Caribbean hair]{.underline},
conversely when she searched \"[professional hair\" images of mostly
coiffed white women emerged]{.underline}, similar google search results
are still seen today. This is due to machine learning -- algorithms; it
collects the most frequently submitted entries and therefore reflects
statistically popular racists sentiments. [These learnt biases are
further strengthened, thus racism continues to be
reinforced.]{.underline} A more perilous example of this is in
data-driven, [predictive policing that uses crime statistics to identify
\"high crime\" areas and then subjects these areas to higher and often
more aggressive levels of policing]{.underline}. Crime happens
everywhere, however when an area is over-policed such as communities of
color that results in more people of color being arrested and flagged as
\"persons of interest\" thus the cycle continues. In 2017, Amnesty
International launched a report called \"trapped in the Matrix\",ix the
report highlighted racially discriminatory practices by the UK police
force and their use of a databasecalled the \"Gangs Matrix\" which
inputs data on \"suspected\" gang members in London. As of October 2017,
there were 3,806 people on the Matrix, 87% of those are from black,
Asian and minority ethnic backgrounds and 78% are black, a
disproportionate number given that the police\'s own figures show that
only 27% of those responsible for serious youth violence are black.
Amnesty stated that some police officers in the UK have been acting like
they are in the \"Wild West\", making false assumptions about people
based on their race, gender, age and socioeconomic status. As a result,
individuals on the Matrix database are subject to chronic overpolicing.
With black people six times more likely to be stopped and searched than
white people, and ten times more likely to be convicted of drug-related
offenses. This system not only interferes with their right to privacy,
Amnesty claims that the police often share the Matrix with other local
agencies such as job centers, housing associations, social services,
schools and colleges. In several cases, this has led to devastating
impacts on people\'s social and economic lives because they are listed
as \"nominal\" gang members, a label which is deliberately vague and
stigmatizing. [The nature of systemic racism means that it is embedded
in all areas of society, the effects of this type of oppression doesn\'t
easily dissipate. Through the continual criminalization and
stigmatization of people of color, systemic racism operates by creating
winners and losers **regardless of what people actually
do**.]{.underline} This is also the way that it redistributes
opportunities and resources based on nothing other than privilege.
[Given that the UK, as well as five other countries are developing fully
autonomous weapons to target, injure and kill based on data-inputs and
pre-programmed algorithms, we can see how **long-standing inherent
biases, pose an ethical** and human rights **threat**]{.underline}.
Where some groups of people will be vastly more vulnerable than others,
**[fully autonomous weapons would not only act to further entrench
already existing inequalities but could exacerbate them]{.underline}**
and lead to deadly consequences. Legalities As AI technology advances,
the question of who will be held accountable for human rights abuses is
becoming increasingly urgent. [Machine learning and AI, effect a range
of human rights including privacy, freedom of expression, freedom of
assembly, the right to non-discrimination and equality, the right to
life and the right to human dignity]{.underline}. Holding those
responsible for the unlawful killings of people of color by law
enforcement and the military is already a huge challenge in many
countries, however, this issue would be further impaired if the unlawful
killing was committed by a fully autonomous weapon. Who would be held
responsible: the programmer, manufacturer, commanding officer, or the
machine itself? [Lethal force by these weapons would make it even easier
for people of color to be at the mercy of unlawful killings]{.underline}
and far more difficult to obtain justice for victims of color and their
families. According to Reni Eddo-Lodge racism perpetuates partly through
malice, carelessness and ignorance, it acts to quietly assist some,
while hindering others.xi It is within this framework that we must
grapple with race and the weaponization of new information technologies.
In this regard, we should ask ourselves who controls these technologies
and what do they think they know about the people they are
\"classifying\"? What are the politics of these relationships and the
deeply-rooted systemic forms of discrimination? Who benefits from these
technologies and how? [There is a long history of people of color being
experimented on for the sake of scientific advances]{.underline} from
which they have suffered greatly but do not benefit. An example of this
is from James Marion Sims, known as the father of gynecology for
reducing maternal death rates in the US, in the 19th century. He
conducted his research by performing painful and grotesque experiments
on enslaved black women. \"All of the early important reproductive
health advances were devised by perfecting experiments on black
women,\".xii Today, the maternal death rate for black women in the US is
three times higher than it is for white women[. Thus, when it comes to
new information technology, facial recognition systems, algorithms and
automated and interactive machine decision-making, communities of color
are often both deprived of their benefits and subjected to their
consequence]{.underline}s. This paradox where science is inflicted on
communities of color rather than aided by it must be addressed. [We must
be vigilant against deeply rooted social problems]{.underline} taking
root in the technical infrastructure that we create. We must work
towards a zero policy on racism in technology, and not weaponize racism
in technology. [If racism and killer robots are allowed to co-exists
these weapons will be used discriminately against people of color and
other marginalized groups]{.underline}. For these and many other
ethical, moral, human rights, legal and humanitarian reasons the
Campaign to Stop Killer Robots, numerous governments, regional groups,
tech workers, experts, scholars and the UN Secretary-General are all
calling for a legally binding instrument to prohibit fully autonomous
weapons xiii We call on the Special Rapporteur on contemporary forms of
racism, racial discrimination, xenophobia and related intolerance to
condemn fully autonomous weapons and the human rights threat they pose
to people of color; and to support a prohibition treaty that will
preserve meaningful human control over the use of force and prohibit
fully autonomous weapons.

## 

## No Solvency

### No Data Sharing

#### AI data sharing fails -- that prevents development.

**Lin-Greenburg 20**(Erik Lin-Greenburg. Assistant Professor of
Political and Member of Security Studies at MIT. \"Allies and Artificial
Intelligence: Obstacles\". Spring 2020. Texas National Security Review.
https://tnsr.org/wp-content/uploads/2020/03/TNSR-Vol-3-Issue-2-Lin-Greenberg.pdf.
6-21-2022.)-cg

To minimize these perceived risks, states often impose restrictions on
information sharing. [One of the most common control measures is sharing
only finished intelligence --- products such as briefings or reports
derived from a variety of different intelligence sources.]{.underline}
These products provide assessments, but generally omit technical data
--- like details about the information source --- that could reveal
intelligence-gathering procedures and methods. [Although data sharing is
a type of intelligence sharing, developing and operating AI-enabled
systems may require the exchange of more complete raw data in far larger
quantities than traditional intelligence sharing]{.underline}. [Raw
data, which includes imagery files and signals intercepts, can include
metadata such as spectral signatures of imagery or characteristics of
electronic emissions that can be used to feed AI systems. Since this
information can expose precise capabilities and shortcomings of a
state's intelligence systems, decision-makers may be hesitant to share
it --- especially in the large quantities needed to develop and run many
AI-enabled systems.]{.underline} There are also technical obstacles to
data sharing. Just as the U.S. intelligence community and military
stores information in non-standardized formats on multiple systems, so
too do national security institutions in other allied states. [Across an
alliance, the same type of data might reside on hundreds of different
networks and in different formats, making it difficult to share data or
to develop interoperable systems]{.underline}. To use data from other
alliance partners, data must first be located, transferred out of a
state's classified computer network, and reformatted into a
standardized, usable form. [Given that the U.S. military has faced
significant data management challenges in its own AI development, we
should expect alliances --- with their greater number of institutional
actors and data sources --- to encounter even greater obstacles to data
sharing.]{.underline}

### No Interoperability

#### Interoperability efforts fail -- EU-US spending gaps and production challenges mean there's no single solution to the AI problem

**Christie '22** (Edward Hunter Christie; Researcher, consultant,
economist, EU affairs professional, former NATO official, public policy
expert; "Defence cooperation in artificial intelligence: Bridging the
transatlantic gap for a stronger Europe," Sage Journals, March 31 2022;
https://journals.sagepub.com/doi/full/10.1177/17816858221089372#)-amc

[The first general challenge to interoperability is the overall **gap
between the US and Europe in terms of total defence
investment**]{.underline}, as well as in terms of civilian technological
attainment with respect to AI and related technologies. [**There is no
single solution to this problem**, which is **much broader in scope than
traditional military--technical standards**, such as those pursued in
the NATO context through existing mechanisms.]{.underline} For this
broad challenge, overall policy decisions relating to national
investment choices and technology policy coordination between the two
sides of the Atlantic are of particular importance. Further discussion
of this follows in the sections on investment challenges and
international security challenges. A second challenge to
interoperability is that, as far as digital technologies are concerned[,
the civilian sector of the economy]{.underline}, on both sides of the
Atlantic, [is]{.underline} more advanced, more dynamic and also
[**not**]{.underline} especially **[oriented towards]{.underline}**
meeting **[military needs]{.underline}**. For decades, [the military
sector has represented only a very small share]{.underline} of the total
sales volume of the computing and semiconductor industries. The same
pattern is repeating itself currently [with AI]{.underline}. This stands
in great contrast to narrower dual-use technologies, for example
aerospace, where the military sector remains inherently important. With
digital technologies, [defence institutions are under]{.underline} much
more [pressure to either adapt to civilian industry]{.underline}
products and [standards or to pay]{.underline} a [significant premium to
suppliers to secure military-grade equipment and software.]{.underline}
A third [challenge]{.underline} to interoperability [lies in how AI is
implemented in practice]{.underline}. [To set up a bespoke
machine-learning algorithm]{.underline} in a given data environment,
best practice in the software industry [is to pursue some variant of
'agile' development]{.underline}. [This involves a very different
product-development cycle]{.underline}, essentially proceeding [with
multiple **rapid iterations of an imperfect product**]{.underline} that
is released in preliminary versions and later revised---like software
products released in various 'beta versions'---with upgrades developed
over time. This [**contrasts** greatly **with**]{.underline} the
[**traditional production** of major military platforms, which puts a
premium on strict quality control and compliance with requirements at
every development step]{.underline}---an approach referred to in the
software industry as 'waterfall' development ([Christie
2021b](https://journals.sagepub.com/doi/full/10.1177/17816858221089372),
87). [Agile product development]{.underline} may [pose challenges to
interoperability]{.underline}. Unless very tight standards are applied,
there is a [considerable risk of divergences in how different national
institutions go about solving]{.underline} a particular [AI]{.underline}
or data analytics [problem.]{.underline}

### No Cooperation

#### NATO won't cooperation -- no trust. US allies fail to see the US as a reliable partner on AI due to lack of confidence and strategies, domestic priorities, and desire to "win" against China. 

**Imbrie et al. '20** (Andrew Imbrie, Senior Fellow at Georgetown\'s
Center for Security and Emerging Technology; Ryan Fedasiuk, Research
Analyst at Georgetown\'s Center for Security and Emerging Technology;
Catherine Aiken, Director of Data Science and Research at Georgetown\'s
Center for Security and Emerging Technology; Tarun Chhabra, nonresident
fellow with the Center for Security, Strategy, and Technology at the
Brookings Institution; Husanjot Chahal, Research Analyst at Georgetown
University\'s Center for Security and Emerging Technology; February
2022; "HOW THE UNITED STATES AND ITS ALLIES CAN DELIVER A DEMOCRATIC WAY
OF AI"; CSET;
<https://cset.georgetown.edu/publication/agile-alliances/>)//akg

Our cross-national [survey of government officials asked questions about
national AI R&D priorities, international coordination and data sharing
preferences, AI talent development strategies, and perceptions of other
countries' approaches to AI.]{.underline} Table 1 outlines the findings
on national AI priorities. Officials cited four primary areas of concern
around AI: domestic social and economic issues, domestic security,
international security, and ethics. Domestic economic and social issues
were the most prevalent area of concern, primarily labor market impacts
and privacy. In terms of optimism, almost all officials focused on AI's
potential to advance domestic industry, services, and governance.
Benefits for health, education, and infrastructure were especially
prevalent. National R&D priorities focus on increasing research
coordination and capabilities and boosting domestic industry. Priorities
to advance capabilities included increasing investment, fostering
technical innovation, establishing AI centers, developing international
research initiatives, and training AI talent. Allies and partners
prioritize AI R&D investments that support [domestic ecosystems, with a
focus on improving health, education, transportation, and public goods
provision]{.underline}. AI R&D priorities are not determined solely by
government actors; industry actors play an important role in the
process, as well. Officials noted multiple channels for industry
consultation and stressed that the voice of the private sector is
important in shaping national AI strategies. Some officials highlighted
that industry takes the lead in determining R&D priorities, with
government backing and support. A few officials noted that national [R&D
priorities are still in flux,]{.underline} indicating room for [U.S.
leadership on this front]{.underline}. All surveyed officials indicated
that they engage with international partners on AI-related issues.
Bilateral efforts were the most cited avenue of international
collaboration on AI, in four cases (EU, Australia, Czech Republic, and
Italy) involving the United States. Multilateral fora were another
common and increasingly relevant avenue for collaboration. The
Organization for Economic Cooperation and Development (OECD) was the
most cited multilateral forum, while multiple officials indicated
engagement through the EU, Group of Seven (G7), Group of Twenty, or the
newly created Global Partnership on AI (GPAI). Current international
efforts focus on developing shared ethical standards, in part following
the lead of the OECD and EU on defining AI standards. Other officials
noted collaboration around workforce challenges, data policies, climate
change, and lethal autonomous weapons systems (LAWS). [**Partners'**
active **engagement**]{.underline} and interest [**in international
collaboration** around **AI** is matched with]{.underline} positive
**[perceptions of the United States']{.underline}** role [as an **AI
partner**]{.underline}. [Eighty percent of]{.underline} officials said
their [country considers]{.underline} the [**United States** to
be]{.underline} a **[reliable partner on AI]{.underline}** issues. The
remaining three officials, representing the EU, Germany, and France,
suggested that while they consider the United States a like-minded ally
and continue to value U.S. partnership, recent exchanges have been less
fruitful and **[current approaches raise]{.underline}** general
[**concerns** regarding **U.S. reliability**]{.underline} as an [**AI**
partner]{.underline}. Officials also rated the United States highly in
terms of commitment to responsible use of AI with an average rating
similar to the average rating of their own countries, or institutions,
in the case of the EU, and a significantly higher rating than they
assigned to China's commitment to responsible AI (7.3, 7.9, and 3.8 out
of 10, respectively). In citing obstacles to collaboration with the
United States, [**officials** were split between]{.underline} placing
**[blame]{.underline}** on the **[United States]{.underline}** and on
their own country. Multiple [officials noted **threats to
industry**]{.underline} and industrial [**competition, trade issues,**
different **domestic priorities**]{.underline}, or a **[lack
of]{.underline}** agreed upon **[strategy]{.underline}** or common
structures [as **obstacles to collaboration**]{.underline}. Others
specified that the [U.S. desire to "**win**"]{.underline} relative [to
**China**]{.underline}, [lack of]{.underline} data **[privacy
protection]{.underline}**, or unwillingness to engage **[inhibited
collaboration]{.underline}**. They also noted a **[lack of
confidence]{.underline}** in [current U.S. goals]{.underline} or
appropriate U.S. points of contact. Obstacles stemming from their own
government included a lack of regulatory framework or set policies,
alternative political priorities, a preference for multilateral fora, or
insufficient resources. The survey results indicate that [there is space
for]{.underline} the [United States to **engage with international
partners**]{.underline} and, despite some [specific]{.underline} but not
insurmountable **[points of difference]{.underline}**, a high degree of
alignment [on AI interests]{.underline} and priorities.

#### NATO countries have fundamentally differing stances on AI

**Heikkila '21** (Melissa Heikkila, senior reporter for AI at Politico
and MIT technology review, March 29^th^ 2021, "NATO wants to set AI
standards. If only its members agreed on the basics." Politico,
<https://www.politico.eu/article/nato-ai-artificial-intelligence-standards-priorities/)-Cayden>
Mayer

On paper, NATO is the ideal organization to go about setting standards
for military applications of artificial intelligence. [But the widely
divergent priorities and budgets of its 30 members could get in the
way.]{.underline} The Western military alliance has identified
artificial intelligence as a key technology needed to maintain an edge
over adversaries, and it wants to lead the way in establishing common
ground rules for its use. "We need each other more than ever. No country
alone or no continent alone can compete in this era of great power
competition," NATO Deputy Secretary-General Mircea Geoană, the
alliance's second in command, said in an interview with POLITICO. [The
standard-setting effort comes as China is pressing ahead with AI
applications in the military largely free of democratic
oversight.]{.underline} David van Weel, NATO's assistant secretary
general for emerging security challenges, said Beijing\'s lack of
concern with the tech\'s ethical implications has sped along the
integration of AI into the military apparatus. \"I\'m \... not sure that
they\'re having the same debates on principles of responsible use or
they\'re definitely not applying our democratic values to these
technologies," he said. Meanwhile, the EU --- which has pledged to roll
out the world\'s first binding rules on AI in coming weeks --- is
seeking closer collaboration with Washington to oversee emerging
technologies, including artificial intelligence. But those efforts have
been slow in getting off the ground. For Geoană, that collaboration will
happen at NATO, which is working closely with the European Union as it
prepares AI regulation focusing on "high risk" applications. The pitch
NATO does not regulate, but "[once NATO sets a standard, it becomes in
terms of defensive security the gold standard in that respective
field]{.underline}," Geoană said. The alliance\'s own AI strategy, to be
released before the summer, will identify ways to operate AI systems
responsibly, identify military applications for the technology, and
provide a "platform for allies to test their AI to see whether it\'s up
to NATO standards," van Weel said. The strategy will also set ethical
guidelines around how to govern AI systems, for example by ensuring
systems can be shut down by a human at all times, and to maintain
accountability by ensuring a human is responsible for the actions of AI
systems. "If an adversary would use autonomous AI powered systems in a
way that is not compatible with our values and morals, it would still
have defense implications because we would need to defend and deter
against those systems," van Weel said. "We need to be aware of that and
we need to flag legislators when we feel that our restrictions are
coming into the realm of \[being detrimental to\] our defense and
deterrence," he continued. Mission impossible? [The problem is that
NATO\'s members are at very different stages when it comes to thinking
about AI in the military context. The U.S., the world\'s biggest
military spender, has prioritized the use of AI in the defense realm.
But in Europe, most countries --- France and the Netherlands excepting
--- barely mention the technology's defense and military implications in
their national AI strategies.]{.underline} "It's absolutely no surprise
that the U.S. had a military AI strategy before it has a national AI
strategy,\" but the Europeans \"did it exactly the other way around,\"
said Ulrike Franke, a senior policy fellow at the European Council on
Foreign Relations, said: That echoes familiar transatlantic differences
--- and previous U.S. President Donald Trump\'s complaints --- over
defense spending, but also highlights the different approaches to AI
regulation more broadly. [The EU\'s AI strategy takes a cautious line,
touting itself as \"human-centric,\" focused on taming corporate
excesses and keeping citizens\' data safe. The U.S., which tends to be
light on regulation and keen on defense, sees things
differently.]{.underline} [There are also divergences over what
technologies the alliance ought to develop, including lethal autonomous
weapons systems --- often dubbed "killer robots" --- programmed to
identify and destroy targets without human control]{.underline}.
Powerful NATO members including France, the U.K., and the U.S. have
developed these technologies and oppose a treaty on these weapons, while
others like Belgium and Germany have expressed serious concerns about
the technology. These weapons systems have also faced fierce public
opposition from civil society and human rights groups, including from
United Nations Secretary-General António Guterres, who in 2018 called
for a ban. Geoană said the alliance has "retained autonomous weapon
systems as part of the interests of NATO." The group hopes that its
upcoming recommendations will allow the ethical use of the technology
without "stifling innovation." Staying relevant These issues threaten to
hamper NATO\'s standard-setting drive. \"I think there's a certain
danger that if NATO doesn't take this on as a real challenge, that it
may be marginalized by other such efforts," Franke said. She pointed to
the U.S.-led AI Partnership for Defense, which consists of 13 countries
from Europe and Asia to collaborate on AI use in the military context
--- a forum which could supplant NATO as the standard-setting body. That
could have consequences for human rights, too. "NATO... is a great place
to responsibly think about how to harness the good parts of this
technology and how to prohibit the parts that would be catastrophic for
humanitarian law and human rights law, and people at the end of the
day," said Verity Coyle, a senior adviser at Amnesty International,
which is part of the Stop Killer Robots campaign. "Without oversight
mechanisms to ensure ethical standards and measures, which would
guarantee that this technology will operate under meaningful human
control" NATO's strategy could head into an "ethical vacuum," Coyle
said. Franke said it\'s better for the alliance to focus on the basics,
like increased data sharing to develop and train military AI and
cooperating on using artificial intelligence in logistics. "If NATO
countries were to cooperate on that, that could create good procedures
and set precedents. And I think we should then move on to the more
controversial things such as autonomous weapons systems," she said.

### Alt Cause

#### There's not enough AI workers to solve\-\--shortages means innovations won't be effective

Stefano **Costalli 21**, Associate Professor of Political Science in the
Department of Political and Social Sciences, University of Florence,
Italy and Research Fellow of the Michael Nicholson Centre for Conflict
and Cooperation, University of Essex. "NATO Decision-Making in the Age
of Big Data and Artificial Intelligence" This publication is the result
of the Conference "NATO Decision-making: promises and perils of the Big
Data age", organized by NATO Allied Command Transformation (ACT), the
University of Bologna and Istituto Affari Internazionali (IAI) of Rome.
<https://www.iai.it/sites/default/files/978195445000.pdf> //pipk

[**[A key requisite for all]{.mark}** organizational
**[innovations]{.mark}** to occur and for Big Data analysis to be
effective [is]{.mark} the development and incorporation of a **[Big Data
culture]{.mark}**]{.underline}. [Chief data officers and senior
data-related leadership positions will acquire crucial importance in the
analysis of information and in the actual decision-making process, but
these [positions require]{.mark} a special mix of [talent and
tools]{.mark} that are **[currently scarce]{.mark}** in many large
organizations]{.underline}, especially in the public sector. [The
organizations]{.underline} that are [[implementing big data
analysis]{.mark} seem [especially]{.mark} in [need]{.mark} of
'[translators']{.mark} -- professionals that can ensure effective
communication between the Big Data analysis unit and other parts of the
organization]{.underline}, where workers are not data scientist and may
not be ready to work directly on complex models. [However, organizations
willing to use Big Data are also in need of real [data scientists and
analysts]{.mark}, because sophisticated [techniques]{.mark} and data
analysis tools eventually [rely on]{.mark} talented [humans]{.mark} who
know how to manage the tools and interpret data]{.underline}. **[As a
result, [attracting new]{.mark}]{.underline}** types of talented young
**[[workers]{.underline}]{.mark}** and retaining them creating new
career paths and opportunities **[[will represent]{.mark} both [an
essential]{.mark} organizational innovation [and]{.mark} an [important
challenge]{.mark}]{.underline}**. In fact, some members of the WG
highlighted that **[[it will not]{.mark} even [be easy to find]{.mark}
many [workers]{.mark} with the appropriate knowledge and skills to
perform the new tasks in old and complex organizations.]{.underline}**
[It is possible to find [computer scientists]{.mark}, but]{.underline}
sometimes [these]{.underline} individuals [[do not]{.underline}]{.mark}
seem to [[fit]{.underline}]{.mark} [well with large organizations whose
main core business has not much to do with computer
science]{.underline}. At the moment, **[[it is even more difficult to
find translators]{.underline}]{.mark}**, since in principle these
workers should be social scientists with an expertise in Big Data
analysis, but most academic institutions are not ready to forge these
profiles. [[For]{.underline}]{.mark} what concerns
**[[NATO]{.underline}]{.mark}** and national armed forces,
[[this]{.mark} educational task [is not]{.mark} even [performed by
military academies]{.mark}, even though some experiments are
emerging]{.underline}. [[The ideal]{.mark} profile [would include
technical awareness, quantitative]{.mark} analytical [skills, broad
vision, flexibility and open-mindedness]{.mark} -- and this explains why
it is [not easy]{.mark} to produce it.]{.underline}
