# Negative

### 1nc -- regulations fail

#### It is impossible to fully and safely regulate complex AI

**Gyulai and Ujlaki 21** (Attila, Centre for Social Sciences - Institute
for Political Science / University of Public Service.  Anna, Centre for
Social Sciences -- Institute for Political Science / Corvinus University
of Budapest, "The political AI: A realist's account of AI regulation",
Információs Társadalom XXI, no. 2 (2021): 29--42,
<https://dx.doi.org/10.22503/inftars.XXI.2021.2.3>)

Recalling the realist viewpoint of the political sphere, it seems that
the only attainable goal is a modus vivendi, which resonates with the
idea that an inherent characteristic of the political world is balancing
the possibilities of two extremes. History of politics supports this
more pessimistic view: occasionally, eruptions of civil war and failed
states still embody the brute reality of the Hobbesian state of nature,
while the existence of authoritarian and totalitarian dictatures
altogether with hybrid regimes are eternal reminders of the
impossibility to limit power in a once-and-for-all manner. In light of
this reality of the political world, [new claims for the regulation of
artificial intelligence, more specifically on weak AI, are less
promising. Debates on the regulation of AI concentrate on the need to
connect principles such as fairness, accountability, safety,
sustainability, and social inclusion,]{.underline} among others, [to AI
governance]{.underline} (for a more exhaustive list, see Hagendorff
2020). Nevertheless, the most discussed issue is
[transparency,]{.underline} which [is among the primary claims for
several AI 38 ethics guidelines released]{.underline} by different
institutions and companies in the past few years. The current boom in
[ethical guidelines for AI involves several criticisms concerning the
effectiveness of such guidelines based on their potential to implement
transparency]{.underline} and other claims effectively. This line of
criticism can be divided into three types of argument. [The first type
challenges the AI guidelines on their extensive list of ethical claims
based on their ineffectiveness.]{.underline} This type, which can be
called 'tick-box criticism,' can be coupled with a proposal of some
different approach, for example, virtue ethics (see Hagendorff 2020).
The second type, which can be called ['double standard criticism', is
more sceptical about the possibility of guiding AI and whether full
transparency can be achieved at all]{.underline}. This criticism builds
on the argument that [it would be a double standard to call for higher
transparency in AI compared to human decision tools and human
reasoning]{.underline} (see Zerilli et al. 2019). The third type of
criticism is more focused, what we call 'specificity criticism', and
argues that [current Artificial Intelligence Guidelines (AIGUs) are not
specific to AI, but they are simply attempting to gain social control
over technology.]{.underline} This criticism also demonstrates that
transparency and explainability are claims that specifically concern AI
because in such cases there is a possibility of the autonomy of AI. In
that case, though, the double standard problem arises (see Héder 2020).
These criticisms imply that [there is a profoundly political
characteristic of A]{.underline}I. On the one hand, there is a relative
autonomy inherent in AI that can be understood in a broader sense. [It
is impossible to regulate in every detail, something that can develop by
itself]{.underline}. On the other hand, concerning the expert systems of
weak AI, the double standard criticism and specificity criticism
correctly acknowledged [that it would be an unfair expectation to
regulate the decision-making of artificial intelligence in domains where
human decision-making cannot be entirely regulated
likewise]{.underline}. However, contrary to the double standard
criticism, we do not base our argument on the similarity between the
obscurity of artificial decision tools and human cognitive processes.
Instead, we build our argument on the political characteristic of AI.
[Using AI as a tool is similar to political authorization: although
accountability is the main virtue in politics, it would be unrealistic
to expect legislative, executive, or judicial officials to act
'perfectly'. We can only hope that they behave to the best of their
knowledge]{.underline}, and while we usually hold them to account for
significant breaches of their power, mostly, we authorize them because
authorization is the only legitimate way to create order without
slipping into a Hobbesian state of nature or a tyrannical regime.

### 1nc -- dod fails

#### DoD needs huge reforms before it could be effective

**Konaev and Nurkin, 22** (Konaev Margarita and Tate Nurkin, Margarita
Konaev is a nonresident senior fellow in the Forward Defense practice of
the Atlantic Council's Scowcroft Center for Strategy and Security.
Konaev's research on international security, armed conflict, non-state
actors and urban warfare in the Middle East, Russia, and Eurasia., Tate
Nurkin is the founder of OTH Intelligence Group and a nonresident senior
fellow with the Scowcroft Center for Strategy and Security at the
Atlantic Council.Substantively, Mr. Nurkin's research and analysis has a
particularly strong focus on US-China competition, defense technology,
the future of military capabilities, and the global defense industry and
its market issues., 5-25-2022, accessed on 7-13-2022, Atlantic Council,
\"Eye to eye in AI: Developing artificial intelligence for national
security and defense\",
https://www.atlanticcouncil.org/in-depth-research-reports/report/eye-to-eye-in-ai/#accelerating-dod)

Currently, there are no shared technical standards for what constitutes
ethical or trustworthy AI systems, which can make it difficult for
nontraditional AI vendors to set expectations and navigate the
bureaucracy. [The DoD is not directly responsible for setting standards.
Rather, the 2021 National Defense Authorization Act (NDAA) expanded the
National Institute of Standards and Technology (NIST) mission "to
include advancing collaborative frameworks, standards, guidelines for
AI, supporting the development of a risk mitigation framework for AI
systems, and supporting the development of technical standards and
guidelines to promote trustworthy AI systems]{.underline}."7979. Pub. L.
116-283, William M. (Mac) Thornberry National Defense Authorization Act
for Fiscal Year 2021, 134 Stat. 3388 (2021),
https://www.congress.gov/116/plaws/publ283/PLAW-116publ283.pdf. In July
2021, the NIST issued a request for information from stakeholders as it
develops its AI Risk Management Framework, meant to help organizations
"incorporate trustworthiness considerations into the design,
development, use, and evaluation of AI products, services, and
systems."8080. "Summary Analysis of Responses to the NIST Artificial
Intelligence Risk Management Framework (AI RMF)---Request for
Information (RFI)," National Institute of Standards and Technology,
October 15, 2021,
https://www.nist.gov/system/files/documents/2021/10/15/AI%20RMF_RFI%20Summary%20.pdf.
[Related to standards are the challenges linked to testing, evaluation,
verification, and validation (TEVV). Testing and verification processes
are meant to "help decision-makers and operators understand and manage
the risks of developing, producing, operating, and sustaining
AI-enabling systems,"]{.underline} and are essential for building trust
in AI.8181 Michele A. Flournoy, Avril Haines, and Gabrielle Chefitz,
"Building Trust through Testing: Adapting DOD's Test & Evaluation,
Validation & Verification (TEVV) Enterprise for Machine Learning
Systems, including Deep Learning Systems," WestExec, October 2020, 3--4,
https://cset.georgetown.edu/wp-content/uploads/Building-Trust-Through-Testing.pdf.
The [**DoD's current TEVV protocols and infrastructure are meant
primarily for major defense acquisition** programs like ships,
airplanes, or tanks; it is linear, sequential, and, ultimately, finite
once the program transitions to production and deployment. **With AI
systems, however, "development is never really finished, so neither is
testing**]{.underline}."8282. Flournoy, Haines, and Chefitz, "Building
Trust through Testing," 3. Adaptive, continuously learning emerging
technologies like AI, therefore, require a more agile and iterative
development-and-testing approach---one that, as the NSCAI recommended,
"integrates testing as a continuous part of requirements specification,
development, deployment, training, and maintenance and includes run-time
monitoring of operational behavior."8383. "Final ," 384. The ethical
code that guides the US military reflects a fundamental commitment to
abiding with the laws of war at a time when authoritarian countries like
China and Russia show little regard for human rights and humanitarian
principles. Concurrently, the DoD's rigorous approach to testing and
assurance of new capabilities is designed to ensure that new weapons are
used responsibly and appropriately, and to minimize the risk from
accidents, misuse, and abuse of systems and capabilities that can have
dangerous, or even catastrophic, effects. These values and principles
that the United States shares with many of its allies and partners are a
strategic asset in the competition against authoritarian countries as
they field AI-enabled military systems. To cement the DoD's advantage in
this arena, we recommend the following steps**[.]{.underline}** The DoD
will not be able to fulfill its ambitions in AI and compete effectively
with the Chinese model of sourcing technology innovation through
military- civil fusion without close partnerships with a broad range of
technology companies. This includes defense-industry leaders with
long-standing ties to the Pentagon, technology giants at the forefront
of global innovation, commercial technology players seeking to expand
their government portfolio, and startups at the cutting edge of AI
development[**. But, the DoD's budget-planning, procurement,
acquisition, contracting, and compliance processes will likely need to
be fundamentally restructured to effectively engage with the entirety of
this vibrant and diverse technology ecosystem. Systemic change is a
slow, arduous process**. But, delaying this transition risks the US
military falling behind]{.underline} on exploiting the advantages AI
promises to deliver, from operational speed to decision dominance. In
the meantime, the following actions could help improve coordination with
industry partners to accelerate the DoD's AI adoption efforts. The DoD
should implement the NSCAI's recommendation to accelerate efforts to
train acquisition professionals on the full range of available options
for acquisition and contracting, and incentivize their use for AI and
digital technologies."88 Moreover, such acquisition- workforce training
initiatives should ensure that acquisition professionals have a
sufficient understanding of the DoD's ethical principles for AI and the
technical dimensions of trusted and responsible AI. The DIU's ethical
guidelines can serve as the foundation for this training. Rather than
building entirely new AI-enabled systems, in the short to medium term,
the DoD will be integrating AI into a range of existing software and
hardware systems---from cyberdefense architectures to fighter jets to
C2. Progress toward implementing AI will, therefore, also depend upon
streamlining collaboration between the startups and nontraditional AI
vendors that the DoD has been courting for their innovative and
cutting-edge technologies and the defense primes responsible for
integrating new capabilities into legacy systems.

### 1nc -- russia AI fails

#### Russia lags behind in AI, means they aren't a threat

**Polyakova '18** (Alina,  President and CEO of the Center for European
Policy Analysis (CEPA) as well as an adjunct professor of European
studies at the Johns Hopkins University's School of Advanced
International Studies (SAIS), "Weapons of the weak: Russia and AI-driven
asymmetric warfare, Accessed 7/9/2022,
brookings.edu/research/weapons-of-the-weak-russia-and-ai-driven-asymmetric-warfare/)

Speaking to Russian students on the first day of the school year in
September 2017, Putin squarely positioned Russia in the technological
arms race for artificial intelligence (AI). Putin's comment (see above)
signaled that, like China and the United States, Russia sees itself
engaged in direct geopolitical competition with the world's great
powers, and AI is the currency that Russia is betting on. But, [unlike
the United States]{.underline} and China**,** [Russia lags behind in
research and development on AI and other emerging technologies. Russia's
economy makes up less than 2 percent of global GDP]{.underline} compared
to 24 percent for the United States and 15 percent for China, which puts
Russia on par with a country like
Spain.[[\[3\]](https://www.brookings.edu/research/weapons-of-the-weak-russia-and-ai-driven-asymmetric-warfare/#footnote-3)
Despite Putin's focus on AI, the Russian government has not released a
strategy, like China has]{.underline}, on how the country plans to lead
in this area. The [Russian government's future investment in AI research
is unknown, but reports estimate that it spends approximately \$12.5
million a
year[\[4\]](https://www.brookings.edu/research/weapons-of-the-weak-russia-and-ai-driven-asymmetric-warfare/#footnote-4)
on AI research, putting it far behind China's plan to invest \$150
billion through 2030. The U.S. Department of Defense alone spends \$7.4
billion annually]{.underline} on unclassified research and development
on AI and related
fields.[^[\[5\]]{.underline}^](https://www.brookings.edu/research/weapons-of-the-weak-russia-and-ai-driven-asymmetric-warfare/#footnote-5)
[Russia's public corruption, decline in rule-of-law, and increasingly
oppressive government regulations have produced a poor business
environment. As a consequence, the country trails]{.underline} the
United States and China in terms of private investment, scientific
research, and the number of AI
start-ups.[^[\[6\]]{.underline}^](https://www.brookings.edu/research/weapons-of-the-weak-russia-and-ai-driven-asymmetric-warfare/#footnote-6)
In [2018, no Russian city entered the top 20 global regional hubs for
the AI
sector]{.underline},[^[\[7\]]{.underline}^](https://www.brookings.edu/research/weapons-of-the-weak-russia-and-ai-driven-asymmetric-warfare/#footnote-7)
despite the much-hyped opening of the "Skolkovo Innovation Center" in
2010, which was designed to be Russia's answer to Silicon Valley. Unlike
Silicon Valley[, Skolkovo did not spur the kind of private investments
and innovation that the Kremlin had hoped for and has since fizzled
out]{.underline}. Russia's new venture, a "technopolis" named Era, which
is set to open in the fall of 2018, now promises to be the new hub for
emerging technologies, but it too is unlikely to spur Silicon Valley
like
innovation.[^[\[8\]]{.underline}^](https://www.brookings.edu/research/weapons-of-the-weak-russia-and-ai-driven-asymmetric-warfare/#footnote-8)
It is telling that despite high-level presidential and administrative
support, there is scant Russian language academic research on AI. It is
not likely that the country's stagnant and hydrocarbon-dependent economy
will do much to improve the government's ability to ramp up investment
in emerging technologies. In the longer term, [Russia's demographic
crisis]{.underline} (Russia is projected to lose 8 percent of its
population by 2050, according the
UN)[^[\[9\]]{.underline}^](https://www.brookings.edu/research/weapons-of-the-weak-russia-and-ai-driven-asymmetric-warfare/#footnote-9)
[will likely lead to shortages in highly skilled workers,]{.underline}
many of whom have already left Russia for better pay and opportunities
elsewhere.[^[\[10\]]{.underline}^](https://www.brookings.edu/research/weapons-of-the-weak-russia-and-ai-driven-asymmetric-warfare/#footnote-10)
[Western sanctions]{.underline} on key sectors of the Russian financial
sector and defense industry, which Europe and the United States imposed
after Russia's annexation of Crimea in 2014 and the United States [has
continued to ramp up]{.underline} since then, put extra pressure on the
Russian economy. Taken together, the economic and demographic trends
signal that in the AI race, [Russia will be unable to match China on
government investment or compete with the United States on private
sector innovation.]{.underline} The Kremlin is undoubtedly aware of the
country's unfavorable position in the global AI competition, even if
such an admission is unlikely to ever be made publicly. Strategically,
such a wide gap between ambition and capacity means that Russia will
need to invest its limited resources carefully. Currently, Moscow is
pursuing investments in at least two directions: select conventional
military and defense technologies where the Kremlin believes it can
still hold comparative advantage over the West and high-impact, low-cost
asymmetric warfare to correct the imbalance between Russia and the West
in the conventional domain. The former---[Russia's development and use
of AI-driven military technologies and weapons---has received
significant
attention.[\[11\]](https://www.brookings.edu/research/weapons-of-the-weak-russia-and-ai-driven-asymmetric-warfare/#footnote-11)]{.underline}
The latter---[the implications of AI for asymmetric political
warfare---remains
unexplored.[\[12\]](https://www.brookings.edu/research/weapons-of-the-weak-russia-and-ai-driven-asymmetric-warfare/#footnote-12)
Yet, such nonconventional tools---cyber-attacks, disinformation
campaigns, political influence, and illicit finance---have become a
central tenet of Russia's strategy toward the West]{.underline} and one
with which Russia has been able to project power and influence beyond
its immediate neighborhood. In particular, AI has the potential to
hyperpower Russia's use of disinformation---the intentional spread of
false and misleading information for the purpose of influencing politics
and societies. And unlike in the conventional military space, the United
States and Europe are ill-equipped to respond to AI-driven asymmetric
warfare (ADAW) in the information space.
