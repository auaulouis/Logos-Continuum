# Aff

## clarity

### 1ac -- impact -- digital authoritarianism 

#### Digital authoritarianism facilitates revisionist great power wars 

**Sherman 21** -- (Justin Sherman is a nonresident fellow at the
Atlantic Council's Cyber Statecraft Initiative, a research fellow at the
Tech, Law & Security Program at American University Washington College
of Law, and a contributor at WIRED Magazine. He was previously a
cybersecurity policy fellow at New America and a fellow at Duke Law
School's Center on Law & Technology, "Digital Authoritarianism and
Implications for US National Security", 3-15-22, The Cyber Defense
Review,
https://cyberdefensereview.army.mil/Portals/6/Documents/2021_winter_cdr/06_CDR_V6N1_Sherman.pdf?ver=\_8pKxD7hOFkcsIANHQZKDw%3d%3d)//=ezg

Private firms worldwide legally or illegally have long been [selling
dual-use digital technologies that can be used to monitor web traffic
and to censor information. That there is a global market for digital
surveillance tools is old news.]{.underline} Companies incorporated in
democracies heavily export these dual-use technologies worldwide,
including, in many documented cases, to despots.\[12\] [Likewise,
companies incorporated in autocracies sell dual-use technologies,
including those that can be used for censorship and surveillance, to
other authoritarian regimes.\[]{.underline}13\] Some studies suggest
that democracies account for a far greater volume of surveillance
technology exports, including to despots, despite attempts to restrict
such exports.\[14\] [The pursuit of digital authoritarianism to bolster
state power magnifies incentives for some countries to acquire dual-use
surveillance tools, and for others to encourage their spread. China's
state leadership, for instance, consistently has advocated a sovereign
and controlled Internet governance model on the global stage, with
practices like censorship and surveillance, as opposed to a global and
open model supported by many liberal democracies.\[]{.underline}15\] In
tandem with this global diplomatic messaging, the [Chinese government
has reportedly conducted trainings on new media or information
management with representatives from dozens of countries, many on record
as pursuing restrictive online practices.\[]{.underline}16\] This has
coincided with countries targeted by the Belt and Road Initiative
passing cybersecurity laws that sometimes mirror laws already enacted in
China, such as Vietnam's recent establishment of data localization
requirements.\[17\] Causality remains unclear in this situation, and
empirical questions remain to be answered about the underlying drivers
of digital authoritarianism in different countries. [Nonetheless, these
patterns and events, coupled with exports of surveillance technologies
from China, raise questions about Beijing's intentions to spread digital
authoritarianism globally, including through a greater focus on, and/or
endorsement of, the sale of digital surveillance and control
capabilities. This could amplify the aforementioned national security
risks, should authoritarian countries acquire the tools and/or knowledge
needed to bolster their power through digital surveillance]{.underline}.
National security analysts have already flagged these potential risks
across Africa. [Many countries China has engaged with]{.underline}
through its Belt and Road [investments have acquired Chinese
surveillance technology, potentially usable for oppressive
purposes]{.underline}. For instance, [Chinese company exports of
surveillance technology to the Ethiopian government have occurred
alongside Chinese government investments]{.underline}.\[18\] Given
China's history of spying on and suppressing political dissidents, this
is hardly a benign fact, and Ethiopia is but one of several examples.
[Should China's leadership be intent on spreading digital
authoritarianism worldwide, to include diffusion of surveillance tools,
this likely could include countries aligned with China's national
security and/or economic interests.]{.underline} Like China, [Russia has
long advocated for cyber sovereignty on the international stage,\[19\]
with President Vladimir Putin repeatedly emphasizing the importance of
information control within a country's sovereign borders.\[20\] As noted
above, Russian companies export surveillance and hacking technologies,
especially to post-Soviet states.\[]{.underline}21\] Andrei Soldatov and
Irina Borogan actually suggest [that Russian surveillance technology
exports to some of these countries are a better fit than Western-made
surveillance applications, because Russian laws and procedures governing
traffic interception are more compatible for these
countries,]{.underline} and the technologies are tailored
accordingly.\[22\] In either case, [these surveillance technology
exports]{.underline} need further study, and they clearly [serve as
tools of political influence in Russia's near-abroad.]{.underline} As
with China, the extent of the Russian government's direct involvement in
and support of such exports needs further study, because the Kremlin's
direct hand in these exports, while visible, is hardly transparent. [The
desire to spread digital authoritarianism may well incentivize the
Kremlin to better spread its surveillance technologies, or to at least
look the other way when they occur, and thereby consolidate power in the
hands of Russian-aligned countries at the expense of US government
interests. This also could threaten vulnerable democracies worldwide,
and facilitate the so-called fracturing of the global Internet, as
countries build out technical and legal regimes that filter the global
and open Internet touching and running through their
borders.\[23\]]{.underline} Again, [the threat here is not only from
governments in China and Russia. Companies incorporated in democracies
also sell a high volume of dual-use surveillance technologies to
despots,]{.underline} and this is something we are better able to
monitor and correct. It is also important to reemphasize the existing
incentives for countries to encourage or allow the spread of these
capabilities to other countries (including the technologies and how to
optimize them). But [growing desires to spread digital authoritarianism
globally not only undermine human rights and developing democracies;
this also exposes US national security to increased risk.]{.underline}

### 

### 2ac -- uniqueness -- digital authoritarianism

#### AI soon to takeover politics to reaffirm authoritarian and fascist ideology to eradicate democracy

**Ünver 18** (Akın, associate professor of IR at Ozyegin University,
former research associate at Oxford University, The 2021 Science Academy
of Turkey's 'Outstanding Young Scientist', "Artificial Intelligence,
Authoritarianism, and the Future of Political Systems", 7/1/18,
https://www.jstor.org/stable/pdf/resrep26084.pdf?refreqid=excelsior%3A1b398a4cb80c5f85d280d26dbc1f9669&ab_segments=&origin=&acceptTC=1)

In a public discussion with Arkady Vorozh, the CEO of Yandex, [Russian
President Vladimir Putin asserted that whichever country would master
the A.I. in the short-term, **'will be the ruler of the world**.' Elon
Musk later shared Putin's words on Twitter, who added 'Competition for
AI superiority at the national level most likely **cause of
WW3'**]{.underline}.26 [Both Putin and Musk statements constitute
high-level affirmations of algorithmic feudalism, given how they both
locate the A.I.'s immediate role within political anarchy and global
leadership]{.underline} to circumvent its effects. Both, furthermore,
view A.I. mastery as an inherently zero-sum game, in which one power has
to dominate and others, become dominated. [Although Putin's statement
referred to both military and non-military applications of A.I., the
rest of his statement specifically referred to threats originating from
the **automation of security tools**]{.underline} (drones,
cybersecurity, 3D printing), instead of finance, healthcare or other
non-military applications of A.I. [Both Putin and Musk deviate
significantly from Emmanuel Macron]{.underline}, for example, who gave
an exclusive interview to Wired [on France's A.I. strategy, where he
focused exclusively on healthcare, finance and political participation
aspects of algorithmic structures, rather than their military
applications]{.underline}.27 'A.I. feudalism' then, has to imply a
political regime, primarily geared towards eliminating systemic anarchy
and revolves directly around security provision, both domestically and
internationally, in exchange for financial and human capital provision.
In domestic politics, it implies riot control and surveillance
industries, whereas internationally it concerns cybersecurity, unmanned
systems and a wide array of communication-related fronts. [The most
immediate impact of A.I. that might reinforce the feudalistic tendencies
of the digital space is to create a production system mimicking
corporatism]{.underline} - namely, the reconfiguration of power
relations through sectoral alliances between coder syndicates and
guilds. This would entail the control of algorithm-building and
maintaining structures that both state and private actors rely on, and
the foundation of the future economic system. [The corporatization of
A.I. could **reinforce power-centralization** through the combination of
corporations that monopolize modes of code and coder production that
will disproportionately influence politics, military and science
affairs]{.underline}. [This will effectively generate a feudal network
that minimizes political participation and representation, leading to
the **eradication of democracy**.]{.underline} The Habermasian
'algorithmic enclosures' that are obscure and inaccessible will
establish robust control mechanisms on the society and in turn, empower
coder oligarchies and corporations in charge of them. [The second
alarmist trend in the popular mainstream is the idea that the A.I. will
create a 'fascist system]{.underline}'28 - [where the over-centralized
A.I.-based decision-making will create a hierarchy of repression in
which control-oriented, top-down practices will restrict expression,
engagement, oversight and political information-seeking
behavior]{.underline}. These fears have been intensified with the rise
of the far-right groups in the US and Europe in the last years,
bolstered by Internet trolls, fake news, and bots. According to the
conceptualization of Foucault29 and Canguilhem30, [the way technology
and science are deployed by fascist regimes snowball into a social
force, bursting their initial utilitarian origins and take on a life of
their own. Technologism then determines the bounds of rights and
freedoms in a society, becoming the real political ideology in fascist
regimes. In 'techno-fascism,' all aspects of social life are controlled
with the purpose of maximizing scientific progress and technological
advances that are in turn, used to exert newer forms of sectoral control
over social life.]{.underline} **[Views that don't conform or fully
converge to the hegemonic ideology are taken out of the equation through
imprisonment and death.]{.underline}** Totalitarianism is different from
authoritarianism in this context since the latter denotes the
centralization of political power without the need to control thoughts
and actions of all citizens through a revolutionary mechanism to change
the human nature or the world at whole. An 'A.I. fascism' or
totalitarianism, therefore, has to entail a bid to change human
relations and social interactions; merely political control and
centralization are not enough on their own

### 2ac -- uniqueness -- china 

#### China implementing mind controlling AI now

**Huang** 7-9-**22** \-- (Joyce Huang, \"China Boasts of
\'Mind-reading\' Artificial Intelligence that Supports \'AI-tocracy\'\",
7-9-2022, VOA,
https://www.voanews.com/a/china-boasts-of-mind-reading-artificial-intelligence-that-supports-ai-tocracy-/6651986.html)//Alk

[An artificial intelligence]{.underline} (AI) institute in Hefei, in
China's Anhui [province]{.underline}, [says it has developed software
that can gauge the loyalty of Communist Party members]{.underline} --
something that, if true, would be considered a breakthrough, but has
sparked public outcry. Analysts said [China has improved its AI-powered
surveillance, using big data, machine learning, facial recognition and
AI to "*get into the brains and minds of its people*]{.underline},"
building what many call a draconian digital dictatorship. The institute
posted a video called "The Smart Political Education Bar," on July 1 to
boast about its "mind-reading" software, which it said would be used on
party members to "further solidify their determination to be grateful to
the party, listen to the party and follow the party." In the video, a
subject was seen scrolling through online material that promotes party
policy at a kiosk, where the institute said its AI software was
monitoring his reaction to see how attentive he was to the party's
thought education. The post, however, was taken down shortly after
sparking a public outcry among Chinese netizens. [Hung Ching-fu, a
professor of political science at National Cheng Kung University
in]{.underline} Tainan, in southern [Taiwan, said that the Communist
Party has abused technological advances to serve its own political
interests. "It has used cutting-edge technology to empower its party
state. China has upgraded from early-day facial recognition to AI
programs that try to get into brains and minds (more) than meet the eye.
Its adoption of advanced AI will reinforce its total controls," Hung
told VOA over the phone]{.underline}. Hung added China's AI-fueled
police state will weigh on its people, who are likely to self-censor or
live in fear. But he cast little confidence in what he called [China's
digital repression]{.underline}, which he said [will likely put *the
Communist Party* in the "dictator's dilemma" -- a political term that
describes a government leader's *failure to win the hearts and minds of
its people*. "The taller you build your wall \[of power\], the further
you're cut off from the people]{.underline}... This constitutes what we
call the 'dictator's dilemma' in politics. That is, despite their
enormous powers, dictators keep out of touch with the people. I don't
think any political systems that are against human nature will sustain,"
Hung added. VOA's calls and emails to the Hefei-based institute for
comment went unanswered. The so-called mind-reading software is but the
latest digital control China has implemented. China reportedly has long
deployed facial recognition in Xinjiang to keep tabs on ethnic Uyghurs
while having enhanced its surveillance in recent years with "one person,
one file" software to make it easier to track its people.

### 2ac -- uniqueness -- turkey 

#### Turkey shifting to digital authoritarianism now 

**Timuçin 21** -- (Fatma Timuçin went to the Graduate School of Social
Sciences at Sabancı University for the degree of Master of Arts, "8-BIT
IRON FIST: DIGITAL AUTHORITARIANISM IN COMPETITIVE AUTHORITARIAN
REGIMES: THE CASES OF TURKEY AND HUNGARY", 7-21, Sabancı University,
https://research.sabanciuniv.edu/id/eprint/42417/1/10337574.pdf)//Alk

Turkish case of digital authoritarianism is an important example for
shedding light on possible pathways for populist regimes' handling of
the free media. As his populist counterparts, [Erdoğan assumed
office]{.underline} with his struggle [against the existing political
framework]{.underline}. The media's dominantly opposing views and
belonging to the 'elite' made him target the media. While [Turkey did
not possess a mainstream conservative media Erdoğan could rely on,
almost every media outlet exhibited a drastic shift in their narrative
after just a decade. The primary reason for this unfolding of events was
the crony capitalism of AKP. While holding the traditional media under
direct control, AKP has also expanded its influence over the internet,
mainly the news sites of said traditional media. As a result, digital
media was regarded as the "alternative" for opposition voters. Just shy
of the end of their second term at the helm of parliament, their
*digital authoritarianism started to materialize* more directly,
especially *in terms of legislative decisions*]{.underline}. Gezi period
pitted AKP against the online presence of opposition voters and created
a demarcation of traditional news and social media on opposing sides of
the political debate. After 2016's failed coup attempt, [Erdoğan
operationalized a crackdown through legal persecution, presidential
decrees, and bans on several platforms. Lastly, the remaining opposition
presence has been plagued by trolls and bot accounts favoring the
government. Through these three sequences, Erdoğan has triumphed over
means of alternative information and created pro-government media to
prolong his incumbency, solidify his voter base, and further *expand his
control over civil society*]{.underline}.

### 2ac -- i/l -- digital authoritarianism

#### Technology and AI key to assure new form of Digital Authoritarianism

**Wright et al 20** (Joseph, Erica Frantz, Andrea Kendall-Taylor, Wright
is a Political Science Professor at Penn State, Co-Director of the
Global and Intl. Studies, with a Ph. D at UCLA. Frantz is an associate
professor at MSU with a Ph.D from UCLA. Kendall-Taylor is a current
director of the Transatlantic Security Program at CNAS, a former Senior
Intelligence Officer for Russia and Eurasia. "The Digital Dictators: How
Technology Strengthens Autocracy", 3/1/2020,
https://www.foreignaffairs.com/articles/china/2020-02-06/digital-dictators?utm_medium=promo_email&utm_source=lo_flows&utm_campaign=registered_user_welcome&utm_term=email_1&utm_content=20220713)

But this wishful vision of a more democratic future proved naive.
Instead, new technologies now afford rulers fresh methods for preserving
power that in many ways rival, if not improve on, the Stasi's tactics.
[Surveillance powered by artificial intelligence (AI), for example,
allows despots to automate the monitoring and tracking of their
opposition in ways that are far less intrusive than traditional
surveillance. Not only do these digital tools enable authoritarian
regimes to cast a wider net than with human-dependent methods; they can
do so using far fewer resources]{.underline}: no one has to pay a
software program to monitor people's text messages, read their social
media posts, or track their movements. [And once citizens learn to
assume that all those things are happening, they alter their behavior
without the regime having to resort to physical repression. This
alarming picture stands in stark contrast to the optimism that
originally accompanied the spread of the Internet, social media, and
other new technologies]{.underline} that have emerged since 2000. Such
[hopefulness peaked in the early
2010s](https://www.aljazeera.com/indepth/opinion/2012/09/2012919115344299848.html)
as social media [facilitated the ouster of four of the world's
longest-ruling dictators,]{.underline} in Egypt, Libya, Tunisia, and
Yemen. [In a world of unfettered access to information and of
individuals empowered by technology, the argument went, autocrats would
no longer be able to maintain the concentration of power that their
systems depend on]{.underline}. It's now clear, however, that technology
does not necessarily favor those seeking to make their voices heard or
stand up to repressive regimes[. Faced with growing pressure and
mounting fear of their own people, authoritarian regimes are evolving.
They are embracing technology to refashion authoritarianism for the
modern age]{.underline}. [Led by China, today's digital autocracies are
using technology---the Internet, social media, AI---to supercharge
long-standing authoritarian survival tactics]{.underline}. They are
harnessing a new arsenal of digital tools to counteract what has become
the most significant threat to the typical authoritarian regime today:
the physical, human force of mass antigovernment protests. [As a result,
digital autocracies have grown far more durable than their pre-tech
predecessors and their less technologically savvy peers.]{.underline}
[In contrast to what technology optimists envisioned at the dawn of the
millennium, autocracies are benefiting from the Internet and other new
technologies, not falling victim to them.]{.underline}

### 2ac -- i/l -- Russia/China

#### NATO currently deemed unsuitable to fight back against Russian and Chinese AI attacks -- Clear Standards needed to deter threats

Jill **Aitoro 18** \[February 15; Senior Vice President of Content
Strategy at CyberRisk Alliance, Editor in Chief at SC Media editor of
Defense News, former executive editor of Business-to-Government Group;
"AI Warfare is Coming, and Some Global Leaders Say NATO Isn't Ready,"
Defense News,
https://www.defensenews.com/smr/munich-security-forum/2018/02/16/ai-warfare-is-coming-and-some-global-leaders-say-nato-isnt-ready/\]

[The future of warfare will involve artificial intelligence systems
acting as lethal weapons]{.underline}, and [[much like cyber a decade
ago,]{.underline}](https://www.fifthdomain.com/dod/2018/01/24/the-next-cyber-arms-race-is-in-artificial-intelligence/)
[NATO allies are ill-equipped to manage the potential
threat]{.underline}, said current and former European leaders speaking
at the Munich Security Conference. Kersti [Kaljulaid, president of
Estonia, estimated a 50 percent chance that by the middle of this
century we will have an AI system capable of launching a lethal attack.
And yet, just as the world was **not prepared** for a cyberattack when
[Russia first launched a cyberattack against Estonia in
2007](https://www.fifthdomain.com/2017/11/21/estonian-official-cyber-must-be-part-of-core-military-education/)]{.underline}
--- bombarding websites of Estonian parliament, banks, ministries, and
news outlets --- [there is no strategy or international law to deter
such tactics of warfare.]{.underline} First, "we need to understand
[[the risks --- what we're afraid
of,]{.underline}](https://www.fifthdomain.com/home/2017/07/14/what-an-artificial-intelligence-researcher-fears-about-ai/)"
said Kaljulaid,, pointing to three: someone using AI disruptively;
intelligence going widespread; and AI depleting energy. "Right now we
know we want to give systems some right of auto decision-making when it
has the necessarily information to react," Kaljulaid said. But once that
is accomplished, "then [we have the responsibility" to establish
standards --- the ability to apply reporting requirements to AI system,
or to even shutdown systems if they are deemed threatening]{.underline}.
The kind of standards gradually being put in place for cybersecurity
"need to apply to the AI world, exactly the same way," she said. [For
such standards to potentially be established for AI, there must be
acceptable models of use in combat, and in conjunction with that, when
there is evidence that AI is deployed outside those established
boundaries, there must be a right to intervene]{.underline}. And much
like nuclear non-proliferation efforts, "if we say that we will have the
right to intervene, we have to have the right to international
inspection," Kaljulaid said. Among the standards advocated by Anders
Fogh Rasmussen, former NATO secretary general, is that AI always involve
human beings. There are three options, he said during the panel: humans
can be in charge, always "in the loop;" humans can be "on the loop"
through a supervisory role, able to intervene; or humans can be "out of
the loop" -- telling the system to attack, and then leaving the rest to
the machine. "I'm in favor of trying to introduce legally binding
\[standards\] that will prevent production and use of these kinds of
autonomous lethal weapons," Rasmussen said, strongly advocating for a
human role. But such standards don't come fast[. It took until 2017 for
NATO to declare that a cyberattack would spur an Article 5
response]{.underline} -- that being, collective defense among allies ---
after a massive computer hack paralyzed portions of government and
businesses in Ukraine before spreading around the globe. In the
meantime, much like cybersecurity, [AI presents an opportunity for
Russia as well as China to use "grey zones," said Rasmussen -- not
initiating open military conflict, but provoking allies enough to
disrupt]{.underline}. So what is [the red line? "The NATO perspective is
clear: ambiguity]{.underline}," Rasmussen said. [**"We never define when
a red line is crossed. We never define how to respond if a certain
member state is attacked. Nobody should know when they cross the line
and how we would react. It's easy- abstain from attacking any NATO ally;
if you do \[attack\], we'll respond decisively. It may be
conventional,** it may be a cyber counterattack, you never know." But to
prevent adversaries from taking advantage of the technological
capability, "we need leadership from the democratic world," Rasmussen
added. "Whenever the democratic countries retrench and retreat they
leave behind a vacuum. And that will be filled by the bad
guys."]{.underline}

### 2ac -- clarity key -- cohesion 

#### Unequal adoption of AI leads to consequences on coalition operations 

**Lin-Greenberg, 20** (Erik Lin-Greenberg, He is Assistant Professor in
the History and Culture of Science and Technology in the Department of
Political Science at the Massachusetts Institute of Technology. His
research examines how emerging military technology affects conflict
dynamics and the use of force., 6-2-2020, accessed on 7-15-2022, Texas
National Security Review, \"Integrating emerging technology in
multinational military operations: The case of artificial intelligence
\| MIT Center for International Studies\",
https://cis.mit.edu/publications/analysis-opinion/2020/integrating-emerging-technology-multinational-military-operations)

[In the military domain, AI has been increasingly used i]{.underline}n
roles that traditionally required human intelligence. In some cases, AI
is employed as part of analytical processes, like the use of machine
learning to help classify geospatial or signals intelligence targets.
Or, it can be part of the software used to operate physical systems,
like self-driving vehicles or aircraft. [States around the world have
already fielded a range of military systems that rely on AI technology.
The **US Department of Defense**, for instance, launched Project Maven
to develop AI to process and exploit the massive volume of video
collected by reconnaissance drones.]{.underline}53 Similarly, Australia
is working with Boeing to develop an advanced autonomous drone intended
for use on combat missions, and the US Navy is exploring the use of
self-operating ships for anti-submarine warfare operations.54 Military
decision-makers look to these systems as ways of increasing the
efficiency and reducing the risk of conducting military operations.
Automating processes like signals analysis can reduce manpower
requirements, [while replacing sailors or soldiers with computers on the
front lines can mitigate the political risks associated with suffering
friendly casualties]{.underline}.55 As states develop AI capabilities,
leaders must consider the challenges that may arise when fielding AI as
part of broader alliance or coalition efforts. [First, alliance leaders
must consider **the unequal rates at which alliance members will adopt
AI**---and the **consequences this could have on alliance and coalition
operations**. Second, leaders must consider how AI will affect two
important components of alliance dynamics: **shared decision-making and
interoperability**]{.underline}.

### 

### 2ac -- at: entaglement turn

 **No US entanglement**

**Beckley, 15** (Michael Beckley, Michael Beckley is Assistant Professor
in the Department of Political Science at Tufts University, 4-1-2015,
accessed on 7-15-2022, MIT press direct, \"The Myth of Entangling
Alliances: Reassessing the Security Risks of U.S. Defense Pacts\",
https://direct.mit.edu/isec/article/39/4/7/12305/The-Myth-of-Entangling-Alliances-Reassessing-the)

[American concerns about entangling alliances are as old as the Republic
itself. During the post-World War II era, however, **there have been
only five ostensible episodes of U.S. entanglement, and even these cases
are questionable**.]{.underline} The case in which alliance obligations
had the largest impact on U.S. decision-making (the 1995--96 Taiwan
Strait crisis) entailed minimal military action, [and the case that
entailed the most military action (the Vietnam War) contained only a
marginal role for alliance politics in U.S.
decision-making.]{.underline} In the other three cases (the 1954--55
Taiwan Strait crisis and the wars in Bosnia and Kosovo), both the effect
of alliance obligations on U.S. policy and the costs suffered by U.S.
forces were moderate. **[And beyond these cases, entanglement was
virtually nonexistent in U.S. foreign policy.]{.underline}** Against
this limited evidence of entanglement are numerous cases in which
alliances restrained the United States. Allies dissuaded the United
States from escalating the Korean War and crises in Laos and Berlin, and
struggled in vain to prevent the United States from entering or
escalating other conflicts, the 2003 Iraq War being only the latest
major example. Indeed, instances of alliance-induced restraint are
evident even within the five cases of entanglement discussed above: in
the 1954--55 Taiwan Strait crisis, concerns about European alliances
discouraged U.S. policymakers from bombing the Chinese mainland and
publicly committing to defend Jinmen and Mazu; in the Vietnam War,
allies impeded U.S. entry into the war and then repeatedly implored the
United States to get out; and in Bosnia and Kosovo, U.S. allies
initially restrained the United States from lashing out violently and
then provided all of the NATO ground forces and most of the postconflict
peacekeepers for the eventual operations. [There also are several cases
in which the United States sidestepped inconvenient alliance
commitments, restrained an ally from attacking a third party, or openly
sided against an ally]{.underline}---and this list could probably be
expanded by looking within other cases, including the five ostensible
cases of entanglement. As explained earlier, the United States blatantly
retracted a pledge to Taiwan to defend Jinmen and Mazu in 1955, refused
to save the French at Dien Bien Phu in 1954, delegated ground operations
and most of the postconflict peacekeeping in Bosnia and Kosovo to
allies, and waited for eight months and the receipt of private security
assurances before responding militarily to China\'s provocative behavior
near Taiwan in 1995--96. **[In sum, the empirical record shows that the
risk of entanglement is real but manageable and that, for better or
worse, U.S. security policy lies firmly in the hands of U.S. leaders and
is shaped primarily by those leaders\' perceptions of the nation\'s core
interests]{.underline}**. [When the United States has overreached
militarily, the main cause has not been entangling alliances but rather
what Richard Betts calls "self-entrapment"---the tendency of U.S.
leaders to define national interests expansively, to exaggerate the
magnitude of foreign threats, and to underestimate the costs of military
intervention.188]{.underline} Developing a disciplined defense policy
therefore will require the emergence of prudent leadership, the
development (or resurrection) of guidelines governing the use of
force,189 the establishment of domestic institutional constraints on the
president\'s authority to send U.S. forces into battle, or some
combination of these.190 **[Scrapping alliances, by contrast, would
simply unleash the United States to act on its interventionist impulses
while leaving it isolated diplomatically and militarily]{.underline}**.

## public trust

### 2ac -- clarity i/l

#### Transparency/clarity key to AI trust and acceptance- study proves

**Shin 20** (Donghee,College of Communication and Media Sciences, Zayed
University, "User Perceptions of Algorithmic Decisions in the
Personalized AI System:Perceptual Evaluation of Fairness,
Accountability, Transparency, and Explainability") 14 Dec 2020

https://www-tandfonline-com.proxy.lib.umich.edu/doi/full/10.1080/08838151.2020.1843357
acd

[The existence of trust is key to promoting technology acceptance (Shin,
[2010](https://www-tandfonline-com.proxy.lib.umich.edu/doi/full/10.1080/08838151.2020.1843357))
as it facilitates openness and transparency in adoption
process.]{.underline} Findings in the work of Lee
([[2018]{.underline}](https://www-tandfonline-com.proxy.lib.umich.edu/doi/full/10.1080/08838151.2020.1843357))
imply that [trust plays a mediating role in algorithm acceptance. In the
news recommendation context, trust is considered a mediator of
relationships between behavioral intentions and individual
characteristics, and algorithm technology]{.underline} (Shin & Park,
[[2019]{.underline}](https://www-tandfonline-com.proxy.lib.umich.edu/doi/full/10.1080/08838151.2020.1843357)).
[It can be reasonably considered trust can play a mediating role in the
context of AI.]{.underline} For estimating indirect effects for FATE and
trust, a non-parametric bootstrapping approach was employed.
Bootsrapping is one of the crucial parts in structural equation
modelling when it comes to test mediation effects. Mediation analyses
assessed the indirect effect of explainability on the association of FAT
and trust. Bootstrapping techniques can be used when examining mediation
to gain confidence limits for specific indirect effects. Variance
accounted for (VAF) is to evaluate the indirect effect and the value of
greater than 80% is full mediation, while greater than 20% but less than
80% is partial mediation (per Hair et al., 2013). The 95% confidence
interval for the indirect effect via trust was obtained using
bootstrapped resampling. Mediation is confirmed if such a confidence
interval does not contain zero. The standardized indirect effect (Table
2) shows that exogenous latent constructs have partial mediation effects
toward emotion through trust. The partial mediations observed in the
test suggest that there are other potential mediators for trust or
emotion.

The findings suggest that [users trust AI recommendations as much as
they understand FATE.]{.underline} The findings of the study provide
evidence of the FATE model of human-AI interaction. The model
illustrates that [interacting with algorithms involves a series of
interrelated cognitive processes wherein features of algorithms are used
to formulate a heuristic of user motivation and to trigger action in AI
services.]{.underline} The findings of this study offer interesting
insights on the relationships between heuristics, quality, and trust in
algorithm. First, using the HSM as a primary framework, this study
examined how [algorithmic features influenced users' trust and emotion
through two different routes of cognitive processing. Users' heuristic
process of FATE influences user trust and increased trust influence
systematic processing of performance expectancy, which is positively
associated with emotion and satisfaction. Just as transparency,
accountability, fairness, and explainability are considered critical
values in social systems, so too are they in algorithm-based
services]{.underline}. [Not only do qualities of FATE play a significant
role in establishing trust, but they also play an anchoring role in
developing user evaluation of performance as well as perceived
usefulness and convenience of using AI. The model shows that users rely
on FATE as heuristic tools to assess trust in algorithm.]{.underline}
The model shows a dual process: Heuristic process is less resource
demanding and less analytical as users normally do not have expertise to
evaluate specialized algorithmic features, whereas systematic process is
more effortful and more deliberate (whether personalized results are
accurate, precise, and private) based on the established trust.
Algorithm users develop their own processes of algorithmic trust based
on cognitive processes related to FATE. User reactions to perceived
usefulness and convenience are not automatic; rather, they are dependent
upon or at least closely related to how users recognize, understand and
process the information regarding FATE. Such a relationship can be
described as heuristic insofar as users rely on FATE to determine their
feelings on usefulness and convenience around algorithm services. In
other words, users figure out usefulness and convenience in the context
of algorithm according to the FATE of content. This finding is in line
with the arguments of previous studies, which have shown the contextual
nature of such variables (Shin & Park,
[[2019]{.underline}](https://www-tandfonline-com.proxy.lib.umich.edu/doi/full/10.1080/08838151.2020.1843357)).
Second, the findings suggest the significant role of trust in
human-AI-interaction. With the pervasive role of algorithms in our
lives, a question is how people trust an algorithm's decision? How trust
is formed and evolves in the course of adoption may provide important
clues in designing and developing AI services. [More and more people are
aware that algorithms are not neutral and concern is rising that AI may
have human prejudices. People would like to understand how algorithms
work, how the processes work, and to what extent the results are fair.
The model in this study shows a clue on how trust is gained and
sustained and with what factors. Trust plays a liaison role by linking
uncertain issues to measurable values and establishing confirmation for
usage and adoption of personalized algorithm]{.underline}s. It\'s worth
noting that the model without trust mediation shows similar results but
is less and powerful than the initial model with trust. Without this
trust factor, overall paths remain significant but less robust with
lower C.R. values. This signifies the role of trust as intermediate
states that foster the development of engagement. Although previous
research consistently has shown the role of trust in AI and algorithm
(e.g., Alexander et al.,
[[2018]{.underline}](https://www-tandfonline-com.proxy.lib.umich.edu/doi/full/10.1080/08838151.2020.1843357)),
this study empirically proves the role of trust in AI, its antecedents,
intermediating role, and heuristic-systematic process. In algorithms,
users get a sense of trust when they are assured with the level of FATE.
When users trust algorithm systems, they tend to believe that system
services are useful and "convenient" just like overriding effects (Shin
& Park,
[[2019]{.underline}](https://www-tandfonline-com.proxy.lib.umich.edu/doi/full/10.1080/08838151.2020.1843357)).
The mediating role of trust between satisfaction and FATE supports the
liaison role of trust in algorithmic processes: linking heuristic and
system evaluation (Ferrario et al.,
[[2020]{.underline}](https://www-tandfonline-com.proxy.lib.umich.edu/doi/full/10.1080/08838151.2020.1843357)).
Trust significantly mediates the effects of FATE on users' satisfaction.
Satisfaction promotes trust, and in turn, influences user perception of
FATE. Higher satisfaction implies greater trust and suggests that users
are more likely to continue to use an algorithm. [Affording more user
trust and assured emotion may warrant users that their individual data
will be used by legitimate and transparent processes, thereby generating
positive trust toward the AIs and the providers, ultimately leading to
heightened levels of satisfaction.]{.underline} [Previous research
findings have confirmed the mediating role of trust in diverse contexts
(Zhang et al.,
[2014](https://www-tandfonline-com.proxy.lib.umich.edu/doi/full/10.1080/08838151.2020.1843357)).
Based on the mediating role, it can be inferred **that trust between
users and algorithms is the underlying key factor in acceptance and
experience of AI.**]{.underline}

#### Gaining Public Trust through clear technical standards key to effective AI usage and implementation 

**Vincent 19** (Brandi, Defense Technology Correspondent at NextGov,
Washington Bureau Desk Assistant at NBC News, Bachelors Degree at
Northwestern State University, "Building the Public's Trust in AI Key to
Coming Guidance, White House Official Says", 10/16/2019,
https://www.nextgov.com/emerging-tech/2019/10/building-publics-trust-ai-key-coming-guidance-white-house-official-says/160652/)

The White House Office of Science and Technology Policy's Assistant
Director for Artificial Intelligence offered [fresh details Wednesday
into a memo being developed to help foster public trust and build
agencies' confidence in regulating artificial intelligence
technologies.  ]{.underline}"This is a [memo directed to agencies that
suggests regulatory and non-regulatory principles for how you oversee
the use of AI in the private sector]{.underline}," Lynne Parker, OSTP's
assistant director for artificial intelligence, said. "[So these will
establish some common principles \[and\] some predictability across
agencies in terms of how they think about regulatory and non-regulatory
approaches to the use of AI."]{.underline} In February, President
[Trump issued an executive order to accelerate American advancements in
AI. One of the key priorities of the order, Parker noted, is to "foster
public trust and confidence in AI technologies and protect civil
liberties, privacy, and American values in their application in order to
fully realize the potential of AI technologies for the American
people."]{.underline}"The question is---how do you implement trust and
confidence?" she asked.  Late last month, Federal Chief Technology
Officer Michael Kratsios initially announced the memo. [Implementing
advanced technological solutions will require modern regulatory
approaches,]{.underline} and Kratsios noted that it will be the first
document that has "legal force" around how agencies should go about
regulating AI tech. The memorandum is currently in the works through
"close coordination" with the Office of Management and Budget's
Information and Regulatory Affairs office. The in-the-making memo marks
one of the key efforts the administration is embarking on to address
issues around worries the general public has around adopting AI. Parker
said the memo's crafters are taking a risk-based approach and [thinking
of AI not as a single monolithic concept, but more unique in terms of
how it works for each specific application domain]{.underline}. She
added that some application domains don't need as many approaches as
others, depending on whether they raise more concerns around
deployment. Though Kratsios and Parker [did]{.underline} not offer up a
timeline, the assistant director of AI said once it's crafted in draft
format, the memo will be released for the public to weigh in. [Parker
noted that that element is critical as the team wholeheartedly aims to
"get it right." Once the input is taken into account, a final memo will
be released. ]{.underline}"After that, agencies will be directed to come
up with their own plans for their own regulatory space, for how they
want to ensure the appropriate regulatory and non-regulatory approaches
for AI within the user application domains that they have oversight in,"
she said. [The order also calls for the establishment of AI technical
standards, and efforts to limit the barriers around testing the
technology and accelerating adoption. Technical standards enable
interoperability across AI systems.]{.underline} Parker noted that
[putting them in place would support the measurement of the system's
performance, accuracy, robustness and trustworthiness. In support of the
administration's initiative, the National Institute of Standards and
Technology recently instituted a plan for federal engagement to develop
the necessary guidelines. ]{.underline}"On the one hand, we say we want
AI that's trustworthy, but on the other hand, we have no way of knowing
how to achieve it---because we don't know the standard for
trustworthiness," Parker said. "[So these technical standards are
critically important]{.underline}."Both the memo and the establishment
of standards will support those deploying AI on the frontline in
measuring bias and addressing concerns around it. Parker said the memo's
approach will allow agencies to consider use cases that present
implications of bias.  She added proper tools must be developed to
determine whether training data for machine-learning systems is
appropriately representative for specific use cases."We also have to
make sure that we are comparing systems to the current state, and the
current state is that people are making decisions, and often, people are
biased," Parker said. "So, we don't want to hold AI systems to an
unreasonable level of perfection when we know that the AI systems can do
better than that current state."

## solvency

### 2ac -- NATO key

#### NATO is specifically key to setting norms on AI

**Ehlert 21** (Dr Ulf Ehlert, 12-16-2021, accessed on 7-9-2022, Nato
Review, \"NATO Review - Why our values should drive our technology
choices\",
https://www.nato.int/docu/review/articles/2021/12/16/why-our-values-should-drive-our-technology-choices/index.html)

[We must strive **to limit potential harm without unduly constraining
the benefits a technology can bring**]{.underline}. Therefore, our
policies should set limits for the application of technologies (such as
genetically optimised super-soldiers) rather than banning entire
technology areas (in this case, biotechnology). We need to understand
when policy changes are necessary and what those changes should be.
Reflecting the diversity of interests, we need to institutionalise a
broad stakeholder engagement that reaches out to all parties affected by
a technology and influencing its evolution. [Within this broadly
applicable framing, **NATO's role is specific.** As the international
organisation committed to defence and security in the North Atlantic
area, it convenes considerable political, military, economic, and
technological power. Building in particular on its political and
intellectual capital, the Alliance can credibly spearhead **norm setting
for technology applications in defence to comply with Western
values.**]{.underline} With its recently published AI Strategy, NATO
fulfils its traditional role in an innovative way. This Strategy
embraces principles of responsible use, which express the value-driven
norms that NATO and its member nations will adhere to in the application
of AI. By making these principles public, [they set an example for other
nations to consider and potentially adopt NATO's principles. This is an
effective approach towards proposing and gradually implementing an
international norm,]{.underline} not unlike the European Union's General
Data Protection Regulation. At the same, NATO responds to the globally
distributed innovation landscape. The NATO2030 initiative highlights the
need to forge new coalitions with likeminded partners beyond the
North-Atlantic region. This broad outreach should not only extend to
governmental organisations, it should in general expand the types of
partners to collaborate with (even within Allied nations), to include
non-governmental organisations, the private sector, academia, and civil
society.

#### NATO's key for military AI

**Gray and Ertan ND** (Maggie Gray and Amy Ertan, Maggie Gray is a NATO
CCDCOE Research Intern and student at Stanford University. her research
focuses on cyber-enabled information warfare and the role of
cybersecurity and technology in the military., Amy Ertan is a NATO
CCDCOE Visiting Researcher and non-resident Cybersecurity Fellow at the
Belfer Center at Harvard University's Kennedy School of Government, No
Date, accessed on 7-13-2022, The NATO Cooperative Cyber Defence Centre
of Excellence, \"Artificial Intelligence and Autonomy in the Military:An
Overview of NATO Member States 'Strategies and Deployment\",
https://ccdcoe.org/library/publications/artificial-intelligence-and-autonomy-in-the-military-an-overview-of-nato-member-states-strategies-and-deployment/)

[**AI and autonomous systems will play an increasingly large part in
enabling future military activities**.]{.underline} [AI-enabled systems
will make warfare faster and more effective by several metrics. ML will
be especially influential, as militaries use it to improve a wide
variety of systems, including autonomous vehicles, air and missile
defence systems, ISR, and logistics support]{.underline}. NATO member
states are, to varying extents, investing and exploring AI-enabled
technology and autonomous military systems. There is an element of
pressure to this, with significant evidence that [**Russia and China are
already actively and aggressively** developing these systems. The
consequences of falling behind technologically could be catastrophic
should AI-enabled systems live up to the current expectations of
many]{.underline}. Military AI and autonomous systems should not be
underestimated, [and incremental implementations can be leveraged to
great effect. **Militaries around the world have begun integrating
AI-enabled and autonomous systems into their militaries**]{.underline},
especially in the categories discussed in this paper -- autonomous
vehicles, autonomous air and missile defence systems, data analytics,
logistics, personnel management, and healthcare. [Whenever possible,
**NATO should facilitate cooperation** and information sharing between
its members to ensure their military systems remain cutting-edge. **It
is important for NATO countries to work together** to ensure that their
military systems are interoperable and secure.]{.underline} When
developing AI-enabled and autonomous systems, it is imperative that
militaries consider security. AI systems are brittle, opaque, and
reliant on good data, and any failure in an AI military system could
have catastrophic consequences.

### 2ac -- solvency -- clarity 

#### Lack of clarity and advancements in AI make Russian deterrence more difficult 

**O\'Hanlon, 18** (Michael E. O\'Hanlon, Michael E. O\'Hanlon is a
senior fellow and director of research in Foreign Policy at the
Brookings Institution, where he specializes in U.S. defense strategy,
the use of military force, and American national security policy.,
11-29-2018, accessed on 7-15-2022, Brookings, \"The role of AI in future
warfare\", https://www.brookings.edu/research/ai-and-future-warfare/)

A hypothetical scenario in which Russia creates a pretext to slice off a
piece of an eastern Baltic state, occupying it in purported "defense" of
native Russian speakers there, [could cause enormous problems if NATO
chose to reverse the aggression.]{.underline} In that event, it could
require a massive deployment of Operation Desert Storm-like proportions
to liberate the territory while facing down any Russian reinforcements
that might be sent. In a less successful case, Russia could interdict
major elements of that attempted NATO deployment through some
combination of cyberattacks, high-altitude nuclear bursts causing
electromagnetic pulse, targeted missile or aerial strikes on ports and
major ships, and perhaps even an "escalate to de-escalate" series of
carefully chosen nuclear detonations against very specific targets on
land or
sea.[\[1\]](https://www.brookings.edu/research/ai-and-future-warfare/#footnote-1)
While the latter concept of nuclear preemption is not formally part of
Russian military doctrine, it could influence actual Russian military
options
today.[\[2\]](https://www.brookings.edu/research/ai-and-future-warfare/#footnote-2)
Alternatively, the NATO deployment could succeed, only to face
subsequent Russian nuclear strikes once evidence of NATO's conventional
superiority on the Baltic battlefields had presented Moscow with the
Hobson's choice of either escalating or
losing.[\[3\]](https://www.brookings.edu/research/ai-and-future-warfare/#footnote-3)
By 2040, some aspects of this kind of scenario could improve for
American and NATO interests. The **[clarity]{.underline}** [and perhaps
the scale of NATO's security commitments to the Baltic states might have
strengthened, reducing the chances of deterrence failure in the first
place and improving the initial capacity for resistance to any Russian
aggression]{.underline}.[\[4\]](https://www.brookings.edu/research/ai-and-future-warfare/#footnote-4)
B**[ut on balance, technological innovation, including advancements in
robotics and AI, makes it quite possible that things could also get
worse.]{.underline}**

### 2ac -- solvency -- cooperation 

#### International cooperation on clarity is [key]{.underline}

**Christie and Stanley-Lockman, 21 (**Edward Hunter Christie and Zoe
Stanley-Lockman, Researcher, consultant, economist, EU affairs
professional, former NATO official, public policy expert., ​Zoe
Stanley-Lockman is an Associate Research Fellow in the Military
Transformations Programme at the Institute of Defence and Strategic
Studies at the S. Rajaratnam School of International Studies in
Singapore. Her research interests are in the areas of defence
innovation, emerging technologies, defence industries, and military
capability development. Previously she worked as a defence analyst at
the European Union Institute for Security Studies., 10-25-2021, accessed
on 7-13-2022, Nato Review, \"NATO Review - An Artificial Intelligence
Strategy for NATO\",
https://www.nato.int/docu/review/articles/2021/10/25/an-artificial-intelligence-strategy-for-nato/index.html)

[Adopting AI in the defence and security context also calls
for]{.underline} effective and responsible governance, in line with the
common values and international commitments of Allied nations. To that
end, [Allied governments]{.underline} have committed to [as a key
component of NATO's AI Strategy.]{.underline} These enduring principles
are also foundational to the discussion and adoption of more detailed
best practices and standards. Allies and NATO can leverage NATO's
consultative mechanisms and NATO's specialised staff and facilities to
work actively towards that goal. NATO's own standardisation and
certification efforts can also be bolstered by coherence with relevant
international standard-setting bodies, including for civilian AI
standards. In addition to best practices and standards, these principles
can also be operationalised via other mechanisms including review
methodologies, risk and impact assessments, and security certification
requirements like threat analysis frameworks and audits, among others.
[Further, NATO's cooperative activities provide the basis to test,
evaluate, validate, and verify (TEVV) AI-enabled capabilities in various
different contexts. More specifically, NATO's experience not only in
operations, but also in **trials, exercises, and** experimentation
provide several avenues in which Allies and NATO can test principles
against intended use cases.]{.underline} This is further reinforced by
NATO's scientific and technical communities, which have worked on issues
such as trust, human-machine and machine-machine interactions, and
human-systems integration, among many others. In addition to these
existing activities, t[he implementation of the AI Strategy will also
benefit from connections with NATO's forthcoming Defence Innovation
Accelerator for the North Atlantic (DIANA). Allied Test Centres
affiliated with DIANA could be used to fulfil the aims set out in the
definitions of the principles. In the future, use of these Test Centres
can help ensure that AI adoption and integration are tested for
robustness and resilience. For example, to ensure that AI is Traceable,
Reliable and Bias-mitigating, Test Centres could synthesise how AI
systems perform in different simulated environments and on different
testing data, or provide independent validation and verification to
assess compliance with standards that focus on responsible engineering
practices.]{.underline} Through the adoption of principles of
responsible use, NATO and Allies are sending a deliberately public
message to their domestic populations, to Allied forces, and to other
states, reiterating the Alliance's enduring values and commitments under
international law. More than just an obligation, this democratic
commitment is also a pre-condition for common policy bases among Allies
-- and for partnership with non-traditional innovators across the
Alliance. Accelerating principled and interoperable adoption With the
ethical aspects of adoption that the principles underscore, [**NATO has
the chance to signal** -- and follow through on -- responsibility at the
core of its outreach efforts. This includes engagement with start-ups,
innovative small and medium enterprises, and academic researchers that
either have not considered working on defence and security solutions, or
simply find the adoption pathways too slow or restrictive for their
business models.]{.underline} In contrast to the development of
traditional military platforms, AI integration entails fast refresh
cycles and requires constant upgrading. This requires a change of
mind-set for iterative, adaptive capability development, in contrast to
sequential development cycles that take years to deliver small numbers
of highly sophisticated platforms. With hostile state and non-state
actors increasing their investments in Emerging and Disruptive
Technologies including AI, this more flexible approach to adoption is
all the more urgent. In this context, with its focus on TEVV and
collaborative activities, the AI Strategy sets the framework for
technological enablers to out-adapt competitors and adversaries. With
more of a focus on agility and adaptation, NATO can make defence and
security a more attractive sector for civilian innovators to partner
with, while also allowing them to maintain other commercial
opportunities. In doing so, efforts to bolster the transatlantic
innovation ecosystem can also serve as a bulwark against undesirable
foreign investment and technology transfers. [NATO's experience not only
in operations, but also in trials, **exercises**]{.underline}**,** and
experimentation provide several avenues in which Allies and NATO can
test principles against intended use cases. This is further reinforced
by NATO's scientific and technical communities, which have worked on
issues such as trust, human-machine and machine-machine interactions,
and human-systems integration, among many others. Pictured: U.S. ground
troops patrol while robots carry their equipment and drones serve as
spotters. Illustration by U.S. Army This work requires coordination
across the NATO Enterprise. Indeed, several stakeholders across the NATO
Enterprise are already involved in the development of AI-related use
cases, concepts, and programmes. With the AI Strategy, these activities
can gain coherence to ensure the proper connections exist between all
innovation stakeholders, including operational end-users. Moving Ahead
To be sure, the implementation of accelerated, principled, and
interoperable AI adoption depends not just on technology, but equally on
the talented and empowered people who drive the technological
state-of-the-art and integration forward. [NATO has also dedicated
attention to other AI inputs, notably through the development of a NATO
Data Exploitation Framework Policy. With actions to treat data as a
strategic asset, develop analytical tools, and store and manage data in
the appropriate infrastructure, the Data Exploitation Framework Policy
sets the conditions for the AI Strategy's succes]{.underline}s. In
addition to the interrelationships between data and AI, [**ensuring
coherence between NATO's efforts on AI** and other Emerging and
Disruptive Technologies such as autonomy, biotechnology, and quantum
computing will be vital. As Allies and NATO seek to fulfil the aim of
this AI Strategy, the linkages between responsible use, accelerated
adoption, interoperability, and safeguarding against threats are
critical.]{.underline} Indeed, these linkages will also apply to NATO's
follow-on work on other Emerging and Disruptive Technologies, including
the development of principles of responsible use. [More broadly, this
entails further coherence between the work strands on these
technologies, understanding that NATO's future technological edge -- and
threats the Alliance will face -- **may depend on their
convergence.**]{.underline} As such, not only does the NATO AI Strategy
apply to this foundational technology, but it also sets the stage for
NATO's and Allies' ambitions with regards to other Emerging and
Disruptive Technologies. For each of them, the future strategic
advantage that comes with NATO innovation efforts will derive from the
connections between ethical leadership, iterative adoption, and
integration that prizes flexibility, interoperability, and trust.

### 2ac -- solvency -- digital authoritarianism

#### NATO's key to AI norms to counter digital authoritarianism 

**Margarita and Nurkin, 22** (Konaev Margarita and Tate Nurkin,
Margarita Konaev is a nonresident senior fellow in the Forward Defense
practice of the Atlantic Council's Scowcroft Center for Strategy and
Security. Konaev's research on international security, armed conflict,
non-state actors and urban warfare in the Middle East, Russia, and
Eurasia., Tate Nurkin is the founder of OTH Intelligence Group and a
nonresident senior fellow with the Scowcroft Center for Strategy and
Security at the Atlantic Council.Substantively, Mr. Nurkin's research
and analysis has a particularly strong focus on US-China competition,
defense technology, the future of military capabilities, and the global
defense industry and its market issues., 5-25-2022, accessed on
7-13-2022, Atlantic Council, \"Eye to eye in AI: Developing artificial
intelligence for national security and defense\",
https://www.atlanticcouncil.org/in-depth-research-reports/report/eye-to-eye-in-ai/#accelerating-dod)

Moreover, the DoD should also leverage the under secretary of defense
for research and engineering's (USDR&E) testing practices and priorities
to ensure planned and deployed AI systems are hardened against adversary
attacks, including data pollution and algorithm corruption.

[The DoD should leverage allies and foreign partners to develop, deploy,
and adopt trusted AI. **Engagement of this nature is vital** for
coordination **on common norms for AI development** and use **that
contain and counter China and Russia's authoritarian technology
models**. Pathways for expanding existing cooperation modes and building
new partnerships can include the following.]{.underline}

[Cross-sharing and implementing joint ethics programs
**with**]{.underline} Five Eyes, [NATO**,**]{.underline} and AUKUS
partners**[.]{.underline}85** [In addition to supporting
interoperability, this **will add a diversity of perspectives** and
experiences, as well as help to ensure that AI development efforts
**limit various forms of bias.** As one former general officer
interviewed for this project noted, "diversity is how we ensure
reliability. It is essential."]{.underline}**86**

[Broadening outreach to allies and partners of varying capabilities and
geographies, including India, South Africa, Vietnam, and Taiwan, to
explore opportunities for bilateral and multilateral
research-and-development efforts and technology-sharing programs that
address the technical attributes of trusted and responsible
AI.]{.underline}87

#### Lack of AI transparency undermines public trust 

**Putoni et al. 21** (Stefano, Behavioral scientist at the Rotterdam
School of Management, Erasmus University, Director of the Psychology of
AI Lab at the Erasmus Centre for Data Analytics, Rebecca Walker Reczek,
[Berry Chair of New Technologies in Marketing,]{.mark} Markus Giesler,
Consumer researcher and Professor of Marketing at the Schulich School of
Business, Simona Botti, PhD in philosophy, "Consumers and Artificial
Intelligence: An Experiential Perspective") Jan 2021
https://web-s-ebscohost-com.proxy.lib.umich.edu/ehost/detail/detail?vid=2&sid=fac31364-8601-4729-91f3-c78d18a8b3b0%40redis&bdata=JnNpdGU9ZWhvc3QtbGl2ZSZzY29wZT1zaXRl#AN=147580050&db=bth
acd

[The listening capability enables AI systems to collect data about
consumers and the environment in which they live. We conceptualize the
resulting experience as \"data capture,\" which includes the different
ways in which data are transferred to the AI]{.underline}. Data can be
intentionally provided by consumers, albeit with different degrees of
understanding of the process: [consumers share data when there is little
or no uncertainty about how the data will be used and by whom, or
consumers surrender data when this uncertainty is high]{.underline}
(\[147\]). Data can also be obtained by AI from the \"shadows\"
consumers leave behind when they engage in daily activities, as in the
case of a shopper perusing a store equipped with facial recognition
technology or of an iRobot Roomba creating a map of a residential space
(\[89\]). The data capture experience provides benefits to consumers
because it can make them feel as if they are served by the AI: the
provision of personal data allows consumers access to customized
services, information, and entertainment, often for free. For example,
consumers who install the Google Photos app let Google capture their
memories but in return get an AI-powered assistant that suggests
context-sensitive actions when viewing photos. Access to customized
services also implies that consumers can enjoy the outcome of decisions
made by digital assistants, which effectively match personal preferences
with available options without having to endure the cognitive and
affective fatigue that decision making can entail (\[
[[4]{.underline}](https://web-s-ebscohost-com.proxy.lib.umich.edu/ehost/detail/detail?vid=2&sid=fac31364-8601-4729-91f3-c78d18a8b3b0%40redis&bdata=JnNpdGU9ZWhvc3QtbGl2ZSZzY29wZT1zaXRl#bib4)\]).
Finally, access to customized services offers unprecedented
opportunities for self-improvement. Consider one of the projects within
Alphabet, in which data from smartphones, genomes, wearables, and
ambient sensors are combined to drive personalized health care
(\[86\]).[Despite AI\'s ability to predict and satisfy preferences,
consumers can feel exploited in data capture experiences, mainly because
they do not understand AI\'s operating criteria. This can be attributed
to several features of AI. First, the modalities of data acquisition are
becoming increasingly intrusive and difficult to avoid. Second, even
when consumers intentionally share information, they are not aware of
how this information is aggregated over time and across contexts.
Finally, data brokers are largely unregulated and often lack
transparency and accountability (\[59\]). As a result, data capture
experiences may threaten consumers\' ownership of personal data and
challenge personal control,]{.underline} that is, the feeling that
events are determined by the self rather than by others or by external
forces and can be stirred toward desired outcomes (\[36\]). We examine
the consequences of this loss of control next from both a sociological
and psychological perspective. In popular culture, [lack of ownership
over personal data has been frequently associated with a loss of
personal control stemming from technology\'s threatening potential to
enable monitoring of human behavior.]{.underline} Stories such as George
Orwell\'s 1984 or Philip K. Dick\'s Minority Report envision systems of
oppression in which, due to lack of privacy and constant surveillance,
people can no longer control their destiny. [This dystopian imagination
is echoed in sociological scholarship that associates data capture with
the rise of a capitalist marketplace in which private information
becomes the central form of capital]{.underline} (\[157\]).Such
dystopian concerns strike a resonant chord when considering Google\'s
move in the early 2000s to transform consumer data from a by-product
into an economic asset that formed the basis of a new type of commerce
driven by the ability to colonize the consumer\'s private experience.
[This commerce contributes to a surveillance marketplace, in which data
surplus is \"fed into advanced manufacturing processes known as
\'machine intelligence\' and fabricated into prediction products that
anticipate what you will do now, soon, and later\"]{.underline}
(\[157\], p. 14, italics in the original). To illustrate the power of
this commerce, targeted ads based on personality characteristics
inferred from the analysis of Facebook likes in combination with online
survey questions can increase conversion rates by about 50% (\[102\]).
In 2018, Facebook\'s revenues from the sales of such tailored ads was
close to \$56 billion (\[111\]). From the perspective of this narrative,
not only are technology companies continually required to find new ways
to make monitoring and surveillance palatable to consumers by linking it
to convenience, productivity, safety, or health and well-being (\[10\]),
but they must also constantly push the boundaries of what private
information consumers should share (\[55\]) through a complex landscape
of notifications, reminders, and nudges intended to initiate behavioral
change. Thus, as consumer behavior becomes increasingly retailored to
the exigencies of behavioral futures, AI can transform consumers into
subjects who are complicit in the commercial exploitation of their own
private experience, thereby undermining personal control and promoting
the concentration of knowledge and power in the hands of those who own
their information. Data capture experiences are characterized by an
underlying tension: c[onsumers recognize that data capture allows AI to
serve them through customization, but AI\'s inherent lack of
transparency makes them feel exploited. These feelings of exploitation
are fueled by actual and perceived loss of personal control, with
important psychological consequences (\[17\]). The first of such
consequences is negative affect, which can turn into demotivation and
helplessness.]{.underline} Consider the case of Leila, a sex worker who
shielded her identity on her Facebook account and reported being shocked
to see some of her regular clients recommended by the \"People You May
Know\" function. According to Leila, \"the worst nightmare of sex
workers is to have your real name out there, and Facebook connecting
people like this is the harbinger of that nightmare.\" For Leila, like
for domestic violence victims or political activists, privacy invasion
is not only frightening, it may become a matter of life, death, or time
in jail (\[73\]). [As being in control is a basic need and a
precondition of psychological welfare (\[93\]), the second consequence
of loss of personal control may be moral outrage.]{.underline} Consider
the case of a German consumer who requested his own data from Amazon and
received transcripts of Alexa\'s interpretations of voice commands, even
though he did not own any Alexa devices. The consumer relayed his story
to a local magazine, which attempted to identify the consumer whose
privacy had been compromised. The magazine staff involved in this
experience described it as follows: \"\[we were able to\] navigate
around a complete stranger\'s private life without his knowledge, and
the immoral, almost voyeuristic nature of what we were doing got our
hair standing on end\" (\[24\]). [The third consequence of loss of
personal control relevant to data capture experiences is psychological
reactance, a state in which a person is motivated to restore control
after a restriction (\[21\]), which causes more negative evaluations of
and hostile behaviors toward the source of the restriction. In
marketing, reactance can decrease the likelihood to repurchase and
follow recommendations]{.underline} (\[48\]). Illustrating reactance in
AI data capture experience is Danielle, a U.S. consumer who installed
Echo devices throughout her home, believing Amazon\'s claims that they
would not invade her privacy. When one of her Alexas recorded a private
conversation and sent it to a random number in her address book,
Danielle said \"I felt invaded\" and concluded, \"I\'m never plugging
that device in again, because I can\'t trust it\" (\[76\]). [In summary,
consumers may experience data capture as a form of exploitation: whereas
technology companies, firms, and governmental agencies gain financial
and political power, consumers lose ownership of their data and feel a
loss of control over their lives.]{.underline} As we discuss next,
managers should gain a better understanding of feelings of exploitation,
as they prevent consumers from seeing the value firms can provide
through data capture. This understanding starts at the organizational
level and is then translated into decisions about experience design. [A
central programmatic task in addressing the issue of consumer
exploitation in AI data capture experiences involves determining and
enhancing the organization\'s level of awareness regarding the
sociological and psychological costs raised in the previous
sections.]{.underline} Companies should strive toward greater
organizational sensitivity around consumer privacy and the current
asymmetry in the level of control over personal data. For instance, they
should use netnographic observation or sentiment analysis to listen
empathetically and at scale to consumers who have experienced
exploitation in AI data capture experiences. Furthermore, rather than
accepting the surveillance society narrative at face value, firms can
use these tools to understand when, how, and whether their own data
capture experiences play into versus subvert this narrative. Likewise,
companies should draw on insights by privacy scholars and activist
movements to question their taken-for-granted beliefs. In doing so, for
instance, companies could realize that their own view on privacy default
settings might differ markedly from that of a vulnerable consumer group
and adjust their processes accordingly (\[101\]).

### 2ac -- solvency -- risk assessment 

#### Opening the "Black Box" of AI is key to creating proper regulations & risk assessment

**Buiten 19** (Miriam, CERRE Research Fellow and Assistant Professor of
Law and Economics at the University of St.Gallen, Switzerland, "Towards
Intelligent Regulation of AI, Published 4/29/2019, Accessed 7/15/2022,
https://www.cambridge.org/core/journals/european-journal-of-risk-regulation/article/towards-intelligent-regulation-of-artificial-intelligence/AF1AD1940B70DB88D2B24202EE933F1B,
EA)

The concern of AI is that it presents new and unknown risks with which
current laws may not be able to cope. We may thus need new rules to
mitigate the risks of AI. [One proposal is to introduce transparency
requirements for AI.29 Such a transparency requirement would be
difficult to implement in practice, if we are not sure what its scope
is]{.underline}. The lack of a clearly defined scope would leave the
industry uncertain whether its activities are covered by the regulation,
and would leave the definition of AI to the courts. The definitions of
AI discussed above are, however, context-sensitive and time-varying in
character. [Take the approach of defining AI in terms of a system's
autonomy. This raises the question of what it means for an application
to act autonomously.]{.underline} Scherer uses the example of autonomous
cars and automated financial advisers that can perform complex 24 Oxford
Dictionaries, "Artificial Intelligence" accessed 22 October 2018. 25 P
Stone et al, "Artificial Intelligence and Life in 2030: One Hundred Year
Study on Artificial Intelligence" (2016) accessed 20 September 2018. 26
C Reed, "How should we regulate artificial intelligence?" (2018) Phil
Trans R Soc A 1. 27 R Calo, "Robotics and the Lessons of Cyberlaw"
(2015) 103 Cal L Rev 513, 529. 28 JM Balkin, "The Path of Robotics Law"
(2015) 6 Cal L Rev 45, 51. 29 See eg European Parliament, supra, note
11, and White House Report, supra, note 8. 2019 Towards Intelligent
Regulation of Artificial Intelligence 45
https://doi.org/10.1017/err.2019.8 Published online by Cambridge
University Press tasks "without active human control or even
supervision". 30 Let us consider the example of cars. Most would agree
that a regular car is not autonomous, but some would say, like Scherer,
that a self-driving car is. What about cars that have some
autonomous-driving features, such as automatic distance keeping? Do they
have sufficient autonomy to be considered AI, and should this
distinction have any legal implications? If we were to require more
transparency from AI, what would be the implications for producers of
regular cars, partly autonomous and fully autonomous cars? Who would be
subject to the requirement, and what would the requirement entail
concretely? [The problem]{.underline}, of course, [would not be
limited]{.underline} to cars, [but to all systems that may be called AI.
Considerable uncertainty would ensue on which systems and applications
fall within the scope of such a rule. Using intelligence to define AI
presents similar problems. Capabilities we consider intelligent today
may not be seen as such in the future.]{.underline} The AI literature
from the 1980s is illustrative, referring to a system's ability to learn
from experience as intelligent.31 Systems capable of playing chess, for
instance, were seen as AI systems.32 Today, chess is considered as a
relatively easy problem for a computer, given that the rules are known
and the number of moves is finite. In 1996, IBM chess computer Deep Blue
defeated the then world champion Gary Kasparov.33 In essence, while AI
is sometimes defined as the capability of machines to perform tasks that
require intelligence, our conception of what constitutes intelligent may
change as machines acquire more capabilities. After all, how can the
task require intelligence if a machine can do it?34 [Even if we overcome
the problem of varying definitions over time, intelligence may be viewed
very differently depending on whom you ask.]{.underline} In 1982, John
Searle essentially took the position that any technology that is
understandable is not intelligent. Computers may seem to understand
concepts but actually do not, as they attach no meaning or
interpretation to any of it.35 On the other end of the spectrum are the
views discussed above of intelligent behaviour as mimicking intelligent
human behaviour, or being perceived as intelligent by humans.36 [The
range of applications that would be considered AI would vary greatly
depending on which approach is taken.]{.underline} In some instances,
features of AI such as autonomy or intelligence may help guide us in
incorporating AI in our laws.37 When asking how laws should respond to
AI, however, I propose to take a step back and demystify this concept.
The narrative of AI as an inscrutable concept may reinforce the idea
that AI is an uncontrollable force shaping our societies. [In essence,
part of the problem is that we cannot control what we do not 30
understand. I therefore propose to "open the black box" of AI and try to
identify the risks and "unknowns". This is not to say that AI does not
present risks, or even that all of these risks are knowable now. In
fact, the majority of the policy challenges regarding AI boil down to
the need to mitigate risks that are unknown or difficult to control. It
is clear that the question of how to address these risks deserves
attention]{.underline}. A first step in doing so is to make these risks
concrete, by considering the underlying computational systems.
[Notwithstanding AI being difficult to understand even for experts, it
is a misconception that AI is completely unknowable. AI is a product of
human invention, and humans continuously improve upon its underlying
technology, algorithms.]{.underline} Algorithms are the basis of the
wide array of AI applications, which may vary widely in their
sophistication and purpose.38 Since the technology of algorithms is what
AI applications have in common[, I propose that we focus the policy
debate on this technology, starting by better understanding the risks
associated with algorithms. Understanding these risks may be possible
without full comprehension of the technology, allowing us to grasp the
risks and flaws that advanced automated systems present.]{.underline}

### 2ac -- solvency -- dod key 

#### US needs dod ai leadership/ai in military 

**NSCAI 21** "final report" National Security Commission on Artificial
Intelligence: Eric Schmidt (Chair) Safra Catz, Steve Chien, Mignon
Clyburn, Chris Darby, Kenneth Ford, José-Marie Griffiths, Robert Work
(Vice Chair), Eric Horvitz, Andrew Jassy, Gilman Louie, William Mark,
Jason Matheny, Katharina McFarland, Andrew Moore
https://www.nscai.gov/wp-content/uploads/2021/03/Full-Report-Digital-1.pdf 

Even with the right artificial intelligence (AI)-ready technology
foundations in place, [**the U.S. military will** still **be at a
battlefield disadvantage if it fails to** adopt the right concepts and
operations to **integrate AI technologies.**]{.underline} Throughout
history, the best adopters and integrators, rather than the best
technologists, have reaped the military rewards of new technology.1
[**The** Department of Defense **(DoD) should not be** a **witness to
the AI revolution** in military affairs, **but** should **deliver** it
with **leadership from the top, new operating concepts, relentless
experimentation, and** a system that rewards **agility and
risk.**]{.underline} A new warfighting paradigm is emerging because of
AI. [Our **competitors are making** substantial **investments to take
advantage of** it. This idea has been called "**algorithmic" or**
"mosaic" warfare2; China's theorists have **called it "intelligentized"
war**.3 All of these terms capture, in various ways, how **a new era of
conflict will** be dominated by AI and **pit algorithms against
algorithms.**]{.underline} Advantage will be determined by the amount
and quality of a military's data, the algorithms it develops, the
AI-enabled networks it connects, the AIenabled weapons it fields, and
the AI-enabled operating concepts it embraces to create new ways of war.
Today's DoD is trying to execute an AI pivot, but without urgency.
Despite pockets of imaginative reform and a few farsighted leaders,
**[DoD]{.underline}** remains locked in an Industrial Age mentality in
which great-power conflict is seen as a contest of massed forces and
monolithic platforms and systems. The emerging ubiquity of AI in the
commercial realm and the speed of digital transformation punctuate the
risk of not pivoting fast enough. [The Department **must** act now to
**integrate AI into critical functions,** existing **systems, exercises,
and wargames to be**come an **AI-ready** force by 2025. Simultaneously,
**DoD must develop** more **creative warfighting concepts** that ar**e
paired with investments in future AI**-enabled technologies **to**
continuously **out-innovate** potential **adversaries. If our forces are
not equipped with AI**-enabled systems **guided by new concepts** that
exceed those of their adversaries, **they will be** outmatched and
**paralyzed by** the **complexity of battle.To compete, deter, and,** if
necessary, **fight** and win in **future conflicts requires** wholesale
**adjustment**s **to operational concepts,**]{.underline} technologies,
organizational structures, and how we integrate allies and partners into
operations. It will also require risk-based assessments of both the
benefits and drawbacks of widespread integration of AI-enabled
capabilities, to include future autonomous weapon systems. Lastly, [**it
will require** a willingness to engage in **bilateral and multilateral
dialogues with** our **allies** and partners **to urge** them to make
similar **AI pivots to ensure** future
**interoperability.**]{.underline} CHAPTER 3 79 p How AI Will Change
Warfare. AI-enabled warfare will not hinge on a single new weapon,
technology, or operational concept; rather, it will center on the
application and integration of AI-enabled technologies into every facet
of warfighting. AI will transform the way war is conducted in every
domain from undersea to outer space, as well as in cyberspace and along
the electromagnetic spectrum. It will impact strategic decision-making,
operational concepts and planning, tactical maneuvers in the field, and
back-office support. In this new kind of warfare, traditional confines
of the battlefield will be expanded through AI-enabled micro-targeting,
disinformation, and cyber operations, as described in Chapter 1 of this
report. [**AI will reshape** many **attributes of war, such as** its
**speed,** tempo, and **scale;** the **relationships** service members
have **with machines**; the **persistence with which the battlefield can
be monitored; and** the discrimination and **precision with** which
**targets** can be attacked.]{.underline} There will be a premium on
speed and accuracy in developing knowledge, acting, and reacting as the
conflict unfolds. [AI will make the process of finding and hitting
targets of military value faster and more efficient.]{.underline} It
will also increase accuracy of target identification and minimize
collateral damage. Currently, this process generally involves passing
data in a serial fashion from a sensor, through a series of humans, to a
platform that can shoot at the target. AI will help automate some of the
intermediate stages of the decision process. [**AI will** also **create
opportunities for** more advanced **processes that** would operate more
akin to a web, fusing multiple sensors and platforms to **manage complex
data flows and transmit**ting actionable **information to human
operators and machines** across all domains.4 In war, many of the
military uses of AI will complement, rather than supplant, the role of
humans.]{.underline} AI tools will improve the way service members
perceive, understand, decide, adapt, and act in the course of their
missions. However, [new concepts for military operations will also need
to account for the changing ways in which humans will be able to
delegate increasingly complex tasks to AI-enabled systems.]{.underline}
In the near term, this will be managed through the military's principle
of "mission command," which stresses decentralized execution and
disciplined initiative by subordinates who follow a commander's intent.
This human-centric approach to fighting should remain the standard for
the foreseeable future. But as AI continues to advance into the
cognitive and neuromorphic domain, and human-machine teaming becomes
more sophisticated, the military will need to develop more imaginative
concepts and organizational constructs that take full advantage of AI
technologies without relinquishing the principles that undergird mission
command. [This list of how AI might transform warfighting principles and
capabilities---as well as others like it---is by no means exhaustive.
**Innovation will lead to future capabilities** that are **unknowable at
present** and will only become clearer in time.  ]{.underline}

# Negative

### 1nc -- regulations fail

#### It is impossible to fully and safely regulate complex AI

**Gyulai and Ujlaki 21** (Attila, Centre for Social Sciences - Institute
for Political Science / University of Public Service.  Anna, Centre for
Social Sciences -- Institute for Political Science / Corvinus University
of Budapest, "The political AI: A realist's account of AI regulation",
Információs Társadalom XXI, no. 2 (2021): 29--42,
<https://dx.doi.org/10.22503/inftars.XXI.2021.2.3>)

Recalling the realist viewpoint of the political sphere, it seems that
the only attainable goal is a modus vivendi, which resonates with the
idea that an inherent characteristic of the political world is balancing
the possibilities of two extremes. History of politics supports this
more pessimistic view: occasionally, eruptions of civil war and failed
states still embody the brute reality of the Hobbesian state of nature,
while the existence of authoritarian and totalitarian dictatures
altogether with hybrid regimes are eternal reminders of the
impossibility to limit power in a once-and-for-all manner. In light of
this reality of the political world, [new claims for the regulation of
artificial intelligence, more specifically on weak AI, are less
promising. Debates on the regulation of AI concentrate on the need to
connect principles such as fairness, accountability, safety,
sustainability, and social inclusion,]{.underline} among others, [to AI
governance]{.underline} (for a more exhaustive list, see Hagendorff
2020). Nevertheless, the most discussed issue is
[transparency,]{.underline} which [is among the primary claims for
several AI 38 ethics guidelines released]{.underline} by different
institutions and companies in the past few years. The current boom in
[ethical guidelines for AI involves several criticisms concerning the
effectiveness of such guidelines based on their potential to implement
transparency]{.underline} and other claims effectively. This line of
criticism can be divided into three types of argument. [The first type
challenges the AI guidelines on their extensive list of ethical claims
based on their ineffectiveness.]{.underline} This type, which can be
called 'tick-box criticism,' can be coupled with a proposal of some
different approach, for example, virtue ethics (see Hagendorff 2020).
The second type, which can be called ['double standard criticism', is
more sceptical about the possibility of guiding AI and whether full
transparency can be achieved at all]{.underline}. This criticism builds
on the argument that [it would be a double standard to call for higher
transparency in AI compared to human decision tools and human
reasoning]{.underline} (see Zerilli et al. 2019). The third type of
criticism is more focused, what we call 'specificity criticism', and
argues that [current Artificial Intelligence Guidelines (AIGUs) are not
specific to AI, but they are simply attempting to gain social control
over technology.]{.underline} This criticism also demonstrates that
transparency and explainability are claims that specifically concern AI
because in such cases there is a possibility of the autonomy of AI. In
that case, though, the double standard problem arises (see Héder 2020).
These criticisms imply that [there is a profoundly political
characteristic of A]{.underline}I. On the one hand, there is a relative
autonomy inherent in AI that can be understood in a broader sense. [It
is impossible to regulate in every detail, something that can develop by
itself]{.underline}. On the other hand, concerning the expert systems of
weak AI, the double standard criticism and specificity criticism
correctly acknowledged [that it would be an unfair expectation to
regulate the decision-making of artificial intelligence in domains where
human decision-making cannot be entirely regulated
likewise]{.underline}. However, contrary to the double standard
criticism, we do not base our argument on the similarity between the
obscurity of artificial decision tools and human cognitive processes.
Instead, we build our argument on the political characteristic of AI.
[Using AI as a tool is similar to political authorization: although
accountability is the main virtue in politics, it would be unrealistic
to expect legislative, executive, or judicial officials to act
'perfectly'. We can only hope that they behave to the best of their
knowledge]{.underline}, and while we usually hold them to account for
significant breaches of their power, mostly, we authorize them because
authorization is the only legitimate way to create order without
slipping into a Hobbesian state of nature or a tyrannical regime.

### 1nc -- dod fails

#### DoD needs huge reforms before it could be effective

**Konaev and Nurkin, 22** (Konaev Margarita and Tate Nurkin, Margarita
Konaev is a nonresident senior fellow in the Forward Defense practice of
the Atlantic Council's Scowcroft Center for Strategy and Security.
Konaev's research on international security, armed conflict, non-state
actors and urban warfare in the Middle East, Russia, and Eurasia., Tate
Nurkin is the founder of OTH Intelligence Group and a nonresident senior
fellow with the Scowcroft Center for Strategy and Security at the
Atlantic Council.Substantively, Mr. Nurkin's research and analysis has a
particularly strong focus on US-China competition, defense technology,
the future of military capabilities, and the global defense industry and
its market issues., 5-25-2022, accessed on 7-13-2022, Atlantic Council,
\"Eye to eye in AI: Developing artificial intelligence for national
security and defense\",
https://www.atlanticcouncil.org/in-depth-research-reports/report/eye-to-eye-in-ai/#accelerating-dod)

Currently, there are no shared technical standards for what constitutes
ethical or trustworthy AI systems, which can make it difficult for
nontraditional AI vendors to set expectations and navigate the
bureaucracy. [The DoD is not directly responsible for setting standards.
Rather, the 2021 National Defense Authorization Act (NDAA) expanded the
National Institute of Standards and Technology (NIST) mission "to
include advancing collaborative frameworks, standards, guidelines for
AI, supporting the development of a risk mitigation framework for AI
systems, and supporting the development of technical standards and
guidelines to promote trustworthy AI systems]{.underline}."7979. Pub. L.
116-283, William M. (Mac) Thornberry National Defense Authorization Act
for Fiscal Year 2021, 134 Stat. 3388 (2021),
https://www.congress.gov/116/plaws/publ283/PLAW-116publ283.pdf. In July
2021, the NIST issued a request for information from stakeholders as it
develops its AI Risk Management Framework, meant to help organizations
"incorporate trustworthiness considerations into the design,
development, use, and evaluation of AI products, services, and
systems."8080. "Summary Analysis of Responses to the NIST Artificial
Intelligence Risk Management Framework (AI RMF)---Request for
Information (RFI)," National Institute of Standards and Technology,
October 15, 2021,
https://www.nist.gov/system/files/documents/2021/10/15/AI%20RMF_RFI%20Summary%20.pdf.
[Related to standards are the challenges linked to testing, evaluation,
verification, and validation (TEVV). Testing and verification processes
are meant to "help decision-makers and operators understand and manage
the risks of developing, producing, operating, and sustaining
AI-enabling systems,"]{.underline} and are essential for building trust
in AI.8181 Michele A. Flournoy, Avril Haines, and Gabrielle Chefitz,
"Building Trust through Testing: Adapting DOD's Test & Evaluation,
Validation & Verification (TEVV) Enterprise for Machine Learning
Systems, including Deep Learning Systems," WestExec, October 2020, 3--4,
https://cset.georgetown.edu/wp-content/uploads/Building-Trust-Through-Testing.pdf.
The [**DoD's current TEVV protocols and infrastructure are meant
primarily for major defense acquisition** programs like ships,
airplanes, or tanks; it is linear, sequential, and, ultimately, finite
once the program transitions to production and deployment. **With AI
systems, however, "development is never really finished, so neither is
testing**]{.underline}."8282. Flournoy, Haines, and Chefitz, "Building
Trust through Testing," 3. Adaptive, continuously learning emerging
technologies like AI, therefore, require a more agile and iterative
development-and-testing approach---one that, as the NSCAI recommended,
"integrates testing as a continuous part of requirements specification,
development, deployment, training, and maintenance and includes run-time
monitoring of operational behavior."8383. "Final ," 384. The ethical
code that guides the US military reflects a fundamental commitment to
abiding with the laws of war at a time when authoritarian countries like
China and Russia show little regard for human rights and humanitarian
principles. Concurrently, the DoD's rigorous approach to testing and
assurance of new capabilities is designed to ensure that new weapons are
used responsibly and appropriately, and to minimize the risk from
accidents, misuse, and abuse of systems and capabilities that can have
dangerous, or even catastrophic, effects. These values and principles
that the United States shares with many of its allies and partners are a
strategic asset in the competition against authoritarian countries as
they field AI-enabled military systems. To cement the DoD's advantage in
this arena, we recommend the following steps**[.]{.underline}** The DoD
will not be able to fulfill its ambitions in AI and compete effectively
with the Chinese model of sourcing technology innovation through
military- civil fusion without close partnerships with a broad range of
technology companies. This includes defense-industry leaders with
long-standing ties to the Pentagon, technology giants at the forefront
of global innovation, commercial technology players seeking to expand
their government portfolio, and startups at the cutting edge of AI
development[**. But, the DoD's budget-planning, procurement,
acquisition, contracting, and compliance processes will likely need to
be fundamentally restructured to effectively engage with the entirety of
this vibrant and diverse technology ecosystem. Systemic change is a
slow, arduous process**. But, delaying this transition risks the US
military falling behind]{.underline} on exploiting the advantages AI
promises to deliver, from operational speed to decision dominance. In
the meantime, the following actions could help improve coordination with
industry partners to accelerate the DoD's AI adoption efforts. The DoD
should implement the NSCAI's recommendation to accelerate efforts to
train acquisition professionals on the full range of available options
for acquisition and contracting, and incentivize their use for AI and
digital technologies."88 Moreover, such acquisition- workforce training
initiatives should ensure that acquisition professionals have a
sufficient understanding of the DoD's ethical principles for AI and the
technical dimensions of trusted and responsible AI. The DIU's ethical
guidelines can serve as the foundation for this training. Rather than
building entirely new AI-enabled systems, in the short to medium term,
the DoD will be integrating AI into a range of existing software and
hardware systems---from cyberdefense architectures to fighter jets to
C2. Progress toward implementing AI will, therefore, also depend upon
streamlining collaboration between the startups and nontraditional AI
vendors that the DoD has been courting for their innovative and
cutting-edge technologies and the defense primes responsible for
integrating new capabilities into legacy systems.

### 1nc -- russia AI fails

#### Russia lags behind in AI, means they aren't a threat

**Polyakova '18** (Alina,  President and CEO of the Center for European
Policy Analysis (CEPA) as well as an adjunct professor of European
studies at the Johns Hopkins University's School of Advanced
International Studies (SAIS), "Weapons of the weak: Russia and AI-driven
asymmetric warfare, Accessed 7/9/2022,
brookings.edu/research/weapons-of-the-weak-russia-and-ai-driven-asymmetric-warfare/)

Speaking to Russian students on the first day of the school year in
September 2017, Putin squarely positioned Russia in the technological
arms race for artificial intelligence (AI). Putin's comment (see above)
signaled that, like China and the United States, Russia sees itself
engaged in direct geopolitical competition with the world's great
powers, and AI is the currency that Russia is betting on. But, [unlike
the United States]{.underline} and China**,** [Russia lags behind in
research and development on AI and other emerging technologies. Russia's
economy makes up less than 2 percent of global GDP]{.underline} compared
to 24 percent for the United States and 15 percent for China, which puts
Russia on par with a country like
Spain.[[\[3\]](https://www.brookings.edu/research/weapons-of-the-weak-russia-and-ai-driven-asymmetric-warfare/#footnote-3)
Despite Putin's focus on AI, the Russian government has not released a
strategy, like China has]{.underline}, on how the country plans to lead
in this area. The [Russian government's future investment in AI research
is unknown, but reports estimate that it spends approximately \$12.5
million a
year[\[4\]](https://www.brookings.edu/research/weapons-of-the-weak-russia-and-ai-driven-asymmetric-warfare/#footnote-4)
on AI research, putting it far behind China's plan to invest \$150
billion through 2030. The U.S. Department of Defense alone spends \$7.4
billion annually]{.underline} on unclassified research and development
on AI and related
fields.[^[\[5\]]{.underline}^](https://www.brookings.edu/research/weapons-of-the-weak-russia-and-ai-driven-asymmetric-warfare/#footnote-5)
[Russia's public corruption, decline in rule-of-law, and increasingly
oppressive government regulations have produced a poor business
environment. As a consequence, the country trails]{.underline} the
United States and China in terms of private investment, scientific
research, and the number of AI
start-ups.[^[\[6\]]{.underline}^](https://www.brookings.edu/research/weapons-of-the-weak-russia-and-ai-driven-asymmetric-warfare/#footnote-6)
In [2018, no Russian city entered the top 20 global regional hubs for
the AI
sector]{.underline},[^[\[7\]]{.underline}^](https://www.brookings.edu/research/weapons-of-the-weak-russia-and-ai-driven-asymmetric-warfare/#footnote-7)
despite the much-hyped opening of the "Skolkovo Innovation Center" in
2010, which was designed to be Russia's answer to Silicon Valley. Unlike
Silicon Valley[, Skolkovo did not spur the kind of private investments
and innovation that the Kremlin had hoped for and has since fizzled
out]{.underline}. Russia's new venture, a "technopolis" named Era, which
is set to open in the fall of 2018, now promises to be the new hub for
emerging technologies, but it too is unlikely to spur Silicon Valley
like
innovation.[^[\[8\]]{.underline}^](https://www.brookings.edu/research/weapons-of-the-weak-russia-and-ai-driven-asymmetric-warfare/#footnote-8)
It is telling that despite high-level presidential and administrative
support, there is scant Russian language academic research on AI. It is
not likely that the country's stagnant and hydrocarbon-dependent economy
will do much to improve the government's ability to ramp up investment
in emerging technologies. In the longer term, [Russia's demographic
crisis]{.underline} (Russia is projected to lose 8 percent of its
population by 2050, according the
UN)[^[\[9\]]{.underline}^](https://www.brookings.edu/research/weapons-of-the-weak-russia-and-ai-driven-asymmetric-warfare/#footnote-9)
[will likely lead to shortages in highly skilled workers,]{.underline}
many of whom have already left Russia for better pay and opportunities
elsewhere.[^[\[10\]]{.underline}^](https://www.brookings.edu/research/weapons-of-the-weak-russia-and-ai-driven-asymmetric-warfare/#footnote-10)
[Western sanctions]{.underline} on key sectors of the Russian financial
sector and defense industry, which Europe and the United States imposed
after Russia's annexation of Crimea in 2014 and the United States [has
continued to ramp up]{.underline} since then, put extra pressure on the
Russian economy. Taken together, the economic and demographic trends
signal that in the AI race, [Russia will be unable to match China on
government investment or compete with the United States on private
sector innovation.]{.underline} The Kremlin is undoubtedly aware of the
country's unfavorable position in the global AI competition, even if
such an admission is unlikely to ever be made publicly. Strategically,
such a wide gap between ambition and capacity means that Russia will
need to invest its limited resources carefully. Currently, Moscow is
pursuing investments in at least two directions: select conventional
military and defense technologies where the Kremlin believes it can
still hold comparative advantage over the West and high-impact, low-cost
asymmetric warfare to correct the imbalance between Russia and the West
in the conventional domain. The former---[Russia's development and use
of AI-driven military technologies and weapons---has received
significant
attention.[\[11\]](https://www.brookings.edu/research/weapons-of-the-weak-russia-and-ai-driven-asymmetric-warfare/#footnote-11)]{.underline}
The latter---[the implications of AI for asymmetric political
warfare---remains
unexplored.[\[12\]](https://www.brookings.edu/research/weapons-of-the-weak-russia-and-ai-driven-asymmetric-warfare/#footnote-12)
Yet, such nonconventional tools---cyber-attacks, disinformation
campaigns, political influence, and illicit finance---have become a
central tenet of Russia's strategy toward the West]{.underline} and one
with which Russia has been able to project power and influence beyond
its immediate neighborhood. In particular, AI has the potential to
hyperpower Russia's use of disinformation---the intentional spread of
false and misleading information for the purpose of influencing politics
and societies. And unlike in the conventional military space, the United
States and Europe are ill-equipped to respond to AI-driven asymmetric
warfare (ADAW) in the information space.
